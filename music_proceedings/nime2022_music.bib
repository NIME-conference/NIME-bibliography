@inproceedings{NIME22_music_1,
 abstract = {“Drawing sound” has been a poetic idea that encouraging artists and musicians exploring intermedia expressions via real-time performance forces through data-driven instruments. Creating mutating mapping algorithms that connects performative actions with the music continuity and different visuals is one of the imaginative attempts in this composition and performance. Pagoda as a piece of architecture reflects history, aesthetics, religion, philosophy among many other cultural elements. Pagoda as a concept reminds me of each unique yet contemplative journey visiting different temples. The rituals of recitation and chanting practice in the temples offer the observers interfaces connecting themselves and the surroundings in different ways. In this piece, the impressions of the three pagodas are depicted in different audiovisual approaches through the real-time interactive performance.},
 address = {Auckland, New Zealand},
 articleno = {1},
 track = {Music},
 author = {Chi Wang},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.de810089},
 issn = {2220-4806},
 month = {jun},
 title = {Impressions of the Pagodas},
 url = {https://doi.org/10.21428/92fbeb44.de810089},
 year = {2022}
}

@inproceedings{NIME22_music_2,
 abstract = {The theme of NIME 2022 is “Decolonizing Musical Interfaces.” This performance recovers a nomadic trait in the violin, a common baroque practice before the triumph of classical “works.” Signal processing reorganizes the musical activity; the coherence of the musical event emerges through the less scripted, less top-down creative activity. The violin, the quintessential western stringed instrument, is reinvented—in fact, this extremely versatile instrument and has always been prone to it, as Dan Trueman has said. The performance is a rendition of Bach’s paradigmatic solo violin work, Chaconne. Machine listening drives a dense variety of synthesis and resampling techniques, including polyphonic live looping, spectral processing, pulsar synthesis, resonant filter banks, granular capture buffers, and subtractive synthesis. The violinist is invited to linger on the thematic materials in order to hear them undergo a captivating electronic transmutation. Ableton Live is used for this performance, employing a variety of custom Max for Live devices. More extended and detailed descriptions of the system as it has evolved through the last several years are available in several publications.},
 address = {Auckland, New Zealand},
 articleno = {2},
 track = {Music},
 author = {Seth Thorn},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.040ffc1e},
 issn = {2220-4806},
 month = {jun},
 title = {Chaconne (2020) for Violin},
 url = {https://doi.org/10.21428/92fbeb44.040ffc1e},
 year = {2022}
}

@inproceedings{NIME22_music_3,
 abstract = {Inside These Waters, music by Anne Hege, text by Camden Richards, is an excerpt from the fifty-minute soundtrack for the artist book Water, Calling created by Camden Richards and Deborah Sibony. Water, Calling is a collaborative artist book that explores the cyclical and omnipresent relationship of water and the self, inviting the reader to reflect upon water as more than a commodity, but rather as life-giving: spirit, flesh, and soul. Using my analog, live-looping instrument, The Tape Machine, and my daily practice improvising on this instrument as a starting place, I recorded improvisations on the instrument and used this as the base tracks for this piece. The Tape Machine is a handbuilt instrument created from two altered cassette players and one altered cassette recorder. The recorder stands in the middle with one player on the left and one on the right. A unique tape loop, created for each playing, is strung between the cassette devices. There are no effects added except what the machine creates. I sing in real-time into the recorder and manage playback from each player, adjusting the volume for discrete speakers hard-panned right and left. The machine highlights the degradation of sound present in the recording, allowing for a clear understanding of what vocals are live and what are echoed through the machine. With live manipulations of the tape, I am able to imitate the pitch-bending effects of water. With this, the Tape Machine emulates the distortions present in the watery world. For Inside these Waters, I layered tape machine samples with field recordings of water, and synthesis samples, while using various effects and filtering to connect the listener to the unique properties of water.},
 address = {Auckland, New Zealand},
 articleno = {3},
 track = {Music},
 author = {Anne Hege},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.9552e483},
 issn = {2220-4806},
 month = {jun},
 title = {Inside These Waters from Water, Calling},
 url = {https://doi.org/10.21428/92fbeb44.9552e483},
 year = {2022}
}

@inproceedings{NIME22_music_4,
 abstract = {(Un)real-time is a gestural performance for video telling the story through sound and movement of our past, present, and future relationship with technology.  Paying homage to early sonic innovations such as the gramophone, the transistor radio, Morse code, the Theremin, the turntable, and the MPC, the piece investigates the line between listening and playing, between percussion and dancing, and between technology and magic.  Composed through gesture and utilising historical recordings, (un)real-time is a provocation. What is real and what is unreal? When is unreal a compliment? When is real uninteresting? And what is a real-time ‘recording’? This video piece is a collaboration between a co-composer/ sound designer , and a percussionist/ instrument designer who plays, dances, gestures and listens with (un)real instruments, (un)real technology and (un)real music-making – all in (un)real-time.},
 address = {Auckland, New Zealand},
 articleno = {4},
 track = {Music},
 author = {Alon Ilsar and Ciaran Frame},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.6545a238},
 issn = {2220-4806},
 month = {jun},
 title = {(Un)real-time},
 url = {https://doi.org/10.21428/92fbeb44.6545a238},
 year = {2022}
}

@inproceedings{NIME22_music_5,
 abstract = {This is a second work submitted for the Tweeter telematic musicking platform developed in response to the COVID-19 pandemic. It reflects the evolution of the platform, the maturation of the emerging aesthetics, and the ensuing complexity, even though the participants were by and large amateur musicians and music enthusiasts. Tweeter enabled uninterrupted operation of the Virginia Tech Linux Laptop Orchestra (L2Ork) throughout the pandemic, while also spawning communities around the world who chose to embrace it for their own telematic collaborations. This, most recent iteration, features 12 performers. Tweeter is uniquely designed to provide a comprehensive support for telematic musicking, including instrument co-creation, improvisation, composition, rehearsal, and performance, as well as audience participation. Its particular focus is on pulse- and pattern-centric tightly-timed musicking that, through the use of control-driven protocol and anticipation of future cues, defies the limits of latency, drift, and bandwidth. Most importantly, Tweeter focuses on the newly coined crowdsourced music genre, where every aspect of creative process and performance is genuinely co-created by multiple stakeholders. The piece was realized in a hybrid environment through a combination of virtual, in-person, and mixed rehearsals.},
 address = {Auckland, New Zealand},
 articleno = {5},
 track = {Music},
 author = {Ivica Bukvic},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.7ff2a42e},
 issn = {2220-4806},
 month = {jun},
 title = {_-=◢▍ ▋░░▒ ▃▅▇░ (pronounced as “4th beat”)},
 url = {https://doi.org/10.21428/92fbeb44.7ff2a42e},
 year = {2022}
}

@inproceedings{NIME22_music_6,
 abstract = {Terracotta is an audiovisual work specifically composed to explore the sounds of arbitrarily shaped drums. Arbitrarily shaped drums are a family of instruments that do not exist in the real world, and are instead formulated according to a mathematical specification for ideal polygonal instruments in two-dimensional space. The sounds of these instruments were simulated using physical modelling synthesis, which was used to produce a linear approximation to their varied timbral spectrums. These instruments were explored thoroughly throughout this composition, which involved experimenting with their combinatorial potential and their distinct timbral differences from the sounds of more traditional percussive instruments. During the compositional process, each of the individual drums were randomly generated, and then tuned sympathetically with one another so as to produce a timbrally cohesive soundworld. Accompanying the drums, there was also the use of a Wacom drawing tablet, which has here been turned into a inharmonic synthesiser, so as to compliment the sounds of the arbitrarily shaped drums. This tablet synthesiser works by using the horizontal axis of the tablet to determine a fundamental frequency, whilst the vertical axis is used to control the amplitude of the sound’s various harmonic partials. To create inharmonic timbres, the individual partials were evenly spaced across the frequency spectrum according to an arbitrary value, resulting in an overall timbre that is a distorted replica of the more traditional harmonic series. In accordance with these themes, the percussive instruments were also represented, distorted and abstracted visually, as part of the performative stimulus for this work. These visuals incorporate the physically modelled simulations of the arbitrarily shaped drums, as well as reacting to the temporal changes in amplitude and style throughout the performance.},
 address = {Auckland, New Zealand},
 articleno = {6},
 track = {Music},
 author = {Lewis Wolstanholme and Francis Devine},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.7c140077},
 issn = {2220-4806},
 month = {jun},
 title = {Terracotta},
 url = {https://doi.org/10.21428/92fbeb44.7c140077},
 year = {2022}
}

@inproceedings{NIME22_music_7,
 abstract = {Embodied Transductions explores alternative ways of knowing bodies—both human and non-human—through hacking biomedical technology for musical expression. Technologically, the instrument consists of a modified stethoscope whose signal routes to a microphone, then to an amplifier, then to a transducer. This vibrates an ironing board, a cedar hope chest, and the performer’s body. The stethoscope listens to heart sounds, breathing, vocals, and the vibrations of the ironing board and cedar chest to create feedback. The simple setup embraces DIY and amateur hack aesthetics that often resist authority. Philosophically, Embodied Transductions critiques Foucauldian medical authority and explores feminist approaches to embodied knowledges. Who or what has the authority to speak truth about health and life? Contrasting the feminized labor of ironing and the hope chest’s traditional role in young women's preparations for marriage, Embodied Transductions repurposes a medical stethoscope to amplify latent resistances and resonances of these materials and a woman's body. The performance demands knowing through embodied experience, and sharing agency with the unpredictability of feedback through physical materials. This work advances the composer/performer's broader agenda exploring alternative epistemologies with biodata—data about human bodies, behavior, thoughts, and feelings, such as heart rate, step count, etc—and the role of this data in everyday meaning-making and and living ‘well’. Biodata is often analyzed to yield insights for 'health' and 'productivity'. Yet, this approach risks reducing the complexity of lived experiences to numbers and, through the perceived authority of data, can delegitimize other ways of knowing. Measurements always leave something out, and measurements not only describe but also co-construct reality. To help explore alternatives, Barad’s concept of intra-action emphasizes the entanglement of self and environment—each measurement makes an agential cut between sensor and what is sensed. Simondon’s transduction emphasizes dynamic transformations of energy from one form to another, leading to individuation of bodies—how we become ourselves. Embodied Transductions repurposes a traditionally medical biosensor to explore a way of knowing the body and mundane domestic objects through the phenomena of their intra-actions, transductions, and becomings.},
 address = {Auckland, New Zealand},
 articleno = {7},
 track = {Music},
 author = {Noura Howell},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.dfcd62f8},
 issn = {2220-4806},
 month = {jun},
 title = {Embodied Transductions},
 url = {https://doi.org/10.21428/92fbeb44.dfcd62f8},
 year = {2022}
}

@inproceedings{NIME22_music_8,
 abstract = {In the performance, "Regreso a la Tierra" (return to the land), the Electronic_Khipu_, a "NIME with a story" presented at the 2020 conference, is used for the first time, along with a new interface, the Kanchay_Yupana// an electronic tangible rhythm sequencer. Both interfaces are inspired by pre-colonial Andean technologies, made invisible by the processes of colonization of Abya Ayala; The Khipu is an ancient textile computer used for the processing and transmission of information encrypted in knots and cords of cotton and wool. On the other hand, the Yupana, also known as the Andean abacus, is a tangible calculator based on the placement of seeds on boards divided into boxes used to perform arithmetic operations. In ancient times the yupana complemented the khipu. The calculations that resulted from the use of the yupana were stored as information encoded in knots of the khipus. In this performance, as a ritual of gratitude, the performer continues the legacy using the same interaction, in this case, Kanchay_Yupana// (Yupana of light in the Quechua language) is played to mark rhythm by placing seeds in carved boxes with photoresistors that complement the experimental sound produced by the manipulation of the Electronic_Khipu_ when knotting its sensitive conductive rubber strings. Both instruments want to generate a distinct experience for live performance and a reflection on the tools used in tangible live coding and electronic music, claiming the original technologies of the Andes braided in today's electronics from a decolonial perspective. For this concert, I use sounds from the Kultrun, the Kaskawillas, and the Metawe from the sample library of Mapuche instruments of Juan Francisco Monsalve and Joaquín Salas.},
 address = {Auckland, New Zealand},
 articleno = {8},
 track = {Music},
 author = {Laddy Patricia Cadavid Hinojosa},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.8df44bd5},
 issn = {2220-4806},
 month = {jun},
 title = {Regreso a la tierra (Return to the land)},
 url = {https://doi.org/10.21428/92fbeb44.8df44bd5},
 year = {2022}
}

@inproceedings{NIME22_music_9,
 abstract = {This work is a musical piece in which the impressions we have of the weather are put into sound and how we feel when we listen to it is expressed through sound. There are two types: live performances and video works. Why does the weather move us? Why do people have different impressions and feelings about the same weather? Based on these backgrounds, I wondered how the music would turn out if the relationship between weather and humans were put to sound. In preparation, sonification of the non-sounding weather is based on scientific facts, historical background such as culture and religion, and general impressions held by people today. Based on this, make a sonification of the performer's impression of the weather. For comparison, the date and time, weather and location information, and a profile of the performer are recorded. In a live performance, the weather sound is generated in real time using object detection and weather api. The performer listens to it and expresses what they feel through sound. The video work is a compilation of the performance. The interesting things about this work is that in the live performance, you can enjoy the combination of sounds which is generated by the impression of weather, and the movement of sounds due to unpredictable changes in weather. In the video work, you can compare and enjoy different weather music depending on the weather, season, location, and people. In contrast to other works on the weather, this work has a clear reason and background for dealing with the weather, and the element of weather cannot be replaced. The novelty lies in the method and process of "Weather Music", which consists of the weather that emit sound, the creative sonification of the rest of the weather, and the sounds that express human emotions.},
 address = {Auckland, New Zealand},
 articleno = {9},
 track = {Music},
 author = {YOSY (POKARI)},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.ad35848c},
 issn = {2220-4806},
 month = {jun},
 title = {Weather Music},
 url = {https://doi.org/10.21428/92fbeb44.ad35848c},
 year = {2022}
}

@inproceedings{NIME22_music_10,
 abstract = {SYNTAX questions technological idealism in an age of ecological disruption and data-driven exploitation. Widely misused technology continues to cause pollution, habitat destruction, global warming, and disinformation. Like two sides of the same coin it is responsible for immeasurable prosperity and immeasurable suffering. It threatens all life on the planet, yet most humans depend on it. To explore this question the composers have programmed the computer to program themselves. By deliberately coding and submitting to an “inversion of control” they evoke the warnings of media theorists like Douglas Rushkoff, that we risk a future wherein our behavior might be irreversibly dictated by the algorithms in the software we use instead of by our own volition. SYNTAX is a performance piece that parallels the dilemmas we face from two sides of the same coin. The collaboration includes a series of eight animated, graphic scores designed to guide the two composers through a generative narrative of improvised sound. This inverts the usual practice of creating visuals in response to sound by creating music in response to visuals. The scores that the artists read are designed as functional symbolism representing a set of rules that suggest musical ideas, sound design techniques, and timed phrases. The scores are regenerated each time the application is executed so that every performance is distinct. The software engineered to generate the graphic scores provides information to the performers in the form of animated and color coded graphics. An additional script was developed to navigate through the movements, display the titles, and transition between each piece. Each movement was written with its own graphical language, algorithms, and directives while maintaining visual and conceptual themes to work as a whole. The composers perform the piece using a combination of hardware synthesizers and software independent from the scores including sequences programmed using the Tidal Cycles live coding environment and manipulated in real time. It is the intent of the human composers for the audience to experience the projected graphic scores and musical performance together.},
 address = {Auckland, New Zealand},
 articleno = {10},
 track = {Music},
 author = {John Keston and Mike Hodnick},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.65f86cc8},
 issn = {2220-4806},
 month = {jun},
 title = {SYNTAX},
 url = {https://doi.org/10.21428/92fbeb44.65f86cc8},
 year = {2022}
}

@inproceedings{NIME22_music_11,
 abstract = {“Don’t Sh Stop Please” is an excerpt of The Furies: A Laptopera, an adaptation of the Electra story. In this scene, the Furies, gods of vengeance, attach tether lines to Orestia and Electra's wrists and chest, a manifestation of the Furies’ hold on them through the guilt that they feel as punishment for their crime, matricide. Here, the tethers play a sonic, visual, and dramatic role. The instrument is designed to create a dramatic task for the Furies chorus. They must pull their tethers, on cue, to change the pitch of their station and thus create the musical texture that supports the vocal line. Simultaneously, they physically pull Electra and Orestia closer and closer to the ground, ready to devour them. Each laptop station independently controls one note of a six-note chord. The instrument is designed so that the right tether determines pitch. The pitch changes are pre-programmed so that a change in velocity triggered by a strong tug on the tether cues the next pitch. The left tether controls volume (z axis) and vibrato (x axis). The tones are kept simple, plain sine tones with reverb. Vibrato creates a warm and living sound, while volume emphasizes pitch change. Specifically, the players were instructed to place an accent at the moment of pitch change. Pitch changes are rhythmic and intended to happen on the downbeat. In support of the embodied metaphor of guilt as weight, both the vocal lines and the tether pitches descend over the course of the duet, collapsing at the end into a low cluster chord. Electra and Orestia physically reinforce this descent with the descent of their bodies, dictated by the pull of each tether. In this way, the instrument design unifies sonic, visual, character, and choreographic elements to support the underlying themes of the narrative.},
 address = {Auckland, New Zealand},
 articleno = {11},
 track = {Music},
 author = {Anne Hege and Camille Noufi and Elena Georgieva and and Ge Wang},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.d91afcee},
 issn = {2220-4806},
 month = {jun},
 title = {The Furies: A Laptopera - "Don't Sh Stop Please"},
 url = {https://doi.org/10.21428/92fbeb44.d91afcee},
 year = {2022}
}

@inproceedings{NIME22_music_12,
 abstract = {Chaos Bells is a very large (2 metres wide and tall) instrument, shown in Figure 1, is designed with both artistic and analytical goals in mind: it is a probe into the exploration of instrument size on performance, while also being a vehicle for Lia Mice’s performance practice. Chaos Bells features 20 gesturally performed pendulums. Chaos Bells' unique sound design in which bell sounds can drone and become chaotic is how it gained its name. Striking the instrument results in a staccato (short) tone, and tilting the pendulum/s results in a drone (sustained tone). The timbral quality of the drone corresponds to the pendulum tilt: somewhere between 45 and 90 degrees on each pendulum produces an unstable system where the drone grows over time and eventually becomes chaotic and distorted as it is clipped by the digital system, finally disintegrating into broadband noise with no clear fundamental tone. Despite its large size, Chaos Bells is also performable with micro-gestures that have the capability to change the overall sonic output of the instrument, a feature influenced through research I conducted in which I interviewed performers of large acoustic instruments. to understand their favourite characteristics of their instruments. Chaos Bells is created from PVC piping and features 20 embedded analog accelerometers connected to 4 Bela minis that run a Karplus-Strong synthesis algorithm on Pure Data. Chaos Bells is unique in both physical aesthetics and sound design. The instrument has a growing list of artists adopting it to create original performances. As a technology probe exploring the impact of instrument design choices on music performance, this instrument has led to findings elucidating the impact of instrument size and tonal layout on music composition and performance.},
 address = {Auckland, New Zealand},
 articleno = {12},
 track = {Music},
 author = {Lia Mice},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.cffb690f},
 issn = {2220-4806},
 month = {jun},
 title = {Chaos Bells},
 url = {https://doi.org/10.21428/92fbeb44.cffb690f},
 year = {2022}
}

@inproceedings{NIME22_music_13,
 abstract = {Machine improvisation for R-IoT playing ball, voice, mouth organ, electric guitar with audiovisual realtime processing and open ensemble. Instrumental sounds interact with audio and video processing in a very intuitive controlling set up. The improvisation is completed by an audio realtime signal processing by fft freeze reverb, classic ring modulation as well as spectral delay together with a multichannel granular synthesis by a computer performer in mutual inducement with the instrumental player. The ball gives rather grip than control to the sound processing in the sense of a modern séance. A R-IoT sensor inside the ball is sending OSC data to a Mac Book with Max. In a mutual influence instrumental sound, electronic representation of audio and interactive visuals blend into a unique piece of art slightly out of control but also very much in artistic an intuitive manner of access.},
 address = {Auckland, New Zealand},
 articleno = {13},
 track = {Music},
 author = {Se-Lien Chuang and Andreas Weixler},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.0b5de81e},
 issn = {2220-4806},
 month = {jun},
 title = {Intuitive access},
 url = {https://doi.org/10.21428/92fbeb44.0b5de81e},
 year = {2022}
}

@inproceedings{NIME22_music_14,
 abstract = {Membrana Neopermeable is a composition for physical acoustic guitar, virtual guitar, mixed reality (MR) and live electronics. It seeks to investigate the potential of the latest developments in MR and machine learning (ML) technology when questioning the boundaries, and compositional opportunities, between physical and digital music instruments; here, the guitar. The piece is performed by interacting with both a physical acoustic guitar and a virtual guitar, where the latter ‘appears’ in the same physical space as the performer. This results in a format which is an interactive, MR, compositional experience, made possible through using: an Oculus Quest 2 head-mounted display (HMD - allowing digital overlays, when worn, to appear in the same physical space as the performer), Myo armband sensors worn on the arms (allowing the performer to make custom gestures within MR), the Unity game engine (hosting the Oculus Quest 2/Passthrough API, the standalone project application itself and allowing for C# scripting, physics mechanics, modelling and digital animation), Max 8 (to receive/process biometric information from the Myo armbands and using such biometric data to generate/manipulate sound materials in real-time), and finally Wekinator (to facilitate the ML of custom musical gestures made by the performer; processing and classifying performer biometric information during performance). Ultimately, through deconstructing the barriers between physical and virtual instrument performance/composition, this piece seeks to observe how future multimodal spaces can be used as compositional assets.},
 address = {Auckland, New Zealand},
 articleno = {14},
 track = {Music},
 author = {Chris Rhodes},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.6e17eaf5},
 issn = {2220-4806},
 month = {jun},
 title = {Membrana Neopermeable},
 url = {https://doi.org/10.21428/92fbeb44.6e17eaf5},
 year = {2022}
}

@inproceedings{NIME22_music_15,
 abstract = {In January of 2021 and during lockdown, an internet subculture emerged with the intention of causing disruption to the US Stockmarket and everything they felt it stood for. Armed with only each other's speculation based on ill-considered analysis of stocks, group momentum and a willingness to succeed due to a lack of consequence, they drove Gamestop (GME) up from 11$ in the October of 2020 to over $480 on the 28th of January 2021. Due to the fact this stock had been massively shorted (bet against) by corporate traders, this caused massive disruption on Wall Street. This composition uses sonification of the high, low, end of day, volume and stock sentiment from October 2020 to September 2021 on an intraday basis to illustrate the momentum and excitement that the stock caused. Other elements crucial to the story, such as most popular daily comments on the main subreddit concerned with the social movement are incorporated and convoluted based on the stocks sentiment with the intention of creating fitting levels of confusion and elation as the stock reaches its highest and lowest points. Other sounds have been recontextualised as typing, coins clicking and engines (expensive cars and other material possessions often purchased with profits) which are then automated based on the stocks charts and sentiment with an interesting counterpoint occurring, in which, the sentiment of the stock overtakes the value of the stock, based on the fact that those who had read about the story in articles arrived slightly too late to reap any financial reward. I feel that this moment was very important socially, highlighting the power of the internet as a tool of group communication during a period of time in which physical contact was limited, something that was completely overlooked by Wall Street itself.},
 address = {Auckland, New Zealand},
 articleno = {15},
 track = {Music},
 author = {George Edmondson},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.ee98ecf7},
 issn = {2220-4806},
 month = {jun},
 title = {The Fragile Value of Nothing – Developing from Storytelling Through Sonification},
 url = {https://doi.org/10.21428/92fbeb44.ee98ecf7},
 year = {2022}
}

@inproceedings{NIME22_music_16,
 abstract = {This performance is part of an ongoing series of collective creations for the mubone augmented trombone and is the culmination of the Mubone Research & Creation Project that instrument designer Travis West and I began in 2019. After 3 years of developing and prototyping primarily in solo contexts, we began exploring new interdisciplinary pathways for the mubone through the creation of three new artworks. Each work pairs me, the mubonist, with an artist from another discipline - choreography/movement, composition/sound art, and new media. The mubone is an augmented instrument that tracks the orientation of the host object, a trombone, which effectively captures the performer’s movements for the purposes of real-time recording, playback, layering, and processing vis-à-vis a virtual 3D environment and a bespoke granular synthesis engine called mugranular. A Nintendo Switch Joy-Con is used for basic interfacing with a Max/MSP patch. Mubones are like trombones; they are a type of instrument that anyone can, in principle, make and play. There is not a single capital-M "Mubone". Garcia is the result of the pairing with choreographer Bettina Szabo. Our collaboration approach focused on centering choreography, movement, and gesture in an attempt to foreground these elements in the work. We mapped out the choreography before drafting the sound score; this approach contributed to a convincing performance that showcases the capacity for the mubone to achieve a symbiotic relationship between the movement, sound, and electronics. The piece is autobiographical and explores themes around immigration, otherness, and social acceptance. These are challenges that we shared as first generation immigrants to Canada. The structure of the piece follows a chronological timeline beginning with naive childhood curiosity and continuing with an adulthood consumed by memory reactivation, and finally ending with the fragility and nostalgia in old age.},
 address = {Auckland, New Zealand},
 articleno = {16},
 track = {Music},
 author = {Kalun Leung},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.e048e4b7},
 issn = {2220-4806},
 month = {jun},
 title = {Garcia (2021) for Mubone},
 url = {https://doi.org/10.21428/92fbeb44.e048e4b7},
 year = {2022}
}

@inproceedings{NIME22_music_17,
 abstract = {Spherical tendency of wrist movement (STOWM) is an artistic project involving two performers, four gestural controllers, reactive video and immersive audio diffusion. It focuses on relating the performer's gesture in the real world with an abstract three-dimensional audiovisual space based on the mapping of gestural data acquired in realtime. Aware of the importance of gesture as a contribution to the intelligibility of musical discourse and performing arts in general, the 3D printed controllers used in STOWM (called IANG) are able to capture the movements of fingers and arms, harking back to a traditional concept of musical instrument but also expanded by the current possibilities offered by technology. By mapping gestural data to control specific audiovisual parameters, a close relationship can be established between performer action, sound and image. The controllers integrate a dual-core processor Wi-Fi embedded board, a high-precision 6-axis motion sensor, and 5 buttons activated by the performer's fingers.},
 address = {Auckland, New Zealand},
 articleno = {17},
 track = {Music},
 author = {Julian Scordato},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.046117c7},
 issn = {2220-4806},
 month = {jun},
 title = {Spherical tendency of wrist movement},
 url = {https://doi.org/10.21428/92fbeb44.046117c7},
 year = {2022}
}

@inproceedings{NIME22_music_18,
 abstract = {Ear to Waipapa Taumata Rau’ is a live-coding performance by performers from two different continents remotely exploring the sonic components of Waipapa Taumata Rau. Our improvised performance would last 15-20 minutes, would feature field recordings collected by the participants of the conference in location, and would make use of multiple live coding platforms. The duo has been developing various strategies for online and hybrid performances which adapt to the uncertain circumstances during and post-pandemic to encourage participation and interaction with a global and inclusive audience. The prompts asked the participants in conference location, to explore their surroundings through soundwalks inspired by John Cage’s "A Dip in the Lake". They are supposed to collect 1-2 minute field recordings from their sound walks and upload the recordings on Freesound using hashtags including NIME2022 and their location (e.g.#NIME2022, #WaipapaTaumataRau). The live-coding performance organizes all these recordings and crowdsourced sounds of the geographical area of the conference, incorporates them into a live processed and synthesized composition. The defining feature of this piece is although the conference participants, listeners and performers are globally distributed, by listening and contributing to the whole composition they can feel in location by their ears. One of the authors has created an online platform that interacts with the audience visually and sonically on their mobile devices.},
 address = {Auckland, New Zealand},
 articleno = {18},
 track = {Music},
 author = {Visda Goudarzi and Anna Xambó},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.8b20f442},
 issn = {2220-4806},
 month = {jun},
 title = {Ear to Waipapa Taumata Rau},
 url = {https://doi.org/10.21428/92fbeb44.8b20f442},
 year = {2022}
}

@inproceedings{NIME22_music_19,
 abstract = {Tselem is the latest in a series of performances featuring the CrackleMask. Tselem confronts latent eschatological belief systems in HCI through esoteric ritual.  Eschatology refers to the study of apocalypse in theology, where apocalypse is not an ending, but a revelation of hidden truths that disrupt an existent social order. Eschatological belief systems are expressed in HCI through the notion that technological innovation will solve humanity’s problems. In the context of esoteric ritual, the CrackleMask serves as a conduit through which embodied histories of spaces may reveal themselves or even possess the performer. The CrackleMask is a wearable instrument that implements Michael Waiswisz’s Cracklebox (Kraakdoos) as a mask form. The Cracklebox circuit takes advantage of the now obsolete LM709’s idiosyncrasies by connecting the LM709’s external frequency compensation, power, and output pins to touch and pressure sensitive contact points on human skin. The circuit produces a chaotic high frequency signal that is brought down to the audible range by using the human body’s capacitive and resistive properties. Crackleboxes are usually implemented in box form with contact points placed on the top. The CrackleMask has contact points on a mask's exterior with a speaker turned inwards towards the user’s mouth, advantaging the mouth’s natural ability to act as a formant filter. The contact points mask's exterior require the performer to touch their face, generating a theatrical performance. A wireless microphone is installed inside the mask. The signal from the wireless microphone is sent to a computer running Supercollider for live processing. The CrackleMask is currently in its second version. The first version was constructed from a flat piece of wood, flattening facial features in an aesthetically interesting but physically uncomfortable manner. The second version is constructed of plaster molded to fit the user's face.},
 address = {Auckland, New Zealand},
 articleno = {19},
 track = {Music},
 author = {Alexander Cohen},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.79e363a5},
 issn = {2220-4806},
 month = {jun},
 title = {Tselem},
 url = {https://doi.org/10.21428/92fbeb44.79e363a5},
 year = {2022}
}

@inproceedings{NIME22_music_20,
 abstract = {The duo of Nicola L. Hein and Axel Dörner unites two busy players on the international scene of improvised music. They perform with an extended pallet of sounds and techniques, multichannel electronics as well as the cybernetic interaction between human musicians and artificial intelligence. For this project they developed intelligent software environments, which spatialise the musical gestures on multichannel systems and in real time are able to learn and transform the human sonic gestures into complex fields of musical potentials.Interacting with their own software alter ego’s, they develop a unique musical language that is characterized by its genesis out of a mixed human machine agency.},
 address = {Auckland, New Zealand},
 articleno = {20},
 track = {Music},
 author = {Nicola Leonard Hein and Axel Dörner},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.79e363a5},
 issn = {2220-4806},
 month = {jun},
 title = {Dörner/Hein},
 url = {https://doi.org/10.21428/92fbeb44.6d34f809},
 year = {2022}
}

@inproceedings{NIME22_music_21,
 abstract = {The work presented here is based on the Hybrid Augmented Saxophone of Gestural Symbioses (HASGS) system with a focus on and its evolution over the last six years, and an emphasis on its functional structure and the repertoire. The HASGS system was intended to retain focus on the performance of the acoustic instrument, keeping gestures centralised within the habitual practice of the instrument, and reducing the use of external devices to control electronic parameters in mixed music. Taking a reduced approach, the technology chosen to prototype HASGS was developed in order to serve the aesthetic intentions of the pieces being written for it. This strategy proved to avoid an overload of solutions that could bring artefacts and superficial use of the augmentation processes, which sometimes occur on augmented instruments, specially prototyped for improvisational intentionality. The definition of an instrumental technique is largely underlying the aesthetics of the pieces that constitute the repertoire of an instrument. The repertoire developed for HASGS is an example of the creative variety that mapping supports. Consequently, the difficulty of accurately defining a standardised instrumental technique is enormous, even when the relationship between an augmented system and an acoustic instrument allows us to establish similarities, insofar shown by how composers made similar use of the technology. The gestural phenomenon of interaction between instrumental and electroacoustic sounds became a fundamental point of interest of contemporary music. A mission of the 20th Century art was to make the invisible visible; in the 21st century artists may become more concerned with finding ways to allow us to sense the invisible as new perceptual modes may be uncovered. This concert features music by Henrique Portovedo, Nicolas Canot, Stewart Engarts and Rodney Duplessis.},
 address = {Auckland, New Zealand},
 articleno = {21},
 track = {Music},
 author = {Henrique Portovedo},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.62f256f8},
 issn = {2220-4806},
 month = {jun},
 title = {Music for HASGS, Stuby 1b},
 url = {https://doi.org/10.21428/92fbeb44.62f256f8},
 year = {2022}
}

@inproceedings{NIME22_music_22,
 abstract = {This performance features the coadaptive audiovisual instrument CAVI for collaborative human-machine improvisation. The system details are presented in a related paper submission for this year’s NIME, entitled “CAVI: A Coadaptive Audiovisual Instrument–Composition”. Briefly, CAVI tracks muscle and motion data of a performer's actions and uses deep learning to generate control signals used in a live sound processing system based on layered time-based effects modules. In addition, CAVI also has a virtual body that is present visually on stage. The artistic motivation of the project is related to how elements of surprise can emerge between a human performer and a computer-based musical agent. We explore this through CAVI, which builds on a dataset collected in a previous laboratory study of the sound-producing actions of guitarists. The particular dataset used in this project consists of electromyogram (EMG) and acceleration (ACC) data of thirty-three guitarists playing a number of basic sound-producing actions (impulsive, sustained, and iterative) and free improvisations. In the performance setup, CAVI continuously monitors the data streamed from a Myo armband located on the right forearm of the guitarist, which consists of 4-channel EMG and 3-channel ACC data. These data streams are used to generate new control signals akin to what will likely come next. CAVI is concerned with (1) how musical agents can interact with a performer’s body motion and (2) how artists can diversify performance repertoires using AI technologies. This results in serendipitous performances, based on the interaction between the human guitarist and the artificial agent. For this NIME performance, CAVI’s creator will perform with the system, using improved mappings, model optimization, sound mix, and spatialization. We will also include the Self-playing Guitars that were presented as part of an online installation, Strings On-Line, during NIME 2020. In doing so, we aim to address this year’s special call for music option, “NIME with a story,” and enhance the piece’s multi-agent structure using acoustic guitars that interact with the environment autonomously via Bela boards, actuators, and sound and motion sensors. Thus, the final performance setup will comprise an electric guitarist human performer, a virtual agent responsible for live sound processing, and six self-playing guitars.},
 address = {Auckland, New Zealand},
 articleno = {22},
 track = {Music},
 author = {Cagri Erdem and Alexander Refsum Jensenius},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.1d9762bd},
 issn = {2220-4806},
 month = {jun},
 title = {Me & My Musical AI "Toddler"},
 url = {https://doi.org/10.21428/92fbeb44.1d9762bd},
 year = {2022}
}

@inproceedings{NIME22_music_23,
 abstract = {SquareFuck is a performance where several Arduino boards are programmed in real time in a live coding setting. It is built sonically through the use of short electrical pulses that are sent to discarded speaker cones without any extra circuitry, generating square waves. Computationally it can be seen as a "hyper-extrapolation" of a Blink Sketch, since it relies, as its synthesis algorithm, on the very simple procedure of setting a pin high, waiting for a set amount of time, and then setting it low. As a performance, it comes from an exploration of the UI/UX of the Arduino IDE and the operating system itself as interfaces. More specifically, it leverages the limitations and constrains of the software ecosystem as affordances for creation. By, for example, not being able to have multiple instances of the IDE open simultaneously targeting different Arduino boards, sonic layers are built from gradual variations of the same base-code which is sent in succession to different boards. This inherent limitation/characteristic of the system led the piece to, structurally, gain a fugato, almost stretto like quality, where sonic material is repeated in rapid succession, with small variations. This interface/system is being developed (much in line with the "NIMEs with a story" submission track) as part of my current PhD research, where I investigate the limits of technical objects in the context of art making - one angle of it being what I named  "saturation of a device", that is, a sensibility deriving from an inquiry into to what extent the creative potential of a piece of technology is depleted once it has already been used for the single piece/performance it was originally developed for.},
 address = {Auckland, New Zealand},
 articleno = {23},
 track = {Music},
 author = {Magno Caliman},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.b23043a2},
 issn = {2220-4806},
 month = {jun},
 title = {SquareFuck},
 url = {https://doi.org/10.21428/92fbeb44.b23043a2},
 year = {2022}
}

@inproceedings{NIME22_music_24,
 abstract = {I often hear about Asian culture and Asian people. I belong to this category by the global standard, but I am from a specific region and culture. I respect other "Asian people" for their uniqueness, too. But, we often are treated as the same thing in the U.S., and it makes me wonder. What is Asian culture anyways? I don't know what that is. Years ago, someone was surprised that I had never meditated to singing bowls or sang ohm. In fact, I didn't know much about the practice until coming to the U.S. These metal bowls are commonly associated with Tibetan Buddhism, but I have heard that they were not really from Tibet. They seem more tied to meditation as a popular spiritual practice in the U.S. This phenomenon seems to me is an example of an imaginative exoticism based on mixed information that slowly earned a cultural myth that people accepted as a foreign culture. In くぼみ — kubomi, I use a singing bowl and a mere décor piece of metal bowl, along with sounds of piano and western flute, to challenge the cultural appropriation and exotic expectations. くぼみ — kubomi is a live interactive performance piece of a custom-designed instrument/controller, kane, that senses the capacitance of the performers’ hands. By stroking the rims of metal bowls, the performer triggers sounds through the capacitive sensing system. With live sounds of hitting the bowls and Western sound sources, this composition encourages us to think about exoticism and quick associations people tend to make about foreign cultures. This composition strives to contemplate all people—how we categorize people based on the look and make quick assumptions, and how we can make ourselves better at understanding each other. “Kubomi” means concave or dent in Japanese.},
 address = {Auckland, New Zealand},
 articleno = {24},
 track = {Music},
 author = {Akiko Hatakeyama},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.0c250d88},
 issn = {2220-4806},
 month = {jun},
 title = {くぼみ — kubomi},
 url = {https://doi.org/10.21428/92fbeb44.0c250d88},
 year = {2022}
}

@inproceedings{NIME22_music_25,
 abstract = {Etu{d,b}e is a series of performances including semi-autonomous musical agents improvising with a live musician performing on the eTube, an augmented instrument using a saxophone mouthpiece and custom controller interface. This instrument's playing techniques are influenced by contemporary saxophone performance. However, the conical saxophone is replaced with a keyless cylindrical tube, presenting a limited, yet interesting sound world to be explored primarily through timbre, texture, overtones, and articulations rather than pitch. Custom electronics based on the ESP32 chip are interfacing the sensors and actuators to relay information between the musician and the computer wirelessly. The instrumentalist’s preferences regarding some inherent qualities of sensors and the topology of the 3D printed controller were at the heart of the design process. The controller is attached to the acoustic instrument without obstructing regular playing technique. The performance evolves around the classical music notion of “étude” through machine listening, human-computer interaction, and a unique sound world created with the acoustic tube and various saxophones. Each étude explores a specific sound palette which is preloaded into the Creative Dynamics of Improvised Interaction (DYCI2) memory. This framework, designed by Jérôme Nika at IRCAM, enables the eTube’s wireless controllers to interact with musical memories and patch settings during real-time performance. We have customized certain functions of the agents to react to specific sound materials for each étude. Live microphone input from the eTube enables the electronics to react to performed musical phrases in real time, while the controller interface enables the improviser to interact with the electronics during performances.},
 address = {Auckland, New Zealand},
 articleno = {25},
 track = {Music},
 author = {Vincent Cusson and Tommy Davis},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.c05957e},
 issn = {2220-4806},
 month = {jun},
 title = {Etu{d,b}e: A Preliminary Conduit},
 url = {https://doi.org/10.21428/92fbeb44.c05957ee},
 year = {2022}
}

@inproceedings{NIME22_music_26,
 abstract = {About a decade ago, I made a piece of electronic music and titled it "Middle Eastern IDM" for a course assignment. After listening to it in class, my professor asked what was Middle Eastern about it. It was only a year after I had left Iran to study in the US, and I didn't know that I could say "I am. I made the piece". So I went back and superimposed a sample of Egyptian protest chants on top of the piece, to make it "sufficiently Middle Eastern". What prejudiced conservatism and performative liberalism share is gatekeeping practices that box one in a preconceived state of otherness. While the former overtly regards that otherness as inferior, the latter exoticises it through patronising paternalism. To me, it is especially troubling when exclusionary practices are driven by some form of "diversity/inclusion" agenda. If you don't fit the diversity box they've made for you, too bad. It's your fault for being "insufficiently diverse". "Poor thing, you've been colonised!", they tell you, as they claim ownership over a collection of frequencies, rhythms, tools, or techniques. When you look at who gets to decide if something's indigenous enough, you see how decolonisation itself has been colonised. When listening to this work, you can keep in mind that it was made by someone from Iran. But I should clarify that this piece has no intentional connections to the patterns of the Persian carpet or the poetry of Rumi. And those moments of "non-western" sonorities reflect the fact that the work has been developed using a custom-designed Max/MSP instrument that completely deconstructs a piece of traditional Iranian music and puts it through multi-layered transformational processes to produce entirely new timbral, temporal, and formal material. So in short, while this piece might not sound like the archetypical Iranian music, I assure you that it is Iranian enough.},
 address = {Auckland, New Zealand},
 articleno = {26},
 track = {Music},
 author = {Mo Zareei},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.2e34544f},
 issn = {2220-4806},
 month = {jun},
 title = {Proof of Identity},
 url = {https://doi.org/10.21428/92fbeb44.2e34544f},
 year = {2022}
}

@inproceedings{NIME22_music_27,
 abstract = {Hadrosaur Variations II is a work for hadrosaur skull instrument, soprano, and live electronics. A Corythosaurus is a duck-billed dinosaur, a lambeosaurine hadrosaur that scientists hypothesize used its large head crest for sound resonation. The hadrosaur skull instrument is a musical instrument created from a replica of a subadult Corythosaurus skull and nasal passages by myself and my collaborator, Sharif Razzaque. The hadrosaur skull instrument was first created as an interactive exhibition, in which participants  give voice to this dinosaur instruments by blowing into a mouthpiece, exciting a larynx mechanism and resonating the sound through the dinosaur’s nasal cavities and skull. Participants know the dinosaur through the controlled exhalation of their breath, how the compression of the lungs leads to a whisper or a roar. In a sense, they fleetingly experience being a dinosaur. The CT scans of the subadult skull fossil and the endocast of its nasal passages and skull were used to 3D model and 3D print the skull. The CT scans and data were provided by Witmer Lab, Ohio University. A mechanical larynx was also created with specifications derived from research on this subadult skull. For instance, by scientists derived estimates of the subadult’s hearing range from measurements of the inner ear fossil, and these guided larynx design.  I created this  larynx with balloons to mimic the flexible material of vocal cords. I make different timbres and pitches by pulling the strings of the larynx, bending the balloons, and putting pressure at different points of the balloons. In this work, I explore how to mimic the dinosaur with the soprano voice, and vice versa. I became particularly interested in coaxing melodies and distinct pitches from the hadrosaur skull instrument, as this was a challenging exercise.},
 address = {Auckland, New Zealand},
 articleno = {27},
 track = {Music},
 author = {Courtney Brown},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.fa9fd7a0},
 issn = {2220-4806},
 month = {jun},
 title = {Hadrosaur Variations II},
 url = {https://doi.org/10.21428/92fbeb44.fa9fd7a0},
 year = {2022}
}

@inproceedings{NIME22_music_28,
 abstract = {ScoreCraft is a multiplayer music game exploring online music making mediated through gameplay. Each player controls the game by producing sounds, so effectively by playing the game the players are making music. ScoreCraft is structured as a modular environment consisting of a collection of mini games and scenarios. Each mini game requires the players to produce a different, particular set of sounds in order to interact with the game, therefore, functioning as low-level musical material. The scenarios determine how the mini games are arranged on a 2d board, organising the low-level musical material embedded in the mini games and driving the high-level musical form.},
 address = {Auckland, New Zealand},
 articleno = {28},
 track = {Music},
 author = {Goni Peles and Yuval Adler},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Raul Masu},
 doi = {10.21428/92fbeb44.0d9f455b},
 issn = {2220-4806},
 month = {jun},
 title = {ScoreCraft: Show Biz},
 url = {https://doi.org/10.21428/92fbeb44.0d9f455b},
 year = {2022}
}