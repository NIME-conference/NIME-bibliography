@inproceedings{nime2025_music_1,
  author = {Ben Swift},
  title = {PANIC!},
  pages = {1--4},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {1},
  track = {Music},
  doi = {10.5281/zenodo.17800975},
  url = {http://nime.org/proceedings/2025/nime2025_music_1.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_1_file01.mp4},
  abstract = {PANIC! (Playground AI Network for Interactive Creativity) is an interactive installation that explores the behaviour of connected AI models. Viewers enter text prompts which are transformed as text/audio/images through a "network" of generative AI models. Each output becomes the input for the next iteration, creating an endless cycle of AI-mediated transformation.},
  format = {Installation},
  numpages = {4}
}

@inproceedings{nime2025_music_2,
  author = {Ivica Ico Bukvic},
  title = {Interstellar},
  pages = {5--8},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {2},
  track = {Music},
  doi = {10.5281/zenodo.17800989},
  url = {http://nime.org/proceedings/2025/nime2025_music_2.pdf},
  presentation-video = {https://www.youtube.com/watch?v=c1O-3g2tkoQ},
  abstract = {"Interstellar" is the latest work co-created by the members of the L2Ork Tweeter International Ensemble. Led by its founder and Director Dr. Ivica Ico Bukvic, the performance features live performers over 5,000 miles apart and integrates projection mapping co-developed by a visual artist Thomas Tucker and Bukvic. Tightly integrated sync of the ensuing telematic electronic music that blends EDM and Ambiental is made possible using L2Ork Tweeter free and open-source software platform that also interfaces with the MadMapper software responsible for the visual projection mapping. "Interstellar" is commissioned by the Alexandria VA Office of the Arts. It is inspired by StudioKCA's "Interstellar Influencer (Make an Impact)" installation on display in Alexandria's Waterfront Park. Like the installation, this piece tells the story of an asteroid whose impact shaped Chesapeake Bay over 35 million years ago.},
  format = {Remote Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_3,
  author = {John R Ferguson and Andrew R Brown},
  title = {MSN/AV: Maximum Silence to Noise/Audio-Visual},
  pages = {9--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {3},
  track = {Music},
  doi = {10.5281/zenodo.17802579},
  url = {http://nime.org/proceedings/2025/nime2025_music_3.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_3_file01.mp4},
  presentation-video = {https://vimeo.com/1054018899/a050c16da0},
  abstract = {MSN/AV (Maximum Silence to Noise/Audio-Visual) is an interactive audiovisual system performed by two musicians. The sound world is created via ring modulation synthesis controlled by multi-dimensional touch gestures, which provides a rich diversity of sonic potential whilst maintaining clear remnants of physical gesture. Each musician uses ROLI Lightpad Blocks and an iPad running TouchOSC in combination with bespoke software written in Pure data. Data from each performer’s interactions with their instruments is passed to a Touch Designer network which generates and/or manipulates visual materials. MSN/AV celebrates physical interplay with gestural interfaces and situates live improvisation with interactive technology within a responsive audiovisual environment. Sensor data is used to generate sound and graphics in real-time, the overall goal is an audiovisual entanglement that provides sonic and visual catalysts ranging from the gentle steering of musical improvisation to more autonomous and potentially disruptive behaviour.},
  format = {Live Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_4,
  author = {Frederick Rodrigues},
  title = {Synthetic Ornithology},
  pages = {13--16},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {4},
  track = {Music},
  doi = {10.5281/zenodo.17801004},
  url = {http://nime.org/proceedings/2025/nime2025_music_4.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_4_file01.mp4},
  presentation-video = {https://vimeo.com/1073767625/d9a3262810},
  abstract = {In Synthetic Ornithology, visitors encounter a compelling simulation of how shifting climates might transform Australia’s vibrant avian soundscapes. Powered by a bespoke ML model trained on an extensive archive of birdsong and corresponding climate data, the installation generates future sonic environments based on user-selected scenarios. The resulting soundscapes reflect the richness and complexity of Australia’s ecosystems, while also revealing how these voices may adapt—or fade—under the pressures of climate change. Ultimately, Synthetic Ornithology underscores the profound impact human interventions may have on the soundscapes of tomorrow, inviting reflection on our collective responsibility to preserve these fragile ecological realms.},
  format = {Installation},
  numpages = {4}
}

@inproceedings{nime2025_music_5,
  author = {Jordan Lacey and Toby Gifford and Mick Harding},
  title = {Sonic Crucible Entanglements},
  pages = {17--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {5},
  track = {Music},
  doi = {10.5281/zenodo.17802597},
  url = {http://nime.org/proceedings/2025/nime2025_music_5.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_5_file01.mp4},
  presentation-video = {https://vimeo.com/1079284705},
  abstract = {Sonic Crucible Entanglements is an ongoing ‘new materialist’ investigation of singing bowls that critiques new age narratives by exploring the ‘spiritual’ potential of inter-cultural material-led collaborations. This live singing bowl performance, also featuring vocal and visual projections, will immerse audiences in an atmospheric cultural encounter combining performance, visuals, sound art and instruments. Taungurung Kulin artist Mick Harding will present a contextual introduction that explores the experimental approach of the collaboration. The artists acknowledge the Ngunnawal and Ngambri peoples, the traditional custodians of the Canberra region. We will introduce light, sound, air and voice as part of our exploration of the vibrancy of materiality. Through the performance of three songs of generosity, knowledge and sharing sung and performed by Mick Harding we embrace Indigenous Research Methods through a combination of traditional and new technologies of sound.},
  format = {Live Performance},
  numpages = {2}
}

@inproceedings{nime2025_music_6,
  author = {Molly Joyce},
  title = {State Change: Merging Adaptive Music Technology with Disability Aesthetics},
  pages = {19--21},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {6},
  track = {Music},
  doi = {10.5281/zenodo.17801014},
  url = {http://nime.org/proceedings/2025/nime2025_music_6.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_6_file01.mp4},
  presentation-video = {https://www.youtube.com/watch?v=a2_Xx39ytQw},
  abstract = {State Change is a forthcoming album that transforms surgical records into musical lyrics and uses aural material generated by accelerometers and motion capture systems. By intertwining the medical and the aural, the album offers a sonic exploration of acquired disability, transforming personal and embodied experiences into a musical language. My interest in disability is rooted in my own lived experience. At the age of seven, I was involved in a car accident that nearly resulted in the amputation of my left hand. This life-altering event set me on a journey of adapting my relationship with music, leading me to experiment with different instruments and, more recently, to explore music technologies that align with my physical capabilities. These include accelerometers and motion capture devices, which have become central to my creative process. The featured track from the album, August 6, 1999, utilizes the MUGIC (Music/User Gesture Interface Control) device, an innovative motion sensor developed by violinist and composer Mari Kimura. The MUGIC translates hand movements into software parameters: “rotation” (pronation and supination) triggers high-frequency sine tones, while “yaw” (vertical axis rotation) triggers low-frequency sine tones. This track integrates lyrical text drawn from the surgical records of my first post-accident operation, alongside layered vocal elements that convey the emotional weight of that experience. Together, these elements create a deeply personal and evocative sonic representation of my journey with disability.},
  format = {Remote Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_7,
  author = {Xuedan Gao and Xinchen Liu},
  title = {Traceless},
  pages = {22--24},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {7},
  track = {Music},
  doi = {10.5281/zenodo.17801018},
  url = {http://nime.org/proceedings/2025/nime2025_music_7.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_7_file01.mp4},
  presentation-video = {https://youtu.be/lR1OMePhHwM?si=gpOlbw7HuWiLFxd4},
  abstract = {The Anthropocene, a time defined by human impact since the 1950s, has led to significant environmental changes, often tied to the Earth's declining health. Glacial ice, formed over centuries through the compression of snow into dense, airless layers, symbolizes the vast, slow forces of nature. However, this balance has been disrupted in the past two centuries. Human activities such as fossil fuel combustion, deforestation, urbanization, and industrialization have accelerated glacier melting, contributing to global warming and altering Earth's ecosystems. The contrast between glaciers' slow formation and rapid retreat shows the profound impact of human activities on natural processes. Traceless is an interactive sound installation with visual projections that invites participants to use handheld controllers for audio and tactile interaction. Ice, a symbol of power in deep time and fragility in the Anthropocene, serves as both medium and collaborator in this work. Traceless includes the installation’s core structure, an interactive audio system, and visual projections. It captures the sounds of ice melting and dripping using contact microphones. The visual projections compare two timescales: the slow, natural progression of glacial formation and the rapid, human-accelerated retreat occurring today. We created animations of various forms of snowflakes combined with the rhythmic sound of dripping water to metaphorically represent the time spectrum of the various stages of glacier formation. To illustrate the impact of human activity on glacier melting, we utilized AI-driven videos generated through Stable Diffusion, along with the Greenland Surface Melt Extent dataset from the NSIDC (National Snow and Ice Data Center). The two visual effects were switched through tactile interactions, bringing different times into the same space. The interaction emphasizes the harmony between human actions and natural processes. When participants hold the controller, the corresponding musical note will play. When participants speak to the installation, their voices blend with the natural sounds of melting ice, which forms an echo that embraces change as part of the natural system. Human language is not parsed semantically and becomes increasingly blurred in the echo. This is a call-and-response process, where the ice and the system actively respond to the participants in their own rhythm that is not influenced by the participants, forming a mutual dialogue. The sounds metaphorically embed the vibrational frequencies of carbon-based life into the ancient resonance systems of geological formations. The sounds across material dimensions constitute dialogues between geological time and the Anthropocene. Traceless allows participants to experience and reflect on the agency of nature within a restricted dialogic framework. The asymmetric interaction exposes the boundaries of human intervention and guides participants to realize, through attempted 'collaboration' with the installation, that symbiosis resides not in technological domination, but in what Donna Haraway calls 'becoming with', an ongoing process of co-creation rather than control. By immersing participants in the multisensory experience, Traceless encourages them to think about the active role of nonhuman entities and to imagine a de-anthropocentric future.},
  format = {Installation},
  numpages = {3}
}

@inproceedings{nime2025_music_8,
  author = {Jarosław Kapuściński and Marc Downie and Paul Kaiser},
  title = {Point Line Piano},
  pages = {25--26},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {8},
  track = {Music},
  doi = {10.5281/zenodo.17801028},
  url = {http://nime.org/proceedings/2025/nime2025_music_8.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_8_file01.mp4},
  presentation-video = {https://vimeo.com/938043123},
  abstract = {Point Line Piano is a VR project that reimagines the composition, performance, and reception of piano music by fusing its modes of creating, playing, and listening. As you interact with it, your ears, eyes, and hands act in concert. You start by drawing lines freely in the space around you, sparking musical notes that are notched as points on the lines as you draw them. These notes quickly accumulate, forming distinct melodic phrases and rhythms, while the computer generates an intricate audiovisual dance all around you. The work enables rare spatial and full-body experience of abstraction. In a live concert setting it can also be used as an audiovisual instrument. },
  format = {Installation},
  numpages = {2}
}

@inproceedings{nime2025_music_9,
  author = {Seth D Thorn},
  title = {Diffractive Constellations: A Modular System for Acoustic Violin Programmed in Max/Gen},
  pages = {27--29},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {9},
  track = {Music},
  doi = {10.5281/zenodo.17801039},
  url = {http://nime.org/proceedings/2025/nime2025_music_9.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_9_file01.mp4},
  abstract = {Diffractive Constellations is a live performance and research project that combines an acoustic violin with a set of bespoke Eurorack modules programmed in Max/Gen and embedded on the Daisy Seed SOM. By capturing, transforming, and re-sequencing violin gestures, these modules operate as compositional fragments that can be patched together in myriad ways. Each module began as a “composition made parametric,” evolving through hardware constraints to support improvisatory practice. This document details the technical design and performance methodologies of six custom modules—called Arvo, Glacial, Detritus, Volution, Widgets, and Forest—highlighting how each processes violin-derived or other input to create layered, time-stretched, and/or autonomous sonic behaviors. These modules collectively shape a system in which performer and electronics co-create emergent musical structures.},
  format = {Remote Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_10,
  author = {Jocelyn Ho},
  title = {Housework Commons: Textile Rhetorics II},
  pages = {30--35},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {10},
  track = {Music},
  doi = {10.5281/zenodo.17801043},
  url = {http://nime.org/proceedings/2025/nime2025_music_10.pdf},
  presentation-video = {https://vimeo.com/1051389504},
  abstract = {Housework Commons, a feminist activist project under the Women’s Labor initiative, transforms domestic tools into Embedded Acoustic Instruments (EAIs) using sensor technologies. It reimagines unpaid, undervalued domestic labor—traditionally private—as a shared act of activism, addressing global gender inequality in domestic work through public engagement with gendered objects. Housework Commons includes two custom instruments: (1) Embedded Iron v.3, based on an early-20th-century ironing board and iron, uses machine learning and sensors to alter pitch based on the iron’s position and sound quality (timbre) depending on fabric color and texture. The board acts as a resonator with a transducer and speaker. (2) Rheostat Rotary Rack, inspired by a rotary dryer, features rheostats, a rotary encoder, and an 8-speaker base. Hanging clothing triggers pitches based on weight, while rotating the rack by hand or wind adds select frequencies. The performance of new composition, Textile Rhetorics II for two performers, will feature Embedded Iron and Rheostat Rotary Rack with other objects from the domestic sphere. Central to Textile Rhetorics are woven textile scores and fabric banners that contain living testimonies from women. These testimonies were collected during a past workshop with a mothers’ group, and additional banners will be created during the proposed NIME workshop. These create a “living archive” installation, a dynamic site where the performance unfolds.},
  format = {Live Performance},
  numpages = {6}
}

@inproceedings{nime2025_music_11,
  author = {Ludwig Zeller and Anselm Bauer},
  title = {The Walkable Instrument: Modular Patches as Entangled Environments in OpenSoundLab},
  pages = {36--39},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {11},
  track = {Music},
  doi = {10.5281/zenodo.17802609},
  url = {http://nime.org/proceedings/2025/nime2025_music_11.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_11_file01.mp4},
  abstract = {What happens if a modular synthesizer patch is no longer constrained to a flat, rigid configuration, but can instead be copied infinitely (and at no expense) and arranged in midair all around the performer? The performance showcases two modular patch designs assembled in OpenSoundLab, a mixed-reality patching system for Meta Quest that was developed in previous research. This platform allows users to freely position basic sound modules (oscillators, samplers, sequencers, effects, etc.) throughout real-world environments. A long-standing challenge in electronic-music performance—whether you’re working with hardware synths, modular rigs, software patches, or full DAW setups—is that the patch architecture and live tweaks remain opaque, even to expert listeners. The system is often too small, too cluttered by patch cables, or simply too complex and hidden in menus, sub-patches, or binary code to be grasped visually. When a musician plays a traditional keyboard or a self-built NIME-style instrument, the audience can readily perceive timing, effort, and skill. In classic synth-tweaking shows, however, such clear audiovisual cues are largely absent. OpenSoundLab resolves this by letting performers position a modular patch at any scale and in any arrangement within the stage space. Freed from rigging and gravity, a modular setup can expose its inner sonic architecture in three dimensions, with less-critical modulation oscillators tucked behind larger, more prominent elements. Multichannel components become visibly entangled around the performers, spreading out in front of—or even encircling—them to map the patch’s full sonic possibility space. During the performance, Anselm Bauer will present two patches that illustrate this concept. Both pieces inhabit the realm of abstract glitch techno. The first features tightly defined sound structures laid out as a clearly readable tree of audio channels and modulations, while the second explores generative, self-modulating processes, resulting in an even more entangled, rhizomatic patch topology.},
  format = {Remote Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_12,
  author = {Yixuan Jin},
  title = {Echoes of Nature’s Heritage – Composed for a Custom-Made Data-Driven Instrument},
  pages = {40--43},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {12},
  track = {Music},
  doi = {10.5281/zenodo.17802618 },
  url = {http://nime.org/proceedings/2025/nime2025_music_12.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_12_file01.mp4},
  presentation-video = {https://youtu.be/a1ZNhHtIV7E},
  abstract = {Inspired by the experience of orthodontic treatment, this piece explores the similarities between the alignment of teeth and the pursuit of environmental harmony. Utilizing a data-driven musical instrument, the performance captures the balance between nature and technology, echoing the stages of free growth, intervention, and equilibrium. As the performer's movements shape the soundscape and visual transformations, the audience contemplates the dynamic tension and potential harmony between the natural world and the mechanical systems we create. It reminds us of our responsibility to seek sustainable balance, promoting the coexistence and mutual prosperity of nature and innovation, paving the way towards a greener future.},
  format = {Remote Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_13,
  author = {Krzysztof Cybulski},
  title = {Kamer/Ton – a system for analog audio-video feedback},
  pages = {44--49},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {13},
  track = {Music},
  doi = {10.5281/zenodo.17802621 },
  url = {http://nime.org/proceedings/2025/nime2025_music_13.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_13_file01.mp4},
  presentation-video = {https://vimeo.com/1080203114/ab3ec0f584},
  abstract = {Kamer/Ton is an experimental audio-visual performance tool, based on audio and video feedback phenomena combined into a single feedback loop. It consists of a couple of devices: two custom-made cameras are directed at a CRT monitor - the picture is transformed into sound, which is consequently sent to a speaker; a microphone directed towards the speaker is in turn connected to a custom-made video synthesizer, producing images displayed on the monitor. The resulting audio-visual feedback can be interacted with by modifying camera and microphone positions and parameters, which influences simultaneously generated, perfectly synchronized sonic and visual structures.},
  format = {Remote Performance},
  numpages = {6}
}

@inproceedings{nime2025_music_14,
  author = {Mel Huang Buntine and Anthony Lyons and Heather Gaunt and Eugene Ughetti},
  title = {Glass and Fans: Spatial Infra-Instrument Framework},
  pages = {50--54},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {14},
  track = {Music},
  doi = {10.5281/zenodo.17801065},
  url = {http://nime.org/proceedings/2025/nime2025_music_14.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_14_file01.mp4},
  presentation-video = {https://zenodo.org/records/15293072},
  abstract = {Glass and Fans are two participatory spatial sound installations created with the Spatial Infra-Instrument Framework (SIF), investigating how interactive interfaces like SIF can expand the scope of music-making to include collaborative, non-linear processes that can evolve with their artists, audiences, and the public over time. SIF utilises open-source tools such as openFrameworks and Kinect v1 to provide a low-cost and flexible framework that supports and encourages the education and artistic practice of building New Interfaces for Musical Expression (NIMEs). Each SIF iteration is a stand-alone structure and has two performance modes: (1) Machine mode, instrument performs as the composer has specified; (2) Human mode, instrument becomes interactive for audience. Glass takes a series of fragile, hand-blown glass percussion and provides access to their tonal and textural qualities. Machine Mode plays audio and visual samples as programmed by the composer, Human Mode invites audiences to compose within the same ecosystem through rear projection and set of spatial triggers. Fans translates the everyday material of computer fans into an infra-instrument. Machine mode performs with the “tuned” fans at set speeds and intervals. Human Mode allows audiences to compose with the electromagnetic field (EMF) microphones by changing their distance and position.},
  format = {Installation},
  numpages = {5}
}

@inproceedings{nime2025_music_15,
  author = {Han Xu and Zehao Wang},
  title = {Bending Nature},
  pages = {55--59},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {15},
  track = {Music},
  doi = {10.5281/zenodo.17801071},
  url = {http://nime.org/proceedings/2025/nime2025_music_15.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_15_file01.mp4},
  presentation-video = {https://www.youtube.com/watch?v=Pqz8INCmDmU&t=45s},
  abstract = {Bending Nature is an architectural sound installation situated in a confined, irregular space. By repurposing part of the raw speakers as microphones and surface transducers placed directly on the walls activate the architectural texture, the work merges natural sounds with feedback loops and creates a resonant dialogue between the building and its environment. Through a visually minimalist presentation, viewers are immersed in a dynamic, ever-changing soundscape that challenges spatial perception and offers a novel sensory experience of sound and structure.},
  format = {Installation},
  numpages = {5}
}

@inproceedings{nime2025_music_16,
  author = {Jacob Hedges and Daniel Garrett and Jaxon Sharp},
  title = {Belly of the Beast},
  pages = {60--62},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {16},
  track = {Music},
  doi = {10.5281/zenodo.17801073},
  url = {http://nime.org/proceedings/2025/nime2025_music_16.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_16_file01.mp4},
  abstract = {Belly of the Beast is an interactive music VR experience that allows users to dynamically manipulate a spatial musical composition in real time by moving sounds in 3D space, rearranging the structure of the composition, and playing virtual instruments along with the music. Leveraging the unique interactions of hand tracking and head tracking with the Meta Quest 3, This work harnesses XR technologies to reimagine the way we interact with music, allowing users to act as both producer, performer, and audience at once. Built using Unity and Wwise, and drawing inspiration from adaptive game music, the project empowers participants to shape the composition in real time through their movements and gestures. The composition presented is a 5-10 minute, semi-linear experience, where the user’s interactions progress the unfolding of the composition. It is a one headset per user experience, designed to be playable in a 4m x 4m square. The intended installation does not require extensive setup, as it operates with the Meta Quest 3 tethered to a Windows computer running the experience software.},
  format = {Installation},
  numpages = {3}
}

@inproceedings{nime2025_music_17,
  author = {Misagh Azimi},
  title = {(un)Stable (dis)Connection},
  pages = {63--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {17},
  track = {Music},
  doi = {10.5281/zenodo.17801081},
  url = {http://nime.org/proceedings/2025/nime2025_music_17.pdf},
  presentation-video = {https://vimeo.com/1072855447/77755dd43e},
  abstract = {(un)Stable (dis)Connection is an improvised performance that merges a fine-tuned text-to-audio latent diffusion model with traditional improvisational techniques in Ableton Live. Acting as a co-creative agent, the AI model instantly generates fresh audio fragments in response to short textual prompts provided by the performer. These fragments are seamlessly imported into Ableton using Max for Live, where they are manipulated and used to improvise in real-time. By fine-tuning an open-source text-to-audio model on the author’s own recordings, the piece achieves some degree of stylistic coherence without sacrificing unpredictability. (un)Stable (dis)Connection highlights the creative potential of ethically sourced, open-source AI in music performance, foregrounding human–machine synergy as both a technical and artistic frontier.},
  format = {Live Performance},
  numpages = {2}
}

@inproceedings{nime2025_music_18,
  author = {Ciaran Frame and Steph O'Hara and Sam Trolland and Alon Ilsar},
  title = {AirPens: Musical Doodling},
  pages = {65--67},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {18},
  track = {Music},
  doi = {10.5281/zenodo.17801083},
  url = {http://nime.org/proceedings/2025/nime2025_music_18.pdf},
  presentation-video = {https://vimeo.com/1053642483/792c534aab},
  abstract = {AirPens is an installation/activation that invites participants to make music while mark-making. NIME attendees are invited to reflect on this year’s conference by doodling, scribbling or sketching onto a whiteboard with one of four AirPens — a gestural DMI that utilises IMU sensors to convert movement into sound. Part of the larger AirSticks project which includes PCB design, 3D printing and fabrication developed at Monash University’s SensiLab, the AirPen takes the physical affordance and potential of a whiteboard marker, and extends the marker’s utility from simply converting movement with ‘2D mark-making,’ to ‘3D music-making.’ AirPens is designed as an ice-breaker experience, inviting attendees to share their thoughts on the state of NIME while improvising gestural music in small groups. While writing, participants may change the way they write to accommodate a different sonic output, or focus more on the ‘sonification’ of their natural writing style. But participants are not restricted to just writing with the AirPen — participants can explore the full scope of the AirPen as a gestural DMI away from the whiteboard, going off into free musical improvisations that may inspire discussions about the technology and project more generally. Each of the four AirPens available will have their own unique mapping that is in harmony with the other AirPens, inviting participants to freely go between writing ideas and improvising gestural music together.},
  format = {Installation},
  numpages = {3}
}

@inproceedings{nime2025_music_19,
  author = {Toby Gifford},
  title = {Breathing with the Forest},
  pages = {68--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {19},
  track = {Music},
  doi = {10.5281/zenodo.17801095},
  url = {http://nime.org/proceedings/2025/nime2025_music_19.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_19_file01.mp3},
  abstract = {As I travelled and camped down the East coast of Australia this summer, a standout feature of the experience was the sheer volume of cicada stridulations. The spatio-temporal pulsations of this sonic mass seemed musical in its phrasing, and felt to me like the sound of the forest breathing. In a literal sense our breath is entangled with the forest as we inhale oxygen that the forest has exhaled. This interactive sound installation is inspired by the notion of breathing with the forest, and aims to emulate that feeling through a breath controlled synthetic sound mass reminiscent of a forest full of cicadas.},
  format = {Installation},
  numpages = {2}
}

@inproceedings{nime2025_music_20,
  author = {Gregg Oliva},
  title = {EDO Artifacts},
  pages = {70--73},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {20},
  track = {Music},
  doi = {10.5281/zenodo.17801099},
  url = {http://nime.org/proceedings/2025/nime2025_music_20.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_20_file01.mp4},
  presentation-video = {https://www.youtube.com/watch?v=cg4VUd2ON9E},
  abstract = {EDO Artifacts is a live-performance piece for a computer-sequenced modular synthesizer to explore equal division of the octave (EDO) tuning systems. The composition is written prior to performance using the ChucK music programming language. During the performance, a computer running this program interfaces with the modular synthesizer, converting signals from the code into voltage to drive oscillators and amplifiers in the system. The flexibility offered by the programmatic composition supports complex arrangements of phrases, sections, and modulating tuning systems, while the modular synthesizer provides the performer precise control over the sound through real-time manipulation of timbre-shaping parameters. In this way, ChucK acts as the “orchestration” or the “brain”, whereas the modular synthesizer is the “instrument” or the “body” of the piece. There are four sections—labeled as fragments—each written using a different EDO tuning: 5EDO, 7EDO, 31EDO, and 15EDO, respectively. Each tuning has been selected to suit the stylistic and textural qualities of its respective fragment, shaping both the compositional approach and the resulting sonic character. The fragments are purposefully brief, serving as previews of the musical potential of each tuning.},
  format = {Live Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_21,
  author = {Joseph Burgess},
  title = {Carpet Entanglement},
  pages = {74--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {21},
  track = {Music},
  doi = {10.5281/zenodo.17801103},
  url = {http://nime.org/proceedings/2025/nime2025_music_21.pdf},
  presentation-video = {https://vimeo.com/1042836127},
  abstract = {Textile production, music, and technology are historically entangled. This performance explores the long-standing interaction between textile arts and musical practice through the carpet tufting gun as a novel electroacoustic interface. This historical connection underscores the deep-rooted interplay between material craft and sonic expression, which has continued to evolve alongside technological advancements. Carpet entanglement emphasises  the ongoing generative interplay between media and mediums highlighting the tufting gun as a site of embodied, material engagement. By leveraging the tufting gun’s distinctive acoustic properties and electromechanical kinetics, this work reimagines the tool as a vehicle for structured musical composition and improvisation. The performance transforms the gun’s utilitarian form and mechanical expressive qualities—such as its rhythmic firing, material resonance, and tactile feedback—into a dynamic paletteof itinerant musical gestures. The tufting gun serves a dual role: as an acoustic sound source and as an interactive performance interface, bridging textile craft and experimental sound art. Carpet Entanglement reinforces the existing connection between textiles, technology, and music, highlighting the tufting gun’s potential as a tool for artistic expression while situating it within a broader historical and technological narrative. Through this performance, the tufting gun becomes a mediator between the tactile, material world of textile production and the ephemeral, sonic world of music, embodying the interwoven processes of making and performing.},
  format = {Live Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_22,
  author = {Jocelyn Ho and Michał Seta and Dirk Stromberg},
  title = {My Sunset Is Your Sunrise},
  pages = {78--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {22},
  track = {Music},
  doi = {10.5281/zenodo.17802633},
  url = {http://nime.org/proceedings/2025/nime2025_music_22.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_22_file01.mp4},
  presentation-video = {https://youtu.be/E6usQM3vlnw},
  abstract = {In “My sunset is your sunrise, yet we touch,” we propose a three-way hybrid-telematic improvisation, by three displaced musicians, featuring 2 DMIs, and an acoustic-prepared piano. The musicians are located in Montreal, Singapore and Canberra at the NIME conference. This performance explores the concept of a multimedia meta-instrument, a collaborative process that integrates audio, video, projection, and lighting into a cohesive whole. The meta-instrument redefines presence and interaction, challenging traditional notions tied to physical co-location. Our approach draws on Karen Barad’s concept of touch as an entangled, relational act. Here, touch extends beyond the physical to encompass the mediated interplay of sound, visuals, gestures, and light across networked spaces. In this environment, the tactility of capacitive keys, the resonances of the prepared piano, and the shifting gestural logic of controller-based sound art merge into a dynamic, co-constitutive system. This telematic improvisation demonstrates how the tactile interactions with instruments, the resonances of sound, and the dynamic interplay of light and video create new forms of connectivity and shared expression across a blurring of physical and virtual spaces. Through this meta-instrument, we aim to push the boundaries of telematic collaboration and artistic interaction.},
  format = {Live Performance},
  numpages = {5}
}

@inproceedings{nime2025_music_23,
  author = {Lilian Zhao},
  title = {Becoming},
  pages = {83--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {23},
  track = {Music},
  doi = {10.5281/zenodo.17801115},
  url = {http://nime.org/proceedings/2025/nime2025_music_23.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_23_file01.mp4},
  presentation-video = {https://www.youtube.com/watch?v=ksYax6dD-38},
  abstract = {'Becoming' recontextualizes a custom playable device as a living entity that responds to both interaction through an analog interface, and audiovisual stimuli detected from the surrounding environment. Using adaptive animations and ambient soundscapes, the piece creates a volatile space that evokes thoughts of transformation, growth, and destruction. It tells the story of a machine grappling with fate and embraces human nature through the eyes of something artificial. The visual component arises through layers upon layers of blood red lines that coalesce into abstract shapes, mirroring processes of growth and decay. The accompanying soundscape layers mechanical noises with ambient sounds like human voices from the surrounding space, emphasizing the tension between artificial systems and organic environments. Through its unpredictable behavior, the work challenges the audience to reflect on questions of agency, determinism, and the fluid nature of identity.},
  format = {Remote Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_24,
  author = {Yukihiro Sugawara and Kotaro Watanabe and Shinnosuke Hirose and Moe Miyake and Kenshiro Taira and Sakura Takada and Ryoho Kobayashi and Yuta Uozumi and Kei Fujiwara and Shinya Fujii},
  title = {Orbis},
  pages = {87--97},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {24},
  track = {Music},
  doi = {10.5281/zenodo.17801119},
  url = {http://nime.org/proceedings/2025/nime2025_music_24.pdf},
  presentation-video = {https://youtu.be/IfhvUUInXSA},
  abstract = {When cells divide, they initiate cell division through ripple-like patterns called “Min waves”. In the field of artificial cell engineering, researchers have successfully generated these waves within artificially constructed cells. Although cell division itself has not yet been achieved, the Min waves observed in artificial cells exhibit diverse behaviors under various conditions. “Orbis” expresses the cycles and spatial movement of Min waves in artificial cells with sound and light, creating a space where the rhythm and energy flow of the artificial cells can be experienced intuitively through hearing and vision. We developed a system to detect the movements of Min waves generated in artificial cells through image analysis and utilized their unique behaviors as an oscillator. For image analysis, Blob Tracking TOP, an OpenCV-based detection model in TouchDesigner, is used to track the positional coordinates of Min waves in the observation footage. The Min wave coordinate data from TouchDesigner is sent to Max via OSC (Open Sound Control) and is reflected in the output of several speakers and light bulbs in a circular arrangement. “Orbis” is an interface that enables the unique behavior of this quasi-life form to be converted into sound and light. By expressing the ‘vitality’ that gradually emerges from matter through auditory and visual information, it encourages the audience to question “What is life?”},
  format = {Installation},
  numpages = {11}
}

@inproceedings{nime2025_music_25,
  author = {Davor Vincze and Maurice Oeser},
  title = {We’re in this together: Expanding Interactive Sonic Spaces},
  pages = {98--102},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {25},
  track = {Music},
  doi = {10.5281/zenodo.17801121},
  url = {http://nime.org/proceedings/2025/nime2025_music_25.pdf},
  presentation-video = {https://youtu.be/U4TCjb_mZmw},
  abstract = {We’re in this together builds on the technical and conceptual frameworks of the Freedom Collective web app but transposes its application from a theatre setting into performance for solo electronics. Hence, the videos provided serve as documentation of previous performances exploring similar concepts; they offer reviewers insight into the musical aesthetics and the functionality of the web-app technology but do not constitute representations of the specific project proposed here. The interaction design leverages the variability of smartphone audio latency and quality to generate distributed sound fields that resonate in real-time with the performance's central sonic themes (e.g. granulation, density, layering and spatial sound distribution). Ideally, the performance space should ideally be set up with an immersive ambisonic sound system for supporting low-frequency content and reinforcing the performer’s electronic sounds. Audience smartphones act as distributed sound sources, enabling a shared participatory instrument. This distributed design also mitigates the need for specialized hardware, making the experience accessible across diverse demographics and venues. In addition to the sonic engagement, the interface includes simple visual prompts on participants' screens, aligning with the performance’s dramaturgy. These features encourage active yet unobtrusive engagement, ensuring that the audience's role complements, rather than distracts from, the sonic focus.},
  format = {Live Performance},
  numpages = {5}
}

@inproceedings{nime2025_music_26,
  author = {Jordan Shier and Xiaowan Yi},
  title = {Diffy},
  pages = {103--106},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {26},
  track = {Music},
  doi = {10.5281/zenodo.17801126},
  url = {http://nime.org/proceedings/2025/nime2025_music_26.pdf},
  presentation-video = {https://youtu.be/mwq-rSJN048},
  abstract = {Diffy is a duo music project comprising a drummer and a sound designer, connected by a set of machine learning-based sound design agents. In this project, we explore and juxtapose a set of three machine learning-based techniques for manipulating the timbral qualities of percussion instruments in real-time with low-latency. These techniques include a neural audio synthesizer trained on non-percussive material, a timbre remapping 808 drum synthesizer, and a modular synthesizer controlled by a neural network. Each sound design agent operates on different modes of timbral understanding -- reacting to the drum performance based on this understanding, and suggesting sonic transformations. Sonic negotiations between the human sound designer and the sound-design agent are relayed back to the drummer, creating a feedback loop that shapes a structured improvisation.},
  format = {Live Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_27,
  author = {Hanyu Qu and Francesco Ardan Dal Rì and Hao Zou},
  title = {O一},
  pages = {107--108},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {27},
  track = {Music},
  doi = {10.5281/zenodo.17801132},
  url = {http://nime.org/proceedings/2025/nime2025_music_27.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_27_file01.mp4},
  abstract = {This performance features “O一”, a Digital Musical Instrument (DMI) that integrates linear and circular conceptions of time, drawing inspiration from Western teleological and Eastern, particularly Chinese, philosophies. The project takes shape as a multi-movement co-composition. The instrument incorporates dual time representations—a linear and a circular LED light board—paired with up to nine light-sensor embedded cubes, each housing an empty patch with protocols and triggers. By working on composing with the instrument we wanted to explore this intimate relationship between entangled time and compositional ideas.},
  format = {Live Performance},
  numpages = {2}
}

@inproceedings{nime2025_music_28,
  author = {Al Wixson and Oliver George-Brown},
  title = {Xylocyclos},
  pages = {109--110},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {28},
  track = {Music},
  doi = {10.5281/zenodo.17801134},
  url = {http://nime.org/proceedings/2025/nime2025_music_28.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_28_file01.mp4},
  presentation-video = {https://www.youtube.com/watch?v=32oAYLdbGbI},
  abstract = {Our project Xylocyclos features desiccated native branches (Juniperus californica, Yucca schidigera, and Cylindropuntia bigelovii) collected during field research in the Mojave Desert. We attach inexpensive transducers and homemade piezo microphones to create feedback loops within the timber itself. The signal is minimally processed: there are no pre-recorded or synthesized sounds, only the natural frequencies of the wood reinforcing themselves and being amplified through a hand-made cajon. Xylocyclos is a portmanteau deriving from the Greek xylo (wood) and cyclos (cycle). Thinking carefully through the provenance, treatment, and materiality of our salvaged timber, we activate physical and poetic resonances across multiple cyclical scales. The sonic feedback operates at the scale of audible sound-waves. Moving the contact microphone mere millimeters can effect a profound shift in the overall sonic texture. We conceive of this as a sonic reimagination of microfluctuations within the natural environment, where minute variations in desert topology determine the way a rivulet chooses its course; where the play of sun and shadows determines the capacity of a plant to photosynthesize; where a pollinator’s peregrinations determine which plants propagate. At this broader ecological scale, we are also thinking about cycles of life and death: our branches are all biologically dead, all in a transitional phase between aliveness and total decomposition. Transplanted into a foreign environment, we practice an ethics of care in preparing the timber for performance. When reanimating the branches as sonic beings we emphasize their roles as co-performers. They are sonically unpredictable: even adorning them with the target-practice cans we found buried in the desert can generate a profusion of new sonorities and overtones. Rather than attempting to control the branches and their sonic output, we instead afford them a degree of agency, performing a non-hierarchical act of creative collaboration with these organic entities. },
  format = {Live Performance},
  numpages = {2}
}

@inproceedings{nime2025_music_29,
  author = {Leah Barclay and Toby Gifford},
  title = {{riversynth}},
  pages = {111--113},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {29},
  track = {Music},
  doi = {10.5281/zenodo.17802652},
  url = {http://nime.org/proceedings/2025/nime2025_music_29.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_29_file01.mp4},
  presentation-video = {https://tobygifford.com/riversynth},
  abstract = {{riversynth} is both an immersive performance and musical interface that mixes live hydrophone streams from six river systems with live processing through a gestural controlled water instrument. Building on a decade of research from the River Listening project, {riversynth} creates an entangled performance ecosystem where unpredictable aquatic soundscapes become both the source material and score for a live performance. The interface consists of a transparent tank filled with water and embedded sensors that detect water movement and light. These parameters modulate, filter and mix the live hydrophone streams via the performer’s hands in the water, creating a dynamic relationship between the real-time river soundscapes and the performer's physical manipulation of water. {riversynth} demonstrates multiple layers of entanglement: temporal (connecting historical River Listneing research with real-time audio), spatial (linking geographically distant river ecosystems in a spatial performance environment), and material (using water to control and mix aquatic soundscapes). This multilayered approach directly engages with NIME 2025's theme by exploring how musical interfaces can bridge environmental monitoring with live artistic experimentation. The technical implementation combines custom-designed sensors, a low-latency streaming network, and a granular synthesis performance tool for live manipulation of the hydrophone streams in surround sound. The interface enables intuitive control while maintaining complexity in the sonic output, allowing for both composed sections and improvisatory responses to the unpredictability of live streams. {riversynth} creates an entangled network where environmental data becomes musical material, and human gestural control is mediated through live engagement with the hydrophones. This creates a feedback loop between performer, technology, and environment that emphasises our interconnected relationship with aquatic ecosystems and their health. While the performers appear to have agency through their gestural engagement with the water, it is ultimately the live river soundscapes that dictate and control how the performance will unfold, mirroring the unpredictability of the natural environment.},
  format = {Live Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_30,
  author = {Michael Gancz and Justin Berry and Shu Wei and Jake Shaker and Kimberly Hieftje and Asher Marks},
  title = {Screenless Optical Theremin with Tremolo (ScOTT)},
  pages = {114--117},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {30},
  track = {Music},
  doi = {10.5281/zenodo.17801140},
  url = {http://nime.org/proceedings/2025/nime2025_music_30.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_30_file01.mp4},
  abstract = {Head-mounted extended-reality (XR) interfaces provide a flexible platform for immersive and embodied musical instrument design. By combining spatial audio, ergonomic first-person gestural control, and networked interactivity, these interfaces can facilitate expressive, interesting, and emotionally resonant performances. Unlike data gloves and hyperinstruments, XR headsets can be calibrated to the physical properties of their individual users. However, these headsets tend to obscure the eyes and other parts of the face, limiting the user’s capacity to establish eye contact and transmit facial expressions. These subtle communicative elements play a crucial role in real-world collaborative musical settings where performers utilize facial cues to negotiate surface parameters such as timing, dynamics, and breath, as well as more complex qualities like atmosphere and interpretive mimesis. In this remote performance, we present the Screenless Optical Theremin with Tremolo (ScOTT), a novel gestural MIDI controller powered by a modified screenless XR headset. ScOTT focuses on the hands and arms, mapping broad gestures to coarse-grained musical parameters (e.g. pitch and velocity) and small movements to more complex musical ornaments (e.g. tremolo width and frequency). In a structured improvisation that highlights the role of ornament in musical texture, we explore the ScOTT’s capacity to balance social presence, embodied interaction, and expressivity.},
  format = {Remote Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_31,
  author = {Sophie Rose},
  title = {Breathing I},
  pages = {118--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {31},
  track = {Music},
  doi = {10.5281/zenodo.17801142},
  url = {http://nime.org/proceedings/2025/nime2025_music_31.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_31_file01.mp4},
  abstract = {Breathing I is the first in a three-part series (Breathing I-III) that externalizes emotional responses to traumatic experiences through breath and bilateral coordination. Bilateral movement techniques, widely used in trauma therapy, engage both hemispheres of the brain, promoting bodily unification and focused attention. This work integrates trauma-informed movement practices with wearable gestural music technology to explore sonic representations of psychological states in a multi-channel spatial audio environment. The performance sonifies panic through asymmetric arm and hand movements, breath-based vocalizations, and visual projections that depict physiological dysregulation. Datagloves capture movement data, modulating live and sampled breath sounds to create a dynamically evolving soundscape. Visual projections, generated in real-time, use torus meshes that expand and contract — an analogy to hemoglobin’s role in oxygen transport. The integration of movement, sound, and visuals reinforces the connection between breath, blood flow, and the body’s autonomic responses. As Adriana Cavarero notes, “Nothing more than the act of breathing is able to testify to the proximity of human beings to one another; nothing else better confirms their communication…” to signify the essence of being alive. Breathing I amplifies this concept, transforming breath into both a personal and collective sonic expression.},
  format = {Live Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_32,
  author = {Sophie Rose},
  title = {Breathing III},
  pages = {122--125},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {32},
  track = {Music},
  doi = {10.5281/zenodo.17801153},
  url = {http://nime.org/proceedings/2025/nime2025_music_32.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_32_file01.mp4},
  abstract = {“Time is the objectification of a biological organism’s act of breathing, which is sensitive and conscious.” Breath is more than survival—it is memory, movement, and connection. Breathing III is the final piece in a three-part series that externalizes emotional responses to trauma, exploring the body’s search for reconnection and resolution. Through midline-crossing and sequenced movement patterns, the performer traces a path from psychological fragmentation to unity. MiMU datagloves capture gestural movement in real time, transforming hand and arm motions into a layered soundscape of live vocal sampling. Open-hand gestures extend outward and draw inward, mapping to sustained melodic lines, while bird-like formations—fingers flexing and unfolding—trigger harmonic layers and spatialized echoes. Prayer-hand movements send reverberant waves through the sound field, reinforcing themes of unity and self-reintegration. Real-time particle cloud projections, generated from movement data, swirl and converge around the performer, evoking shamanic ritual and animist traditions. As breath, movement, and sound merge, Breathing III invites the audience into a space of transformation—where embodied memory dissolves, and a new sense of self begins to take shape.},
  format = {Live Performance},
  numpages = {4}
}

@inproceedings{nime2025_music_33,
  author = {Tara Pattenden},
  title = {Tentacle Orbits},
  pages = {126--128},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {33},
  track = {Music},
  doi = {10.5281/zenodo.17801157},
  url = {http://nime.org/proceedings/2025/nime2025_music_33.pdf},
  presentation-video = {https://vimeo.com/1048397360/038e6d170c},
  abstract = {Tentacle Orbits is a participatory performance composed with malleable Tentacle Instruments that are played through manipulations such as squeezing or stretching. The performance of these custom-built instruments is supported by live electronics that build a framework for playful improvisation. During the performance, the audience is invited to don and play the instruments, squeezing and bending them to create a variety of sounds. The colourful appearance and glitchy sonic palette of the instruments inspire playful musicking, culminating in a cacophony of freeform play. The invitation techniques of the participatory performance vary in response to the performance site, considering the atmospheric and social aspects of the performance venue. The Tentacle Instruments’ sounds range from bright square waves built with digital logic circuitry to glitchy samplers built with the Daisy Seed platform. The parameters of their electronic components, the affordances of their code, and the nuanced expression afforded by the soft interface’s malleability inform each instrument’s sonic direction.},
  format = {Live Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_34,
  author = {Takuma Kikuchi and Riki Saito and Risako Shibata and Atsuya Tsuchida and Kenshiro Taira and Nimisha Anand and Ryoho Kobayashi and Yuta Uozumi and Shinya Fujii},
  title = {transcriptions},
  pages = {129--137},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {34},
  track = {Music},
  doi = {10.5281/zenodo.17801160},
  url = {http://nime.org/proceedings/2025/nime2025_music_34.pdf},
  presentation-video = {https://www.youtube.com/watch?v=F0hwyvr1jEM},
  abstract = {“transcriptions” is an improvisational musical performance piece in which two performers recursively mimic each other's movements, utilizing postural sensing and tactile feedback. The misalignments and errors that occur in the process of imitation and the dynamic changes in the relationship between the performers caused by the interveners (the System Jockey and the Intervener) are intertwined to produce unpredictable movements and sounds. Performers wear special suits equipped with gyro-sensors and exciters. The gyro-sensor converts the postural movement data into vibrations, which are transmitted to the exciter of the other performer. The performers respond to these vibrations and move, mimicking each other's postures. As this chain of imitation is repeated, errors and misalignments due to tactile perception, physical ability, initial position, differences in posture, spatial constraints, and other factors accumulate, and new movements emerge. In this work, all sounds are generated from the performers' movements. Contact microphones are attached to the performers' suits to capture the sound generated by their movements. The sounds are processed and output, and electronic sounds are generated from its volume information, so that the relationships between the performers and the changes in their movements can be expressed sonically. The relationships among the performers, and the interaction between the system and the performance environment generate the performers' movements and sounds, and the system jockey and the intervener intervene in these interactions. The result is an improvisational performance in which nonlinear changes in movement and sound are intertwined with intentional control. “transcriptions” is a work that presents a new form of improvisational expression by actively utilizing creative emergence through chains of imitation.},
  format = {Live Performance},
  numpages = {9}
}

@inproceedings{nime2025_music_35,
  author = {Courtney Brown and Cezary Gajewski},
  title = {Dinosaur Choir: Adult Corythosaurus},
  pages = {138--141},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {35},
  track = {Music},
  doi = {10.5281/zenodo.17801164},
  url = {http://nime.org/proceedings/2025/nime2025_music_35.pdf},
  presentation-video = {https://vimeo.com/1026355696},
  abstract = {Lambeosaurine hadrosaurs are duck-billed dinosaurs known for their large head crests encasing elaborate nasal passages. Researchers hypothesize these large crests were resonators for their vocalizations. Dinosaur Choir brings these calls to life as singing dinosaur musical instruments. Musicians and participants give voice to these dinosaur instruments by blowing into a mouthpiece, exciting a syrinx (bird vocal box) computational model and resonating the sound through the dinosaur’s nasal cavities and skull.},
  format = {Installation},
  numpages = {4}
}

@inproceedings{nime2025_music_36,
  author = {Palle Dahlstedt},
  title = {Adaptive Elusion - an improvisation for pianist and real-time machine learning},
  pages = {142--144},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {36},
  track = {Music},
  doi = {10.5281/zenodo.17801166},
  url = {http://nime.org/proceedings/2025/nime2025_music_36.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_36_file01.mp4},
  presentation-video = {https://youtu.be/kclJz0j1cto?si=7Yw_-1b1zkPHSBZA},
  abstract = {A continuation of my experiments with minimal algorithms, investigating how small an interactive musical algorithm can be and still invoke the feeling of "somebody there". Here, a small set of adaptive algorithms react to a live pianist, trying to imitate, elude and counteract his playing, while at the same time being completely dependent on it as a source of patterns and sounds. The piece explores real-time training as a primary modus of interaction, in a cat-and-mouse game of sorts. It is also an example of what I call entangled musicianship. What the pianist plays is a reaction to what the algorithm plays, and at the same time shapes the future playing of the algorithm, hence entangling performance and control.The musical response is generated by a small machine-learning algorithm that starts empty and is trained in real-time on what I am playing. It can also gradually forget what it has learnt.},
  format = {Live Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_37,
  author = {Palle Dahlstedt},
  title = {What We Do (Differently) Together},
  pages = {145--146},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {37},
  track = {Music},
  doi = {10.5281/zenodo.17801168},
  url = {http://nime.org/proceedings/2025/nime2025_music_37.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_37_file01.mp4},
  presentation-video = {https://youtu.be/4Z6bO1s3Mu8?si=EbchJ8rl_Yx0MqO4},
  abstract = {In this performance-lecture, the pianist performs and improvises on (or with) five different interactive algorithms, and simultaneously discusses how it feels playing with them, how their different interactive qualities affect the musical outcome, and what the human-machine situation does to us. It is a statement about the essence of process, the nature of agency, and what different types of algorithms bring to human creative process, told from a situation of being entangled with the algorithms, while trying to make music.},
  format = {Live Performance},
  numpages = {2}
}

@inproceedings{nime2025_music_38,
  author = {Vijay Thillaimuthu},
  title = {Tesseract},
  pages = {147--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {38},
  track = {Music},
  doi = {10.5281/zenodo.17801170},
  url = {http://nime.org/proceedings/2025/nime2025_music_38.pdf},
  presentation-video = {https://vimeo.com/1013750398/cacafe0c7c},
  abstract = {Tesseract is a live audiovisual work designed to provoke a questioning of the mechanisms of perception. The work creates interactions between two dimensional and three-dimensional visualisations of sound using lasers and video projections. This sound voltage is derived from a modular synthesiser that is kinaesthetically controlled by the performer using a theremin controller. The work is developed in reference to the tesseract, a four-dimensional hypercube beyond our ability to properly visualise, analogous to what a three-dimensional cube is to a two-dimensional square. It is inspired by a notion of the movement of electrons, as quantum waves, in relation to the movement of the underlying voltage that defines sound synthesis. The work considers the microcosm of the subatomic in relation to the macrocosm of experience.},
  format = {Live Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_39,
  author = {Risako Shibata and Miki Kanda and Kenta Tanaka and Ryoho Kobayashi and Yuta Uozumi and Shinya Fujii},
  title = {Playing the sound image – individual immersive sound performance by a performer wearing parametric speakers},
  pages = {150--157},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {39},
  track = {Music},
  doi = {10.5281/zenodo.17801174},
  url = {http://nime.org/proceedings/2025/nime2025_music_39.pdf},
  presentation-video = {https://www.youtube.com/watch?v=fPONmF3s8nM},
  abstract = {This is a sound performance in which dynamic and improvisational spatial movement of sound is created by a performer wearing parametric  speakers that have the characteristic of directing sound. The direct sound from the parametric speakers has a super narrow directivity, which creates the sensation that the sound field is close by, as well as tactile auditory stimulation. It is also easily reflected by walls and objects, creating a virtual sound source on the spot. In this work, parametric speakers are attached to both palms and shins of the performer and made wireless, making the sound source mobile. The composer also remotely selects and adjusts the parameters of the sound source played from the parametric speakers. This allows the performer to move between the audience, changing the distance and direction from the  audience and the wall, creating a movement  of the sound image. In addition, the switching between direct and reflected sound creates an auditory experience in which the localization changes abruptly. In this work, the choreography of the performers and the music are composed in parallel, for example, by taking advantage of the changes in pitch created by the high-speed movement of the parametric speakers. The movement of the performer changes the hearing, response, and relationship between the performer themselves, the composer, and the viewer, bringing to light a more complex sound source movement and improvisational acoustic experience.},
  format = {Live Performance},
  numpages = {8}
}

@inproceedings{nime2025_music_40,
  author = {Botao Amber Hu and Yuemin Huang and Mingze Chai and Xiaobo Aaron Hu and Yilan Elan Tao and Rem RunGu Lin},
  title = {Improvising within "GravField": A Participatory Live-coding Performance Exploring How Digital Objects Mediate Intercorporeal Movements in Collocated Mixed Reality},
  pages = {158--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {40},
  track = {Music},
  doi = {10.5281/zenodo.17801178},
  url = {http://nime.org/proceedings/2025/nime2025_music_40.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_40_file01.mp4},
  presentation-video = {https://vimeo.com/955522087},
  abstract = {As mixed reality technologies evolve, they blur the boundaries between digital and physical worlds, prompting us to reevaluate how we engage with digital objects, our bodies, and our consensus reality. Inspired by contact improvisation, we introduce GravField, a live performance mixed reality system that uses metaphorical audiovisual mediators to guide interdependent behaviors among participants. These mediators—incorporating springs, ropes, and magnetic fields—shape bodily movement and social dynamics through dynamic audio feedback, assigned by a live-musician/live-coder. We invite participants to join performances to explore how digital objects mediate intercorporeal interaction and entangled embodied behavior with technology. This work investigates the concept of "digital physics" in mixed reality, where digital objects shape somatic experiences to examine the relationships between embodiment, interpretation, alterity, and background, drawing from post-phenomenology.},
  format = {Live Performance},
  numpages = {6}
}

@inproceedings{nime2025_music_41,
  author = {Héloïse Garry},
  title = {Mulholland Revisited},
  pages = {164--168},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {41},
  track = {Music},
  doi = {10.5281/zenodo.17801184},
  url = {http://nime.org/proceedings/2025/nime2025_music_41.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_41_file01.mp4},
  presentation-video = {https://www.youtube.com/watch?v=viQeJR-QNjc},
  abstract = {This project explores the intersection of real-time algorithmic sound generation and live piano performance through ChucK and MIDI-based interaction. Inspired by the interplay between dream and reality in David Lynch’s "Mulholland Drive" (2001), the piece utilizes a series of ChucK scripts that dynamically respond to the pianist’s input, shaping an evolving soundscape. The system employs MIDI-triggered processes to generate and manipulate electronic textures in real-time, expanding the piano’s expressive range beyond its traditional acoustic boundaries. The work is structured around four key sonic gestures, each corresponding to a moment in "Mulholland Drive": (1) a synthesized telephone bell signifying Diane’s psychological rupture, (2) a dynamically generated arpeggio that mirrors the tension of her conversation with Camilla, and (3-4) progressively complex textural layers that blur the line between live performance and algorithmic sound synthesis. By integrating reactive electronic sound with live piano, this piece demonstrates how real-time programming can transform the piano from a traditional interface into a multidimensional, interactive system.},
  format = {Live Performance},
  numpages = {5}
}

@inproceedings{nime2025_music_42,
  author = {Taurin Barrera},
  title = {Gaia is hanging by a thread...},
  pages = {169--171},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {42},
  track = {Music},
  doi = {10.5281/zenodo.17801191},
  url = {http://nime.org/proceedings/2025/nime2025_music_42.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_42_file01.mp4},
  abstract = {“Gaia is hanging by a thread…" is an interactive audiovisual performance that explores the delicate balance between technology, humanity, and our planet’s ecosystem. This approximately 6-minute performance aligns primarily with solarpunk aesthetics, offering a critical contemplation of electronic art, sustainability, and collective ecoconsciousness through audience interaction and connection. At the heart of this performance is the Solar Punk Console, a solar-powered capacitive synthesizer that embodies the solarpunk ethos of sustainable technology and community interconnectedness. This synthesizer not only harnesses solar energy but also invites audience participation, as conductive threads connected to the Solar Punk Console’s synthesis control inputs are deployed into the audience during performance. Participants are encouraged to hold, manipulate, or pass these threads, forming a complex electronic circuit within the performance venue. This interaction directly impacts both the audio output and the visual atmosphere of the performance space using audiovisual processing software created in Max. This piece utilizes feedback in both audio and visual systems to blur the lines between performer, audience, and environment, as collective actions of the audience shape a interwoven synesthetic sensory experience in real-time.},
  format = {Live Performance},
  numpages = {3}
}

@inproceedings{nime2025_music_43,
  author = {Ji Won Yoon and Woon Seung Yeo},
  title = {Echoic Defiance of Gravity},
  pages = {172--174},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Sophie Rose and Jos Mulder and Nicole Carroll},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {43},
  track = {Music},
  doi = {10.5281/zenodo.17801197},
  url = {http://nime.org/proceedings/2025/nime2025_music_43.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2025/nime2025_music_43_file01.mp4},
  abstract = {“Echoic Defiance of Gravity" is a visual music piece featuring "Bouncy Echo"—a virtual audiovisual instrument based on a computer simulation of the motion of bouncing balls, showcasing its potential as a versatile, expressive tool for real-time music performance. Here, the simulation not only governs the motion of the balls in the visual domain but also determines the musical outcome of the piece in the auditory domain, thereby providing a unique cross-modal experience for both the audience and the performer. In addition to the fact that this piece may invoke intriguing discussions in terms of simulation-based algorithmic composition, it is especially noteworthy that it is a unique amalgamation of a fixed, deterministic set of musical notes and partly uncertain, indeterministic follow-ups generated by computer simulation in the visual domain. Although the exact time and the frequency of occurrence of musical events are chance-based and effectively random, they are perfectly relevant to the motion of the ball presented simultaneously. Regarding the musical organization of the piece, the artists focus on exploring a wide range of parameter combinations, keep evaluating the musical versatility of the system employed, and hope to bring more unique outcomes to the stage at the conference than those featured in the accompanying video.},
  format = {Live Performance},
  numpages = {3}
}
