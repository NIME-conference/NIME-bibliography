@inproceedings{NIME22_music_1,
 abstract = {Mechanophore was inspired by the force-sensitive molecular units of the same name. As mechanophores are subjected to physical forces, they activate chemical reactions that can communicate their state (e.g. color change) or even heal themselves. The musical work represents this process of increasing tension to the point of ring opening, out of which a texture whose nature ascends and heals emerges. The second section represents a particular mechanophore, spiropyran, more literally by tracing the molecule’s skeletal structure in its pitch contours. Just as force makes spiropyran transform into a different molecule (merocyanine), the musical theme morphs into new configurations as it progresses. After another ring opening, the final section of the work represents interactions between individual polymers within a material, which can be characterized by entanglement, bridging, paths of motion, qualities, sizes, velocities, densities and loops. More philosophically, the piece shows the wonder and complexity of the microscopic world through sonic elements that border on the threshold of perceptibility. As polymer science brings the distinction between the ideas of organic and synthetic into focus, the music illustrates the continuum between these poles through virtual and acoustic instruments (including the robotic string instruments PAM and Cyther) that are combined and manipulated in a panoply of ways. Spiropyran elastomers were used as membranes for PVC drums played by robotic actuators made from 3D-printed PLA, thus connecting the metaphors of the work to its physical realization. Musical robots were design and built by WPI’s Music, Perception and Robotics Lab and EMMI.},
 address = {Shanghai, China},
 articleno = {1},
 track = {Music},
 author = {Scott Barton},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.f40bd267},
 issn = {2220-4806},
 month = {June},
 title = {Mechanophore},
 url = {https://doi.org/10.21428/92fbeb44.f40bd267},
 year = {2021}
}

@inproceedings{NIME22_music_2,
 abstract = {“Glorious Guilt” is an excerpt of The Furies: A Laptopera, originally premiered by the Stanford Laptop Orchestra (SLOrk).  In this scene, a collective controller, The Rope, acts as a barrier between the Furies and their victims (Electra and Orestia) as well as an instrument guiding harmony and rhythm accompanying the voices of the Fury chorus. The Furies surround the rope, collectively raising and lowering the rope with wave-like gestures to perform their ritual, “Glorious Guilt.” The rope is attached to six GameTrak Tether controllers spaced evenly around the rope. Each tether is attached to a laptop and gathers six axes of spatial information. This information determines the playback of various samples or sound synthesis depending on the instrument design. The sound is spatialized across hemispherical speakers so that each player has a localized instrument. The length and rate of change of the tethers control the playing of specific harmonic samples.  Percussive samples are networked to a constant pulse controlled by a server computer and automated variations are introduced throughout the piece.Players control volume and, through volume, collective panning in real-time.  As the rope gets pulled in different directions, the sound swells around the space. Through practice, the ensemble perfects a kind of Ouija board-like playing where the rope communally moves around the outskirts of the space, panning the beat as it travels. The basic song form of “Glorious Guilt” creates something familiar and, with the repetition, supports the ritualistic drama while allowing time for the audience to become acquainted with the instruments. This piece aptly introduces the laptopera, showcasing its potential to use physical, group-based, embodied interfaces to support dramatic elements and character relationships.},
 address = {Shanghai, China},
 articleno = {2},
 track = {Music},
 author = {Anne Hege and Ge Wang and Camille Noufi and Elena Georgieva},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.7fbceebc},
 issn = {2220-4806},
 month = {June},
 title = {The Furies: A Laptopera - "Glorious Guilt"},
 url = {https://doi.org/10.21428/92fbeb44.7fbceebc},
 year = {2021}
}

@inproceedings{NIME22_music_3,
 abstract = {Mermy is an augmented ceramic instrument shaped as a female marine creature, acting as the player's avatar during the performance. The physical body of Mermy offers multiple playing possibilities as an instrument: idiophone, chordophone, and aerophone all together. An embedded contact microphone is used to pick up the acoustic sound, for both amplification and processing purpose. Five windows around the body serve as tangible sensors, which control digital treatments and trigger LED lights from within. This music performance is an illustration of an uncanny world through the sound of Mermy. Interactions between the performer and the instrument are subtle, precise and intimate, adding to the immersive, ritualistic atmosphere of the piece.},
 address = {Shanghai, China},
 articleno = {3},
 track = {Music},
 author = {Shan Ni},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.a641fb52},
 issn = {2220-4806},
 month = {June},
 title = {Sound of Mermy},
 url = {https://doi.org/10.21428/92fbeb44.a641fb52},
 year = {2021}
}

@inproceedings{NIME22_music_4,
 abstract = {Crow Chases Red-tailed Hawk is a structured improvisation for custom bullroarers, whistles and other aerophones with live electronics. Bullroarers and related instruments are some of the most ancient and widespread instruments in the world. These custom versions were created using generative design principles and produced with 3D fabrication techniques. The sounds of the instruments are processed by audio algorithms that are driven by the speed, intensity and direction of the instruments. The work takes its title from a common interaction in the natural world: one species defending its territory from another species that has been identified as a predator.},
 address = {Shanghai, China},
 articleno = {4},
 track = {Music},
 author = {Holland Hopson},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.10df6264},
 issn = {2220-4806},
 month = {June},
 title = {Crow Chases Red-tailed Hawk},
 url = {https://doi.org/10.21428/92fbeb44.10df6264},
 year = {2021}
}

@inproceedings{NIME22_music_5,
 abstract = {Metamorphosis is a real-time interactive audio-visual composition for one human performer and two artificial intellectual performers. Three performers will start by playing the same virtual ancient Chinese percussion instrument: Bianqing (磬). Then, by learning, imitating, having conflict and cooperating with each other, this instrument's shape and sound will evolve gradually and the performers themselves. From ancient to modern, concrete to abstract, the fusion of sound, image and live performance creates an immersive experience exploring the dramatic shift and co-evolution between human and AI.},
 address = {Shanghai, China},
 articleno = {5},
 track = {Music},
 author = {Hongshuo Fan},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.5b50a42f},
 issn = {2220-4806},
 month = {June},
 title = {Metamorphosis},
 url = {https://doi.org/10.21428/92fbeb44.5b50a42f},
 year = {2021}
}

@inproceedings{NIME22_music_6,
 abstract = {Action-Reaction is a real-time interactive composition of approximately eight minutes in duration for two GameTrak controllers, Max/MSP, and Kyma. In classical mechanics, Newton’s third law of motion states that for every action, there is an equal and opposite reaction. The action and reaction form a single interaction, they are simultaneous and neither force exists without the other. The GameTrak’s retractable tethers interact with the performer’s push, pull, release and free movements, causing the tethers deviating from and aligning with the rest state. The performer sometimes makes subtle and sparse push movements while sometimes pull and drag the tethers dramatically. However, after releasing the tethers, they are retracted back to the rest state, creating predictable yet unique realignment path each time. The data measured from the physical movements are mapped to various parameters in the sound producing algorithms, creating musical expressions that are both superimposed and nuanced.},
 address = {Shanghai, China},
 articleno = {6},
 track = {Music},
 author = {Chi Wang},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.7a93b7c9},
 issn = {2220-4806},
 month = {June},
 title = {Action-Reaction},
 url = {https://doi.org/10.21428/92fbeb44.7a93b7c9},
 year = {2021}
}

@inproceedings{NIME22_music_7,
 abstract = {This work shows the alternation and opposition of two musical emotions. The musical gestures in traditional music score are amplified and deepened by live electronics. Through the performer's expression of the score and the blessing of different electronic effects, the contrast between the various parts of the work is more vivid. An interactive system that can be controlled by the performer himself has been established for the performer, who can use traditional piano performance techniques to play the piano and live electronics simultaneously. For the performer, the subtle changes in electronic effects are a brand-new experience. He can achieve the fusion effect of traditional piano and technology by learning and controlling the characteristic data during the performance. The work is divided into 10 sections (ABCDEFGHIJ), containing 10 interactive scenarios. The on-site microphone detects the piano performer’s 5 performance characteristic parameters (attack peak, loudness, pitch, spectrum centroid, partials) in real time, and then maps them to the internal parameters 4 modules (4 electronic effects). The interaction scenarios include the use of a single module and the use of multiple modules.},
 address = {Shanghai, China},
 articleno = {7},
 track = {Music},
 author = {Yixuan Zhao},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.99631411},
 issn = {2220-4806},
 month = {June},
 title = {Still Life —for piano and live electronics},
 url = {https://doi.org/10.21428/92fbeb44.99631411},
 year = {2021}
}

@inproceedings{NIME22_music_8,
 abstract = {Even before the Corona Pandemic became known, I had the idea for an interactive piece with my violin automaton. Besides the pitch, the distance of the player to the machine was a parameter for the communication with the machine. Striking coincidence with social distancing and loneliness during lockdown. The title of the work has a double meaning: musical and physical approach of the flautist to the automaton.On the one hand, the interactive system with the violin automaton represents a flexible improvisation system for the flutist. On the other hand, it is an enigmatic labyrinth in which the performer moves. Developing this requires difficult learning with surprises. The learning effects are the result of testing the reaction algorithms by playful improvisation of the flautist with the system. In parallel, the system was refined. This simultaneity in learning has demands not known in traditional music. The piece is dedicated to the performer of the 2020 premiere Ms Karina Erhard.The use of technology is rather playful and does not compare to robotics. The automaton allows extended sounds and rhythms by means of three bows, vibrato, percussion on the strings and ponticello. New sounds emerge from conflict rhythms between vibrato and tremolo.},
 address = {Shanghai, China},
 articleno = {8},
 track = {Music},
 author = {Karl F. Gerber},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.a9149ce2},
 issn = {2220-4806},
 month = {June},
 title = {Approaches for Flute and Interactive Violin Automaton},
 url = {https://doi.org/10.21428/92fbeb44.a9149ce2},
 year = {2021}
}

@inproceedings{NIME22_music_9,
 abstract = {3BP is a new international trio of improvisors. Born of and in the pandemic and thus native to online exchange of real-time audio, the trio explores network instabilities, data loss, compression artefacts and latency as affordances for a creative approach to negotiating the physical and virtual materialities and spaces of online improvisation. The performance features instruments and interfaces already presented at NIME along with new inventions, chained together in complex signal and feedback loops across the network. A new video work in the style of a cinematic ‘essay film’ accompanies the performance, combining algorithmically edited collage, personal archival and documentary footage, and live visualizations. The concert is a document of the evolution and adaption of both instruments and performers as they become entangled in distributed and temporally asynchronous ecologies across continents.},
 address = {Shanghai, China},
 articleno = {9},
 track = {Music},
 author = {Adam Pultz Melbye and Paul Stapleton and and John Bowers},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.75c3b1fd},
 issn = {2220-4806},
 month = {June},
 title = {Networked performance by 3BP},
 url = {https://doi.org/10.21428/92fbeb44.75c3b1fd},
 year = {2021}
}

@inproceedings{NIME22_music_10,
 abstract = {"Into the Abyss" is the first collaborative work composed and performed using a new L2Ork Tweeter platform for socially distant synchronous instrument design, improvisation, composition, rehearsal, and performance. Consequently, every aspect of this work is devised collaboratively, including instrument design, structure, and performance. The work was originally premiered in December 2020.The work portrays a metaphorical journey of facing one's fears and taking that very first leap of faith off a cliff into a deep water well below. With a thundering splash, a terrifying free fall suddenly transforms into a soothing weightlessness, a life-changing experience that makes us question our own limits. Inspired by the COVID-19 crisis, this metaphor in many ways reflects the motivation behind the piece and the supporting infrastructure.},
 address = {Shanghai, China},
 articleno = {10},
 track = {Music},
 author = {Ivica Bukvic},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.de47a33c},
 issn = {2220-4806},
 month = {June},
 title = {Into the Abyss},
 url = {https://doi.org/10.21428/92fbeb44.de47a33c},
 year = {2021}
}

@inproceedings{NIME22_music_11,
 abstract = {Our program begins before it begins with the amalgamation & transmutation of assets offered up by the S.E.A.L. We five, through shared words, golden dice & a prism of  frequency, invoke one another to activate a portal of boxes within boxes. We are observed & connected across lay/latency lines, stitching together a tapestry of sight & sound...and for 35 minutes life is less lonely for these ghosts in the machine. We create an experience that transcends the academic contexts in which we are trained with a more punk aesthetic with a real beat. The performance swerves between the avant-garde and a Bond-theme song for a movie set in the ocean of the future. To increase our telepresence we each have an identical golden rotten luck die that we roll to determine playing our identical custom made theremins and performance techniques including barks, tonal honks, grunts, growls, roars, moans, and pup contact calls. Our improvisation is asynchronous; we’re in different timezones with zoom lags, no single stream of the performance was the same for any viewer or performer, beautifully isolated experiences in this interconnected world. We’ll never know exactly what anyone experiences each in their own Covid bubbles.},
 address = {Shanghai, China},
 articleno = {11},
 track = {Music},
 author = {Sofy Yuditskaya and Meg Schedel and Sophia Sun and Ria Rajan and Susie Green},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.e340d105},
 issn = {2220-4806},
 month = {June},
 title = {The Seals International Debut},
 url = {https://doi.org/10.21428/92fbeb44.e340d105},
 year = {2021}
}

@inproceedings{NIME22_music_12,
 abstract = {What is it that engenders the sense of “authentic” concert experience? Philippe Auslander asserts that the authentic object for listeners in pop music concerts has changed from performers’ actual musicianship to a simulation of recorded performances that have never existed in the real, but were created with the extensive use of studio production techniques (e.g., lip-syncing on recorded tracks). Drawing on this historical transition of authenticity in concerts from an actual to fictional performance, our composition proposes the ambiguity between corporeal performance and electroacoustic imagery as a new authentic experience in a concert. To this end, the piece focuses on a gradual disintegration of the causal link from a gesture to a sound. The piece starts with a state where a single gesture such as jabbing, tilting and swinging causes a single sonic event such as initiation, transition and termination. As the piece unfolds, the causal link is untied through asynchronization between the gesture and the sound in time, and displacement between the performing bodies and sound positions. Finally, the performing bodies vanish from the stage while the T-Sticks autonomously resonate, remaining only the vestige of human performers.  At this point, we authenticate the fictional performance over the actual one as if the unreal is more real than the real – hyperreal.},
 address = {Shanghai, China},
 articleno = {12},
 track = {Music},
 author = {Takuto Fukuda and Ana Dall'ara-Majek},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.bae5271d},
 issn = {2220-4806},
 month = {June},
 title = {Higher Order Gestalt Fromage},
 url = {https://doi.org/10.21428/92fbeb44.bae5271d},
 year = {2021}
}

@inproceedings{NIME22_music_13,
 abstract = {Black box fading, for sensor-augmented flute (Chaosflöte) and improvisation machine (AIYA), is an immersive experience that draws upon the performance interplay between human and machine and the shifting perceptions of machine agency and human-machine interactions that result from it. This work is presented as a 360° video with first-order Ambisonics and head-tracking in VR. Modulating sound behaviors, reactive visual projections, shifting perceptions of space and scale, and the unconventional 360° editing techniques play along the spectrums of reality & surreality and between performance & edited composition, eventually transforming the work into a multifaceted experience of its own. Listeners are invited to explore the virtual space as both a concert and installation, a journey inside a box.},
 address = {Shanghai, China},
 articleno = {13},
 track = {Music},
 author = {Melody Chua},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.efecbb70},
 issn = {2220-4806},
 month = {June},
 title = {Black box fading},
 url = {https://doi.org/10.21428/92fbeb44.efecbb70},
 year = {2021}
}

@inproceedings{NIME22_music_14,
 abstract = {For this collaborative performance, two musicians will improvise together over the internet using custom gestural controllers, the AirSticks. The AirSticks utilise off-the-shelf VR controllers and bespoke software to trigger and manipulate sound and graphics through hand movements. 3D point clouds — captured using commodity depth sensors — are streamed in real-time into a shared virtual stage. The performer’s gestures create and manipulate the audio-visual environment, as the ‘VJ’ curates the audience’s porthole into the space. With the rise of online musical experiences over Zoom, this performance brings new 3D flavour for both the musicians and the audience. Audio and graphic latency is reduced by sending MIDI and OSC data over the internet, rendering the sound on each end, while images streamed from the depth cameras utilise state-of-the-art compression algorithms. Future work will be dedicated to allowing the audience to enter the virtual space using VR or AR.},
 address = {Shanghai, China},
 articleno = {14},
 track = {Music},
 author = {Alon Ilsar and Matthew Hughes},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.84c23721},
 issn = {2220-4806},
 month = {June},
 title = {AirStream: A Collaborative Gestural Virtual Performance},
 url = {https://doi.org/10.21428/92fbeb44.84c23721},
 year = {2021}
}

@inproceedings{NIME22_music_15,
 abstract = {In the ‘embodied gestures’ project we discuss the beneficial aspects of incorporating energy-motion models as a design pattern in musical interface design. These models can be understood as archetypes of motion trajectories which are commonly applied in the analysis and composition of acousmatic music. Four interfaces were especially designed to emphasise four particular energy-motion profiles: oscillation, granularity, friction and flexion. In this performance, which will take the format of an improvisation, two musicians are exploring the physical and musical gestures suggested from the interfaces’ affordances.},
 address = {Shanghai, China},
 articleno = {15},
 track = {Music},
 author = {Enrique Tomás and Thomas Gorbach},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.84c23721},
 issn = {2220-4806},
 month = {June},
 title = {Bend-Rotation Nr. 3: Improvisation for Embodied Gestures Instruments},
 url = {https://doi.org/10.21428/92fbeb44.d7e7f121},
 year = {2021}
}

@inproceedings{NIME22_music_16,
 abstract = {This performance with the TCP/Indeterminate Place quartet activates a telematic connection in which two concert hall pipe organs, in two different locations in Europe, are controlled remotely. It is a realization of a new scenario in the Global Hyperorgan project, in which the quartet is divided in two duos, situated in different locations. Such interactions are emblematic of the intersections of the technical, artistic and social aspects at the heart of the Global Hyperorgan and illustrative of the thick and pervasive mediating dynamics endemic to all musicking. Global Hyperorgan participants are compelled to develop new models of instrumentality for new modes of musicking. TCP, in the name of the quartet, refers to the combination of Telepresence and Copresence, in the neologism Tele-Copresence, as an expression of our central interest in the sense of presence in musical interactions enabled by telematic performance. By Indeterminate Place, we refer to the experience of place in tele-copresence as sometimes characterised by a mediated, liminal space. The Global Hyperorgan contributes a particular potential for exploration of concert spaces that carry particular acoustic affordances, as well as socio-historical characteristics. In tele-copresence, these are at times molded together in Indeterminate Place.},
 address = {Shanghai, China},
 articleno = {16},
 track = {Music},
 author = {Robert Ek and Stefan Östersjö and Federico Visi and Mattias Petersson},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.7714e834},
 issn = {2220-4806},
 month = {June},
 title = {Bend-Rotation Nr. 3: Improvisation for Embodied Gestures Instruments},
 url = {https://doi.org/10.21428/92fbeb44.7714e834},
 year = {2021}
}

@inproceedings{NIME22_music_17,
 abstract = {#otherbeats is a sound piece that lives on the web, or a website that makes sound. The piece is made from an archive of “social” rhythm that I collected from over 50 participants around the globe during the pandemic. Each participant interpreted the prompts that I circulated in their own way while mostly being in isolation and with little to no access to instruments or professional recording technology. The prompts asked them, in several ways, to explore differences between musical concepts such as “tempo,” “beat” “rhythm” or “alternative metronome” in tiny 3-minute performances ideally in the outdoors, and so the collected archive showcases a wide range of approaches to humans keeping time. The collected audio exhibits versions of human-made regularity and manifold ways of deviation thereof. #otherbeats organizes all these audio recordings on a website, with the use of HTML and the Web Audio API, and equips them with a visual appearance that acts at once as a purposely under-explained and nebulous user interface and an immersive experience with the aim to create a sense of getting lost within the sonic and visual ephemera. The rhythm recordings, on the website, are spread and arranged visually in space, and with the Web Audio API, an experience of distance, or proximity respectively, is produced. By scrolling, the user approaches ever new audio files that gradually appear on the virtual “horizon” and become louder, clearer, and dryer. On the other hand, those sounds left behind disappear in the distance. The lowpass filter’s cutoff frequency is lowered, the amount of convolution reverb increased, and the gain reduced. If they click on one of the few clickable items, a JavaScript random algorithm takes them to a new location within the website rather far away and results in a swift change of the sonic carpet. All audio processing is computed directly on the user’s browser with the user’s left-right and top-down scrolling, and clicking, as the sole input variables. The user navigates their own way through the archive of rhythm recordings, all of which play constantly and simultaneously in loop but never quite sync up with one another – given the precarious nature of their making. They find themselves left with a quite blurry visual and sonic space that is ever changing and yet constantly evades being grabbed and pinned down.},
 address = {Shanghai, China},
 articleno = {17},
 track = {Music},
 author = {Marcel Zaes},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.2f210153},
 issn = {2220-4806},
 month = {June},
 title = {#otherbeats: Performing a Participatory Archive of Social Rhythm},
 url = {https://doi.org/10.21428/92fbeb44.2f210153},
 year = {2021}
}

@inproceedings{NIME22_music_18,
 abstract = {This interdisciplinary project is a musical mime performance that utilizes the mime’s movements to generate sounds in real-time. For this purpose, the performers wear gloves with embedded wireless motion sensors called MUGIC. The receiving data from the sensors generate real-time sounds through a MAX/MSP patch and creates the required music for the performance. Silent Music uses the mime’s body as a musical instrument and creates a particular relationship between musical events and gestures. Panopticon is a type of architecture and a system of control that is designed to observe and punish prisoners. In a panopticon prison, the inmates never know when they are being watched. With a certain observation technique, the prisoners are forced to act like they are always being watched. The video that you are about to hear and see is a scene from my upcoming movie,  Silent Music/Musical Mime. This scene along with six others depicts an original story describing the life of a censored artist who is writing a piece about a prison called Panop. Silent Music is a multidisciplinary performance integrating mime, music, and motion sensor technologies. This project is based on the concept of censorship, silence, and silencing. For this scene, I wear hand-worn MUGIC motion sensors. MUGIC tracks the movements of my hand and generates data; the resulting data produces the required music in real-time.},
 address = {Shanghai, China},
 articleno = {18},
 track = {Music},
 author = {Adib Ghorbani},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.a647b10e},
 issn = {2220-4806},
 month = {June},
 title = {Silent Music - What happened in Panop?},
 url = {https://doi.org/10.21428/92fbeb44.a647b10e},
 year = {2021}
}

@inproceedings{NIME22_music_19,
 abstract = {The audiovisual content in "Flowing Light" is presented as a fantasy scene composed of virtual landforms and ambient sounds. It’s based on the author's creative intention to guide the order in the virtual space with the light of real space. Through digital means, the author transforms ordinary illumination light into a control source, forming a deep-seated sensory penetration and highly integrated audio-visual performance.},
 address = {Shanghai, China},
 articleno = {19},
 track = {Music},
 author = {Shihua MA},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.16eb8ea4},
 issn = {2220-4806},
 month = {June},
 title = {Flowing Light},
 url = {https://doi.org/10.21428/92fbeb44.16eb8ea4},
 year = {2021}
}

@inproceedings{NIME22_music_20,
 abstract = {MEMORIAS is an artistic project by Jessica A. Rodríguez that unfolded in six esolangs for audio-visual creation drawing from Live Coding and Electronic Literature practices. The languages are based on six short autobiographical texts and have a hybridized syntax using natural and computational languages. MEMORIAS explores literary/textual materialities, as well as different audio-visual modalities; mixing voices, Cello and Paetzold samples, sonic and visual syntheses, and pre-recorded video clips. This project explores how speech — in its written mode — can be used as an interface that allows the interpreter to communicate with the computer and the audience. In parallel, speech — in its sonic mode — expands the possibilities of spoken literature producing limitless variations of the "original" autobiographical stories. During the live performance, the user activates audio-visual languages through the evaluation of written speech (esolangs).},
 address = {Shanghai, China},
 articleno = {20},
 track = {Music},
 author = {Jessica Rodriguez},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.1ba52c5e},
 issn = {2220-4806},
 month = {June},
 title = {MEMORIAS},
 url = {https://doi.org/10.21428/92fbeb44.1ba52c5e},
 year = {2021}
}

@inproceedings{NIME22_music_21,
 abstract = {Students of the University of the Arts Utrecht, school of music technology experienced the same depressing situation as all of us when Covid-19 struck in March 2020. The school proposed to use the pandemic as an opportunity for new interactive performance formats and interactive physical online installations. Positioning the assignment as an exhilarating state-of-the-art novelty functioned as an extra motivation for the students. While many school subjects suffered from motivation loss, the students active within this project reported an increased working ethos compared to what they experience doing a ‘normal’ physical presentation. Conceptual focus, often an issue with these kinds of projects for beginning bachelor students, came easy. Themes like ‘Skin Hunger’ and ‘Escapism’ had a strong connection with the students’ own predicaments. The performance at NIME will deal with the end of the epidemic. This time, the student’s assignment is to translate the uncertainty of the current situation. The multifaceted circumstance sublimates in an online live streamed hybrid performance featuring interactive installations and performances on self-made instruments. Visitors of the NIME conference will literally crash the party at what hopefully be the height of the event.},
 address = {Shanghai, China},
 articleno = {21},
 track = {Music},
 author = {Hans Leeuw and Dianne Verdonk},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.e2c711b7},
 issn = {2220-4806},
 month = {June},
 title = {Ether noise},
 url = {https://doi.org/10.21428/92fbeb44.e2c711b7},
 year = {2021}
}

@inproceedings{NIME22_music_22,
 abstract = {The theme of NIME 2021 is “Learning to Play, Playing to Learn”, and this performance involves the exploration of how novel machine learning and audio decomposition tools can be integrated into a NIME that is already well-established, and has been performed with publicly for fourteen years. While much NIME research has focused on novelty within musical human-computer interaction, there has been a significant body of work which has demonstrated that much can be learnt from examining not only issues of longevity within NIME, but also how the performance practices of musicians working within the NIME field can provide fruitful sites of knowledge. This piece was created using new tools from the Fluid Corpus Manipulation (FluCoMa) project, from the University of Huddersfield. The project studies how creative coders and technologists work with and incorporate new digital tools for deconstructing audio in novel ways: “FluCoMa instigates new musical ways of exploiting ever-growing baks of sound and gestures within the digital composition process, by bringing breakthroughs of signal decomposition DSP and machine learning to the toolset of techno-fluent computer composers, creative coders and digital artists”. In this piece, I explore these tools through an embodied approach to segmentation, slicing, and layering of sound in real time. Extensive use of the micro-sound technique pulsar synthesis is employed which is explored through the use of tangible controllers. Using the FluCoMa toolkit, I was able to incorporate novel machine learning techniques in Max which deal with exploring large corpora of sound files. Specifically, this work involves, among other relevant AI techniques, machine learning in order to train based on preference; sort and select based on descriptors; and then concatenate percussion sounds from a large collection of drum machine samples. More broadly, my improvisation instrument that I have been developing and performing with since 2007 is heavily based on machine listening techniques such as transient detection and pitch detection. While the former is linked not only to its origins involving the hybrid piano [3], but also its heavily percussive or attack based aesthetics, the latter has always afforded an element of unpredictability, given the sonic material that I work with. Using FluCoMa’s toolkit, I was able to explore not only transient detection, but other amplitude-based models. Furthermore, pitch detection involved ‘confidence’ estimates, rather than simply delivering values. In general, my approach to improvisation involves designing mutually affecting networks between my hardware and software. By introducing machine learning, I hope to explore this further so that performance remains less about decision making and control, and more about navigation, vulnerability, and play.},
 address = {Shanghai, China},
 articleno = {22},
 track = {Music},
 author = {Lauren Hayes},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.f3163249},
 issn = {2220-4806},
 month = {June},
 title = {Moon via Spirit (2019) for hybrid analogue/digital live electronics},
 url = {https://doi.org/10.21428/92fbeb44.f3163249},
 year = {2021}
}

@inproceedings{NIME22_music_23,
 abstract = {Potarhythm is a MIDI-controlled musical performance device that uses the sound of water droplets colliding with an object as its sound source. The frequency of the water droplet sound is determined by the combination of the diameter of the droplet and the velocity at which it lands. I have developed a water droplet generation mechanism that controls the droplet diameter by electrically controlling the opening and closing of a solenoid valve. I used this mechanism to roughly control the pitch and volume of the water droplets and also used it as a MIDI sound source to enable music sequencing and real-time performance. Potarhythm can extend the tone by changing the objects that collide with the water droplets. Images and sounds of flowing water, ripples on the surface of the water, and ripples are called ASMR (Autonomous Sensory Meridian Response) and are appreciated as one of the phenomena that induce a state of euphoria and relaxation. In Japanese gardens, there are also examples of decorations using water, such as Suikinkutsu and Shishiodoshi. These are created to appreciate the movement and sound of water. In the field of musical works, various artists have presented works using the sound of water. For example, Junichi Kamiyama in 1989, and Cornelius in 2001 and 2016 have presented works that use the sound of water. Water sounds can be used as a method of musical expression. However, not much reproducible information has been published on how to record and perform live water sounds. I wanted to create a musical performance using not only sampled sounds but also sounds generated by the physical phenomenon of water droplets using real water. In this performance, I combined not only the real-time sound of Potarhythm but also reverb, delay, and looping to create more complex rhythms. This performance was also a combination of guitar and synthesizer to create and perform ambient music. I created the music to create a sound that expresses a sense of floating and instability.},
 address = {Shanghai, China},
 articleno = {23},
 track = {Music},
 author = {Taisei Kato},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.ce443fea},
 issn = {2220-4806},
 month = {June},
 title = {Potarhythm},
 url = {https://doi.org/10.21428/92fbeb44.ce443fea},
 year = {2021}
}

@inproceedings{NIME22_music_24,
 abstract = {This live coding performance is a collaboration between a human live coder and a virtual agent (VA). MIRLCa is a self-built SuperCollider extension and a follow-up of the also self-built SuperCollider extension MIRLC.  The system combines machine learning algorithms with music information retrieval techniques to retrieve crowdsourced sounds from the online database Freesound.org, which results in a sound-based music style.  In this performance, the live coder will explore the online database by only retrieving sounds predicted as “good” sounds when using the retrieval methods from the live coding system. This approach aims at facilitating serendipity instead of randomness in the retrieval of crowdsourced sounds. The VA has been trained to learn from the musical preference of a live coder within context-dependent decisions, ‘situated musical actions’. A binary classifier based on a multilayer perceptron (MLP) neural network has been used for sound prediction. The themes of legibility, agency and negotiability in performance will be sought through the collaboration between the human live coder, the virtual agent live coder and the audience. This project has been funded by the EPSRC HDI Network Plus Grant - Art, Music, and Culture theme.},
 address = {Shanghai, China},
 articleno = {24},
 track = {Music},
 author = {Anna Xambó},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.7299536b},
 issn = {2220-4806},
 month = {June},
 title = {A Live Coding Session With the Cloud and a Virtual Agent},
 url = {https://doi.org/10.21428/92fbeb44.7299536b},
 year = {2021}
}

@inproceedings{NIME22_music_25,
 abstract = {Snared, Wired, Crashed is a new composition for percussion and live electronics, created in collaboration with percussionist Josh Perry.  The composition is built around a hybrid acoustic-electronic-digital instrument that places contact mics and transducer drivers on a snare and cymbal (with an extra pair of drivers on a kick drum and thunder sheet) and various delay and feedback presets built into a Max patch controlled by the percussionist with his feet.},
 address = {Shanghai, China},
 articleno = {25},
 track = {Music},
 author = {Adam Mirza},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.23aa2d7b},
 issn = {2220-4806},
 month = {June},
 title = {Snared, Wired, Crashed},
 url = {https://doi.org/10.21428/92fbeb44.23aa2d7b},
 year = {2021}
}

@inproceedings{NIME22_music_26,
 abstract = {UnStumm - Artificial Liveness is run by Claudia Schmitz (time-based media artist) and Nicola L. Hein (sound artist) and focuses on the equal communication of human and machine actors within the framework of audiovisual real-time performances in telematic virtual reality settings. In UnStumm - Artificial Liveness, the boundaries between human and machine are blurred, forming an artificial system that unfolds an nonartificial liveness and questions the boundaries of human and machine through collaborative creation and real-time performance. The two project leaders (performing with moving image onto virtual sculpture and guitar/electronics respectively) will perform together with the two AI artists programmed by them. It will realize a virtual reality performance in front of an audience over a duration of 15 minutes. The intermedial and transhuman conversation between 2 video artists and 2 sound artists will take place on the virtual reality real-time platform already programmed by UnStumm, where humans and machine actors will meet as artistic partners. The machine artists will use technologies of machine learning in order to interact. The audience will be able to stream the performances through this internet platform and use their own smartphones as AR glasses to stream the AR performances visually and auditorily. Sound will be audible through headphones and a binaural audio stream.},
 address = {Shanghai, China},
 articleno = {26},
 track = {Music},
 author = {Claudia Schmitz and Nicola Leonard Hein},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.a1d9cdc3},
 issn = {2220-4806},
 month = {June},
 title = {UnStumm - Artificial Liveness},
 url = {https://doi.org/10.21428/92fbeb44.a1d9cdc3},
 year = {2021}
}

@inproceedings{NIME22_music_27,
 abstract = {Transsonic is a intermedia project of Viola Yip and Nicola L. Hein and aims to develop new site-specific works that communicate intermedial conceptions of music consisting of site-specific light installations, electric guitar, multichannel live electronics and relays working as oscillators. The project centers around the development of “light as musical material” and questions the ontology of music from this intermedial perspective. In light of the ongoing digital revolution we develop translational potentials of electronic and digital media to develop our working concept of music across different media. By using guitar pickups that microphone the changes in electricity used to control the flickering of the lights by placing them amongst the electricity carrying cables, introducing the sharp clicking sound of 220 Volts lighting up the lightbulbs of Viola Yip’s lightbulb instrument. These sounds are picked up and transformed by the guitar pickups instantly being sent through the line of guitar effects and live-electronics written in Max/MSP for Nicola Hein’s multichannel electronic set up thus sonically fusing both instruments. Both, the sound of the light bulbs and the electric guitar are, in real time, fed into a machine learning based Max/MSP patch which performs frequency analysis based multi channel audio spatialisation and acts as a musically autonomous agent. The software does, in real time based on mel-frequency and knn analysis, learn the playing and interaction style of guitar and light bulbs and answers/interacts with the learned audio material. On the other hand, a knn-analysis controlled, audio-corpus based synthesis process forms a site of contextual resonance, lets the sound of the light bulbs resonate in myriad grains of guitar sound and vice versa. With its 15 minute telematic performance, Transsonic explores the rich nexus between light and sound, that sees both as equally important and yet dialectical musical counterpoints that enrich our musical experience.},
 address = {Shanghai, China},
 articleno = {27},
 track = {Music},
 author = {Viola Yip and Nicola Leonard Hein},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.7e245eca},
 issn = {2220-4806},
 month = {June},
 title = {Transsonic},
 url = {https://doi.org/10.21428/92fbeb44.7e245eca},
 year = {2021}
}

@inproceedings{NIME22_music_28,
 abstract = {Alt F in Front of the Body showcases three instruments: karlax, sopranino t-stick and phallophone. The composite creative work around these DMIs spans twenty years, coinciding with the emergence of the NIME community, itself. The development of the karlax commenced in 2001, with the developers (Dury and Garabédian) constructing the initial TUB-x prototype. In 2006, the first t-stick was developed at the Input Devices and Music Interaction Lab at McGill University, led by a technologist-composer duo (Malloch and Stewart). The phallophone, described by its inventor (Stromberg) as a physical manifestation of a trans-visual/aural experience in performance, was completed in 2015. The creative undercurrent of Alt F is twofold. Firstly, these instruments, positioned in front of the body, are extensions of the performers' physiology; DMIs of this nature cease to be instrument-objects and become areas of sensitivity, extending the scope and active radius of touch and listening outward from the body. Secondly, Alt F is a response to alternative facts. Alt F is a tribute to those who seek truth by opening new paths of play, questioning, appreciating and discovering new approaches. Play is a taproot from which original art springs; it is the raw stuff that the artist channels and organises with all their learning and technique (Nachmanovitch, 1990).},
 address = {Shanghai, China},
 articleno = {28},
 track = {Music},
 author = {D. Andrew Stewart and Michał Seta and and Dirk Stromberg},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.e96a58fb},
 issn = {2220-4806},
 month = {June},
 title = {Alt F in Front of the Body},
 url = {https://doi.org/10.21428/92fbeb44.e96a58fb},
 year = {2021}
}

@inproceedings{NIME22_music_29,
 abstract = {The composition Uncertainty Etude #2 is based on a narrative that keeps the performer remain in unconfident state of performing. The composition provides musical phrases followed by ever-shifting new sounds. Uncertainty Etude #2 is a composition written for the AI-terity instrument that comprises GANSpaceSynth deep learning hybrid method for generating relevant audio samples for real-time audio synthesis. The autonomous behaviour of the AI-terity instrument keeps the performer in an uncertainty state in the performance. Appearance of new sounds and being able to move through timber-changes in latent space allow performer to explore a whole new range of musical possibilities with the AI-terity instrument. Composition turns into a continuous state of playing and opening up new variety of musical demands.},
 address = {Shanghai, China},
 articleno = {29},
 track = {Music},
 author = {Koray Tahiroğlu},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.15ad0c07},
 issn = {2220-4806},
 month = {June},
 title = {Uncertainty Etude #2},
 url = {https://doi.org/10.21428/92fbeb44.15ad0c07},
 year = {2021}
}

@inproceedings{NIME22_music_30,
 abstract = {Global live coding ensemble SuperContinent will use the Estuary browser-based, collaborative live coding platform for an improvised, multi-lingual, live coding, audiovisual performance (music plus generative visuals). The performance features sounds and images gathered by the ensemble members for this performance (and shared for posterity via our CC0 sample library), captured in locations as diverse as the locations from which we will perform (Canada, Colombia, India, Israel, Japan, South Africa, United Kingdom).},
 address = {Shanghai, China},
 articleno = {30},
 track = {Music},
 author = {Celeste Betancur and Abhinay Khoparzi and Shelly Knotts and Melandri Laubscher and Mynah Marie and David Ogborn and Chiho Oka and Eldad Tsabary},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.718da650},
 issn = {2220-4806},
 month = {June},
 title = {SuperContinent: Global, collective live coding improvisation},
 url = {https://doi.org/10.21428/92fbeb44.718da650},
 year = {2021}
}

@inproceedings{NIME22_music_31,
 abstract = {In the first performance of this piece, two pianos were set in two different buildings and were connected through network. One piano was tuned in 440hz and the other in 442hz. MIDI signals were sent on OSC and the Disklavier was interactively played by the pianist’s performance. Audio information was transmitted through DANTE in order to get high-speed response. "Diastema Ⅱ”is performed in two remote places. The two places are audio-visually connected. Because of several layers of latency (LAN, audio processing, hardware transmission, etc.), the audience will not only see the time difference between the two but also hear the tuning difference between the instruments. Interval of the pitches, separation of the places and time difference are the three dimension of the piece. The audience can see and hear all the three dimensions from an objective point.},
 address = {Shanghai, China},
 articleno = {31},
 track = {Music},
 author = {Mikako Mizuno and Yoshihisa Suzuki},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.9078fa2d},
 issn = {2220-4806},
 month = {June},
 title = {Diastema for the time floating between the networked remote places},
 url = {https://doi.org/10.21428/92fbeb44.9078fa2d},
 year = {2021}
}

@inproceedings{NIME22_music_32,
 abstract = {Summer (blackflies and other bugs) is a site-specific intermedia performance that uses homemade instruments, digital signal processing, and interactive electronics to create a closed-loop system for a human-insect encounter with rural space. While human-insect interaction is a consistent feature of the rural imaginary, it is typically represented within settler narratives of conquest and perseverance. In an effort to engage insects outside of these frameworks, this piece places two light sources in a pasture in rural Vermont. A human participant produces frequencies by beating wing-like paddles in response to the intensity of the light sources. The system also measures the presence of insects attracted to the light, and adjusts the intensity of both lights in response to insect and human activity. Human and insect sound is captured and processed for a virtual audience.  Through biomimicry and autopoeisis, this system offers a unique opportunity for engaging insect spatial practices outside of human-oriented pioneer narratives of rural space.},
 address = {Shanghai, China},
 articleno = {32},
 track = {Music},
 author = {Sean Clute and Otto Muller and and Leif Hunneman},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.cdddc154},
 issn = {2220-4806},
 month = {June},
 title = {Summer (blackflies and other bugs)},
 url = {https://doi.org/10.21428/92fbeb44.cdddc154},
 year = {2021}
}

@inproceedings{NIME22_music_33,
 abstract = {The title was taken from “Rossby Wave”, a natural atmospheric phenomenon relating to rotation of the planet. When the Rossby Wave ‘breaks’, it could cause extreme climate conditions as El Niño/La Niña. The piece consists of a ‘flexible ostinato’, a somewhat regular ‘wave’ of violin sound that’s processed and flows. The motion sensor MUGIC® detects and interprets the character of the bowing movements which affect ‘ostinato’ in its timbre and character. Some irregular and disruptive motive starts to affect the regular ostinato, sometimes mangling it. Eventually both the violin and ostinato flows freely, sometimes and sometimes not, affecting each other. The accompanying video was created by media artist Liubo Borissov, using Artmatic Voyager and Max (Jitter). Rossby Waving is dedicated to the memory of Jean-Claude Risset who passed away in November, 2016.},
 address = {Shanghai, China},
 articleno = {33},
 track = {Music},
 author = {Mari Kimura},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.99072877},
 issn = {2220-4806},
 month = {June},
 title = {Rossby Waving for Violin, MUGIC® sensor, and video},
 url = {https://doi.org/10.21428/92fbeb44.99072877},
 year = {2021}
}

@inproceedings{NIME22_music_34,
 abstract = {In this work, we will explore the idea of divergent realities as they manifest for three performers and an audience during a live performance. We will use a collaborative live-coding environment that we have developed to carry out the performance. The live-coding environment combines collaborative editing of source code with live execution of code. We separate editing and execution so the performers can choose when to execute fragments of the document on their own and the other performers' machines. As the performance progresses, the performers will code various non-deterministic processes, for example, probabilistic frequency sequences and signal processors. The stochastic behaviour of the growing, collaboratively written program means each performer will hear an increasingly divergent interpretation of the piece. There will be no definitive sonic interpretation of the code. We will make each of the performers' divergent live streams available in performance so the audience can choose to which they wish to listen.},
 address = {Shanghai, China},
 articleno = {34},
 track = {Music},
 author = {Mick Grierson and Matthew Yee-King and Louis McCallum},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.c86215ad},
 issn = {2220-4806},
 month = {June},
 title = {Executive Order},
 url = {https://doi.org/10.21428/92fbeb44.c86215ad},
 year = {2021}
}

@inproceedings{NIME22_music_35,
 abstract = {Does the Earth dream? Can our dreams mesh? In the Wandering Mind, we merge audience dreams and a massive data set of audio field recordings. In dreams, we set sail on open ocean. Freed from the sense-making constraints of the waking mind, the currents pull us through a process of wandering transformations, influenced by both our subconscious minds and our surroundings. This dream-work reminds us of the operations of AI algorithms called representation learning neural networks, which uncover relationships between data samples that can facilitate travel outside of space and time. Applied to global recordings, the latent space connections are like sensory wormholes. To create the Wandering Mind platform, we fed ~70,000 field recordings scraped from the internet to a neural network called YAMNet , producing ~30 million 521-dimensional points corresponding to 1-second audio clips. We further reduce the dimensionality to 2-d, creating a traversable sound map. In the 30 minute performance, we navigate over the map to create a continuous, morphing soundscape that mixes as many as 30 distinct recordings together at any given time, while the audience drifts between waking and sleep. For the best experience, we recommend finding a darkened space and a comfortable spot to lie down. There is no requirement to sleep. We only encourage you to allow the mind to wander.},
 address = {Shanghai, China},
 articleno = {35},
 track = {Music},
 author = {Gershon Dublon and Xin Liu and Nicholas Gillian and Nan Zhao},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 editor = {Eric Parren and Wei Chen},
 doi = {10.21428/92fbeb44.56a514bb},
 issn = {2220-4806},
 month = {June},
 title = {The Wandering Mind: Planetary Scale Dreaming in Latent Spaces},
 url = {https://doi.org/10.21428/92fbeb44.56a514bb},
 year = {2021}
}