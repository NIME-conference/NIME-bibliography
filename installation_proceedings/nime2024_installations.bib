@inproceedings{nime2024_installations_1,
  author = {Monica Lim and Jarrod Knibbe and Bingqing Chen and Ying Sima and Melanie Huang},
  title = {Echo Chamber},
  pages = {1--3},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {1},
  track = {Installations},
  doi = {10.5281/zenodo.15027150},
  url = {http://nime.org/proceedings/2024/nime2024_installations_1.pdf},
  presentation-video = {https://vimeo.com/938949011},
  abstract = {Echo Chamber is a participatory sound installation using MusicGen’s audio generative AI. Participants play a short melody on a piano, which is then used as a reference for MusicGen to generate multiple versions of itself. These evolving echoes of the original are played back through a multi-channel speaker installation. The work alludes to concerns about generative AI creating a monoculture through a feedback cycle of data scraping, copying and regenerating. By deliberately engaging with (and simultaneously serving as a critique of) MusicGen’s innate tendency to stay within predictable melodic and harmonic structures, we can layer and loop multiple versions of the AI-generated audio while staying musically coherent. The layering of multiple samples also allows us to produce a real-time participatory work despite the time required for the AI generations (which currently take longer to generate than the length of the audio samples generated). A piano was chosen as the interface through which an audio reference is created by participants, as it plays on the familiarity and ubiquity of the instrument, which has become both a symbol and a tool of colonisation. Not only has the keyboard become a dominant interface for music-making, the equal-tempered scale has become pervasive across the globe in most popular music cultures, relegating other modes and scales to the fringe – a pattern repeated by generative AI amplifying what is already amplified.},
  numpages = {3}
}

@inproceedings{nime2024_installations_2,
  author = {Marta Rossi},
  title = {Synaptic Sanctum Neuroscape: real-time emotionally adaptive music in a VR immersive environment },
  pages = {4--9},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {2},
  track = {Installations},
  doi = {10.5281/zenodo.15027650},
  url = {http://nime.org/proceedings/2024/nime2024_installations_2.pdf},
  presentation-video = {},
  abstract = {Synaptic Sanctum Neuroscape is an immersive EEG-driven project that intertwines the realms of neuroscience, music composition, and virtual reality. Developed using Max/MSP and Unreal Engine 5, this immersive experience employs a custom EEG device to read users’ brainwaves, transforming their neural activity into emotionally adaptive music and particle systems. The sound spatialization uses ambisonics, and music and environmental sounds are rendered in binaural format for the VR headset’s headphones. This real-time EEG-driven composition, while reflecting the user's alert/relaxed states, also adapts the musical landscape in response to the user’s evolving emotional dynamics, and the auditory experience becomes an intimate collaboration between their neural patterns and the generative musical algorithm. The EEG data also drives particle systems and manages the duration of the 3D granulator’s grains. This granulator, built with the Max/MSP CataRT library, manifests as glowing spheres within the virtual space, utilizing concatenative synthesis where each sphere encapsulates a unique sonic fragment, and its position and size are given by its sonic features. The visual elements resonate with the music, creating a multisensory symphony that transcends the boundaries of traditional VR experiences. The spatialization of this dynamically generated music is managed by the ambisonics library Spat5: sound is encoded in ambisonics and decoded in binaural format, allowing for a 3D perception of the music, and enhancing the overall sense of presence and engagement. This project merges cheap custom EEG technology and free VR development tools, dark aesthetics and generative art, and invites participants to embark on a synesthetic journey through the realms of their minds, where the convergence of science and art allows users to explore what technology can show us of ourselves.},
  numpages = {6}
}

@inproceedings{nime2024_installations_3,
  author = {Hugo Flores Garcia and Stephan Moore},
  title = {Token Telephone},
  pages = {10--13},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {3},
  track = {Installations},
  doi = {10.5281/zenodo.17552961},
  url = {http://nime.org/proceedings/2024/nime2024_installations_3.pdf},
  presentation-video = {https://youtu.be/vEaYoEgtSUo},
  abstract = {Token Telephone is a co-creative AI sound installation. Participants enter a space equipped with a microphone and a quartet of generative sound neural networks, each represented by a loudspeaker. Upon vocalizing into the microphone, the participants' utterance is transformed into neural acoustic tokens and played back, initiating a game of telephone between the neural networks. Each network encodes, processes and reconstructs the sound, distorting the original utterance into new textures guided by the network's training data. The newly reconfigured sound is then passed to the next network/loudspeaker in a clockwise direction, and the process repeats. The sound produced by the fourth network is passed back to the first network in the cycle, creating a feedback loop wherein the original utterance incrementally loses all of its original characteristics and disintegrates into textures that reflect the inherent biases of the generative models in play. In time, the resonant properties of the processes are revealed in front of the participant. Inspired by the popular children's game of telephone, Token Telephone illuminates the gradual formation of hallucinations through the iterative processing and re-processing of audio, reflecting the biases introduced by the model's understanding of sound objects, as well as the data that was provided to it.  },
  numpages = {4}
}

@inproceedings{nime2024_installations_4,
  author = {Luciana Perc},
  title = {The Listening Canvas},
  pages = {14--16},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {4},
  track = {Installations},
  doi = {10.5281/zenodo.17552982},
  url = {http://nime.org/proceedings/2024/nime2024_installations_4.pdf},
  presentation-video = {},
  abstract = {‘The Listening Canvas’ is an interactive sound installation that invites audiences to discover the sounds of found objects by way of engaging in tactile interactions with organic (grains, leaves, stems) and synthetic (plastics, cardboard, fabric) matter within an open canvas. This sound installation displays a canvas made of multiple synthetic materials surrounded by hanging found objects. Audiences are invited to produce sounds by touching the surface of the canvas with their hands or with leaves and other vegetal materials provided. This tactile interaction is captured by contact microphones placed on the verso of the canvas. The sounds are modified and diffused in real time through two transducers attached to hanging found objects used as resonators, such as a frying pan and a watering can. The installation enables a two-sided experience in which listening and sound-making are equally encouraged. The aim of this artwork is to engage wide audiences, including adults and children, in experimental sound-making and active listening practices. They will be offered the opportunity to actively participate by way of touching art objects in display, which is traditionally forbidden in exhibition spaces. The organic and synthetic matter used to produce sounds through the canvas surface is explored sonically in a way that fosters different modes of listening, challenging boundaries between nature and culture towards an ecofeminist configuration.},
  numpages = {3}
}

@inproceedings{nime2024_installations_5,
  author = {Yunyu Ong and Emma Smith and Lee McIver and Maddie Duncan},
  title = {Be Your Own Rain God},
  pages = {17--20},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {5},
  track = {Installations},
  doi = {10.5281/zenodo.17552998},
  url = {http://nime.org/proceedings/2024/nime2024_installations_5.pdf},
  presentation-video = {https://vimeo.com/942206916/761cb98da1},
  abstract = {In this immersive installation, participants take on the role of rainmakers, orchestrating their own thunderstorms through two distinctive modes. The first, a free-playing mode, invites individuals to generate their personalized storms by interacting with an electric taiko display. The intensity of their performance directly influences the gathering pace of storm clouds, culminating in the triumphant arrival of rain. The second mode, a precision-playing experience, challenges participants to execute a specific musical phrase, vying for the esteemed title of successor to a retiring rain god.},
  numpages = {4}
}

@inproceedings{nime2024_installations_6,
  author = {Leon Eckard},
  title = {What Is It (not) Like to Be a Bat?},
  pages = {21--25},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {6},
  track = {Installations},
  doi = {10.5281/zenodo.17048764},
  url = {http://nime.org/proceedings/2024/nime2024_installations_6.pdf},
  presentation-video = {https://vimeo.com/912907360},
  abstract = {The work ‘What Is It (not) Like to Be a Bat?’ is a contemporary interpretation of the famous paper ‘What Is It Like to Be a Bat’ (1974) by the American philosopher Thomas Nagel. The work consists of a self-built headset which uses three ultrasonic sensors and a gyroscope, translating the data in real-time into sound — analogous to the sonar of a bat — creating an interactive musical ‘Bat-Theme’. Hence, it is not only questioning the hard problem of consciousness and the line between subjectivity and intersubjectivity, but also the urge to of science, philosophy and technology to solve it, which might be impossible by nature.},
  numpages = {5}
}

@inproceedings{nime2024_installations_7,
  author = {Iván Paz and Lina Bautista},
  title = {Maria Choir},
  pages = {26--27},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {7},
  track = {Installations},
  doi = {10.5281/zenodo.15027164},
  url = {http://nime.org/proceedings/2024/nime2024_installations_7.pdf},
  presentation-video = {},
  abstract = {Interactive installation where a synthetic reproduction (using a timbre transfer model) of the Catalan singer Maria Arnal’s voice harmonizes in real time what the visitor sings, forming a hybrid choir. The harmonization is performed by analysing the input voice and using different singing voice models (trained for different vocal registers). The harmonization process also includes a set of Maria Arnal's samples triggered accordingly to the result of the analysis of the input signal. This work explores on the timber transfer possibilities to extend the human voice and reflects on how timber transfer techniques could offer new expressive possibilities.},
  numpages = {2}
}

@inproceedings{nime2024_installations_8,
  author = {Enrico Dorigatti},
  title = {[in.tangibile]},
  pages = {28--31},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {8},
  track = {Installations},
  doi = {10.5281/zenodo.17846330} 
  url = {http://nime.org/proceedings/2024/nime2024_installations_8.pdf},
  presentation-video = {},
  abstract = {[in.tangibile] is an interactive sound art installation confronting visitors with two main concepts. In the first instance, it proposes a reflection on the materiality of sound, which, although often referred to as an ’object’ in the sonic arts context, remains nevertheless formless and elusive. Yet, it is a physical phenomenon that impacts—albeit usually imperceptibly—the surrounding environment. Is its supposed intangibility, thus, intrinsic to the sound event, or is it related to our perceptual limitations? Secondly [in.tangibile] centralises the audience’s role in the artistic creation process. According to Umberto Eco’s open work concept, an artwork is not an accomplished and unchangeable fact but rather an artefact that only finds its completeness in combination with the interaction of the audience and the subsequent interpretative process it enacts. [in.tangibile], through experimental interfaces—recalling the tradition of graphic scores—and generative processes, aims to provide an experimental, tactile dimension to the sound phenomenon whilst elevating the audience at the role of co-creator, granting freedom of exploration and creative agency to explore the affordances of the artistic system and its grounding concept.},
  numpages = {4}
}

@inproceedings{nime2024_installations_9,
  author = {Fabian Werfel and Nicole Luján},
  title = {Loco Loop},
  pages = {32--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {9},
  track = {Installations},
  doi = {10.5281/zenodo.17553006},
  url = {http://nime.org/proceedings/2024/nime2024_installations_9.pdf},
  presentation-video = {},
  abstract = {Loco Loop is an installation for up to eight participants to record sounds into a loop, which they can create and manipulate together in real time. It has the shape of an octagonal table with eight pairs of headphones in which the surroundings can be heard through two microphones embedded in the table top. There are three main controls in the center of the table: one button to record, one button to erase and a rotary knob to change the speed – and thus the pitch – of the loop. There are two additional knobs on the sides for master effects such as reverb, delay or distortion. Playing with this collaborative looper results in an attention shift towards auditory perception. In continuous repetition, every sound becomes music. Ultimately, the minimalist interface allows all participants – both with and without musical education – to have a musical experience through sound manipulation.},
  numpages = {5}
}

@inproceedings{nime2024_installations_10,
  author = {Kenshiro Taira and Sogen Handa and Risako Shibata and Nimisha Anand and Len Matsuda and Victoria Maki and Ryotaro Hoshino and Kenta Tanaka and Ryoho Kobayashi and Yuta Uozumi and Shinya Fujii},
  title = {Sonic Rhopalia},
  pages = {37--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {10},
  track = {Installations},
  doi = {10.5281/zenodo.15027173},
  url = {http://nime.org/proceedings/2024/nime2024_installations_10.pdf},
  presentation-video = {https://youtu.be/HBFbDtB_Nsw},
  abstract = {The Jellyfish is a species with one of the most simplistic and complete Umwelts. Its pulsating movements are caused by a refined network, composing only of a few neurons. These neural receptors, named Rhopalia, receive external simulation which determines their pulses. In the ocean, these pulses then ripple into waves in the water, that is sensed by the Rhopalia, triggering the pulses of the next jellyfish. Such interactions (that cannot be seen by the naked eye) in repetition over time, have sustained the existence of the jellyfish species across 6 billion years. This mechanical system causes organic pulses, radiating an inexplicable feeling of vitality. ‘Sonic Rhopalia’ is a sound installation that is produced by the pulsation of four Aurelia aurita (a.k.a. moon jellyfish). By using Touch Designer and a web camera to capture the movement of each jellyfish, we detect its pulse through image analysis, using OpenCV and a combination of pixel color channel classification and area calculation. Once the pulse of a jellyfish is detected, it sends a bang to M4L (Max for Live) using OSC (OpenSound Control), where it is used as a trigger for music composition. Sounds are played in response to each pulse, and when the jellyfish pulse at the same time, in synchronization, different chords and sounds are produced. The work develops across one loop comprising of three phases with varying soundscapes, all determined by the number pulse synchronizations. By reframing jellyfish as an interface for music generation, the rhythms and movements of the pulsating jellyfish come together create a unique soundscape, that allows the audience to experience the vitality of jellyfish in a completely new dimension from conventional methods of bio-marine appreciation.},
  numpages = {6}
}

@inproceedings{nime2024_installations_11,
  author = {Riki Saito and Fushi Sano and Rikuto Shinmi and Minna Hosaka and Haruru Muramatsu and Kenta Tanaka and Ryoho Kobayashi and Yuta Uozumi and Shinya Fujii},
  title = {Yadorigi},
  pages = {43--48},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {11},
  track = {Installations},
  doi = {10.5281/zenodo.15027175},
  url = {http://nime.org/proceedings/2024/nime2024_installations_11.pdf},
  presentation-video = {https://youtu.be/ML-aZeJ32UQ},
  abstract = {This work is a sound installation that utilizes the surrounding human flow information to dynamically transform the visual and auditory expressions of a ‘Hokora’ integrating changes in light, shadow, and music based on the movements and tendencies of people, ultimately creating an interactive environment where the shrine and individuals mutually influence each other. ‘Hokora’ is a miniature Shinto shrine on a street side, and in Japan, it is a common practice to stop or bow when passing by. Through the faith in the shrine, the environment, or human flow, also undergoes changes. In this work, the information from the surrounding human flow is reflected in the presentation of light and shadow reminiscent of dappled sunlight, causing the visual and auditory expressions of the Hokora to change based on people's actions and further influencing the environment. In this work, three ultrasonic distance sensor values are used as input, and the detection frequency and distance values of the past 10 detections are recorded for each sensor. The recorded detection frequency is used to calculate the congestion level of the surrounding human flow. Based on these two pieces of human flow information, changes are introduced in the movement of lights controlled by motors (DC and servo), creating shadow play and altering the music output. This contributes to the portrayal of light inspired by dappled sunlight. The density of human flow influences the left and right movement of lights, reflecting the intensity of the light's movement. On the other hand, in the realm of music, the density of human flow modulates the frequency of high-pass and low-pass filters. By offering viewers an expression of the Hokora that changes based on human flow information, this installation creates an effect where the Hokora and people mutually influence each other, a phenomenon not traditionally seen in Hokora.},
  numpages = {6}
}

@inproceedings{nime2024_installations_12,
  author = {Christian Faubel},
  title = {Five Robots Playing Pentatonic, Polyrhythmic Songs},
  pages = {49--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {12},
  track = {Installations},
  doi = {10.5281/zenodo.17553007},
  url = {http://nime.org/proceedings/2024/nime2024_installations_12.pdf},
  presentation-video = {https://vimeo.com/912950719},
  abstract = {Five robots playing five tuned bells by striking them with a mallet. Each robot is controlled by a minimalist, neurally inspired analogue oscillator. These oscillators are mutually connected and connected to a pacemaker oscillator, and through network connectivity, they will synchronize at different ratios with the pacemaker. Slow-running analog oscillators of the same type switch on and off the connectivity and modulate the activation of the robots' oscillators. Only through these changing network configurations does a musical score that alternates between coordinated polyrhythms, random structure and moments of silence emerges. The complex interaction between the analogue oscillators is visualized with a computer program that graphically displays the changing relations of the oscillators as multiple x-y plots.},
  numpages = {5}
}

@inproceedings{nime2024_installations_13,
  author = {Ayaka Sakakibara},
  title = {Ocean Pandæmonium -The Noisy Plasticscape-},
  pages = {54--59},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Laurel Smith Pardue and Palle Dahlstedt},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {13},
  track = {Installations},
  doi = {10.5281/zenodo.15027179},
  url = {http://nime.org/proceedings/2024/nime2024_installations_13.pdf},
  presentation-video = {https://youtu.be/KLEhXtMFkgU},
  abstract = {This work is a sound installation that uses plastic debris to consider the interrelationship between people and the ocean, and the history of the ocean. Today, the toxicity of plastic in the marine environment is being debated, and data shows the increase of plastic in the oceans. Although the data accurately represents the facts, it is somewhat cold attitude and we are rarely aware of our personal involvement in the data. Also, the ocean viewed from land is calm and beautiful, and there is a gap between the actual pollution in the ocean and what humans see from land. A critical situation for marine life is occurring in the ocean. We humans cannot live underwater and cannot truly understand the creatures of the sea. The project creates a virtual sea made of bottles to represent the gap between the sea as seen from the land and the sea inside, and emotionally translates the data to appeal to the human senses, looking into the situation and history of the sea through the lens of sound.},
  numpages = {6}
}
