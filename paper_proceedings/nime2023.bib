@inproceedings{nime2023_1,
  author = {Jack Armitage and Thor Magnusson and Andrew McPherson},
  title = {Studying Subtle and Detailed Digital Lutherie: Motivational Contexts and Technical Needs},
  pages = {1--9},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {1},
  track = {Papers},
  doi = {10.5281/zenodo.11189088},
  url = {http://nime.org/proceedings/2023/nime2023_1.pdf},
  abstract = {Subtlety and detail are fundamental to what makes musical instruments special, but accounts of their development in digital lutherie have been constrained to ethnographies, in-the-wild studies, and personal reflections. Though insightful, these accounts are imprecise, incomparable, and inefficient for understanding how fluency with the subtle details of digital musical instruments (DMIs) develops. We have been designing DMI design probes and activities for closed and constrained observation of subtle and detailed DMI design, but in two previous studies these failed to motivate subtle and detailed responses. In this paper we report on our third attempt, where we designed a tuned percussion DMI and a hybrid handcraft tool for sculpting its sound using clay, and a one hour activity. Among 26 study participants were digital luthiers, violin luthiers and musicians, who all engaged with what we define as micro scale DMI design. We observed technical desires and needs for experiencing and comparing subtle details systematically, and also widely varying, subjective emotional and artistic relationships with detail in participants' own practices. We reflect on the contexts that motivate subtle and detailed digital lutherie, and discuss the implications for DMI design researchers and technologists for studying and supporting this aspect of DMI design and craft practice in future.},
  numpages = {9}
}

@inproceedings{nime2023_2,
  author = {Pedro P Lucas and Stefano Fasciani},
  title = {A Human-Agents Music Performance System in an Extended Reality Environment},
  pages = {10--20},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {2},
  track = {Papers},
  doi = {10.5281/zenodo.11189090},
  url = {http://nime.org/proceedings/2023/nime2023_2.pdf},
  abstract = {This paper proposes a human-machine interactive music system for live performances based on autonomous agents, implemented through immersive extended reality. The interaction between humans and agents is grounded in concepts related to Swarm Intelligence and Multi-Agent systems, which are reflected in a technological platform that involves a 3D physical-virtual solution. This approach requires visual, auditory, haptic, and proprioceptive modalities, making it necessary to integrate technologies capable of providing such a multimodal environment. The prototype of the proposed system is implemented by combining Motion Capture, Spatial Audio, and Mixed Reality technologies. The system is evaluated in terms of objective measurements and tested with users through music improvisation sessions. The results demonstrate that the system is used as intended with respect to multimodal interaction for musical agents. Furthermore, the results validate the novel design and integration of the required technologies presented in this paper.},
  numpages = {11}
}

@inproceedings{nime2023_3,
  author = {Nathan D Villicana-Shaw and Dale Carnegie and Jim Murphy and Mo Zareei},
  title = {Explorator Genus: Designing Transportable Mechatronic Sound Objects for Outdoor Installation Art},
  pages = {21--29},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {3},
  track = {Papers},
  doi = {10.5281/zenodo.11189092},
  url = {http://nime.org/proceedings/2023/nime2023_3.pdf},
  abstract = {The Explorator genus is a set of hardware and firmware systems, artistic motivations, and physical construction methods designed to support the creation of transportable environmentally-responsive mechatronic sound objects for exhibition outdoors. In order to enable the realization of installation scenarios with varied cochlear needs, we developed a generalized hardware and firmware system that can be reused between projects and which supports the development of purpose-built feedback mechanisms.
We introduce five distinct hardware instances that serve as test cases for the Explorator genus. The hardware instances are introduced as Explorator “species”. Each species shares core hardware and firmware systems but uses distinct soundscape augmentation feedback mechanisms to support unique installation scenarios. Initial subjective and objective observations, findings, and data are provided from fieldwork conducted in four American states. These initial test installations highlight the Explorator genus as a modular, transportable, environmentally reactive, environmentally protected, self-powered system for creating novel mechatronic sound objects for outdoor sonic installation art.},
  numpages = {9}
}

@inproceedings{nime2023_4,
  author = {Andreas Förster and Alarith Uhde and Mathias Komesker and Christina Komesker and Irina Schmidt},
  title = {LoopBoxes - Evaluation of a Collaborative Accessible Digital Musical Instrument},
  pages = {30--39},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {4},
  track = {Papers},
  doi = {10.5281/zenodo.11189094},
  url = {http://nime.org/proceedings/2023/nime2023_4.pdf},
  abstract = {LoopBoxes is an accessible digital musical instrument designed to create an intuitive access to loop based music making for children with special educational needs (SEN). This paper describes the evaluation of the instrument in the form of a pilot study during a music festival in Berlin, Germany, as well as a case study with children and music teachers in a SEN school setting. We created a modular system composed of three modules that afford single user as well as collaborative music making. The pilot study was evaluated using informal observation and questionnaires (n = 39), and indicated that the instrument affords music making for people with and without prior musical knowledge across all age groups and fosters collaborative musical processes. The case study was based on observation and a qualitative interview. It confirmed that the instrument meets the needs of the school settings and indicated how future versions could expand access to all students.
especially those experiencing complex disabilities. In addition, out-of-the-box functionality seems to be crucial for the long-term implementation of the instrument in a school setting.},
  numpages = {10}
}

@inproceedings{nime2023_5,
  author = {Nathan D Villicana-Shaw and Dale Carnegie and Jim Murphy and Mo Zareei},
  title = {Legatus: Design and Exhibition of Loudspeaker-Based, Environmentally-Reactive, Soundscape Augmentation Artifacts in Outdoor Natural Environments},
  pages = {40--47},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {5},
  track = {Papers},
  doi = {10.5281/zenodo.11189096},
  url = {http://nime.org/proceedings/2023/nime2023_5.pdf},
  abstract = {Legatus is a three-legged audio and environmentally-reactive soundscape augmentation artifact created for outdoor exhibitions in locations without access to mains electricity. Legatus has an approximate ingress protection rating of IP54, is self-powered, and is easy to transport weighing approximately a kilogram while fitting within a 185 mm tall by 110 mm diameter cylinder. With LED-based visual feedback and a cochlear loudspeaker-based vocalization system, Legatus seeks to capture attention and redirect it to the in-situ sonic environment.
Informed by related historical and contemporary outdoor sonic installation artworks, we conceptualized and tested four installation scenarios in 2021. Installations were presented following a soundscape-specific pop-up exhibition strategy, where the exhibition venue and artifact placement are determined by in-situ sonic conditions. Legatus artifacts use high-level audio features and real-time environmental conditions including ambient temperature, humidity, and brightness levels to influence the timing and parameters of sample playback routines, audio synthesis, and audio recording.
Having developed and tested for nine months, Legatus has emerged as a portable, rugged, affordable, adaptable, lightweight, and simple tool for augmenting natural sonic environments that can provide last-mile distributions of sonic installation art experiences to places and communities where these works are rarely exhibited.},
  numpages = {8}
}

@inproceedings{nime2023_6,
  author = {S. M. Astrid Bin},
  title = {Where Few NIMEs Have Gone Before: Lessons in instrument design from Star Trek},
  pages = {48--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {6},
  track = {Papers},
  doi = {10.5281/zenodo.11189098},
  url = {http://nime.org/proceedings/2023/nime2023_6.pdf},
  abstract = {Since 1966 Star Trek has been exploring imaginary and futuristic worlds in which humanity comes in contact with alien cultures. Music has always been a method through which alien cultures are made relatable to humans, and musical instruments become props through which we learn about an alien culture that is totally different to that of humans. These musical instruments were not designed with musical use in mind; rather they are designed as storytelling devices, and never intended to work or make sound. After discovering one of these instruments I realised that recreating it in the way it was imagined and making it functional would require consideration of the instrument's storytelling function above all else, including the technology. In this paper I describe the process of re-creating an instrument from Star Trek as a functional DMI, a process in which design decisions were guided by what the storytelling intentions were for this imagined instrument, and what I found out by having to make technical choices that supported them (not the other way around). As well as reporting the design and implementation process I summarise the important design lesson that came from having to emphasise the intended mood and presence of an instrument, instead of the design being steered by technical affordances.},
  numpages = {6}
}

@inproceedings{nime2023_7,
  author = {Romain Michon and Joseph Bizien and Maxime Popoff and Tanguy Risset},
  title = {Making Frugal Spatial Audio Systems Using Field-Programmable Gate Arrays},
  pages = {54--59},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {7},
  track = {Papers},
  doi = {10.5281/zenodo.11189100},
  url = {http://nime.org/proceedings/2023/nime2023_7.pdf},
  abstract = {Spatial audio systems are expensive, mostly because they usually imply the use of a wide range of speakers and hence audio outputs. Some techniques such as Wave Field Synthesis (WFS) are especially demanding in that regard making them out of reach to many individuals or even institutions. In this paper, we propose to leverage recent progress made using Field-Programmable Gate Arrays (FPGA) in the context of real-time audio signal processing to implement frugal spatial audio systems. We focus on the case of WFS and we demonstrate how to build a 32 speakers system that can manage multiple sources in parallel for less than 800 USD (including speakers). We believe that this approach contributes to making advanced spatial audio techniques more accessible.},
  numpages = {6}
}

@inproceedings{nime2023_8,
  author = {Marcelo Wanderley},
  title = {Prehistoric NIME: Revisiting Research on New Musical Interfaces in the Computer Music Community before NIME},
  pages = {60--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {8},
  track = {Papers},
  doi = {10.5281/zenodo.11189104},
  url = {http://nime.org/proceedings/2023/nime2023_8.pdf},
  abstract = {The history of the New Interfaces for Musical Expression (NIME) conference starts with the first workshop on NIME during the ACM Conference on Human Factors in Computing Systems in 2001. But research on musical interfaces has a rich ”prehistoric” phase with a substantial amount of relevant research material published before 2001. This paper highlights the variety and importance of musical interface-related research between the mid-1970s and 2000 published in two major computer music research venues: the International Computer Music Conference and the Computer Music Journal. It discusses some early examples of research on musical interfaces published in these venues, then reviews five other sources of related literature that pre-date the original NIME CHI workshop. It then presents a series of implications of this research and introduces a collaborative website that compiles many of these references in one place. This work is meant as a step into a more inclusive approach to interface design by facilitating the integration of as many relevant references as possible into future NIME research.},
  numpages = {10}
}

@inproceedings{nime2023_9,
  author = {Teodoro Dannemann and Nick Bryan-Kinns and Andrew McPherson},
  title = {Self-Sabotage Workshop: a starting point to unravel sabotaging of instruments as a design practice},
  pages = {70--78},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {9},
  track = {Papers},
  doi = {10.5281/zenodo.11189106},
  url = {http://nime.org/proceedings/2023/nime2023_9.pdf},
  abstract = {Within the music improvisation and jazz scenes, playing a wrong note may be seen as a source of creativity and novelty, where an initially undesired factor (the mistaken note) invites the musician to leverage their skills to transform it into new musical material. How does this idea, however, translate into more experimental scenes like NIME, where control and virtuosity are not necessarily the performance's aim? 
Moreover, within NIME communities the addition of randomness or constraints to musical instruments is often an intended aesthetic decision rather than a source of mistakes. To explore this contrast, we invited four NIME practitioners to participate in the Self-Sabotage Workshop, where each practitioner had to build their own sabotaging elements for their musical instruments and to give a short demonstration with them. We gathered participants' impressions of self-sabotating in a  focus group, inquiring about control and musicality, and also the strategies they developed for coping with the self-sabotaged instruments. We discuss the emergent ideas of planned and unplanned sabotaging, and we propose a starting point towards the idea of self-sabotaging as a continuous design and musical process where designers/musicians try to overcome barriers that they impose upon themselves.},
  numpages = {9}
}

@inproceedings{nime2023_10,
  author = {Teodoro Dannemann},
  title = {Music jamming as a participatory design method. A case study with disabled musicians},
  pages = {79--85},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {10},
  track = {Papers},
  doi = {10.5281/zenodo.11189108},
  url = {http://nime.org/proceedings/2023/nime2023_10.pdf},
  abstract = {We propose a method that uses music jamming as a tool for the design of musical instruments. Both designers and musicians collaborate in the music making process for the subsequent development of individual “music performer’s profiles” which account for four dimensions: (i) movements and embodiment, (ii) musical preferences, (iii) difficulties, and (iv) capabilities. These profiles converge into proposed prototypes that transform into final designs after experts and performers' examination and feedback. We ground this method in the context of physically disabled musicians, and we show that the method provides a decolonial view to disability, as its purpose moves from the classical view of technology as an aid for allowing disabled communities to access well-established instruments, towards a new paradigm where technologies are used for the augmentation of expressive capabilities, the strengthening of social engagement, and the empowerment of music makers.},
  numpages = {7}
}

@inproceedings{nime2023_11,
  author = {Eduardo A. L. Meneses and Thomas Piquet and Jason Noble and Marcelo Wanderley},
  title = {The Puara Framework: Hiding complexity and modularity for reproducibility and usability in NIMEs},
  pages = {86--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {11},
  track = {Papers},
  doi = {10.5281/zenodo.11189110},
  url = {http://nime.org/proceedings/2023/nime2023_11.pdf},
  abstract = {This paper presents Puara, a framework created to tackle problems commonly associated with instrument design, immersive environments, and prototyping. We discuss how exploring Digital Musical Instruments (DMIs) in a collaborative environment led to generalizing procedures that constitute a starting point to solve technical challenges when building, maintaining, and performing with instruments. These challenges guided the framework organization and focus on maintainability, integrability, and modularity. Puara was employed in self-contained systems using 3 % hard-to-implement  DMI building blocks (network manager, gestural descriptors, Media Processing Unit) and supporting 3 established DMIs (GuitarAMI, T-Stick, Probatio) and one new instrument (AMIWrist).  We validated Puara with two use cases where parts of the framework were used. Finally, we accessed the influence of frameworks when exploring predefined NIMEs without concern about the inner workings, or shifting composition paradigms between event-based and gesture-based approaches.},
  numpages = {8}
}

@inproceedings{nime2023_12,
  author = {Joaquín R. Díaz Durán and Laia Turmo Vidal and Ana Tajadura-Jiménez},
  title = {Joakinator: An Interface for Transforming Body Movement and Perception through Machine Learning and Sonification of Muscle-Tone and Force.},
  pages = {94--97},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {12},
  track = {Demos},
  doi = {10.5281/zenodo.11189114},
  url = {http://nime.org/proceedings/2023/nime2023_12.pdf},
  abstract = {Joakinator is a wearable interactive interface that allows users to activate different media materials, such as sound, music, and video, through body gestures. The device, designed in the context of music and performing arts, integrates surface electromyogram, force sensors, and machine learning algorithms with tailored-made software for sonifying muscle-tone and force. This allows the body to reflect expressively the content of the media and the architecture of the device. Recently, we have started to investigate the potential of Joakinator to alter body perception in the context of Joakinator, a European Research Council Project focused on the transformations of body perception through the use of interactive sound/haptics technology. At NIME-2023, we will showcase Joakinator and invite visitors to experience the device firsthand. Visitors will have the opportunity to try on the device, observe others using it, and reflect on its capabilities to transform body movement and perception through the sonification of muscle-tone and force. Overall, Joakinator is a technology that pushes the boundaries of body-computer interaction and opens new possibilities for human-computer interaction and expression.},
  numpages = {4}
}

@inproceedings{nime2023_13,
  author = {Brady Boettcher and Eduardo A. L. Meneses and Christian Frisson and Marcelo Wanderley and Joseph Malloch},
  title = {Addressing Barriers for Entry and Operation of a Distributed Signal Mapping Framework},
  pages = {98--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {13},
  track = {Papers},
  doi = {10.5281/zenodo.11189118},
  url = {http://nime.org/proceedings/2023/nime2023_13.pdf},
  abstract = {The novelty and usefulness of the distributed signal mapping framework libmapper has been demonstrated in many projects and publications, yet its technical entry and operation requirements are often too high to be feasible as a mapping option for less-technical users. This paper focuses on completing key development tasks to overcome these barriers including improvements to software distribution and mapping session management. The impact of these changes was evaluated by asking several artists to design an interactive audiovisual installation using libmapper. Observations and feedback from the artists throughout their projects let us assess the impact of the developments on the usability of the framework, suggesting key development principles for related tools created in research contexts.},
  numpages = {8}
}

@inproceedings{nime2023_14,
  author = {Raul Masu and Fabio Morreale and Alexander Refsum Jensenius},
  title = {The O in NIME: Reflecting on the Importance of Reusing and Repurposing Old Musical Instruments},
  pages = {106--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {14},
  track = {Papers},
  doi = {10.5281/zenodo.11189120},
  url = {http://nime.org/proceedings/2023/nime2023_14.pdf},
  abstract = {In this paper, we reflect on the focus of “newness” in NIME research and practice and argue that there is a missing O (for “Old”) in framing our academic discourse. A systematic review of the last year’s conference proceedings reveals that most papers do, indeed, present new instruments, interfaces, or pieces of technology. Comparably few papers focus on the prolongation of existing NIMEs. Our meta-analysis identifies four main categories from these papers: (1) reuse, (2) update, (3) complement, and (4) long-term engagement. We discuss how focusing more on these four types of NIME development and engagement can be seen as an approach to increase sustainability.},
  numpages = {10}
}

@inproceedings{nime2023_15,
  author = {Torin Hopkins and Emily Doherty and Netta Ofer and Suibi Che-Chuan Weng and Peter Gyory and Chad Tobin and Leanne Hirshfield and Ellen Yi-Luen Do},
  title = {Stringesthesia: Dynamically Shifting Musical Agency Between Audience and Performer Based on Trust in an Interactive and Improvised Performance},
  pages = {116--122},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {15},
  track = {Papers},
  doi = {10.5281/zenodo.11189125},
  url = {http://nime.org/proceedings/2023/nime2023_15.pdf},
  abstract = {This paper introduces Stringesthesia, an interactive and improvised performance paradigm. Stringesthesia was designed to explore the connection between performer and audience by using real-time neuroimaging technology that gave the audience direct access to the performer's internal mental state and determined the extent of how the audience could participate with the performer throughout the performance. Functional near-infrared spectroscopy (fNIRS) technology was used to assess metabolic activity in a network of brain areas collectively associated with a metric we call “trust”. The real-time measurement of the performer’s level of trust was visualized behind the performer and used to dynamically restrict or promote audience participation: e.g., as the performer’s trust in the audience grew, more participatory stations for playing drums and selecting the performer’s chords were activated. Throughout the paper we discuss prior work that heavily influenced our design, conceptual and methodological issues with using fNIRS technology, and our system architecture. We then describe an employment of this paradigm with a solo guitar player.},
  numpages = {7}
}

@inproceedings{nime2023_16,
  author = {Ted Moore and Jean Brazeau},
  title = {Serge Modular Archive Instrument (SMAI): Bridging Skeuomorphic & Machine Learning Enabled Interfaces},
  pages = {123--127},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {16},
  track = {Papers},
  doi = {10.5281/zenodo.11189127},
  url = {http://nime.org/proceedings/2023/nime2023_16.pdf},
  abstract = {The Serge Modular Archive Instrument (SMAI) is a sample-based computer emulation of selected patches on the vintage Serge Modular instrument that is housed at (redacted). Hours of recorded audio created by specified parameter combinations have been analyzed using audio descriptors and machine learning algorithms in the FluCoMa toolkit. Sound is controlled via (1) a machine learning dimensionality reduction plot showing all the recorded samples and/or (2) a skeuomorphic graphical user interface of the patches used to record the sounds. Flexible MIDI and OSC control of the software enables custom modulation and performance of this archive from outside the software. Differing from many software synthesis-based emulations, the SMAI aims to capture and archive the idiosyncrasies of vintage hardware as digital audio samples; compare and contrast skeuomorphic and machine learning enabled modes of exploring vintage sounds; and create a flexible instrument for creatively performing this archive.},
  numpages = {5}
}

@inproceedings{nime2023_17,
  author = {Yichen Wang and Mingze Xi and Matt Adcock and Charles Patrick Martin},
  title = {Mobility, Space and Sound Activate Expressive Musical Experience in Augmented Reality},
  pages = {128--133},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {17},
  track = {Papers},
  doi = {10.5281/zenodo.11189129},
  url = {http://nime.org/proceedings/2023/nime2023_17.pdf},
  abstract = {We present a study of a freehand musical system to investigate musicians' experiences related to performance in augmented reality (AR). Head-mounted mixed reality computers present opportunities for natural gestural control in three dimensions, particularly when using hand-tracking in a creative interface. Existing musical interfaces with head-mounted displays use dedicated input devices that are not designed specifically for musical gestures and may not support appropriate interactions. We are yet to see widespread adoption of head-mounted AR musical instruments. We conducted an empirical study to evaluate musicians' (N=20) experience of performing with a freehand musical interface. The results suggest that the design of freehand musical interaction in the AR space is highly learnable and explorable, and that such systems can leverage unique aspects of mobility, space and sound to deliver an engaging and playful musical experience. The mobile musical experience with a spatial interface design allowed performers to be more bodily engaged and facilitated gestural exploration for musical creativity. This work contributes to a more developed understanding of potentials and challenges in AR-based interface design for musical creativity.},
  numpages = {6}
}

@inproceedings{nime2023_18,
  author = {Anna-Kaisa Kaila and Petra Jääskeläinen and Andre Holzapfel},
  title = {Ethically Aligned Stakeholder Elicitation (EASE): Case Study in Music-AI},
  pages = {134--141},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {18},
  track = {Papers},
  doi = {10.5281/zenodo.11189131},
  url = {http://nime.org/proceedings/2023/nime2023_18.pdf},
  abstract = {Engineering communities that feed the current proliferation of artificial intelligence (AI) have historically been slow to recognise the spectrum of societal impacts of their work. Frequent controversies around AI applications in creative domains demonstrate insufficient consideration of ethical predicaments, but the abstract principles of current AI and data ethics documents provide little practical guidance.
Pragmatic methods are urgently needed to support developers in ethical reflection of their work on creative-AI tools. 

In the wider context of value sensitive, people-oriented design, we present an analytical method that implements an ethically informed and power-sensitive stakeholder identification and mapping: Ethically Aligned Stakeholder Elicitation (EASE). As a case study, we test our method in workshops with six research groups that develop AI in musical contexts. Our results demonstrate that EASE supports
critical self-reflection of the research and outreach practices among developers, discloses power relations and value tensions in the development processes, and foregrounds opportunities for stakeholder engagement. This can guide developers and the wider NIME community towards ethically aligned research and development of creative-AI.},
  numpages = {8}
}

@inproceedings{nime2023_19,
  author = {Juan Ignacio Mendoza Garay},
  title = {The Rearranger Ball: Delayed Gestural Control of Musical Sound using Online Unsupervised Temporal Segmentation},
  pages = {142--146},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {19},
  track = {Papers},
  doi = {10.5281/zenodo.11189133},
  url = {http://nime.org/proceedings/2023/nime2023_19.pdf},
  abstract = {The state-of-the-art recognition of continuous gestures for control of musical sound by means of machine learning has two notable constraints. The first is that the system needs to be trained with individual example gestures, the starting and ending points of which need to be well defined. The second constraint is time required for the system to recognise that a gesture has occurred, which may prevent the quick action that musical performance typically requires. This article describes how a method for unsupervised segmentation of gestures, may be used for delayed gestural control of a musical system. The system allows a user to perform without explicitly indicating the starting and ending of gestures in order to train the machine learning algorithm. To demonstrate the feasibility of the system, an apparatus for control of musical sound was devised incorporating the time required by the process into the interaction paradigm. The unsupervised automatic segmentation method and the concept of delayed control are further proposed to be exploited in the design and implementation of systems that facilitate seamless human-machine musical interaction without the need for quick response time, for example when using broad motion of the human body.},
  numpages = {5}
}

@inproceedings{nime2023_20,
  author = {Francesco Dal Rì and Francesca Zanghellini and Raul Masu},
  title = {Sharing the Same Sound: Reflecting on Interactions between a Live Coder and a Violinist},
  pages = {147--154},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {20},
  track = {Papers},
  doi = {10.5281/zenodo.11189135},
  url = {http://nime.org/proceedings/2023/nime2023_20.pdf},
  abstract = {This paper introduces a performative ecosystem created with the aim of promoting a joint expression between a live coder and an instrumentalist. The live coding environment is based on TidalCycles, controlling a sample machine implemented in SuperCollider. The instrumentalist can record short samples of his/her playing in different buffers, which the live coder can then process. The ecosystem was intensively used by the first and the second author of this paper (respectively live coder and violinist) to develop a performance. At the end of this paper, we provide a number of reflections on the entanglement of the different roles and agencies that emerged during the rehearsals.},
  numpages = {8}
}

@inproceedings{nime2023_21,
  author = {Paul Buser and Kasey LV Pocius and Linnea Kirby and Marcelo Wanderley},
  title = {Towards the T-Tree 2.0: Lessons Learned From Performance With a Novel DMI and Instrument Hub},
  pages = {155--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {21},
  track = {Papers},
  doi = {10.5281/zenodo.11189139},
  url = {http://nime.org/proceedings/2023/nime2023_21.pdf},
  abstract = {In this paper, the authors describe working with and on the T-Tree, a device that integrates multiple instances of a gestural controller known as the T-Stick. The T-Tree is used in two public performance contexts; the results of those performances are summarized, potential improvements to the design of the hardware and software are introduced, and issues are identified. Improvements in the T-Tree from the first version are also discussed. Finally, the authors present future design improvements for the T-Tree 2.0.},
  numpages = {5}
}

@inproceedings{nime2023_22,
  author = {Teresa Pelinski and Rodrigo Diaz and Adan L. Benito Temprano and Andrew McPherson},
  title = {Pipeline for recording datasets and running neural networks on the Bela embedded hardware platform},
  pages = {160--166},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {22},
  track = {Papers},
  doi = {10.5281/zenodo.11189141},
  url = {http://nime.org/proceedings/2023/nime2023_22.pdf},
  abstract = {Deploying deep learning models on embedded devices is an arduous task: oftentimes, there exist no platform-specific instructions, and compilation times can be considerably large due to the limited computational resources available on-device. Moreover, many music-making applications demand real-time inference. Embedded hardware platforms for audio, such as Bela, offer an entry point for beginners into physical audio computing; however, the need for cross-compilation environments and low-level software development tools for deploying embedded deep learning models imposes high entry barriers on non-expert users.

We present a pipeline for deploying neural networks in the Bela embedded hardware platform. In our pipeline, we include a tool to record a multichannel dataset of sensor signals. Additionally, we provide a dockerised cross-compilation environment for faster compilation. With this pipeline, we aim to provide a template for programmers and makers to prototype and experiment with neural networks for real-time embedded musical applications.},
  numpages = {7}
}

@inproceedings{nime2023_23,
  author = {Juan M Ramos and Pablo Riera and Esteban Calcagno},
  title = {An embedded wavetable synthesizer for the electronic bandoneon with parameter mappings based on acoustical measurements},
  pages = {167--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {23},
  track = {Papers},
  doi = {10.5281/zenodo.11189144},
  url = {http://nime.org/proceedings/2023/nime2023_23.pdf},
  abstract = {The bandoneon is a free-reed instrument of great cultural value that is currently struggling to ensure its conservation as heritage, mainly due to its complex constitution, the lack of sufficient manufacturers to satisfy the demand, and the high sales prices that this entails. Our research group has been working on the task of revitalizing the instrument from a modern perspective, carrying out musical and scientific research for the creation of an accessible electronic bandoneon. As the next step in this endeavor, we present a method for synthesizing the bandoneon sound using multiple wavetable interpolation, and parameter mappings based on acoustic measurements. We discuss a method for capturing and selecting the wavetables, the implementation on an embedded platform (Bela Mini), and the trade-offs between realistic sound and computational efficiency. The synthesizer runs in real-time and has a polyphony of approximately 12 voices, allowing for an autonomously sounding electronic instrument.},
  numpages = {7}
}

@inproceedings{nime2023_24,
  author = {Claudio Panariello and Chiara Percivati},
  title = {“WYPYM”: A Study for Feedback-Augmented Bass Clarinet},
  pages = {174--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {24},
  track = {Papers},
  doi = {10.5281/zenodo.11189146},
  url = {http://nime.org/proceedings/2023/nime2023_24.pdf},
  abstract = {This paper explores the concept of co-creativity in a performance for feedback-augmented bass clarinet. The bass clarinet is augmented using a loudspeaker placed on the bell and a supercardiod microphone placed inside the instrument's body, allowing for the generation of feedback that is subsequently processed by a computational system to create new sound material. This feedback loop creates a symbiotic relationship between the performer and the electronics, resulting in the co-creation of the final piece, with the performer and the electronics influencing each other. The result is a unique and ever-evolving musical experience that poses interesting challenges to the traditional instrument--electronics and composer--opera relationship. This paper reports on both the hardware and software augmentation of the bass clarinet, and presents "WYPYM - Were you a part of your mother?", a piece written especially for this augmented instrument and its feedback system.},
  numpages = {6}
}

@inproceedings{nime2023_25,
  author = {Jaehoon Choi},
  title = {Brushing Interface - DIY multi-touch interface for expressive gestural performance},
  pages = {180--185},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {25},
  track = {Papers},
  doi = {10.5281/zenodo.11189148},
  url = {http://nime.org/proceedings/2023/nime2023_25.pdf},
  abstract = {This paper presents the design of the Brushing Interface, which aims to transform brushing gestures into a genuine and expressive musical/sonic performance. To achieve this, a hardware system consisting of a grid of 216 self-made force sensitive resistor(FSR) sensors and 8 piezo microphones was implemented, which enables high-fidelity gesture tracking and sound production closely tied with brushing gestures. The hardware system, including the sensor itself, was made in a DIY approach, which provides an economical and high-quality design strategy for implementing a multi-touch interface. Moreover, it is combined with a unique gesture mapping strategy that integrates multi-dimensional parameter mapping and continuous gesture tracking, enabling an expressive performance that is highly flexible to configure in various settings.},
  numpages = {6}
}

@inproceedings{nime2023_26,
  author = {Suso Romaris},
  title = {DIGITL A Reduction of Guitar},
  pages = {186--190},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {26},
  track = {Papers},
  doi = {10.5281/zenodo.11189150},
  url = {http://nime.org/proceedings/2023/nime2023_26.pdf},
  abstract = {This paper presents the Digitl, a digital processing system using a reduced electric guitar as control input. The physical design is based on three fundamental elements: String, Body and Electromagnetic Pickup. The main characteristic of the instrument lies is the linear matrix x-y configuration of the strings and frets. The purpose of the instrument is the application of individual signal processing at each X-Y position.
It is described the technical aspects of the Digitl, including the design of the matrix configuration and the digital signal processing algorithms. Specifically, a set of Max/MSP patches that routes the signals from the strings to the processing engine. 
The experimental results confirm the importance of the design and configuration of musical instruments in the context of expressive performance.},
  numpages = {5}
}

@inproceedings{nime2023_27,
  author = {Nicolo Merendino and Giacomo Lepri and Antonio Rodà and Raul Masu},
  title = {Redesigning the Chowndolo: a Reflection-on-action Analysis to Identify Sustainable Strategies for NIMEs Design},
  pages = {191--199},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {27},
  track = {Papers},
  doi = {10.5281/zenodo.11189153},
  url = {http://nime.org/proceedings/2023/nime2023_27.pdf},
  abstract = {The sustainability of Digital Musical Instruments (DMIs) is a crucial concern within the NIME community, not only in the design of the instruments but also in terms of sustaining the instrument over a prolonged period, promoting longevity, and minimizing obsolescence. The risk of designing advanced instruments becoming debris quickly is real if longevity is not actively considered. In this paper, we present the process of redesigning a crafted DMI to fit a small-scale production process while considering strategies that render the final design more sustainable and maximize the object's lifespan. We present the results of a critical analysis of this process through a sustainability lens. From this analysis, we distilled a number of reflections that could help similar design processes or NIME crafting activities. The most innovative reflections are related to inscribing sustainability into the practice of using the instruments. From this perspective, we suggest considering the future user as a designer capable of fixing, adjusting, redesigning, or hacking the DMI and actively provide possible solutions that can significantly extend the lifespan of a DMI and, consequently, its sustainability.},
  numpages = {9}
}

@inproceedings{nime2023_28,
  author = {Michael Mulshine and Ge Wang and Chris Chafe and Jack Atherton and Terry Feng and Celeste Betancur},
  title = {WebChucK: Computer Music Programming on the Web},
  pages = {200--205},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {28},
  track = {Papers},
  doi = {10.5281/zenodo.11189155},
  url = {http://nime.org/proceedings/2023/nime2023_28.pdf},
  abstract = {WebChucK is ChucK—a strongly-timed computer music programming language—running on the web. Recent advancements in browser technology (including WebAssembly and the Web Audio API’s AudioWorklet interface) have enabled languages written in C/C++ (like ChucK) to run in web browsers with nearly native-code performance. Early adopters have explored the many practical and creative possibilities that WebChucK enables, ranging from a WebChucK integrated development environment to interactive browser-based audiovisual experiences. WebChucK has also been adopted as the programming platform in an introductory computer music course at Stanford University. Importantly, by running in any browser, WebChucK broadens and simplifies access to computer music programming, opening the door for new users and creative workflows. In this paper, we discuss WebChucK and its applications to date, explain how the tool was designed and implemented, and evaluate the unique affordances of combining computer music programming with a web development workflow.},
  numpages = {6}
}

@inproceedings{nime2023_29,
  author = {Fernando Lopez-Lezcano and Michael Mulshine},
  title = {Affordable Speaker Arrays for Education and Artists},
  pages = {206--211},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {29},
  track = {Papers},
  doi = {10.5281/zenodo.11189157},
  url = {http://nime.org/proceedings/2023/nime2023_29.pdf},
  abstract = {We present an affordable, lightweight, and highly portable multichannel audio solution for surround sound applications and installations. The system was developed for the “Sound in Space” course, taught by on of the authors at CCRMA in the winter quarter of 2021, when education was fully remote. Students in the course were able to listen to and create surround sound compositions from their homes or dorm rooms. Beyond the course, artists have demonstrated the versatility and creative affordances of this cheap, lightweight, and highly portable setup in sound installations and other custom speaker arrays. Such an affordable and versatile system has the potential to provide more students and artists access to spatialized sound production and multichannel audio in their work, enabling deeper technical education and creative applications ranging from Ambisonics to sound installations. Importantly, the transportability and ease of assembling this system enables multichannel audio work to be developed outside of the physical confines of academic institutions, including in spaces like apartments, garages, the outdoors, and more. This paper steps through the process of creating such a system, detailing the challenges faced and reflecting on the affordances in educational and creative usage.},
  numpages = {6}
}

@inproceedings{nime2023_30,
  author = {Albert-Ngabo Niyonsenga and Marcelo Wanderley},
  title = {Tools and Techniques for the Maintenance and Support of Digital Musical Instruments},
  pages = {212--218},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {30},
  track = {Papers},
  doi = {10.5281/zenodo.11189159},
  url = {http://nime.org/proceedings/2023/nime2023_30.pdf},
  abstract = {There are multiple barriers to the long term use of digital musical instruments. Among several issues related to instrument accessibility, many DMIs remain as prototypes in research labs never becoming a robust and stable instrument. Technical support is an important part of the long term use of a DMI. Though all musical instruments can eventually break, managing how they are going to be fixed and built within a research organisation can help with the continued usage of the instrument. We apply reliability analysis techniques to estimate the reliability, availability and maintainability characteristics of the T-Stick. Using these characteristics we estimate the amount of spare parts needed to maintain a 99% availability target for the T-Stick. This analysis provides insights on expected maintenance time, costs, and personnel needed when supporting and building DMIs.},
  numpages = {7}
}

@inproceedings{nime2023_31,
  author = {Suraj Jaiswal and Vipul Arora},
  title = {HarMIDI: Sensor System To Read MIDI from Indian Harmoniums},
  pages = {219--223},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {31},
  track = {Papers},
  doi = {10.5281/zenodo.11189162},
  url = {http://nime.org/proceedings/2023/nime2023_31.pdf},
  abstract = {While digital music technologies are rapidly growing, music communities using traditional acoustic instruments are sometimes unable to take full advantage of all of the digital processing techniques available to electronic musicians. One way to include them in the latest developments is to develop interfaces connecting non-electronic instruments to the digital world. This paper presents HarMIDI, a sensor system to convert keystrokes on an Indian harmonium to MIDI. The paper presents the sensor assembly, calibration methods, and the algorithm to output MIDI. The calibration methods calibrate the notes and temporally synchronize the MIDI stream with audio. The system has been evaluated for time synchronization of onsets and offsets. The sensor setup is affordable, portable and can be used with any existing harmonium.},
  numpages = {5}
}

@inproceedings{nime2023_32,
  author = {Victor Shepardson and Thor Magnusson},
  title = {The Living Looper: Rethinking the Musical Loop as a Machine Action-Perception Loop},
  pages = {224--231},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {32},
  track = {Papers},
  doi = {10.5281/zenodo.11189164},
  url = {http://nime.org/proceedings/2023/nime2023_32.pdf},
  abstract = {We describe the Living Looper, a real-time software system for prediction and continuation of audio signals in the format of a looping pedal. Each of several channels is activated by a footswitch and repeats or continues incoming audio using neural synthesis. The live looping pedal format is familiar to electric guitarists and electronic musicians, which helps the instrument to serve as a boundary object for musicians and technologists of different backgrounds. Each Living Loop channel learns in the context of what the other channels are doing, including those which are momentarily controlled by human players. This leads to shifting networks of agency and control between players and Living Loops. In this paper we present the ongoing design of the Living Looper as well as preliminary encounters with musicians in a workshop and concert setting.},
  numpages = {8}
}

@inproceedings{nime2023_33,
  author = {Hugo Scurto and Ludmila Postel},
  title = {Soundwalking Deep Latent Spaces},
  pages = {232--235},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {33},
  track = {Papers},
  doi = {10.5281/zenodo.11189166},
  url = {http://nime.org/proceedings/2023/nime2023_33.pdf},
  abstract = {This paper relates an early art-research collaboration between two practitioners in machine learning and virtual worlds toward new embodied musical experiences of Artificial Intelligence (AI). Instead of a digital music instrument or a music-generating agent, we propose to craft a soundwalk experience where a human person moves through a three-dimensional virtual world to explore a latent sound space generated by deep learning. We report on the diffractive prototyping and iterative crafting of three such soundwalks through/out deep latent spaces, using nn~ and New Atlantis as computational platforms for AI audio processing and virtual world experimentation. We share critical perspectives emerging from our latent soundwalking practice, with the hope that they contribute to ongoing community-wide reflections toward new AI for musical expression.},
  numpages = {4}
}

@inproceedings{nime2023_34,
  author = {Behzad Haki and Teresa Pelinski and Marina Nieto Giménez and Sergi Jordà},
  title = {Completing Audio Drum Loops with Symbolic Drum Suggestions},
  pages = {236--243},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {34},
  track = {Papers},
  doi = {10.5281/zenodo.11189168},
  url = {http://nime.org/proceedings/2023/nime2023_34.pdf},
  abstract = {Sampled drums can be used as an affordable way of creating human-like drum tracks, or perhaps more interestingly, can be used as a mean of experimentation with rhythm and groove. Similarly, AI-based drum generation tools can focus on creating human-like drum patterns, or alternatively, focus on providing producers/musicians with means of experimentation with rhythm. In this work, we aimed to explore the latter approach. To this end, we present a suite of Transformer-based models aimed at completing audio drum loops with stylistically consistent symbolic drum events. Our proposed models rely on a reduced spectral representation of the drum loop, striking a balance between a raw audio recording and an exact symbolic transcription. Using a number of objective evaluations, we explore the validity of our approach and identify several challenges that need to be further studied in future iterations of this work. Lastly, we provide a real-time VST plugin that allows musicians/producers to utilize the models in real-time production settings.},
  numpages = {8}
}

@inproceedings{nime2023_35,
  author = {Notto J. W. Thelle and Bernt Isak Wærstad},
  title = {Co-Creatives Spaces: The machine as a collaborator},
  pages = {244--250},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {35},
  track = {Papers},
  doi = {10.5281/zenodo.11189170},
  url = {http://nime.org/proceedings/2023/nime2023_35.pdf},
  abstract = {People have always used new technology to experiment with new forms of music creation. However, the latest devel- opments in artificial intelligence (AI) suggest that machines are on the verge of becoming more than mere tools—they can also be co-creators. In this article, we follow four mu- sicians in the project Co-Creative Spaces through a six- month long collaborative process, where they created music through improvising with each other and with computer- based imitations of themselves. These musical agents were trained through machine learning to generate output in the style of the musicians. What happens to musical co-creation when AI is included in the creative cycle? The musicians are from Norway and Kenya—two countries with fundamen- tally different musical traditions. How is the collaboration affected by cultural biases inherent in the technology, and in the musicians themselves?
These questions were examined through focus groups as part of two five-day workshops. An analysis shows how the musicians moved between an understanding of machine as tool and machine as co-creator, and between the idea of music as object and music as process. These different interpretative repertoires were used interchangeably and paint a complex picture of what it is like being in the intersection between different musical and cultural paradigms.},
  numpages = {7}
}

@inproceedings{nime2023_36,
  author = {Erik Stifjell},
  title = {A FLexible musical instrument Augmentation that is Programmable, Integrated in a Box (FLAPIBox)},
  pages = {251--255},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {36},
  track = {Papers},
  doi = {10.5281/zenodo.11189174},
  url = {http://nime.org/proceedings/2023/nime2023_36.pdf},
  abstract = {Most musical instrument augmentations aim to only fit one specific instrument and depend on an external sound system to work as intended. In a more acoustic concert setting this often alienates the electronic sound component. The FLAPIBox is an integrated solution that fits most acoustic instruments and use its own resonance for playing electronic sound in a more organic way—through the instrument itself. Reviewing related works and exploring different hardware and software components, a modular prototype has been built. The results of this preliminary study make the body of planning and building the first integrated breadboard prototype. Because of its flexible design, the FLAPIBox can use several different microphone, and loudspeaker technologies. Using inexpensive components and developing open-source software, the FLAPIBox is both affordable and accessible. The development of the FLAPIBox aim to result in a stable and predictable platform, yet open and versatile enough for further development.},
  numpages = {5}
}

@inproceedings{nime2023_37,
  author = {Alexandros Drymonitis},
  title = {LiveLily: An Expressive Live Sequencing and Live Scoring System Through Live Coding With the Lilypond Language},
  pages = {256--261},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {37},
  track = {Papers},
  doi = {10.5281/zenodo.11189176},
  url = {http://nime.org/proceedings/2023/nime2023_37.pdf},
  abstract = {LiveLily is an open-source system for live sequencing and live scoring through live coding in a subset of the Lilypond language. It is written in openFrameworks and consists of four distinct parts, the text editor, the language parser, the sequencer, and the music score. It supports the MIDI and OSC protocols to communicate the sequencer with other software or hardware, as LiveLily does not produce any sound. It can be combined with audio synthesis software that supports OSC, like Pure Data, SuperCollider, and others, or hardware synthesizers that support MIDI. This way, the users can create their sounds in another, audio-complete framework or device, and use LiveLily to control their music.
LiveLily can also be used as a live scoring system to write music scores for acoustic instruments live. This feature can be combined with its live sequencing capabilities, so acoustic instruments can be combined with live electronics. Both live scoring and live sequencing in LiveLily provide expressiveness to a great extent, as many musical gestures can be included either in the score or the sequencer. Such gestures include dynamics, articulation, and arbitrary text that can be interpreted in any desired way, much like the way Western-music notation scores are written.},
  numpages = {6}
}

@inproceedings{nime2023_38,
  author = {Seyed Mojtaba Karbasi and Alexander Refsum Jensenius and Rolf Inge Godøy and Jim Torresen},
  title = {Exploring Emerging Drumming Patterns in a Chaotic Dynamical System using ZRob},
  pages = {262--267},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {38},
  track = {Papers},
  doi = {10.5281/zenodo.11189178},
  url = {http://nime.org/proceedings/2023/nime2023_38.pdf},
  abstract = {ZRob is a robotic system designed for playing a snare drum. The robot is constructed with a passive flexible spring-based joint inspired by the human hand. This paper describes a study exploring rhythmic patterns by exploiting the chaotic dynamics of two ZRobs. In the experiment, we explored the control configurations of each arm by trying to create unpredictable patterns. Over 200 samples have been recorded and analyzed. We show how the chaotic dynamics of ZRob can be used for creating new drumming patterns.},
  numpages = {6}
}

@inproceedings{nime2023_39,
  author = {Tommy Davis and Kasey LV Pocius and Vincent Cusson and Marcelo Wanderley and Philippe Pasquier},
  title = {eTu{d,b}e: case studies in playing with musical agents},
  pages = {268--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {39},
  track = {Papers},
  doi = {10.5281/zenodo.11189180},
  url = {http://nime.org/proceedings/2023/nime2023_39.pdf},
  abstract = {The eTu{d,b}e framework adapts existing improvising musical agents (MA) for performance with an augmented instrument called the eTube. This instrument has been developed with deliberate musical and technological limitations including a simple two-button controller and restricted pitch capacity. We will present case studies which outline our research-creation framework for mapping the eTube controller, developing corpora for the MAs, and testing interactive and machine listening settings which will also be demonstrated by performance examples. A general summary of the MAs will be followed by specific descriptions of the features we have utilised in our work, and finally a comparison of the MAs based on these features. Few papers discuss the process for learning to work with and adapt existing MAs and we will finish by describing challenges experienced as other users with these technologies.},
  numpages = {9}
}

@inproceedings{nime2023_40,
  author = {Lucie F Jones and Jeffrey  Boyd and Jeremy Brown and Hua Shen},
  title = {A Wearable Technology For Wind Musicians: Does It Matter How You Breathe?},
  pages = {277--287},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {40},
  track = {Papers},
  doi = {10.5281/zenodo.11189184},
  url = {http://nime.org/proceedings/2023/nime2023_40.pdf},
  abstract = {This paper presents an affordable and accessible wearable technology for wind musicians which provides real-time biofeedback on their breathing. We developed the abdominal thoracic expansion measurement prototype wearable technology (ATEM-P), to measure a wind musician’s breathing-induced expansion and contraction while they are playing.
Our first study validates the ATEM-P with the gold standard of medical grade respiratory exertion measurement devices, the respiratory plethysmography inductance system (RIP). The results show that the ATEM-P has a strong correlation to the RIP system.
Our second study provides quantitative and qualitative data about the correlation between a musician’s breathing technique and the quality of their performance. We expected the results to show a correlation between the ATEM-P peak amplitudes and the quality of performance, i.e. better breathing-induced expansion leads to better quality of performance, however this was not the case. The results did show that there is a correlation between a musician’s quality of performance and breath period.
Results from the studies show that the ATEM-P has potential as an affordable and accessible wearable technology for wind musicians: a performance enhancement tool and an educational tool.},
  numpages = {11}
}

@inproceedings{nime2023_41,
  author = {Carla Tapparo and Brooke Chalmers and Victor Zappi},
  title = {Leveraging Android Phones to Democratize Low-level Audio Programming},
  pages = {288--294},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {41},
  track = {Papers},
  doi = {10.5281/zenodo.11189186},
  url = {http://nime.org/proceedings/2023/nime2023_41.pdf},
  abstract = {In this work we introduce LDSP, a novel technology capable of turning any Android phone into a high-performance embedded platform for digital musical instrument (DMI) design. Embedded platforms are powerful technologies that changed the way we design and even think of DMIs. Their widespread adoption has popularized low-level audio programming, enabling engineers and artists alike to create highly responsive, self-contained digital musical instruments that have direct access to hardware resources. However, if we shift our focus away from the wealthy countries of the `Global North', embedded platforms become a commodity that only a few can afford. DMI researchers, artists and students from Latin America have discussed at great lengths the effects that the lack of access to these otherwise common resources have on their practices. And while some solutions have been proposed, a large gap can still be perceived. By means of appropriating possibly the most widespread and accessible technology in the world (Android) and turn it into an embedded platform, LDSP creates an effective opportunity to close this gap. Throughout the paper, we provide technical details of the full LDSP environment, along with insights on the surprising performances of the first DMIs that have been designed with it.},
  numpages = {7}
}

@inproceedings{nime2023_42,
  author = {Matthew Goodheart},
  title = {Reembodied Sound and Transducer-actuated Instruments in Refraction Interlude},
  pages = {295--300},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {42},
  track = {Papers},
  doi = {10.5281/zenodo.11189190},
  url = {http://nime.org/proceedings/2023/nime2023_42.pdf},
  abstract = {“Reembodied sound” refers to the electroacoustic practice of projecting sound into resonating objects, thereby turning these objects into a kind of speaker. The practice, which typically uses an audio transducer attached to the surface of the object being resonated, lies in a middle-ground between loudspeaker-based music and augmented/actuated instruments, allowing practitioners to draw upon and fuse multiple paradigms of new and emerging technologies. This article examines Refraction Interlude, an interactive environment for solo performer and transducer-actuated metal percussion instruments. Building on a decade of reembodied sound research, the work combines augmented and actuated instruments, physical modeling, pre-recorded performer input, interactivity, and sound spatialization in a manner that facilitates adaptability to performer creativity and to the acoustic properties of the actuated instruments. The computational processes were minimized, designed to forefront the interaction and integration between these multiple domains.},
  numpages = {6}
}

@inproceedings{nime2023_43,
  author = {Iran Sanadzadeh and Chloë Sobek},
  title = {A sustained relationship with large instruments - a case against the convenient interface},
  pages = {301--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {43},
  track = {Papers},
  doi = {10.5281/zenodo.11189192},
  url = {http://nime.org/proceedings/2023/nime2023_43.pdf},
  abstract = {In recent decades, with the innovation in sensor technology, the trend towards smaller digital controllers for instruments has expanded. New generations of performance styles are growing that rely on compact instruments that can travel easily and are thus versatile. This article cites two interactive performance practices to illustrate how larger instruments change the nature of interaction and sonic outcomes of performance. Pressure-sensitive Floors, a wooden set of platforms for performing electronic music, are compared with a practice on the Renaissance violone with electronics. Large instruments offer unique additions to performance and music making that are not accessible in small instruments. They have their own specific affordances and limitations that affect the musical decisions of the performer and therefore contribute unique ways of conceptualising performance. The instruments in this paper have been chosen as the authors have a 'sustained relationship’ with them and these practices merely act as examples of the embodied knowledge gained through staying committed to a particular large instrument. We demonstrate how with such a practice, the performance is recentered around human presence. This offers a deeper communication between performer and audience. It creates new avenues for the performance of contemporary music where the entire body is engaged in movement and sounding. We argue that overlooking large instruments in favour of their smaller counterparts would result in the loss of a unique aesthetic as well as conceptual and performance approaches.},
  numpages = {6}
}

@inproceedings{nime2023_44,
  author = {Patty  J Preece and Melania Jack and Giacomo Lepri},
  title = {Oscillations: Composing a Performance Ecosystem through a Sonic Cyberfeminist Lens },
  pages = {307--313},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {44},
  track = {Papers},
  doi = {10.5281/zenodo.11189194},
  url = {http://nime.org/proceedings/2023/nime2023_44.pdf},
  abstract = {The audiovisual installation Oscillations, turns irons and ironing boards into electronic instruments, in an attempt to deconstruct stereotypical ideas of gender and its assigned roles. The project aims to investigate the relationships we have with domestic objects, and ponder their structures and significance through the design and performance of an interactive ecosystem. The project uses a sonic cyberfeminisms lens to critically explore aesthetic and relational hierarchies at the intersection of sound, gender and technology. Three irons and ironing boards have been hacked and retrofitted with embedded electronic instruments that together create a complex feedback network. While the audience is invited to physically interact with the irons instruments and manipulate samples, the sonic state of the installation also changes based on the audio information detected in the environment.},
  numpages = {7}
}

@inproceedings{nime2023_45,
  author = {Anne K Hege and Curtis Ullerich},
  title = {Principles of Instrument and System Design for LaptOperas},
  pages = {314--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {45},
  track = {Papers},
  doi = {10.5281/zenodo.11189196},
  url = {http://nime.org/proceedings/2023/nime2023_45.pdf},
  abstract = {In this article, we explore practical and artistic considerations of instrument design and the creation of an instrument ensemble control system for The Furies: A LaptOpera, an opera for laptop orchestra and live vocalists based on the Greek tragedy Electra.  We outline the artistic principles that guided the creation of the rope instrument and, specifically, our use of instrument design to forge direct and visceral connections between the music, the narrative, and the relationship between characters. This discussion is followed by an overview of the practical considerations that inspired the creation of an instrument ensemble control system for the opera and the principles that guided this system's design. Through a detailed description of the development of the rope instrument, the growth of this instrument through the course of the opera, and the design of the instrument ensemble control system, this paper offers tools and reflections on the potential of instrument design to invigorate an embodied connection to opera and useful design strategies to support rehearsal and performance of evening-length multimedia works.},
  numpages = {5}
}

@inproceedings{nime2023_46,
  author = {Théo Jourdan and Baptiste Caramiaux},
  title = {Machine Learning for Musical Expression: A Systematic Literature Review},
  pages = {319--331},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {46},
  track = {Papers},
  doi = {10.5281/zenodo.11189198},
  url = {http://nime.org/proceedings/2023/nime2023_46.pdf},
  abstract = {For several decades NIME community has always been appropriating machine learning (ML) to apply for various tasks such as gesture-sound mapping or sound synthesis for digital musical instruments. Recently, the use of ML methods seems to have increased and the objectives have diversified. Despite its increasing use, few contributions have studied what constitutes the culture of learning technologies for this specific practice. This paper presents an analysis of 69 contributions selected from a systematic review of the NIME conference over the last 10 years. This paper aims at analysing the practices involving ML in terms of the techniques and the task used and the ways to interact this technology. It thus contributes to a deeper understanding of the specific goals and motivation in using ML for musical expression. This study allows us to propose new perspectives in the practice of these techniques.},
  numpages = {13}
}

@inproceedings{nime2023_47,
  author = {Théo Jourdan and Baptiste Caramiaux},
  title = {Culture and Politics of Machine Learning in NIME: A Preliminary Qualitative Inquiry},
  pages = {332--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {47},
  track = {Papers},
  doi = {10.5281/zenodo.11189202},
  url = {http://nime.org/proceedings/2023/nime2023_47.pdf},
  abstract = {For several years, the various practices around ML techniques have been increasingly present and diversified. However, the literature associated with these techniques rarely reveals the cultural and political sides of these practices. In order to explore how practitioners in the NIME community engage with ML techniques, we conducted interviews with seven researchers in the NIME community and analysed them through a thematic analysis. Firstly, we propose findings at the level of the individual, resisting technological determinism and redefining sense making in interactive ML. Secondly, we propose findings at the level of the community, revealing mitigated adoption with respect to ML. This paper aims to provide the community with some reflections on the use of ML in order to initiate a discussion about cultural, political and ethical issues surrounding these techniques as their use grows within the community.},
  numpages = {7}
}

@inproceedings{nime2023_48,
  author = {Jack Armitage and Thor Magnusson and Andrew McPherson},
  title = {A Scale-Based Ontology of Musical Instrument Design},
  pages = {339--349},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {48},
  track = {Papers},
  doi = {10.5281/zenodo.11189204},
  url = {http://nime.org/proceedings/2023/nime2023_48.pdf},
  abstract = {Subtlety and detail are fundamental to what makes musical instruments special, and worth dedicating a life's practice to, for designer, maker, player and listener alike. However, research into digital musical instrument (DMI) design tools and processes have so far mainly focused on high-level conceptual concerns and low-level technical abstractions, leaving subtlety and detail underexplored and undervalued.  These nuances, and the processes they result from, cannot be fully articulated in words alone, yet they largely define an instrument's quality, and it is therefore important to understand how they come to be. We introduce a scale-based ontology that divides design details into three levels - macro, meso and micro - and we present a literature review of DMI design from the perspective of this ontology. Finally we extrapolate the ontology to consider its utility in broader contexts, and consider future directions.},
  numpages = {11}
}

@inproceedings{nime2023_49,
  author = {Eevee Zayas-Garin and Charlotte Nordmoen and Andrew McPherson},
  title = {Transmitting Digital Lutherie Knowledge: The Rashomon Effect for DMI Designers},
  pages = {350--357},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {49},
  track = {Papers},
  doi = {10.5281/zenodo.11189206},
  url = {http://nime.org/proceedings/2023/nime2023_49.pdf},
  abstract = {As the field around computer-mediated musical interaction drives attention to its sociotechnical, political and epistemological exigencies, it becomes important to be guided by disability studies, and for researchers and designers of accessible digital musical instruments (ADMIs) to foreground the lived experience of disabled musicians. This resonates with the movement to promote disability justice in HCI. In this paper, we introduce a case study of the design of a string-less guitar, which was developed in collaboration with a guitarist who lost his ability to play due to impairment. We present this work as an exploration of the Rashomon effect, a term that refers to the phenomenon of multiple witnesses describing the same event from their own perspective. We argue that the Rashomon effect is a useful way to explore how digital musical instrument (DMI) designers respond to NIME's interdisciplinarity, and to reflect on how we produce and transmit knowledge within our field.},
  numpages = {8}
}

@inproceedings{nime2023_50,
  author = {Olivia B Smith and Matthew Rodger and Maarten van Walstijn and Miguel Ortiz},
  title = {Sound guiding action: the effect of timbre on learning a new percussive DMI for beginner musicians},
  pages = {358--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {50},
  track = {Papers},
  doi = {10.5281/zenodo.11189208},
  url = {http://nime.org/proceedings/2023/nime2023_50.pdf},
  abstract = {Learning to play a digital musical instrument (DMI) may be affected by the acoustic behaviour of that instrument, in addition to its physical characteristics and form. However, how the timbral properties of an instrument affect learning has received little systematic empirical research. In an exploratory study, we assessed whether timbral feedback from a physical model based percussive DMI influences beginner players’ performance in a musical learning task. We contrasted the timbral richness of a metallic plate physical model with an amplitude modulated pink-noise signal that was comparable in response to input controls but with relatively reduced timbral features. Two groups of participants practiced three sets of simple beats using their respective version of the instrument (physical model or pink noise), over the course of an hour. Their performance was recorded throughout and assessed in the form of rhythmic timing accuracy. Results showed that participants’ performance in both sound groups significantly improved throughout the task. Timing accuracy was significantly better in the physical model group for one out of three sets of beats. We argue that the timbral feedback of a musical instrument may influence beginner’s playing experience, encouraging further research into how this could benefit DMI design.},
  numpages = {6}
}

@inproceedings{nime2023_51,
  author = {Halldor Ulfarsson},
  title = {Ergodynamics of String Feedback},
  pages = {364--370},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {51},
  track = {Papers},
  doi = {10.5281/zenodo.11189210},
  url = {http://nime.org/proceedings/2023/nime2023_51.pdf},
  abstract = {This paper describes the latest iteration of signal path routing and mixing control for the halldorophone, an experimental electro-acoustic string instrument intended for music making with string feedback and describes the design thinking behind the work which is informed by long term contact with dedicated users. Specifically, here we discuss the intended “feel” or ergodynamic design of how the affordances of the instrument are presented and the delicate task of reducing cognitive load for early use while not limiting options for expert users.},
  numpages = {7}
}

@inproceedings{nime2023_52,
  author = {Derek Holzer and Luka Aron and Andre Holzapfel},
  title = {Laser Phase Synthesis},
  pages = {371--378},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {52},
  track = {Papers},
  doi = {10.5281/zenodo.11189212},
  url = {http://nime.org/proceedings/2023/nime2023_52.pdf},
  abstract = {This paper presents a new interface – Laser Phase Synthesis — designed for audiovisual performance expression. The instrument is informed by the historical Audio/Video/Laser system developed by Lowell Cross and Carson Jeffries for use by David Tudor and Experiments in Arts and Technology (E.A.T.) at the 1970 Japan World Exposition in Osaka, Japan. The current work employs digital audio synthesis, modern laser display technology, and close collaboration be- tween sound and image composition to illustrate the har- monic progression of a musical work. The authors present a micro-history of audiovisual laser displays, a brief introduction to the process of drawing visual figures with sound, a description of the Pure Data software and laser display hardware systems used for the Laser Phase Synthesis instrument, and a discussion of how this instrument shaped the composition process of one audiovisual performance of electroacoustic music. The paper concludes with speculations on how the system can be further developed with other kinds of live performers, specifically vocalists.},
  numpages = {8}
}

@inproceedings{nime2023_53,
  author = {Kobi Hartley and Steve Hodges and Joe Finney},
  title = {Jacdac-for-Max: Plug-and-Play Physical Prototyping of Musical Interfaces},
  pages = {379--386},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {53},
  track = {Papers},
  doi = {10.5281/zenodo.11189216},
  url = {http://nime.org/proceedings/2023/nime2023_53.pdf},
  abstract = {This article presents Jacdac-for-Max: a cross-platform, open-source set of node.js scripts and custom Cycling ’74 Max objects which enable the use of Jacdac, an open, modular plug-and-play hardware prototyping platform, with Max visual programming language frequently used for audio-visual applications. We discuss the design and implementation of Jacdac-for-Max, and explore a number of example applications. Through this we show how Jacdac-for-Max can be used to rapidly prototype digital musical interfaces based on a range of input devices. Additionally, we discuss these qualities within the context of established principles for designing musical hardware, and the emerging concepts of long-tail hardware and frugal innovation. We believe that through Jacdac-for-Max, Jacdac provides a compelling approach to prototyping musical interfaces while supporting the evolution beyond a prototype with more robust and scalable solutions.},
  numpages = {8}
}

@inproceedings{nime2023_54,
  author = {Nicola Privato and Thor Magnusson and Einar Torfi Einarsson},
  title = {Magnetic Interactions as a Somatosensory Interface},
  pages = {387--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {54},
  track = {Papers},
  doi = {10.5281/zenodo.11189218},
  url = {http://nime.org/proceedings/2023/nime2023_54.pdf},
  abstract = {Thales is a composed instrument consisting of two hand-held magnetic controllers whose interactions with each other and with other magnets produce the somatosensory manifestation of a tangible interface that the musician generates and shapes in the act of performing. In this paper we provide a background for the development of Thales by describing the application of permanent magnets in HCI and musical interfaces. We also introduce the instrument’s sound generation based on a neural synthesis model and contextualise the system in relation with the concept of magnetic scores. We report on our preliminary user study and discuss the somatosensory response that characterise Thales, observing the interaction between the opposing magnetic field of the controllers as a tangible magnetic interface. Finally, we investigate its nature from the perspective of performative posthumanist ontologies.},
  numpages = {7}
}

@inproceedings{nime2023_55,
  author = {Diemo Schwarz},
  title = {Touch Interaction for Corpus-based Audio–Visual Synthesis},
  pages = {394--401},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {55},
  track = {Papers},
  doi = {10.5281/zenodo.11189220},
  url = {http://nime.org/proceedings/2023/nime2023_55.pdf},
  abstract = {Audio–visual corpus-based synthesis extends the principle of concatenative sound synthesis to the visual domain, where, in addition to the sound corpus (i.e. a collection of segments of recorded sound with a perceptual description of their sound character), the artist uses a corpus of still images with visual perceptual description (colour, texture, brightness, entropy), in order to create an audio–visual musical performance by navigating in real-time through these descriptor spaces, i.e. through the collection of sound grains in a space of perceptual audio descriptors, and at the same time through the visual descriptor space, i.e. selecting images from the visual corpus for rendering, and thus navigate in parallel through both corpora interactively with gestural control via movement sensors.
The artistic–scientific question that is explored here is how to control at the same time the navigation through the audio and the image descriptor spaces with gesture sensors, in other words, how to link the gesture sensing to both the image descriptors and the sound descriptors in order to create a symbiotic multi-modal embodied audio–visual experience.},
  numpages = {8}
}

@inproceedings{nime2023_56,
  author = {Tom Mudd and Akira Brown},
  title = {Musical pathways through the no-input mixer},
  pages = {402--408},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {56},
  track = {Papers},
  doi = {10.5281/zenodo.11189224},
  url = {http://nime.org/proceedings/2023/nime2023_56.pdf},
  abstract = {This paper examines the use of the no-input mixing desk—or feedback mixer—across a range of musical practices. The research draws on twenty two artist interviews conducted by the authors, and on magazine and forum archives. We focus particularly on how the properties of the no-input mixer connect with the musical, aesthetic and practical concerns of these practices. The affordability, accessibility, and non-hierarchical nature of the instrument are examined as factors that help the idea spread, and that can be important political dimensions for artists.
The material, social and cultural aspects are brought together to provide a detailed picture of the instrument that goes beyond technical description. This provides a useful case study for NIME in thinking through these intercon- nections, particularly in looking outwards to how musical instruments and associated musical ideas travel, and how they can effect change and be changed themselves in their encounters with real-world musical contexts.},
  numpages = {7}
}

@inproceedings{nime2023_57,
  author = {Jeffrey Snyder and Davis Polito and Matthew Wang},
  title = {The Electrosteel: An Electronic Instrument Inspired by the Pedal Steel Guitar},
  pages = {409--416},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {57},
  track = {Papers},
  doi = {10.5281/zenodo.11189228},
  url = {http://nime.org/proceedings/2023/nime2023_57.pdf},
  urlsuppl1 = {http://nime.org/proceedings/2023/nime2023_57_video.mp4},
  abstract = {The Electrosteel is a new electronic instrument inspired by the user interface of the pedal steel guitar (PSG). The Electrosteel uses the interface concepts of the PSG (a bar in the left hand, plucked strings for the right hand, foot pedals, knee levers, etc) as a control paradigm for digital synthesis. The instrument allows performers with skill on the PSG to expand their sonic range, and creates a powerful new multi-dimensional way to control synthesis. This paper describes the development of the instrument and its custom embedded synthesis engine, with a focus on the design challenges posed by mapping an existing performer interface to a new instrument.},
  numpages = {8}
}

@inproceedings{nime2023_58,
  author = {John M Bowers and John Richards and Tim Shaw and Robin Foster and Akihiro Kubota},
  title = {Raw Data, Rough Mix: Towards an Integrated Practice of Making, Performance and Pedagogy},
  pages = {417--427},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {58},
  track = {Papers},
  doi = {10.5281/zenodo.11189230},
  url = {http://nime.org/proceedings/2023/nime2023_58.pdf},
  abstract = {This paper describes an extended intercontinental collaboration between multiple artists, institutions, and their publics, to develop an integrated musical practice which combines experimental making, performance, and pedagogy. We build on contributions to NIME which work with art and design-led methods to explore alternatives to, for example, more engineering-oriented approaches, without loss of practical utility and theoretical potential. We describe two week-long workshop-residencies and three performance-installations done under the provocative title Raw Data, Rough Mix which was intended to encourage exploration of basic processes in physical, mechanical, electrical, electronic and computational domains to develop musical artefacts that were frugal in their resource-demands but enabled the interrogation of human/non-human relationships, performativity, musical ecologies, aesthetics, and other matters. We close by elaborating our contribution to NIME as offering an integrated practice combining making, playing and learning, which is critically informed and practically productive.},
  numpages = {11}
}

@inproceedings{nime2023_59,
  author = {Colin Malloy and George Tzanetakis},
  title = {Steelpan-specific pitch detection: a dataset and deep learning model},
  pages = {428--435},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {59},
  track = {Papers},
  doi = {10.5281/zenodo.11189232},
  url = {http://nime.org/proceedings/2023/nime2023_59.pdf},
  abstract = {The steelpan is a pitched percussion instrument that although generally known by listeners is typically not included in music instrument audio datasets.  This means that it is usually underrepresented in existing data-driven deep learning models for fundamental frequency estimation. Furthermore, the steelpan has complex acoustic properties that make fundamental frequency estimation challenging when using deep learning models for general fundamental frequency estimation for any music instrument. Fundamental frequency estimation or pitch detection is a fundamental task in music information retrieval and it is interesting to explore methods that are tailored to  specific instruments and whether they can outperform general methods.  To address this, we present SASS, the Steelpan Analysis Sample Set that can be used to train steel-pan specific pitch detection algorithms as well as propose a custom-trained deep learning model for steelpan fundamental frequency estimation. This model outperforms general state-of-the-art methods such as pYin and CREPE on steelpan audio - even while having significantly fewer parameters and operating on a shorter analysis window. This reduces minimum system latency, allowing for deployment to a real-time system that can be used in live music contexts.},
  numpages = {8}
}

@inproceedings{nime2023_60,
  author = {Daniel Jones},
  title = {AbletonOSC: A unified control API for Ableton Live},
  pages = {436--440},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {60},
  track = {Papers},
  doi = {10.5281/zenodo.11189234},
  url = {http://nime.org/proceedings/2023/nime2023_60.pdf},
  abstract = {This paper describes AbletonOSC, an Open Sound Control API whose objective is to expose the complete Ableton Live Object Model via OSC. Embedded within Live by harnessing its internal Python scripting interface, AbletonOSC allows external processes to exert real-time control over any element of a Live set, ranging from generating new melodic sequences to modulating deeply-nested synthesis parameters. We describe the motivations and historical precedents behind AbletonOSC, provide an overview of its OSC namespace and the classes of functionality that are exposed by the API, and look at a series of applied case studies that demonstrate the new types of musical interface that AbletonOSC enables.},
  numpages = {5}
}

@inproceedings{nime2023_61,
  author = {Adan L. Benito Temprano and Teodoro Dannemann and Andrew McPherson},
  title = {Exploring the (un)ambiguous guitar: A Qualitative Study on the use of Gesture Disambiguation in Augmented Instrument Design},
  pages = {441--450},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {61},
  track = {Papers},
  doi = {10.5281/zenodo.11189236},
  url = {http://nime.org/proceedings/2023/nime2023_61.pdf},
  abstract = {Some of the performer’s gestures, despite corresponding to different physical interactions, might produce a similar sonic output. This is the case of upward and downward string bends on the guitar where stretching the string shifts the pitch upwards. Bending represents  
an expressive resource that extends across many different styles of guitar playing. 
In this study, we presented performers with an augmented electric guitar on which the gesture-to-sound relationship of downward bending gestures is changed depending on how the instrument is configured. Participants were asked to explore and perform a short improvisation under three different conditions, two augmentations that correspond to different auditory imagery and a constrained scenario. The different sessions of the experiment were recorded to conduct thematic analysis as an examination of how gestural disambiguation can be exploited in the design of augmentations that focus on reusing performer's expertise and how the gesture-to-sound entanglement of the different modalities supports or encumbers the performer's embodied relationship with the instrument.},
  numpages = {10}
}

@inproceedings{nime2023_62,
  author = {Jonathan Diaz},
  title = {Sensattice: An emerging collaborative and modular sound sculpture},
  pages = {451--456},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {62},
  track = {Papers},
  doi = {10.5281/zenodo.11189238},
  url = {http://nime.org/proceedings/2023/nime2023_62.pdf},
  abstract = {The concept of sound sculpture can embrace a rich variety of artistic manifestations and disciplines since it contains music, plastic arts, and performance, to say the least. Even the conceptual and space design or the skills and crafts necessary to transform physical materials demonstrates its interdisciplinary potential.
Sensattice is an emerging sound sculpture proposal, which takes advantage of organic raw materials considered waste to convert them into biopolymers and explores their acoustic and haptic potential taking "skin and bone" as conceptual premises to synthesize two fundamental materials. Such materials were obtained by applying biomaterial engineering and 3D modeling and printing as parallel processes.
Sensattice seems to be an emerging system since it is not reduced to mere materials but involves people and situated epistemic approaches that literally shape a sculptural lattice through the sensory and symbolic perception of skin and bones that can be sounded before, during and after the sculptural construction.},
  numpages = {6}
}

@inproceedings{nime2023_63,
  author = {Anastasia Clarke and Anastasia Clarke},
  title = {Shard-Speakers: An Inquiry into the History, Sonic Properties, and Musical Possibilities of Quartz Crystal Singing Bowls},
  pages = {457--462},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {63},
  track = {Papers},
  doi = {10.5281/zenodo.11189240},
  url = {http://nime.org/proceedings/2023/nime2023_63.pdf},
  abstract = {In this paper, the NIME “shard-speakers” is situated within the cultural context of the typical uses of crystal singing bowls, specifically acknowledging the origins of crystal bowls as re-purposed by-products of the silicon chip manufaturing process, and their subsequent adoption into the toolkits of New Age sound healing practitioners. Following this discussion is a first-person anecdotal account of the author/composer’s sonic explorations using crystal singing bowls in combination with the shards of broken bowls and custom electronics to create a body of recorded, acoustic, and electroacoustic musical works named Crushed Matrices #1-7. The last section of this paper explains how the extended musical techniques unearthed through the Crushed Matrices investigations informed the creation of the shard-speakers, and the electronically-generated musical content that was composed for them in the form of a sound artwork, Ode on Crushed Matrices. This recording was fed into the shard-speakers via tactile transducers on resonating bodies for the 2022 inaugural installation of the work, which at the time of writing is the only installation of the work to date. The paper’s conclusion addresses the relationship of this body of work to the NIME 2023 conference’s theme of “Frugal Music Innovation,” correlating or otherwise characterizing its relationship to several of the core competencies set forth by the Frugal Innovation Hub: adaptability, lightness of weight, mobile design, affordability, local material sourcing, and ruggedness.},
  numpages = {6}
}

@inproceedings{nime2023_64,
  author = {William C Payne and Matthew Kaney and Yuhua Cao and Eric Xu and Xinran Shen and Katrina Lee and Amy Hurst},
  title = {Live Coding Ensemble as Accessible Classroom},
  pages = {463--471},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {64},
  track = {Papers},
  doi = {10.5281/zenodo.11189248},
  url = {http://nime.org/proceedings/2023/nime2023_64.pdf},
  abstract = {In music and computer science classrooms, Blind and Visually Impaired (BVI) learners are often not given alternatives to visual technologies and materials. FiLOrk, an ensemble at the Filomen M. D'Agostino Greenberg Music School, is made up of five BVI high school learners who studied and performed computer music using the live coding language Tidal Cycles over the course of a semester. To make FiLOrk approachable and accessible we wrote a new curriculum featuring audio/tactile learning materials, and we designed a collaborative web editor for use with learners' assistive technologies, including screen readers and braille displays. In this article, we describe findings from classroom observations and interviews. We highlight how learners wrestled with persistent accessibility challenges, connected pre-existing music knowledge with Tidal Cycles concepts, created a culture of respect and support, and made suggestions for improving FiLOrk. We conclude by discussing opportunities to make live coding ensembles accessible to both BVI people and high school learners.},
  numpages = {9}
}

@inproceedings{nime2023_65,
  author = {Michał Seta and Dirk J Stromberg and D Andrew Stewart},
  title = {Building hybrid performances with DMIs, Hubs and Faust},
  pages = {472--478},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {65},
  track = {Papers},
  doi = {10.5281/zenodo.11189250},
  url = {http://nime.org/proceedings/2023/nime2023_65.pdf},
  abstract = {In this article, we describe the challenges of an artistic residency that included: a distributed improvisation in VR, performances using Digital Musical Instruments (DMIs), and Open Source software as much as possible. For this residency, we were constrained to using Mozilla’s Hubs as the Metaverse platform. We describe the shortcomings of the platform as a performance space in light of our experience, musical cultures, and the social aspects of a musical performance. We also address select technical issues pertaining to the context of a hybrid musical performance (simultaneously in Virtual Reality (VR) and in-real-life (IRL)) using this particular technology stack. Furthermore, we describe the challenges and surprises that occurred with Faust (Function Audio Stream), which was our choice of synthesis engine for the project. We conclude this paper by identifying some possible avenues for future research, exploration, and performances of a similar nature. We wish to clarify that although we will be talking a lot about Hubs, which was the Virtual Reality (VR) platform used for the residency, we were not endorsed by Mozilla.},
  numpages = {7}
}

@inproceedings{nime2023_66,
  author = {Erin M Gee},
  title = {The BioSynth—an affective biofeedback device grounded in feminist thought},
  pages = {479--485},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {66},
  track = {Papers},
  doi = {10.5281/zenodo.11189254},
  url = {http://nime.org/proceedings/2023/nime2023_66.pdf},
  abstract = {This paper presents the BioSynth, an affective biofeedback device for generating electronic music developed over a decade as part of my research-creation practice. The BioSynth has facilitated the creation of work involving performers from a variety of ages and professional experiences, contributing to  knowledge regarding emotional performance, exposing the differences between perceived and felt emotion within biofeedback art, extending emotional quantification techniques to notions of emotional performance technique, emotional labor, and what feminist Alva Gotby calls emotional reproduction. The design of the BioSynth privileges relational and real-world interactions as well as feminist thought regarding gendered hierarchies between body, mind, musical notation, social context, emotion and reason, and the division between performers and composers. This feminist inquiry has led to the development of alternatives to traditional frameworks for biofeedback music that rely on metaphors of musical instrumentation. After an introduction presenting two lived scenarios, this article is divided into three sections: hardware, software, and wetware. The hardware section describes the BioSynth through its design, which privileges ease-of-use for non-expert users. The software section describes mapping considerations based on feminist principles of measuring the emotional subject only against itself. Finally, in the wetware section I describe a feminist-inspired approach to emotional performance that embraces artificiality, irony, play, pleasure, and performance in biofeedback art, implying novel models for composer-instrument-performer relations.},
  numpages = {7}
}

@inproceedings{nime2023_67,
  author = {David Fierro and Alain Bonardi and Atau Tanaka},
  title = {FAUST Multiplatform toolbox for Body Brain Digital Musical Instruments},
  pages = {486--493},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {67},
  track = {Papers},
  doi = {10.5281/zenodo.11189256},
  url = {http://nime.org/proceedings/2023/nime2023_67.pdf},
  abstract = {This article presents new tools developed in FAUST language to create musical interactions using electrophysiological signals as input. The developed tools are centered around signal processing and simulation of electrophysiological signals. These techniques are used to clean and process the electrophysiological signals and subsequently provide real-time interactions to feed the control of sound processes. The system provides modules that are highly musically expressive especially in the domain of spatial sound.
These tools also allow to set up a  testing environment by replacing the need of electrophysiological capturing devices.
The findings of this exploration provide a better understanding of how the FAUST language can be used in conjunction with electrophysiological signals and exposes interesting opportunities to explore further possibilities in music creation in an open source environment with the possibility of multitarget compilation, allowing our modules to be used either in such softwares as Max or embedded on microcontrollers.},
  numpages = {8}
}

@inproceedings{nime2023_68,
  author = {Hugh A von Arnim and Stefano Fasciani and Çağrı Erdem},
  title = {The Feedback Mop Cello: An Instrument for Interacting with Acoustic Feedback Loops},
  pages = {494--499},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {68},
  track = {Papers},
  doi = {10.5281/zenodo.11189258},
  url = {http://nime.org/proceedings/2023/nime2023_68.pdf},
  abstract = {This paper presents the Feedback Mop Cello, a feedback instrument integrating acoustic feedback loops generated through a microphone and loudspeaker combination with a control interface inspired by the cello. Current paradigms of interaction with feedback instruments are based around ideas of negotiation with autonomous systems rather than control. We explore the possibility of integration of negotiated and controlled elements through a design focused on isolating the acoustic feedback loop signal path from the signal path to which sound processing is applied. We focus on three musical parameters of timbre, pitch, and dynamics. We present timbre as a parameter to mainly be negotiated within the feedback loop, while pitch and dynamics are parameters that can be explicitly controlled through the interface. An approach is taken to minimize components within the feedback loop in order to foreground the choice of loudspeaker as an integral part of the instrument’s sound. A preliminary user study is carried out involving five semi-professional musicians, focusing on their reflection regarding their interaction with the acoustic feedback loop.},
  numpages = {6}
}

@inproceedings{nime2023_69,
  author = {Raghavasimhan Sankaranarayanan and  Nitin Hugar and Qinying Lei and Thomas Ottolin and Hardik Goel and Gil Weinberg},
  title = {Mixboard - A Co-Creative Mashup Application for Novices},
  pages = {500--505},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {69},
  track = {Papers},
  doi = {10.5281/zenodo.11189260},
  url = {http://nime.org/proceedings/2023/nime2023_69.pdf},
  abstract = {Mixboard is a web / iOS application that allows music lovers to create and share personalized musical mashups. The app allows users to choose and organize up to four songs within four different lanes. The system automatically separates the songs' sources into corresponding stems, calculates an appropriate tempo and key for the mashup, and chooses song segments according to users' visual creation. Unlike other professional applications used for mashups, Mixboard does not require experience with Digital Audio Workstations (DAWs) or waveform editing and supports unlimited library of usable songs. In a co-creative fashion, users can explore their creativity while the system contributes its own creative input utilizing Music Information Retrieval (MIR), Digital Signal Processing (DSP), and compositional templates. User studies were conducted to evaluate Mixboard's success in achieving an effective balance between system automation and user control. Results indicate strong metrics for user creative expression, engagement, and ownership, as well as high satisfaction with the final musical outcome. Results also suggest a number of modifications to the balance between user control and system automation, which will be addressed in future work.},
  numpages = {6}
}

@inproceedings{nime2023_70,
  author = {Nicholas Canny},
  title = {The implementation of envelope based complex mapping strategies to extend and augment human control},
  pages = {506--510},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {70},
  track = {Papers},
  doi = {10.5281/zenodo.11189262},
  url = {http://nime.org/proceedings/2023/nime2023_70.pdf},
  abstract = {This paper presents complex mapping strategies that offer flexibility for improvising with elaborate digital environments by allowing for more human control with less physical input. The intention is not to reduce human physicality, but instead actions are further extended and altered through complex envelopes. This software was originally designed for the augmented guitar, to address the issue of a lack of spare bandwidth (Cook, 2001) that is inherent to guitar playing. This makes it challenging to simultaneously control digital interfaces without compromising guitar technique. The Slider MultiMap software discussed in this paper helps to overcome this dilemma by enabling a guitarist to control multiple audio effects with a single gesture while individually customising how each parameter is controlled prior to the performance. At the same time, it explores the delegation of tasks to the computer in situations where indirect control is more desirable.},
  numpages = {5}
}

@inproceedings{nime2023_71,
  author = {John M Bowers},
  title = {A Hapless But Entertaining Roar: Developing a Room Feedback System through Artistic Research and Aesthetic Reflection},
  pages = {511--520},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {71},
  track = {Papers},
  doi = {10.5281/zenodo.11189266},
  url = {http://nime.org/proceedings/2023/nime2023_71.pdf},
  abstract = {This paper presents a room feedback system which the author has been developing and performing with for nearly three years. The design emerged from an artistic research process which emphasises multiple explorations coexisting around a research topic while having a sensitivity to the practicalities of a customary gig (short set-up time, unpredictable acoustics). Typically enabled by a stereo room-mic and a pair of speakers, many algorithms have been explored in the loop with some being tributes to historical feedback works. An overall design is offered where all feedback pathways are simultaneously available and mutually interfere via the room. Each algorithm is designed to have one significant performable parameter but how this is mapped to sensors or widgets is itself performable with various behaviours available, including some explorations of self-programming and ‘intra-active’ ideas. Concert experience in solo and small ensemble formats is discussed and a number of contributions are identified in how the work: extends room feedback research to explore multiple parallel processes of varied spectro-morphological character, offers connections to historical work in a pedagogically interesting fashion, demonstrates several novel algorithms, while exemplifying a characteristic artistic research method. The paper closes with a speculative ‘feedback aesthetics’ to help configure future work.},
  numpages = {10}
}

@inproceedings{nime2023_72,
  author = {Douglas A {Bowman Jr} and Daniel Manesh and Sang Won Lee},
  title = {SHARP: Supporting Exploration and Rapid State Navigation in Live Coding Music},
  pages = {521--524},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {72},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189270},
  url = {http://nime.org/proceedings/2023/nime2023_72.pdf},
  abstract = {How do live coders simultaneously develop new creations and master previous ones? Using conclusions drawn from previous studies about exploratory programming and our experience practicing live coding, we identified a need to support creation and mastery in the live coding space—specifically in the realm of live coding pertaining to musical creations. We developed a tool, SHARP, which attempted to empower live coders in both their exploration and performances. SHARP is a code editor extension that visualizes the history of each instrument that the live coder creates; the visualization can then be used to revisit the previous states of the instrument and create new ones. We believe that this extension will support live coders’ exploration in practice as well as enable novel musical aesthetics in performance contexts. We did an initial evaluation of SHARP using an autoethnographic approach where one researcher used the tool over multiple sessions to compose a piece. From the autoethnography, we saw that SHARP supported composition by making it easier to explore different musical ideas and to revisit past states. Our analysis also hints at new possible features, such as being able to combine multiple previous states together using SHARP.},
  numpages = {4}
}

@inproceedings{nime2023_73,
  author = {Courtney D Brown and Thomas Dudgeon and Cezary Gajewski},
  title = {Dinosaur Choir: Designing for Scientific Exploration, Outreach, and Experimental Music},
  pages = {525--530},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {73},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189272},
  url = {http://nime.org/proceedings/2023/nime2023_73.pdf},
  abstract = {Lambeosaurine hadrosaurs are duck-billed dinosaurs. Scientists hypothesize that their large, bony crests which encapsulate complicated, hollow nasal passages function as resonators for vocal calls. This paper discusses the work-in-process, Dinosaur Choir, which recreates these vocal capabilities as musical skull instruments. The skull and nasal passages are fabricated based on Computed Topology (CT) scans of hadrosaur skulls, and larynx design is informed by scientific research. Musicians and participants voice the instruments by blowing into a mouthpiece or microphone, and a larynx mechanism creates the sound in response, which is then resonated through the nasal passages. The instruments are intended both for interactive exhibition and for on-going musical performance practice. Dinosaur Choir aims to give life to the voices of dinosaurs, allowing an embodied experience with extinct animals long lost to the past. This paper focuses on the development of the first musical instrument in the series, based on an adult Corythosaurus skull. We consider how scientific research as well as musical and practical concerns impact the design process and what trade-offs must be contemplated and made in order to achieve our aims of dinosaurian embodied sound.},
  numpages = {6}
}

@inproceedings{nime2023_74,
  author = {Takuto Fukuda and Marcelo Wanderley},
  title = {T-Patch: a software application for T-Stick Digital Musical Instruments},
  pages = {531--535},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {74},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189274},
  url = {http://nime.org/proceedings/2023/nime2023_74.pdf},
  abstract = {This paper introduces the T-Patch, a software application that streamlines the use of T-Stick Digital Musical Instruments (DMIs). It offers a user-friendly interface for gesture extraction, mapping, signal conditioning, sound synthesis, and sequencing with cues, enabling composers to create music without programming. Our main contribution is two-fold: (1) providing a versatile software solution to address the current lack of music-making support for T-Stick DMIs, and (2) highlighting the importance of demonstration content, such as a video, to showcase the instrument’s capabilities and inspire new users. The T-Patch reduces the barrier to entry for using the T-Stick DMI and offers a shared software solution for various music-making scenarios.},
  numpages = {5}
}

@inproceedings{nime2023_75,
  author = {Yue Yang and Zhaowen Wang and Zijin Li},
  title = {MuGeVI: A Multi-Functional Gesture-Controlled Virtual Instrument},
  pages = {536--541},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {75},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189278},
  url = {http://nime.org/proceedings/2023/nime2023_75.pdf},
  abstract = {Currently, most of the digital musical instruments cannot leave the use of dedicated hardware devices, making them limited in terms of user popularity and resource conservation. In this paper, we propose a new computer vision-based interactive multi-functional musical instrument, called MuGeVI, which requires no additional hardware circuits or sensors, and allows users to create or play music through different hand gestures and positions. It firstly uses deep neural network models for hand key point detection to obtain gesture information, secondly maps it to pitch, chord or other information based on the current mode, then passes it to Max/MSP via the OSC protocol, and finally implements the generation and processing of MIDI or audio. MuGeVI is now available in four modes: performance mode, accompaniment mode, control mode, and audio effects mode, and can be conveniently used with just a personal computer with a camera. Designed to be human-centric, MuGeVI is feature-rich, simple to use, affordable, scalable and programmable, and is certainly a frugal musical innovation. All the material about this work can be found in https://yewlife.github.io/MuGeVI/.},
  numpages = {6}
}

@inproceedings{nime2023_76,
  author = {Costa K Colachis Glass and Fabio Morreale},
  title = {TAILSPIN: AN INQUIRY INTO THE EQUILIBRIUM OF AUDIO-VISUAL FEEDBACK},
  pages = {542--548},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {76},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189282},
  url = {http://nime.org/proceedings/2023/nime2023_76.pdf},
  abstract = {This paper is an exploration and creative inquiry into the equilibrium of audio-visual feedback. Following a Research-Through Design approach, we actualized this inquiry by designing an ad-hoc audio-visual instrument: TAILSPIN. In this instrument, a closed audio-visual and physical loop is created between a microphone and its speaker, and a camera and its display, which are controlled by a performer. The tenets of feedback are then understood through the contextual research of cycles and loops in our natural environment. In this paper, we present the technical details of the instrument and offer novel insights into the audio-visual equilibrium within the context and intricacies of our own natural environment and organic feedback systems.},
  numpages = {7}
}

@inproceedings{nime2023_77,
  author = {Jonathan Lane-Smith and Derrek Chow and Sahand Ajami and Jeremy Cooperstock},
  title = {The Hapstrument: A Bimanual Haptic Interface for Musical Expression},
  pages = {549--553},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {77},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189284},
  url = {http://nime.org/proceedings/2023/nime2023_77.pdf},
  abstract = {In this paper, we present the Hapstrument: a bimanual haptic interface for musical expression. This DMI uses two low-cost 2-DoF haptic force-feedback devices, one for each hand. The left device controls pitch selection, while the right device controls excitation by simulating the feeling of bowing or plucking a string. A user study was run to evaluate the effectiveness of the Hapstrument. This evaluation
received a wide range of reviews, from excellent to poor. Ultimately, the musical backgrounds of the participants greatly impacted their experiences with the Hapstrument. For participants whose expectations aligned with what the instrument could provide, it was an effective DMI that uses force feedback to enhance musical expression.},
  numpages = {5}
}

@inproceedings{nime2023_78,
  author = {sergey k kasich},
  title = {Morphological evolution of musical interface: design case studies},
  pages = {554--559},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {78},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189286},
  url = {http://nime.org/proceedings/2023/nime2023_78.pdf},
  abstract = {The recent advancements in digital fabrication has led to a wider access to prototyping in all sorts of fields. Beginning of the last decade was marked by the word “revolution” in relation to “maker’s culture” at least in some publications. This has influenced the sphere of physical computing in arts and NIME sphere as well. As currently there are more and more possibilities to create new instruments, we think that it can be useful to think of approaches to conceptualize these creations. This paper is an attempt to suggest methodology for NIME prototyping, based on evolutionary metaphor.
First we observe the application of evolutionary concepts to the field of music technology, briefly discussing its appearance in related publications. We then assemble our own operational concept, which can be used for the direct prototyping of interfaces. Mainly by introducing metaphorical “DNA”, inside which the “gene” of “interactive kinematic concept” is of a particular interest, and also by applying the now obsolete but useful “Meckel–Serres recapitulation hypothesis” (embryological parallelism) as a model for rapid prototyping. 
Understanding the speculative nature of such an approach we do not offer it as a scientific basis for classification, research or prediction, but as a workable concept for development, which can lead to valuable results. 
In the end we describe two case studies of NIMEs, which were prototyped in the discussed fashion, showing illustrations and reflecting on the practicalities.},
  numpages = {6}
}

@inproceedings{nime2023_79,
  author = {Aoi Uyama and Danny Hynds and Dingding Zheng and George Chernyshov and Tatsuya Saito and Kai Kunze and Kouta Minamizawa},
  title = {Feel What You Don't Hear: A New Framework for Non-aural Music Experiences},
  pages = {560--565},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {79},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189288},
  url = {http://nime.org/proceedings/2023/nime2023_79.pdf},
  abstract = {Just as the way a performer is moved differs even among audiences who have the same impression of the performance, the sensations and experiences felt by the performers themselves and the audiences' experiences also differ. The purpose of this research is to create a new listening experience by analyzing and extracting the performer's introspection of rests, groove, and rhythm, and physically presenting it to the audience. Although these elements are important in shaping music, they are not always directly expressed as auditory sounds.
Our hypothesis is that this introspection, such as a sense of rhythm and groove, is latent and observable in physiological states such as breathing and heartbeat. By sensing and presenting them to the audience, music appreciation that includes introspection could become possible. In other words, by sensing and presenting introspection to the audience, the music listening experience itself can be redesigned to include a physicality that is closer to the performer's experience of the music, rather than being passive in an auditory sense. In this study, preliminary experiments were conducted on the extraction of the performer's introspection, and a device was designed to present it to the audience.},
  numpages = {6}
}

@inproceedings{nime2023_80,
  author = {Atau Tanaka and David Fierro and Francesco Di Maggio and Martin Klang and Stephen Whitmarsh},
  title = {The EAVI ExG Muscle/brain hybrid physiological sensing},
  pages = {566--568},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {80},
  track = {Demos},
  doi = {10.5281/zenodo.11189290},
  url = {http://nime.org/proceedings/2023/nime2023_80.pdf},
  abstract = {We present an update on the EAVI physiological interface, a wireless, microcontroller based hardware design for the acquisition of bioelectrical signals. The system has been updated to process electroencephalogram brain signals in addition to muscle electromyogram. The hardware/firmware system interfaces with host software carrying out feature extraction and signal processing.
Recent advances in electronics have made physiological computing applications practical and feasible. However, there is a gap between high end biomedical equipment and consumer DIY solutions. The hardware design we present here bridges this gap, and combines a specialized biosignal acquisition chip mated with a general-purpose microcontroller. It is based on the Texas Instruments ADS129x family a single chip integrated solution for high quality biosignal amplification and digitization. It serves as analogue front end via programmable gain amplifiers to a 24bit delta-sigma analog-digital converter.  The microcontroller is the STMicroelectronics STM32F427, a Cortex-M4 family microcontroller with floating point unit . In addition to EMG acquisition, the board includes a Kionix KX122 three-axis accelerometer . The TI and Kionix sensing chipts communicate with the ST microcontroller over an I2C digital serial bus. The board communicates with the host computer or rest of the music system wirelessly over Bluetooth LE 4.2 using an ST SPBTLE-1S transceiver. The board can also communicate over USB where it registers with the host as a class compliant audio and MIDI device. Audio and physiological signals are treated in the same signal processing chain using the OWL framework.
The demo will show multichannel EMG, and single channel EEG. We call this hybridization “ExG”. We will present documentation of the EAVI board used in the lab and on stage, in user studies with neuro-diverse musicians and trained instrumentalists, as well as in performance with the experimental all-female band, Chicks on Speed.},
  numpages = {3}
}

@inproceedings{nime2023_81,
  author = {Rodrigo Diaz and Charalampos Saitis and Mark B Sandler},
  title = {Interactive Neural Resonators},
  pages = {569--573},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {81},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189296},
  url = {http://nime.org/proceedings/2023/nime2023_81.pdf},
  abstract = {In this work, we propose a method for the controllable synthesis of real-time contact sounds using neural resonators. Previous works have used physically inspired statistical methods and physical modelling for object materials and excitation signals. Our method incorporates differentiable second-order resonators and estimates their coefficients using a neural network that is conditioned on physical parameters. This allows for interactive dynamic control and the generation of novel sounds in an intuitive manner. We demonstrate the practical implementation of our method and explore its potential creative applications.},
  numpages = {5}
}

@inproceedings{nime2023_82,
  author = {Teodoro Dannemann and Nick Bryan-Kinns},
  title = {The Sabotaging Piano: key-to-pitch remapping as a source of new techniques in piano improvisation},
  pages = {574--578},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {82},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189298},
  url = {http://nime.org/proceedings/2023/nime2023_82.pdf},
  abstract = {In this paper we present the Sabotaging Piano, a prepared electronic piano that alters key-to-pitch correspondence by reassigning adjacent pitches (i.e. one semi-tone higher or lower) to each key. Performers can control how many keys to remap through an expression pedal. If the pedal is not pressed the Sabotaging Piano works as a normal piano. When fully pressed, each key is remapped one semi-tone up or down with equal probability. Each new performance (i.e. when the piano is turned on) triggers a new and unknown remapping pattern, but the specific pattern remains fixed throughout the whole performance. This aims to provide a balance of uncertain but still explorable and learnable behaviour. 
We invited three professional piano improvisers to rehearse with our piano in order to prepare a final improvisation concert. We aimed to explore how much can be rehearsed or prepared with a piano that will behave somewhat differently for each new performance. We asked pianists to document their rehearsal processes to witness the appearing of strategies or techniques with the Sabotaging Piano. 
Through analysis of the rehearsals reports and the MIDI data collected in the final concert, here we show that the three pianists not only developed different techniques with the Sabotaging Piano, but they also leveraged the particularities of it to use them as creative resources.},
  numpages = {5}
}

@inproceedings{nime2023_83,
  author = {Hugh Aynsley and Tom Mitchell and Dave Meckin},
  title = {Participatory Conceptual Design of Accessible Digital Musical Instruments using Generative AI},
  pages = {579--583},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {83},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189302},
  url = {http://nime.org/proceedings/2023/nime2023_83.pdf},
  abstract = {This paper explores the potential of AI text-to-image diffusion models (e.g. DALLE-2 and Midjourney) to support the early phase design of new digital musical instruments in collaboration with Disabled musicians. The paper presents initial findings from two speculative design workshops attended by Disabled participants who are affiliated with the London-based inclusive arts organisation Joy of Sound. The workshops included activities enabling participants to co-create speculative images of new instruments, drawing on their contributions. These included the overall appearance of the instrument, constituent materials and other design characteristics. The paper discusses the generated images and examines how diffusion models can be a useful tool to support the conceptual co-design phase of bespoke accessible instruments. The project findings indicate that diffusion models can be useful as a facilitatory tool for idea generation in the initial stages of bespoke instrument design.},
  numpages = {5}
}

@inproceedings{nime2023_84,
  author = {Jonathan M Pigrem and Jennifer MacRitchie and Andrew McPherson},
  title = {Instructions Not Included: Dementia-Friendly approaches to DMI Design},
  pages = {584--589},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {84},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189304},
  url = {http://nime.org/proceedings/2023/nime2023_84.pdf},
  abstract = {The development of bespoke musical tools such as many accessible digital musical instruments (ADMI) can necessitate specific design constraints. Within a field which often promotes out of the box thinking and new interactions with experimental technologies, how do we design for user groups where these notions of interaction will be less familiar, and/or increasingly challenging due to the progression of cognitive decline?
The relationship between age and the use of technology is understood within the wider context of human computer interaction (HCI), however, how this applies specifically to musical interaction or contributes to a ‘dementia-friendly’ approach to digital musical instrument (DMI) design is drastically underrepresented within the NIME community. Following a scoping review of technology for arts activities designed for older adults with cognitive decline, we ran a series of involvement activities with a range of stakeholders living with, or caring for those living with dementia. Consolidating the knowledge and experience shared at these events, we propose five considerations for designing dementia-friendly digital musical instruments. We illustrate our approach with a range of new instruments co-designed to enable increased interaction with music for people living with dementia.},
  numpages = {6}
}

@inproceedings{nime2023_85,
  author = {William Francis Wilson and Niccolo Granieri and Islah Ali-Maclachlan},
  title = {Time's up for the Myo? The smartwatch as a ubiquitous alternative for audio-gestural analyses.},
  pages = {590--593},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {85},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189306},
  url = {http://nime.org/proceedings/2023/nime2023_85.pdf},
  abstract = {The utility of gestural technologies in broadening analytical- and expressive-interface possibilities has been documented extensively; both within the sphere of NIME and beyond. 

Wearable gestural sensors have proved integral components of many past NIMEs. Previous implementations have typically made use of specialist, IMU and EMG based gestural technologies. Few have proved, singularly, as popular as the Myo armband. An informal review of the NIME archives found that the Myo has featured in 21 NIME publications, since an initial declaration of the Myo’s promise as “a new standard controller in the NIME community” by Nyomen et al. in 2015. Ten of those found were published after the Myo’s discontinuation in 2018, including three as recently as 2022.

This paper details an assessment of smartwatch-based IMU and audio logging as a ubiquitous, accessible alternative to the IMU capabilities of the Myo armband. Six violinists were recorded performing a number of exercises using VioLogger; a purpose-built application for the Apple Watch. Participants were simultaneously recorded using a Myo armband and a freestanding microphone. Initial testing upon this pilot dataset indicated promising results for the purposes of audio-gestural analysis; both implementations demonstrated similar efficacy for the purposes of MLP-based bow-stroke classification.},
  numpages = {4}
}

@inproceedings{nime2023_86,
  author = {Adam G Schmidt and Michael Gurevich},
  title = {The Hummellaphone: An Electromagnetically Actuated Instrument and Open-Source Toolkit},
  pages = {594--599},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {86},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189308},
  url = {http://nime.org/proceedings/2023/nime2023_86.pdf},
  abstract = {This paper presents the Hummellaphone, a highly-reconfigurable, open-source, electromagnetically actuated instrument being developed for research in engineering learning, haptics, and human-computer interaction (HCI). The reconfigurable performance interface promotes experimentation with gestural control and mapping. Haptic feedback reintroduces the tangible bilateral communication between performer and instrument that is present in many acoustic and electro-acoustic instruments but missing in most digital musical instruments. The overall aim of the project is to create an open-source, accessible toolkit for facilitating the development of and research with electromagnetically actuated musical instruments. This paper describes the hardware and design of the musical instrument and control interface as well as example research applications.},
  numpages = {6}
}

@inproceedings{nime2023_87,
  author = {Juan C Duarte Regino},
  title = {AUGURY : an interface for generating soundscapes inspired by ancient divination},
  pages = {600--603},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {87},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189310},
  url = {http://nime.org/proceedings/2023/nime2023_87.pdf},
  abstract = {This paper presents the development of a multichannel sound installation about atmospheric processes. This instrument is an example of taking inspiration from ancient cultures for NIME design, and of sensing weather to extend the perception of the performer, who also then becomes a listener of atmospheric processes.  The interface channels dynamics found in the atmosphere: wind's force and direction, air quality, atmospheric pressure, and electromagnetism. These sources are translated into sound by mapping sensor data into a multichannel sonification composition. The paper outlines the artistic context and expands on its interaction overview.},
  numpages = {4}
}

@inproceedings{nime2023_88,
  author = {Gerardo  Meza},
  title = {Exploring the potential of interactive Machine Learning for Sound Generation: A preliminary study with sound artists},
  pages = {604--607},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {88},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189313},
  url = {http://nime.org/proceedings/2023/nime2023_88.pdf},
  abstract = {Interactive Machine Learning (IML) is an approach previously explored in music discipline. However, its adaptation in sound synthesis as an algorithmic method of creation has not been examined. This article presents the prototype ASCIML, an Assistant for Sound Creation with Interactive Machine Learning, that allows musicians to use IML to create personalized datasets and generate new sounds. Additionally, a preliminary study is presented which aims to evaluate the potential of ASCIML as a tool for sound synthesis and to gather feedback and suggestions for future improvements. The prototype can be used in Google Colaboratory and is divided into four main stages: Data Design, Training, Evaluation and Audio Creation. Results from the study, which involved 27 musicians with no prior knowledge of Machine Learning (ML), showed that most participants preferred using microphone recording and synthesis to design their dataset and that the Envelopegram visualization was found to be particularly meaningful to understand sound datasets. It was also found that the majority of participants preferred to implement a pre-trained model on their data and relied on hearing the audio reconstruction provided by the interface to evaluate the model performance. Overall, the study demonstrates the potential of ASCIML as a tool for hands-on neural audio sound synthesis and provides valuable insights for future developments in the field.},
  numpages = {4}
}

@inproceedings{nime2023_89,
  author = {Sophie Rose},
  title = {CALM: Mapping yoga practice for gestural control to externalise traumatic experiences},
  pages = {608--611},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {89},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189317},
  url = {http://nime.org/proceedings/2023/nime2023_89.pdf},
  abstract = {CALM is a performance piece from a collection of works that explore trauma through trauma-informed therapeutic models, such as bi-lateral coordination drawing, yoga, and tapping, and existing movement practices, such as yoga, Pilates, dance, and conducting, to control and manipulate sound in performance. This work draws from yoga practice to control the volumes and audio effects on pre-composed audio layers through use of datagloves (MiMu with their proprietary software Glover (MI.MU Gloves Ltd, 2010), though this is not specific to the constraints of the MiMu/Glover system) and Max/MSP (Cycling ’74, 2018). 

Yoga is a movement practice often recommended to manage symptoms of trauma and anxiety due to the focus on one’s body and generally meditative nature or the practice.  However, in cases of sexual trauma, yoga may yield the opposite of the desired results when not used in a trauma-sensitive context (Khoudari, 2021; Levine et al., 2010). This is because the individual tries to focus on the body in which they do not feel safe and encounter unresolved trauma. Thus, instead of a grounding effect, the individual hears the mental and physical pain that they have endured repeating itself in the present. To reflect this, “stillness” audio material is routed to scream-like and abrasive sounds, while “movement” audio quiets the listener’s internal landscape. Movements used in the live piece were chosen based on providing extramusical benefit to the composer-performer (and areas that are typically carrying tension as a result of the trauma) without contributing to any negative effects, for example, the pose “Happy Baby/Ananda Balasana” was excluded and Malasana (a deep squat pose) was used in its place as it puts the performer in a less vulnerable position by being on one’s feet. },
  numpages = {4}
}

@inproceedings{nime2023_90,
  author = {Eric Easthope},
  title = {SnakeSynth: New Interactions for Generative Audio Synthesis},
  pages = {612--619},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {90},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189319},
  url = {http://nime.org/proceedings/2023/nime2023_90.pdf},
  abstract = {I present "SnakeSynth," a web-based lightweight audio synthesizer that combines audio generated by a deep generative model and real-time continuous two-dimensional (2D) input to create and control variable-length generative sounds through 2D interaction gestures. Interaction gestures are touch and mobile-compatible and made with analogies to strummed, bowed, brushed, and plucked musical instrument controls. Point-and-click and drag-and-drop gestures directly control audio playback length and intensity. I show that I can modulate sound length and intensity by interacting with a programmable 2D grid and leveraging the speed and ubiquity of web browser-based audio and hardware acceleration to generate time-varying high-fidelity sounds with real-time interactivity. SnakeSynth adaptively reproduces and interpolates between sounds encountered during model training, notably without long training times, and I briefly discuss possible futures for deep generative models as an interactive paradigm for musical expression.},
  numpages = {8}
}

@inproceedings{nime2023_91,
  author = {Ilya Borovik and Vladimir Viro},
  title = {Real-Time Co-Creation of Expressive Music Performances Using Speech and Gestures},
  pages = {620--625},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {91},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189321},
  url = {http://nime.org/proceedings/2023/nime2023_91.pdf},
  abstract = {We present a system for interactive co-creation of expressive performances of notated music using speech and gestures. The system provides real-time or near-real-time dialog-based control of performance rendering and interaction in multiple modalities. It is accessible to people regardless of their musical background via smartphones. The system is trained using sheet music and associated performances, in particular using notated performance directions and user-system interaction data to ground performance directions in performances. Users can listen to an autonomously generated performance or actively engage in the performance process. A speech- and gesture-based feedback loop and online learning from past user interactions improve the accuracy of the performance rendering control. There are two important assumptions behind our approach: a) that many people can express nuanced aspects of expressive performance using natural human expressive faculties, such as speech, voice, and gesture, and b) that by doing so and hearing the music follow their direction with low latency, they can enjoy playing the music that would otherwise be inaccessible to them. The ultimate goal of this work is to enable fulfilling and accessible music making experiences for a large number of people who are not currently musically active.},
  numpages = {6}
}

@inproceedings{nime2023_92,
  author = {Budhaditya Chattopadhyay},
  title = {Dhvāni: Sacred Sounds and Decolonial Machines},
  pages = {626--628},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {92},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189323},
  url = {http://nime.org/proceedings/2023/nime2023_92.pdf},
  abstract = {This paper provides an entry into a decolonial approach to AI driven music and sound arts by describing an ongoing artistic research project Dhvāni. The project is a series of responsive, self-regulating, and autonomous installations driven by Artificial Intelligence and Machine Learning and incorporating ritual and sacred sounds from South Asia. Such mélange re-emphasizes and advocates for the values of interconnectivity, codependence, network, and community with a decolonial approach. By giving the AI an autonomous agency, the project aims to reimagine the future of AI with an inter-subjective reciprocity in human-machine assemblages transcending the technologically deterministic approach to AI-driven live art, media arts and music. Through unpacking the project, this paper underscores the necessity to dehegemonize the AI-driven music field towards a transcultural exchange, thereby transcend the field’s Eurocentric bias.},
  numpages = {3}
}

@inproceedings{nime2023_93,
  author = {Jose M Corredera},
  title = {EMG Sonification as a Tool for Functional Rehabilitation of Spinal-Cord Injury.},
  pages = {629--632},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {93},
  track = {Work in Progress},
  doi = {10.5281/zenodo.11189327},
  url = {http://nime.org/proceedings/2023/nime2023_93.pdf},
  abstract = {Spinal cord injury is one of the most serious causes of disability that can affect people's lives. In tetraplegia, the loss of mobility of the upper limb has a devastating effect on the quality of life and independence of these patients, so their rehabilitation is considered a crucial objective. We present a tool for functional motor rehabilitation of the upper limb in patients with spinal cord injury, based on the use of bio-sensors and the sonification of EMG activity during the repetitive execution of a specific gesture.  During the hospital stay, the patient has a wide range of therapies available to improve motor function or compensate for loss of mobility, including execution of different maneuvers. The repetitive and continuous performance of these tasks is a key element in motor recovery. However, in many cases, these tasks do not include sufficient feedback mechanisms to help the patient or to motivate him/her during execution. Through the sonification of movement and the design of adapted interaction strategies, our research aims to offer a new therapeutic tool that musically transforms the gesture and expands the patient's mechanisms of expression, proprioception and cognition, in order to optimize, correct and motivate movement.},
  numpages = {4}
}

@inproceedings{nime2023_94,
  author = {Atsuya Kobayashi and Ryo Nishikado and Nao Tokui},
  title = {Improvise+=Chain: Listening to the Ensemble Improvisation of an Autoregressive Generative Model},
  pages = {633--636},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {94},
  track = {Demos},
  doi = {10.5281/zenodo.11189329},
  url = {http://nime.org/proceedings/2023/nime2023_94.pdf},
  abstract = {This paper describes Improvise+=Chain, an audio-visual installation artwork of autonomous musical performance using artificial intelligence technology. The work is designed to provide the audience with an experience exploring the differences between human and AI-based virtual musicians. Using a transformer decoder, we developed a four-track (melody, bass, chords and accompaniment, and drums) symbolic music generation model. The model generates each track in real time to create an endless chain of phrases, and 3D visuals and LED lights represent the attention information between four tracks, i.e., four virtual musicians, calculated within the model. This work aims to highlight the differences for viewers to consider between humans and artificial intelligence in music jams by visualizing the only information virtual musicians can communicate with while humans interact in multiple modals during the performance.},
  numpages = {4}
}

@inproceedings{nime2023_95,
  author = {Sebastian Lobbers and George Fazekas},
  title = {SketchSynth: a browser-based sketching interface for sound control},
  pages = {637--641},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {95},
  track = {Demos},
  doi = {10.5281/zenodo.11189331},
  url = {http://nime.org/proceedings/2023/nime2023_95.pdf},
  abstract = {SketchSynth is an interface that allows users to create mappings between synthesised sound and a graphical sketch input based on human cross-modal perception. The project is rooted in the authors' research which collected 2692 sound-sketches from 178 participants representing their associations with various sounds. The interface extracts sketch features in real-time that were shown to correlate with sound characteristics and can be mapped to synthesis and audio effect parameters via Open Sound Control (OSC). This modular approach allows for an easy integration into an existing workflow and can be tailored to individual preferences. The interface can be accessed online through a web-browser on a computer, laptop, smartphone or tablet and does not require specialised hard- or software. We demonstrate SketchSynth with an iPad for sketch input to control synthesis and audio effect parameters in the Ableton Live digital audio workstation (DAW). A MIDI controller is used to play notes and trigger pre-recorded accompaniment. This work serves as an example of how perceptual research can help create strong, meaningful gesture-to-sound mappings.},
  numpages = {5}
}

@inproceedings{nime2023_96,
  author = {Johann Diedrick},
  title = {The Harvester: A DIY Sampler and Synthesizer - Demo},
  pages = {642--643},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {96},
  track = {Demos},
  doi = {10.5281/zenodo.11189333},
  url = {http://nime.org/proceedings/2023/nime2023_96.pdf},
  abstract = {This paper describes the Harvester, a DIY sampler and synthesizer. The Harvester provides users with a low-cost, accessible platform for making music with everyday sounds via open-source hardware and software tools that anyone can use or modify. This paper goes over the motivation, methodology, features, and use cases of the Harvester instrument, with the intention of the instrument being demonstrated for people to play with and use at NIME 2023.},
  numpages = {2}
}

@inproceedings{nime2023_97,
  author = {Juan  P Martinez Avila},
  title = {Kraakavera: A Tribute to Michel Waisvisz},
  pages = {644--645},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {97},
  track = {Demos},
  doi = {10.5281/zenodo.11189337},
  url = {http://nime.org/proceedings/2023/nime2023_97.pdf},
  abstract = {The Kraakavera (a portmanteau of Kraakdoos—aka the Crackle box, and “calavera”—i.e., skull in Spanish) is an instrument that honours Michel Waisvisz’s memory by tributing one of his classic instruments—an exemplary of circuit bending that originated from STEIM in the 1960s. Inspired by the original design which used six metal contacts as inputs, I have used conductive paint to paint six pads on a ceramic skull which interact with the Kraakdoos circuit (using a uA709 IC). The skull depicts a sugar skull which is a traditional Mexican sweet that is often seen in altars to honour diseased relatives and loved ones during the Day of the Dead, but that is also consumed as a treat by children during these festivities. In this case, I have constructed an altar for Waisvisz, which doubles as an instrument, where the sugar skull—the centrepiece of the altar (below a picture of Waisvisz) serves both as traditional decoration but also the main point of contact with the instrument. Hence, the altar invites the musician to pay their respects by playing the instrument through the sugar skull. The Kraakavera also features a second mode which can be accessed by patching the skull’s inputs to another circuit which features a Trill Craft capacitive sensing board and a Bela board, which processes a secondary sound output consisting of a sample of a ceramic whistle running through a granular synthesizer patched in Pure Data (corresponding to the six pads on the skull). Lastly, the Kraakavera presents a syncretism of Mexican folklore and circuit bending traditions and a juxtaposition of classic and upcoming DMIs.},
  numpages = {2}
}

@inproceedings{nime2023_98,
  author = {Ranger Y Liu and Satchel Peterson and Richard T Lee and Mark Santolucito},
  title = {MaxPy: An open-source Python package for text-based generation of MaxMSP patches},
  pages = {646--649},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {98},
  track = {Demos},
  doi = {10.5281/zenodo.11189339},
  url = {http://nime.org/proceedings/2023/nime2023_98.pdf},
  abstract = {MaxMSP is a visual programming language for creating interactive audiovisual media that has found great success as a flexible and accessible option for computer music. However, the visual interface requires manual object placement and connection, which can be inefficient. Automated patch editing is possible either by visual programming with the [thispatcher] object or text-based programming with the [js] object. However, these objects cannot automatically create and save new patches, and they operate at run-time only, requiring live input to trigger patch construction.  There is no solution for automated creation of multiple patches at compile-time, such that the constructed patches do not contain their own constructors. To this end, we present MaxPy, an open-source Python package for programmatic construction and manipulation of MaxMSP patches. MaxPy replaces the manual actions of placing objects, connecting patchcords, and saving patch files with text-based Python functions, thus enabling dynamic, procedural, high-volume patch generation at compile-time. MaxPy also includes the ability to import existing patches, allowing users to move freely between text-based Python programming and visual programming with the Max GUI. MaxPy enables composers, programmers, and creators to explore expanded possibilities for complex, dynamic, and algorithmic patch construction through text-based Python programming of MaxMSP.},
  numpages = {4}
}

@inproceedings{nime2023_99,
  author = {Xavier Barriga-Abril and Andres Basantes},
  title = {Missing the hubbub: Memory and Identity in the Interactive Audio},
  pages = {650--652},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Miguel Ortiz and Adnan Marquez-Borbon},
  year = {2023},
  month = {May},
  address = {Mexico City, Mexico},
  issn = {2220-4806},
  articleno = {99},
  track = {Demos},
  doi = {10.5281/zenodo.11189341},
  url = {http://nime.org/proceedings/2023/nime2023_99.pdf},
  abstract = {In this demo we present an interactive object called Barahúnda Boba, that was developed through the exploration of Quito’s identity (a city placed in the mountains in Latin América). The product is an audio container system that reproduces environmental sounds of the city to preserve the memory of Quito. It was built after studying the Baroque’s concepts, as a period, extrapolated to the Baroque’s culture. The program that plays and stores the audio is written originally in JavaScript under the p5.js’s library. The object is a decorative product, handcrafted in pine wood. The components are assembled in an Arduino controller and they are embedded in the product. Although the object has a user interface, the product (just like the noise of the city) can not be fully controlled.},
  numpages = {3}
}
