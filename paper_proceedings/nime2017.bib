@inproceedings{rrooyen2017,
  author = {Robert Van Rooyen and Andrew Schloss and George Tzanetakis},
  title = {Voice Coil Actuators for Percussion Robotics},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176149},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0001.pdf},
  abstract = {Percussion robots have successfully used a variety of actuator technologies to activate a wide array of striking mechanisms. Popular types of actuators include solenoids and DC motors. However, the use of industrial strength voice coil actuators provides a compelling alternative given a desirable set of heterogeneous features and requirements that span traditional devices.  Their characteristics such as high acceleration and accurate positioning enable the exploration of rendering highly accurate and expressive percussion performances.}
}

@inproceedings{mdonneaud2017,
  author = {Maurin Donneaud and Cedric Honnet and Paul Strohmeier},
  title = {Designing a Multi-Touch eTextile for Music Performances},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176151},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0002.pdf},
  abstract = {We present a textile pressure sensor matrix, designed to be used as a musical multi-touch input device.  An evaluation of our design demonstrated that the sensors pressure response profile fits a logarithmic curve (R = 0.98). The input delay of the sensor is 2.1ms. The average absolute error in one direction of the sensor was measured to be less than 10% of one of the matrix's strips (M = 1.8mm, SD = 1.37mm). We intend this technology to be easy to use and implement by experts and novices alike: We ensure the ease of use by providing a host application that tracks touch points and passes these on as OSC or MIDI messages. We make our design easy to implement by providing open source software and hardware and by choosing evaluation methods that use accessible tools, making quantitative comparisons between different branches of the design easy. We chose to work with textile to take advantage of its tactile properties and its malleability of form and to pay tribute to textile's rich cultural heritage. }
}

@inproceedings{pwilliams2017,
  author = {Peter Williams and Daniel Overholt},
  title = {bEADS Extended Actuated Digital Shaker},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176153},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0003.pdf},
  abstract = {While there are a great variety of digital musical interfaces available to the working musician, few o
er the level of immediate, nuanced and instinctive control that one
nds in an acoustic shaker.  bEADS is a prototype of a digital musical instrument that utilises the gestural vocabulary associated with shaken idiophones and expands on the techniques and sonic possibilities associated with them.  By using a bespoke physically informed synthesis engine, in conjunction with accelerometer and pressure sensor data, an actuated handheld instrument has been built that allows for quickly switching between widely di
ering percussive sound textures. The prototype has been evaluated by three experts with di
erent levels of involvement in professional music making.}
}

@inproceedings{rmichon2017,
  author = {Romain Michon and Julius O. Smith and Matthew Wright and Chris Chafe and John Granzow and Ge Wang},
  title = {Passively Augmenting Mobile Devices Towards Hybrid Musical Instrument Design},
  pages = {19--24},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176155},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0004.pdf},
  abstract = {Mobile devices constitute a generic platform to make standalone musical instruments for live performance. However, they were not designed for such use and have multiple limitations when compared to other types of instruments.  We introduce a framework to quickly design and prototype passive mobile device augmentations to leverage existing features of the device for the end goal of mobile musical instruments.  An extended list of examples is provided and the results of a workshop, organized partly to evaluate our framework, are provided.}
}

@inproceedings{aeldridge2017,
  author = {Alice Eldridge and Chris Kiefer},
  title = {Self-resonating Feedback Cello: Interfacing gestural and generative processes in improvised performance},
  pages = {25--29},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176157},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0005.pdf},
  abstract = {The Feedback Cello is a new electroacoustic actuated instrument in which feedback can be induced independently on each string. Built from retro-fitted acoustic cellos, the signals from electromagnetic pickups sitting under each string are passed to a speaker built into the back of the instrument and to transducers clamped in varying places across the instrument body.  Placement of acoustic and mechanical actuators on the resonant body of the cello mean that this simple analogue feedback system is capable of a wide range of complex self-resonating behaviours. This paper describes the motivations for building these instruments as both a physical extension to live coding practice and an electroacoustic augmentation of cello. The design and physical construction is outlined, and modes of performance described with reference to the first six months of performances and installations. Future developments and planned investigations are outlined.}
}

@inproceedings{dhaddad2017,
  author = {Don Derek Haddad and Xiao Xiao and Tod Machover and Joseph Paradiso},
  title = {Fragile Instruments: Constructing Destructible Musical Interfaces},
  pages = {30--33},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176159},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0006.pdf},
  abstract = {We introduce a family of fragile electronic musical instruments designed to be "played" through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.  This paper presents several variations of Fragile Instruments we have created, discussing their circuit design as well as choices of material for the outer shell and tools of destruction. We conclude by considering other approaches to create intentionally destructible electronic musical instruments.  }
}

@inproceedings{fheller2017,
  author = {Florian Heller and Irene Meying Cheung Ruiz and Jan Borchers},
  title = {An Augmented Flute for Beginners},
  pages = {34--37},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176161},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0007.pdf},
  abstract = {Learning to play the transverse flute is not an easy task, at least not for everyone.  Since the flute does not have a reed to resonate, the player must provide a steady, focused stream of air that will cause the flute to resonate and thereby produce sound.  In order to achieve this, the player has to be aware of the embouchure position to generate an adequate air jet.  For a beginner, this can be a difficult task due to the lack of visual cues or indicators of the air jet and lips position.  This paper attempts to address this problem by presenting an augmented flute that can make the gestures related to the embouchure visible and measurable.  The augmented flute shows information about the area covered by the lower lip, estimates the lip hole shape based on noise analysis, and it shows graphically the air jet direction.  Additionally, the augmented flute provides directional and continuous feedback in real time, based on data acquired by experienced flutists.}
}

@inproceedings{gisaac2017,
  author = {Gabriella Isaac and Lauren Hayes and Todd Ingalls},
  title = {Cross-Modal Terrains: Navigating Sonic Space through Haptic Feedback},
  pages = {38--41},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176163},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0008.pdf},
  abstract = {This paper explores the idea of using virtual textural terrains as a means of generating haptic profiles for force-feedback controllers. This approach breaks from the paradigm established within audio-haptic research over the last few decades where physical models within virtual environments are designed to transduce gesture into sonic output.  We outline a method for generating multimodal terrains using basis functions, which are rendered into monochromatic visual representations for inspection. This visual terrain is traversed using a haptic controller, the NovInt Falcon, which in turn receives force information based on the grayscale value of its location in this virtual space. As the image is traversed by a performer the levels of resistance vary, and the image is realized as a physical terrain. We discuss the potential of this approach to afford engaging musical experiences for both the performer and the audience as iterated through numerous performances.}
}

@inproceedings{jwu2017,
  author = {Jiayue Wu and Mark Rau and Yun Zhang and Yijun Zhou and Matt Wright},
  title = {Towards Robust Tracking with an Unreliable Motion Sensor Using Machine Learning},
  pages = {42--47},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176165},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0009.pdf},
  abstract = {This paper presents solutions to improve reliability and to work around challenges of using a Leap Motion; sensor as a gestural control and input device in digital music instrument (DMI) design. We implement supervised learning algorithms (k-nearest neighbors, support vector machine, binary decision tree, and artificial neural network) to estimate hand motion data, which is not typically captured by the sensor. Two problems are addressed: 1) the sensor cannot detect overlapping hands 2) The sensor's limited detection range. Training examples included 7 kinds of overlapping hand gestures as well as hand trajectories where a hand goes out of the sensor's range. The overlapping gestures were treated as a classification problem and the best performing model was k-nearest neighbors with 62% accuracy. The out-of-range problem was treated first as a clustering problem to group the training examples into a small number of trajectory types, then as a classification problem to predict trajectory type based on the hand's motion before going out of range. The best performing model was k-nearest neighbors with an accuracy of 30%. The prediction models were implemented in an ongoing multimedia electroacoustic vocal performance and an educational project named Embodied Sonic Meditation (ESM).  }
}

@inproceedings{abarbosa2017,
  author = {Álvaro Barbosa and Thomas Tsang},
  title = {Sounding Architecture: Inter-Disciplinary Studio at HKU},
  pages = {48--51},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176167},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0010.pdf},
  abstract = {Sounding Architecture, is the first collaborative teaching development between Department of Architecture and Department of Music at the University of Hong Kong (HKU), introduced in Fall 2016. In this paper we present critical observations about the studio after a final public presentation of all projects. The Review was conducted with demonstrations by groups of students supervised by different Lecturer, in each case focusing on a different strategy to create a connection between Sound, Music, Acoustics, Space and Architectural Design. There was an assumption that the core working process would have to include the design of a new musical instrument, which in some cases became the final deliverable of the Studio and in other cases a step in a process that leads to a different outcome (such as an architectural Design, a performance or social experiment). One other relevant aspect was that Digital technology was used in the design and fabrication of the physical instruments' prototypes, but in very few cases, it was used in the actual generation or enhancement of sound, with the instruments relying almost exclusively in acoustic and mechanical sound.  }
}

@inproceedings{mlerner2017,
  author = {Matus Lerner, Martín},
  title = {Osiris: a liquid based digital musical instrument},
  pages = {52--55},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176169},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0011.pdf},
  abstract = {This paper describes the process of creation of a new digital musical instrument: Osiris. This device is based on the circulation of liquids for the generation of musical notes. Besides the system of liquid distribution, a module that generates MIDI events was designed and built based on the Arduino platform; such module is employed together with a Proteus 2000 sound generator.  The programming of the control module as well as the choice of sound-generating module had as their main objective that the instrument should provide an ample variety of sound and musical possibilities, controllable in real time.}
}

@inproceedings{sstasis2017,
  author = {Spyridon Stasis and Jason Hockman and Ryan Stables},
  title = {Navigating Descriptive Sub-Representations of Musical Timbre},
  pages = {56--61},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176171},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0012.pdf},
  abstract = {Musicians, audio engineers and producers often make use of common timbral adjectives to describe musical signals and transformations. However, the subjective nature of these terms, and the variability with respect to musical context often leads to inconsistencies in their definition. In this study, a model is proposed for controlling an equaliser by navigating clusters of datapoints, which represent grouped parameter settings with the same timbral description. The interface allows users to identify the nearest cluster to their current parameter setting and recommends changes based on its relationship to a cluster centroid. To do this, we apply dimensionality reduction to a dataset of equaliser curves described as warm and bright using a stacked autoencoder, then group the entries using an agglomerative clustering algorithm with a coherence based distance criterion. To test the efficacy of the system, we implement listening tests and show that subjects are able to match datapoints to their respective sub-representations with 93.75% mean accuracy.}
}

@inproceedings{pwilliams:2017a,
  author = {Peter Williams and Daniel Overholt},
  title = {Pitch Fork: A Novel tactile Digital Musical Instrument},
  pages = {62--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176173},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0013.pdf},
  abstract = {Pitch Fork is a prototype of an alternate, actuated digital musical instrument (DMI). It uses 5 infra-red and 4 piezoelectric sensors to control an additive synthesis engine. Iron bars are used as the physical point of contact in interaction with the aim of using material computation to control aspects of the digitally produced sound. This choice of material was also chosen to affect player experience.  Sensor readings are relayed to a Macbook via an Arduino Mega. Mappings and audio output signal is carried out with Pure Data Extended.}
}

@inproceedings{cerdem2017,
  author = {Cagri Erdem and Anil Camci and Angus Forbes},
  title = {Biostomp: A Biocontrol System for Embodied Performance Using Mechanomyography},
  pages = {65--70},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176175},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0014.pdf},
  abstract = {Biostomp is a new musical interface that relies on the use mechanomyography (MMG) as a biocontrol mechanism in live performance situations.  Designed in the form of a stomp box, Biostomp translates a performer's muscle movements into control signals. A custom MMG sensor captures the acoustic output of muscle tissue oscillations resulting from contractions. An analog circuit amplifies and filters these signals, and a micro-controller translates the processed signals into pulses. These pulses are used to activate a stepper motor mechanism, which is designed to be mounted on parameter knobs on effects pedals.  The primary goal in designing Biostomp is to offer a robust, inexpensive, and easy-to-operate platform for integrating biological signals into both traditional and contemporary music performance practices without requiring an intermediary computer software. In this paper, we discuss the design, implementation and evaluation of Biostomp. Following an overview of related work on the use of biological signals in artistic projects, we offer a discussion of our approach to conceptualizing and fabricating a biocontrol mechanism as a new musical interface. We then discuss the results of an evaluation study conducted with 21 professional musicians. A video abstract for Biostomp can be viewed at vimeo.com/biostomp/video.}
}

@inproceedings{eknudsen2017,
  author = {Esben W. Knudsen and Malte L. Hølledig and Mads Juel Nielsen and Rikke K. Petersen and Sebastian Bach-Nielsen and Bogdan-Constantin Zanescu and Daniel Overholt and Hendrik Purwins and Kim Helweg},
  title = {Audio-Visual Feedback for Self-monitoring Posture in Ballet Training},
  pages = {71--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181422},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0015.pdf},
  abstract = {An application for ballet training is presented that monitors the posture position (straightness of the spine and rotation of the pelvis) deviation from the ideal position in real-time. The human skeletal data is acquired through a Microsoft Kinect v2.  The movement of the student is mirrored through an abstract skeletal figure and instructions are provided through a virtual teacher.  Posture deviation is measured in the following way: Torso misalignment is calculated by comparing hip center joint, shoulder center joint and neck joint position with an ideal posture position retrieved in an initial calibration procedure. Pelvis deviation is expressed as the xz-rotation of the hip-center joint. The posture deviation is sonified via a varying cut-off frequency of a high-pass filter applied to floating water sound. The posture deviation is visualized via a curve and a rigged skeleton in which the misaligned torso parts are color-coded. In an experiment with 9-12 year-old dance students from a ballet school, comparing the audio-visual feedback modality with no feedback leads to an increase in posture accuracy (p &lt; 0.001, Cohen's d = 1.047). Reaction card feedback and expert interviews indicate that the feedback is considered fun and useful for training independently from the teacher.}
}

@inproceedings{rlindell2017,
  author = {Rikard Lindell and Tomas Kumlin},
  title = {Augmented Embodied Performance – Extended Artistic Room, Enacted Teacher, and Humanisation of Technology},
  pages = {77--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176177},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0016.pdf},
  abstract = {We explore the phenomenology of embodiment based on research through design and reflection on the design of artefacts for augmenting embodied performance. We present three designs for highly trained musicians; the designs rely on the musicians' mastery acquired from years of practice. Through the knowledge of the living body their instruments &#8211; saxophone, cello, and flute &#8211; are extensions of themselves; thus, we can explore technology with rich nuances and precision in corporeal schemas. With the help of Merleau-Ponty's phenomenology of embodiment we present three hypotheses for augmented embodied performance: the extended artistic room, the interactively enacted teacher, and the humanisation of technology.  }
}

@inproceedings{jvetter2017,
  author = {Jens Vetter and Sarah Leimcke},
  title = {Homo Restis --- Constructive Control Through Modular String Topologies},
  pages = {83--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176179},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0017.pdf},
  abstract = {In this paper we discuss a modular instrument system for musical expression consisting of multiple devices using string detection, sound synthesis and wireless communication.  The design of the system allows for different physical arrangements, which we define as topologies.  In particular we will explain our concept and requirements, the system architecture including custom magnetic string sensors and our network communication and discuss its use in the performance HOMO RESTIS.}
}

@inproceedings{jbarbosa2017,
  author = {Jeronimo Barbosa and Marcelo M. Wanderley and Stéphane Huot},
  title = {Exploring Playfulness in NIME Design: The Case of Live Looping Tools},
  pages = {87--92},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176181},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0018.pdf},
  abstract = {Play and playfulness compose an essential part of our lives as human beings. From childhood to adultness, playfulness is often associated with remarkable positive experiences related to fun, pleasure, intimate social activities, imagination, and creativity. Perhaps not surprisingly, playfulness has been recurrently used in NIME designs as a strategy to engage people, often non-expert, in short term musical activities. Yet, designing for playfulness remains a challenging task, as little knowledge is available for designers to support their decisions.  To address this issue, we follow a design rationale approach using the context of Live Looping (LL) as a case study. We start by surveying 101 LL tools, summarizing our analysis into a new design space. We then use this design space to discuss potential guidelines to address playfulness in a design process. These guidelines are implemented and discussed in a new LL tool&#8212;called the "Voice Reaping Machine". Finally, we contrast our guidelines with previous works in the literature.}
}

@inproceedings{dmanesh2017,
  author = {Daniel Manesh and Eran Egozy},
  title = {Exquisite Score: A System for Collaborative Musical Composition},
  pages = {93--98},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176183},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0019.pdf},
  abstract = {Exquisite Score is a web application which allows users to collaborate on short musical compositions using the paradigm of the parlor game exquisite corpse. Through a MIDI-sequencer interface, composers each contribute a section to a piece of music, only seeing the very end of the preceding section.  Exquisite Score is both a fun and accessible compositional game as well as a system for encouraging highly novel musical compositions. Exquisite Score was tested by several students and musicians. Several short pieces were created and a brief discussion and analysis of these pieces is included.}
}

@inproceedings{sstenslie2017,
  author = {Stahl Stenslie and Kjell Tore Innervik and Ivar Frounberg and Thom Johansen},
  title = {Somatic Sound in Performative Contexts},
  pages = {99--103},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176185},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0020.pdf},
  abstract = {This paper presents a new spherical shaped capacitive sensor device for creating interactive compositions and embodied user experiences inside of a periphonic, 3D sound space. The Somatic Sound project is here presented as a) technological innovative musical instrument, and b) an experiential art installation. One of the main research foci is to explore embodied experiences through moving, interactive and somatic sound. The term somatic is here understood and used as in relating to the body in a physical, holistic and immersive manner.}
}

@inproceedings{jlarsen2017,
  author = {Jeppe Veirum Larsen and Hendrik Knoche},
  title = {States and Sound: Modelling Interactions with Musical User Interfaces},
  pages = {104--109},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176187},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0021.pdf},
  abstract = {Musical instruments and musical user interfaces provide rich input and feedback through mostly tangible interactions, resulting in complex behavior.  However, publications of novel interfaces often lack the required detail due to the complexity or the focus on a specific part of the interfaces and absence of a specific template or structure to describe these interactions. Drawing on and synthesizing models from interaction design and music making we propose a way for modeling musical interfaces by providing a scheme and visual language to describe, design, analyze, and compare interfaces for music making. To illustrate its capabilities we apply the proposed model to a range of assistive musical instruments, which often draw on multi-modal in- and output, resulting in complex designs and descriptions thereof.}
}

@inproceedings{gxia2017,
  author = {Guangyu Xia and Roger Dannenberg},
  title = {Improvised Duet Interaction: Learning Improvisation Techniques for Automatic Accompaniment},
  pages = {110--114},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176189},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0022.pdf},
  abstract = {The interaction between music improvisers is studied in the context of piano duets, where one improviser embellishes a melody, and the other plays a chordal accompaniment with great freedom. We created an automated accompaniment player that learns to play from example performances.  Accompaniments are constructed by selecting and concatenating one-measure score units from actual performances. An important innovation is the ability to learn how the improvised accompaniment should respond to variations in the melody performance, using tempo and embellishment complexity as features, resulting in a truly interactive performance within a conventional musical framework. We conducted both objective and subjective evaluations, showing that the learned improviser performs more interactive, musical, and human-like accompaniment compared with the less responsive, rule-based baseline algorithm.}
}

@inproceedings{pdahlstedt2017,
  author = {Palle Dahlstedt},
  title = {Physical Interactions with Digital Strings --- A hybrid approach to a digital keyboard instrument},
  pages = {115--120},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176191},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0023.pdf},
  abstract = {A new hybrid approach to digital keyboard playing is presented, where the actual acoustic sounds from a digital keyboard are captured with contact microphones and applied as excitation signals to a digital model of a prepared piano, i.e., an extended wave-guide model of strings with the possibility of stopping and muting the strings at arbitrary positions. The parameters of the string model are controlled through TouchKeys multitouch sensors on each key, combined with MIDI data and acoustic signals from the digital keyboard frame, using a novel mapping.  The instrument is evaluated from a performing musician's perspective, and emerging playing techniques are discussed. Since the instrument is a hybrid acoustic-digital system with several feedback paths between the domains, it provides for expressive and dynamic playing, with qualities approaching that of an acoustic instrument, yet with new kinds of control.  The contributions are two-fold. First, the use of acoustic sounds from a physical keyboard for excitations and resonances results in a novel hybrid keyboard instrument in itself. Second, the digital model of "inside piano" playing, using multitouch keyboard data, allows for performance techniques going far beyond conventional keyboard playing.}
}

@inproceedings{croberts2017,
  author = {Charles Roberts and Graham Wakefield},
  title = {gibberwocky: New Live-Coding Instruments for Musical Performance},
  pages = {121--126},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176193},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0024.pdf},
  abstract = {We describe two new versions of the gibberwocky live-coding system. One integrates with Max/MSP while the second targets MIDI output and runs entirely in the browser. We discuss commonalities and differences between the three environments, and how they fit into the live-coding landscape. We also describe lessons learned while performing with the original version of gibberwocky, both from our perspective and the perspective of others. These lessons informed the addition of animated sparkline visualizations depicting modulations to performers and audiences in all three versions.}
}

@inproceedings{sleitman2017,
  author = {Sasha Leitman},
  title = {Current Iteration of a Course on Physical Interaction Design for Music},
  pages = {127--132},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176197},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0025.pdf},
  abstract = {This paper is an overview of the current state of a course on New Interfaces for Musical Expression taught at Stanford University. It gives an overview of the various technologies and methodologies used to teach the interdisciplinary work of new musical interfaces.}
}

@inproceedings{ahofmann2017,
  author = {Alex Hofmann and Bernt Isak Waerstad and Saranya Balasubramanian and Kristoffer E. Koch},
  title = {From interface design to the software instrument --- Mapping as an approach to FX-instrument building},
  pages = {133--138},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176199},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0026.pdf},
  abstract = {To build electronic musical instruments, a mapping between the real-time audio processing software and the physical controllers is required.  Different strategies of mapping were developed and discussed within the NIME community to improve musical expression in live performances. This paper discusses an interface focussed instrument design approach, which starts from the physical controller and its functionality. From this definition, the required, underlying software instrument is derived. A proof of concept is implemented as a framework for effect instruments. This framework comprises a library of real-time effects for Csound, a proposition for a JSON-based mapping format, and a mapping-to-instrument converter that outputs Csound instrument files. Advantages, limitations and possible future extensions are discussed.}
}

@inproceedings{mmarchini2017,
  author = {Marco Marchini and François Pachet and Benoît Carré},
  title = {Rethinking Reflexive Looper for structured pop music},
  pages = {139--144},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176201},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0027.pdf},
  abstract = {Reflexive Looper (RL) is a live-looping system which allows a solo musician to incarnate the different roles of a whole rhythm section by looping rhythms, chord progressions, bassline and more. The loop pedal, is still the most used device for those types of performances, accounting for many of the cover songs performances on youtube, but not all kinds of song apply.  Unlike a common loop pedal, each layer of sound in RL is produced by an intelligent looping-agent which adapts to the musician and respects given constraints, using constrained optimization.  In its original form, RL worked well for jazz guitar improvisation but was unsuited to structured music such as pop songs. In order to bring the system on pop stage, we revisited the system interaction, following the guidelines of professional users who tested it extensively. We describe the revisited system which can accommodate both pop and jazz. Thanks to intuitive pedal interaction and structure-constraints, the new RL deals with pop music and has been already used in several in live concert situations.}
}

@inproceedings{vzappi2017,
  author = {Victor Zappi and Andrew Allen and Sidney Fels},
  title = {Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments},
  pages = {145--150},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176203},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0028.pdf},
  abstract = {Physical modelling is a sophisticated synthesis technique, often used in the design of Digital Musical Instruments (DMIs). Some of the most precise physical simulations of sound propagation are based on Finite-Difference Time-Domain (FDTD) methods, which are stable, highly parameterizable but characterized by an extremely heavy computational load. This drawback hinders the spread of FDTD from the domain of off-line simulations to the one of DMIs. With this paper, we present a novel approach to real-time physical modelling synthesis, which implements a 2D FDTD solver as a shader program running on the GPU directly within the graphics pipeline. The result is a system capable of running fully interactive, massively sized simulation domains, suitable for novel DMI design. With the help of diagrams and code snippets, we provide the implementation details of a first interactive application, a drum head simulator whose source code is available online. Finally, we evaluate the proposed system, showing how this new approach can work as a valuable alternative to classic GPGPU modelling.}
}

@inproceedings{djohnson2017,
  author = {David Johnson and George Tzanetakis},
  title = {VRMin: Using Mixed Reality to Augment the Theremin for Musical Tutoring},
  pages = {151--156},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176205},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0029.pdf},
  abstract = {The recent resurgence of Virtual Reality (VR) technologies provide new platforms for augmenting traditional music instruments. Instrument augmentation is a common approach for designing new interfaces for musical expression, as shown through hyperinstrument research.  New visual affordances present in VR give designers new methods for augmenting instruments to extend not only their expressivity, but also their capabilities for computer assisted tutoring. In this work, we present VRMin, a mobile Mixed Reality (MR) application for augmenting a physical theremin, with an immersive virtual environment (VE), for real time computer assisted tutoring. We augment a physical theremin with 3D visual cues to indicate correct hand positioning for performing given notes and volumes.  The physical theremin acts as a domain specific controller for the resulting MR environment.  The initial effectiveness of this approach is measured by analyzing a performer's hand position while training with and without the VRMin. We also evaluate the usability of the interface using heuristic evaluation based on a newly proposed set of guidelines designed for VR musical environments.}
}

@inproceedings{rgraham2017,
  author = {Richard Graham and Brian Bridges and Christopher Manzione and William Brent},
  title = {Exploring Pitch and Timbre through 3D Spaces: Embodied Models in Virtual Reality as a Basis for Performance Systems Design},
  pages = {157--162},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176207},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0030.pdf},
  abstract = {Our paper builds on an ongoing collaboration between theorists and practitioners within the computer music community, with a specific focus on three-dimensional environments as an incubator for performance systems design. In particular, we are concerned with how to provide accessible means of controlling spatialization and timbral shaping in an integrated manner through the collection of performance data from various modalities from an electric guitar with a multichannel audio output. This paper will focus specifically on the combination of pitch data treated within tonal models and the detection of physical performance gestures using timbral feature extraction algorithms. We discuss how these tracked gestures may be connected to concepts and dynamic relationships from embodied cognition, expanding on performative models for pitch and timbre spaces. Finally, we explore how these ideas support connections between sonic, formal and performative dimensions. This includes instrumental technique detection scenes and mapping strategies aimed at bridging music performance gestures across physical and conceptual planes. }
}

@inproceedings{mgurevich2017,
  author = {Michael Gurevich},
  title = {Discovering Instruments in Scores: A Repertoire-Driven Approach to Designing New Interfaces for Musical Expression},
  pages = {163--168},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176209},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0031.pdf},
  abstract = {This paper situates NIME practice with respect to models of social interaction among human agents. It argues that the conventional model of composer-performer-listener, and the underlying mid-20th century metaphor of music as communication upon which it relies, cannot reflect the richness of interaction and possibility afforded by interactive digital technologies.  Building on Paul Lansky's vision of an expanded and dynamic social network, an alternative, ecological view of music-making is presented, in which meaning emerges not from "messages" communicated between individuals, but instead from the "noise" that arises through the uncertainty in their interactions. However, in our tendency in NIME to collapse the various roles in this network into a single individual, we place the increased potential afforded by digital systems at risk. Using examples from the author's NIME practices, the paper uses a practice-based methodology to describe approaches to designing instruments that respond to the technologies that form the interfaces of the network, which can include scores and stylistic conventions. In doing so, the paper demonstrates that a repertoire&#8212;a seemingly anachronistic concept&#8212;and a corresponding repertoire-driven approach to creating NIMEs can in fact be a catalyst for invention and creativity.}
}

@inproceedings{jcantrell2017,
  author = {Joe Cantrell},
  title = {Designing Intent: Defining Critical Meaning for NIME Practitioners},
  pages = {169--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176211},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0032.pdf},
  abstract = {The ideation, conception and implementation of new musical interfaces and instruments provide more than the mere construction of digital objects. As physical and digital assemblages, interfaces also act as traces of the authoring entities that created them. Their intentions, likes, dislikes, and ultimate determinations of what is creatively useful all get embedded into the available choices of the interface. In this light, the self-perception of the musical HCI and instrument designer can be seen as occupying a primary importance in the instruments and interfaces that eventually come to be created. The work of a designer who self-identifies as an artist may result in a vastly different outcome than one who considers him or herself to be an entrepreneur, or a scientist, for example. These differing definitions of self as well as their HCI outcomes require their own means of critique, understanding and expectations. All too often, these definitions are unclear, or the considerations of overlapping means of critique remain unexamined.}
}

@inproceedings{jvasquez2017,
  author = {Juan Vasquez and Koray Tahiroğlu and Johan Kildal},
  title = {Idiomatic Composition Practices for New Musical Instruments: Context, Background and Current Applications},
  pages = {174--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181424},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0033.pdf},
  abstract = {One of the reasons of why some musical instruments more successfully continue their evolution and actively take part in the history of music is partially attributed to the existing compositions made specifically for them, pieces that remain and are still played over a long period of time. This is something we know, performing these compositions keeps the characteristics of the instruments alive and able to survive. This paper presents our contribution to this discussion with a context and historical background for idiomatic compositions. Looking beyond the classical era, we discuss how the concept of idiomatic music has influenced research and composition practices in the NIME community; drawing more attention in the way current idiomatic composition practices considered specific NIME affordances for sonic, social and spatial interaction. We present particular projects that establish idiomatic writing as a part of a new repertoire for new musical instruments. The idiomatic writing approach to composing music for NIME can shift the unique characteristics of new instruments to a more established musical identity, providing a shared understanding and a common literature to the community.}
}

@inproceedings{fberthaut2017,
  author = {Florent Berthaut and Cagan Arslan and Laurent Grisoni},
  title = {Revgest: Augmenting Gestural Musical Instruments with Revealed Virtual Objects},
  pages = {180--185},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176213},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0034.pdf},
  abstract = {Gestural interfaces, which make use of physiological signals, hand / body postures or movements, have become widespread for musical expression.  While they may increase the transparency and expressiveness of instruments, they may also result in limited agency, for musicians as well as for spectators. This problem becomes especially true when the implemented mappings between gesture and music are subtle or complex. These instruments may also restrict the appropriation possibilities of controls, by comparison to physical interfaces.  Most existing solutions to these issues are based on distant and/or limited visual feedback (LEDs, small screens).  Our approach is to augment the gestures themselves with revealed virtual objects.  Our contributions are, first a novel approach of visual feedback that allow for additional expressiveness, second a software pipeline for pixel-level feedback and control that ensures tight coupling between sound and visuals, and third, a design space for extending gestural control using revealed interfaces. We also demonstrate and evaluate our approach with the augmentation of three existing gestural musical instruments.}
}

@inproceedings{atroyer2017,
  author = {Akito van Troyer},
  title = {MM-RT: A Tabletop Musical Instrument for Musical Wonderers},
  pages = {186--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176215},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0035.pdf},
  abstract = {MM-RT (material and magnet --- rhythm and timbre) is a tabletop musical instrument equipped with electromagnetic actuators to offer a new paradigm of musical expression and exploration. After expanding on prior work with electromagnetic instrument actuation and tabletop musical interfaces, the paper explains why and how MM-RT, through its physicality and ergonomics, has been designed specifically for musical wonderers: people who want to know more about music in installation, concert, and everyday contexts. Those wonderers aspire to interpret and explore music rather than focussing on a technically correct realization of music. Informed by this vision, we then describe the design and technical implementation of this tabletop musical instrument. The paper concludes with discussions about future works and how to trigger musical wonderers' sonic curiosity to encounter, explore, invent, and organize sounds for music creation using a musical instrument like MM-RT.}
}

@inproceedings{fmorreale2017,
  author = {Fabio Morreale and Andrew McPherson},
  title = {Design for Longevity: Ongoing Use of Instruments from NIME 2010-14},
  pages = {192--197},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176218},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0036.pdf},
  abstract = {Every new edition of NIME brings dozens of new DMIs and the feeling that only a few of them will eventually break through. Previous work tried to address this issue with a deductive approach by formulating design frameworks; we addressed this issue with a inductive approach by elaborating on successes and failures of previous DMIs. We contacted 97 DMI makers that presented a new instrument at five successive editions of NIME (2010-2014); 70 answered. They were asked to indicate the original motivation for designing the DMI and to present information about its uptake. Results confirmed that most of the instruments have difficulties establishing themselves. Also, they were asked to reflect on the specific factors that facilitated and those that hindered instrument longevity. By grounding these reflections on existing reserach on NIME and HCI, we propose a series of design considerations for future DMIs.  }
}

@inproceedings{sdelalez2017,
  author = {Samuel Delalez and Christophe d'Alessandro},
  title = {Vokinesis: Syllabic Control Points for Performative Singing Synthesis},
  pages = {198--203},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176220},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0037.pdf},
  abstract = {Performative control of voice is the process of real-time speech synthesis or modification by the means of hands or feet gestures. Vokinesis, a system for real-time rhythm and pitch modification and control of singing is presented.  Pitch and vocal effort are controlled by a stylus on a graphic tablet. The concept of Syllabic Control Points (SCP) is introduced for timing and rhythm control.  A chain of phonetic syllables have two types of temporal phases : the steady phases, which correspond to the vocalic nuclei, and the transient phases, which correspond to the attacks and/or codas. Thus, syllabic rhythm control methods need transient and steady phases control points, corresponding to the ancient concept of the arsis and thesis is prosodic theory. SCP allow for accurate control of articulation, using hand or feet. In the \emph{Tap mode}, SCP are triggered by pressing and releasing a control button. In the \emph{Fader mode}, continuous variation of the SCP sequencing rate is controlled with expression pedals.  Vokinesis has been tested successfully in musical performances, using both syllabic rhythm control modes. This system opens new musical possibilities, and can be extended to other types of sounds beyond voice. }
}

@inproceedings{gyoung2017,
  author = {Gareth Young and Dave Murphy and Jeffrey Weeter},
  title = {A Qualitative Analysis of Haptic Feedback in Music Focused Exercises},
  pages = {204--209},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176222},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0038.pdf},
  abstract = {We present the findings of a pilot-study that analysed the role of haptic feedback in a musical context. To examine the role of haptics in Digital Musical Instrument (DMI) design an experiment was formulated to measure the users' perception of device usability across four separate feedback stages: fully haptic (force and tactile combined), constant force only, vibrotactile only, and no feedback. The study was piloted over extended periods with the intention of exploring the application and integration of DMIs in real-world musical contexts. Applying a music orientated analysis of this type enabled the investigative process to not only take place over a comprehensive period, but allowed for the exploration of DMI integration in everyday compositional practices. As with any investigation that involves creativity, it was important that the participants did not feel rushed or restricted. That is, they were given sufficient time to explore and assess the different feedback types without constraint. This provided an accurate and representational set of qualitative data for validating the participants' experience with the different feedback types they were presented with.}
}

@inproceedings{jhe2017,
  author = {Jingyin He and Jim Murphy and Dale A. Carnegie and Ajay Kapur},
  title = {Towards Related-Dedicated Input Devices for Parametrically Rich Mechatronic Musical Instruments},
  pages = {210--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176224},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0039.pdf},
  abstract = {In the recent years, mechatronic musical instruments (MMI) have become increasingly parametrically rich. Researchers have developed different interaction strategies to negotiate the challenge of interfacing with each of the MMI's high-resolution parameters in real time. While mapping strategies hold an important aspect of the musical interaction paradigm for MMI, attention on dedicated input devices to perform these instruments live should not be neglected. This paper presents the findings of a user study conducted with participants possessing specialized musicianship skills for MMI music performance and composition. Study participants are given three musical tasks to complete using a mechatronic chordophone with high dimensionality of control via different musical input interfaces (one input device at a time). This representative user study reveals the features of related-dedicated input controllers, how they compare against the typical MIDI keyboard/sequencer paradigm in human-MMI interaction, and provide an indication of the musical function that expert users prefer for each input interface.}
}

@inproceedings{ablatherwick2017,
  author = {Asha Blatherwick and Luke Woodbury and Tom Davis},
  title = {Design Considerations for Instruments for Users with Complex Needs in SEN Settings},
  pages = {216--221},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176226},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0040.pdf},
  abstract = {Music technology can provide unique opportunities to allow access to music making for those with complex needs in special educational needs (SEN) settings. Whilst there is a growing trend of research in this area, technology has been shown to face a variety of issues leading to underuse in this context.  This paper reviews issues raised in literature and in practice for the use of music technology in SEN settings. The paper then reviews existing principles and frameworks for designing digital musical instruments (DMIs.) The reviews of literature and current frameworks are then used to inform a set of design considerations for instruments for users with complex needs, and in SEN settings.  18 design considerations are presented with connections to literature and practice. An implementation example including future work is presented, and finally a conclusion is then offered.  }
}

@inproceedings{ahindle2017,
  author = {Abram Hindle and Daryl Posnett},
  title = {Performance with an Electronically Excited Didgeridoo},
  pages = {222--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176228},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0041.pdf},
  abstract = {The didgeridoo is a wind instrument composed of a single large tube often used as drone instrument for backing up the mids and lows of an ensemble. A didgeridoo is played by buzzing the lips and blowing air into the didgeridoo. To play a didgeridoo continously one can employ circular breathing but the volume of air required poses a real challenge to novice players. In this paper we replace the expense of circular breathing and lip buzzing with electronic excitation, thus creating an electro-acoustic didgeridoo or electronic didgeridoo. Thus we describe the didgeridoo excitation signal, how to replicate it, and the hardware necessary to make an electro-acoustic didgeridoo driven by speakers and controllable from a computer. To properly drive the didgeridoo we rely upon 4th-order ported bandpass speaker boxes to help guide our excitation signals into an attached acoustic didgeridoo. The results somewhat replicate human didgeridoo playing, enabling a new kind of mid to low electro-acoustic accompaniment without the need for circular breathing.  }
}

@inproceedings{mzbyszynski2017,
  author = {Michael Zbyszyński and Mick Grierson and Matthew Yee-King},
  title = {Rapid Prototyping of New Instruments with CodeCircle},
  pages = {227--230},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181420},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0042.pdf},
  abstract = {Our research examines the use of CodeCircle, an online, collaborative HTML, CSS, and JavaScript editor, as a rapid prototyping environment for musically expressive instruments. In CodeCircle, we use two primary libraries: MaxiLib and RapidLib. MaxiLib is a synthesis and sample processing library, ported from the C++ library Maximillian, which interfaces with the Web Audio API for sound generation in the browser. RapidLib is a product of the Rapid-Mix project, and allows users to implement interactive machine learning, using "programming by demonstration" to design new expressive interactions.}
}

@inproceedings{fvisi2017,
  author = {Federico Visi and Baptiste Caramiaux and Michael Mcloughlin and Eduardo Miranda},
  title = {A Knowledge-based, Data-driven Method for Action-sound Mapping},
  pages = {231--236},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176230},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0043.pdf},
  abstract = {This paper presents a knowledge-based, data-driven method for using data describing action-sound couplings collected from a group of people to generate multiple complex mappings between the performance movements of a musician and sound synthesis.  This is done by using a database of multimodal motion data collected from multiple subjects coupled with sound synthesis parameters.  A series of sound stimuli is synthesised using the sound engine that will be used in performance. Multimodal motion data is collected by asking each participant to listen to each sound stimulus and move as if they were producing the sound using a musical instrument they are given.  Multimodal data is recorded during each performance, and paired with the synthesis parameters used for generating the sound stimulus.  The dataset created using this method is then used to build a topological representation of the performance movements of the subjects. This representation is then used to interactively generate training data for machine learning algorithms, and define mappings for real-time performance.  To better illustrate each step of the procedure, we describe an implementation involving clarinet, motion capture, wearable sensor armbands, and waveguide synthesis.}
}

@inproceedings{ssalazar2017,
  author = {Spencer Salazar and Mark Cerqueira},
  title = {ChuckPad: Social Coding for Computer Music},
  pages = {237--240},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176232},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0044.pdf},
  abstract = {ChuckPad is a network-based platform for sharing code, modules, patches, and even entire musical works written on the ChucK programming language and other music programming platforms. ChuckPad provides a single repository and record of musical code from supported musical programming systems, an interface for organizing, browsing, and searching this body of code, and a readily accessible means of evaluating the musical output of code in the repository.  ChuckPad consists of an open-source modular backend service to be run on a network server or cloud infrastructure and a client library to facilitate integrating end-user applications with the platform. While ChuckPad has been initially developed for sharing ChucK source code, its design can accommodate any type of music programming system oriented around small text- or binary-format documents. To this end, ChuckPad has also been extended to the Auraglyph handwriting-based graphical music programming system.}
}

@inproceedings{aberndt2017,
  author = {Axel Berndt and Simon Waloschek and Aristotelis Hadjakos and Alexander Leemhuis},
  title = {AmbiDice: An Ambient Music Interface for Tabletop Role-Playing Games},
  pages = {241--244},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176234},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0045.pdf},
  abstract = {Tabletop role-playing games are a collaborative narrative experience. Throughout gaming sessions, Ambient music and noises are frequently used to enrich and facilitate the narration. With AmbiDice we introduce a tangible interface and music generator specially devised for this application scenario. We detail the technical implementation of the device, the software architecture of the music system (AmbientMusicBox) and the scripting language to compose Ambient music and soundscapes. AmbiDice was presented to experienced players and gained positive feedback and constructive suggestions for further development.}
}

@inproceedings{sferguson2017,
  author = {Sam Ferguson and Anthony Rowe and Oliver Bown and Liam Birtles and Chris Bennewith},
  title = {Sound Design for a System of 1000 Distributed Independent Audio-Visual Devices},
  pages = {245--250},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176236},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0046.pdf},
  abstract = {This paper describes the sound design for Bloom, a light and sound installation made up of 1000 distributed independent audio-visual pixel devices, each with RGB LEDs, Wifi, Accelerometer, GPS sensor, and sound hardware. These types of systems have been explored previously, but only a few systems have exceeded 30-50 devices and very few have included sound capability, and therefore the sound design possibilities for large systems of distributed audio devices are not yet well understood. In this article we describe the hardware and software implementation of sound synthesis for this system, and the implications for design of media for this context.}
}

@inproceedings{rvogl2017,
  author = {Richard Vogl and Peter Knees},
  title = {An Intelligent Drum Machine for Electronic Dance Music Production and Performance},
  pages = {251--256},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176238},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0047.pdf},
  abstract = {An important part of electronic dance music (EDM) is the so-called beat.  It is defined by the drum track of the piece and is a style defining element.  While producing EDM, creating the drum track tends to be delicate, yet labor intensive work.  In this work we present a touch-interface-based prototype with the goal to simplify this task.  The prototype aims at supporting musicians to create rhythmic patterns in the context of EDM production and live performances.  Starting with a seed pattern which is provided by the user, a list of variations with varying degree of deviation from the seed pattern is generated.  The interface provides simple ways to enter, edit, visualize and browse through the patterns.  Variations are generated by means of an artificial neural network which is trained on a database of drum rhythm patterns extracted from a commercial drum loop library.  To evaluate the user interface and pattern generation quality a user study with experts in EDM production was conducted.  It was found that participants responded positively to the user interface and the quality of the generated patterns.  Furthermore, the experts consider the prototype helpful for both studio production situations and live performances.}
}

@inproceedings{mjensen2017,
  author = {Martin Snejbjerg Jensen and Ole Adrian Heggli and Patricia Alves Da Mota and Peter Vuust},
  title = {A low-cost MRI compatible keyboard},
  pages = {257--260},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176240},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0048.pdf},
  abstract = {Neuroimaging is a powerful tool to explore how and why humans engage in music. Magnetic resonance imaging (MRI) has allowed us to identify brain networks and regions implicated in a range of cognitive tasks including music perception and performance. However, MRI-scanners are noisy and cramped, presenting a challenging environment for playing an instrument. Here, we present an MRI-compatible polyphonic keyboard with a materials cost of 850 USD, designed and tested for safe use in 3T (three Tesla) MRI-scanners. We describe design considerations, and prior work in the field. In addition, we provide recommendations for future designs and comment on the possibility of using the keyboard in magnetoencephalography (MEG) systems. Preliminary results indicate a comfortable playing experience with no disturbance of the imaging process.}
}

@inproceedings{slee2017,
  author = {Sang Won Lee and Jungho Bang and Georg Essl},
  title = {Live Coding YouTube: Organizing Streaming Media for an Audiovisual Performance},
  pages = {261--266},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176242},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0049.pdf},
  abstract = {Music listening has changed greatly with the emergence of music streaming services, such as Spotify or YouTube. In this paper, we discuss an artistic practice that organizes streaming videos to perform a real-time improvisation via live coding. A live coder uses any available video from YouTube, a video streaming service, as source material to perform an improvised audiovisual piece. The challenge is to manipulate the emerging media that are streamed from a networked service. The musical gesture can be limited due to the provided functionalities of the YouTube API. However, the potential sonic and visual space that a musician can explore is practically infinite. The practice embraces the juxtaposition of manipulating emerging media in old-fashioned ways similar to experimental musicians in the 60's physically manipulating tape loops or scratching vinyl records on a phonograph while exploring the possibility of doing so by drawing on the gigantic repository of all kinds of videos. In this paper, we discuss the challenges of using streaming videos from the platform as musical materials in computer music and introduce a live coding environment that we developed for real-time improvisation.  }
}

@inproceedings{skiratli2017,
  author = {Solen Kiratli and Akshay Cadambi and Yon Visell},
  title = {HIVE: An Interactive Sculpture for Musical Expression},
  pages = {267--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176244},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0050.pdf},
  abstract = {In this paper we present HIVE, a parametrically designed interactive sound sculpture with embedded multi-channel digital audio which explores the intersection of sculptural form and musical instrument design. We examine sculpture as an integral part of music composition and performance, expanding the definition of musical instrument to include the gestalt of loudspeakers, architectural spaces, and material form. After examining some related works, we frame HIVE as an interactive sculpture for musical expression.  We then describe our design and production process, which hinges on the relationship between sound, space, and sculptural form. Finally, we discuss the installation and its implications.}
}

@inproceedings{mblessing2017,
  author = {Matthew Blessing and Edgar Berdahl},
  title = {The JoyStyx: A Quartet of Embedded Acoustic Instruments},
  pages = {271--274},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176246},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0051.pdf},
  abstract = {The JoyStyx Quartet is a series of four embedded acoustic instruments. Each of these instruments is a five-voice granular synthesizer which processes a different sound source to give each a unique timbre and range. The performer interacts with these voices individually with five joysticks positioned to lay under the performer's fingertips.  The JoyStyx uses a custom-designed printed circuit board. This board provides the joystick layout and connects them to an Arduino Micro, which serializes the ten analog X/Y position values and the five digital button presses. This data controls the granular and spatial parameters of a Pure Data patch running on a Raspberry Pi 2.  The nature of the JoyStyx construction causes the frequency response to be coloured by the materials and their geometry, leading to a unique timbre. This endows the instrument with a more ``analog'' or ``natural'' sound, despite relying on computer-based algorithms. In concert, the quartet performance with the JoyStyx may potentially be the first performance ever with a quartet of Embedded Acoustic Instruments.}
}

@inproceedings{gwakefield2017,
  author = {Graham Wakefield and Charles Roberts},
  title = {A Virtual Machine for Live Coding Language Design},
  pages = {275--278},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176248},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0052.pdf},
  abstract = {The growth of the live coding community has been coupled with a rich development of experimentation in new domain-specific languages, sometimes idiosyncratic to the interests of their performers. Nevertheless, programming language design may seem foreboding to many, steeped in computer science that is distant from the expertise of music performance. To broaden access to designing unique languages-as-instruments we developed an online programming environment that offers liveness in the process of language design as well as performance.  The editor utilizes the Parsing Expression Grammar formalism for language design, and a virtual machine featuring collaborative multitasking for execution, in order to support a diversity of language concepts and affordances. The editor is coupled with online tutorial documentation aimed at the computer music community, with live examples embedded. This paper documents the design and use of the editor and its underlying virtual machine.}
}

@inproceedings{tdavis2017,
  author = {Tom Davis},
  title = {The Feral Cello: A Philosophically Informed Approach to an Actuated Instrument},
  pages = {279--282},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176250},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0053.pdf},
  abstract = {There have been many NIME papers over the years on augmented or actuated instruments [2][10][19][22].  Many of these papers have focused on the technical description of how these instruments have been produced, or as in the case of Machover's #8216;Hyperinstruments' [19], on producing instruments over which performers have &#8216;absolute control' and emphasise &#8216;learnability. perfectibility and repeatability' [19]. In contrast to this approach, this paper outlines a philosophical position concerning the relationship between instruments and performers in improvisational contexts that recognises the agency of the instrument within the performance process. It builds on a post-phenomenological understanding of the human/instrument relationship in which the human and the instrument are understood as co-defining entities without fixed boundaries; an approach that actively challenges notions of instrumental mastery and &#8216;absolute control'. This paper then takes a practice-based approach to outline how such philosophical concerns have fed into the design of an augmented, actuated cello system, The Feral Cello, that has been designed to explicitly explore these concerns through practice.  }
}

@inproceedings{fbernardo2017,
  author = {Francisco Bernardo and Nicholas Arner and Paul Batchelor},
  title = {O Soli Mio: Exploring Millimeter Wave Radar for Musical Interaction},
  pages = {283--286},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176252},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0054.pdf},
  abstract = {This paper describes an exploratory study of the potential for musical interaction of Soli, a new radar-based sensing technology developed by Google's Advanced Technology and Projects Group (ATAP). We report on our hand-on experience and outcomes within the Soli Alpha Developers program. We present early experiments demonstrating the use of Soli for creativity in musical contexts. We discuss the tools, workflow, the affordances of the prototypes for music making, and the potential for design of future NIME projects that may integrate Soli.}
}

@inproceedings{clevican2017,
  author = {Constanza Levican and Andres Aparicio and Vernon Belaunde and Rodrigo Cadiz},
  title = {Insight2OSC: using the brain and the body as a musical instrument with the Emotiv Insight},
  pages = {287--290},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176254},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0055.pdf},
  abstract = {Brain computer interfaces are being widely adopted for music creation and interpretation, and they are becoming a truly new category of musical instruments. Indeed, Miranda has coined the term Brain-computer Musical Interface (BCMI) to refer to this category. There are no "plug-n-play" solutions for a BCMI, these kinds of tools usually require the setup and implementation of particular software configurations, customized for each EEG device. The Emotiv Insight is a low-cost EEG apparatus that outputs several kinds of data, such as EEG rhythms or facial expressions, from the user's brain activity. We have developed a BCMI, in the form of a freely available middle-ware, using the Emotiv Insight for EEG input and signal processing. The obtained data, via blue-tooth is broad-casted over the network formatted for the OSC protocol. Using this software, we tested the device's adequacy as a BCMI by using the provided data in order to control different sound synthesis algorithms in MaxMSP. We conclude that the Emotiv Insight is an interesting choice for a BCMI due to its low-cost and ease of use, but we also question its reliability and robustness.}
}

@inproceedings{bsmith2017,
  author = {Benjamin Smith and Neal Anderson},
  title = {ArraYnger: New Interface for Interactive 360° Spatialization},
  pages = {291--295},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176256},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0056.pdf},
  abstract = {Interactive real-time spatialization of audio over large immersive speaker arrays poses significant interface and control challenges for live performers. Fluidly moving and mixing numerous sound objects over unique speaker configurations requires specifically designed software interfaces and systems.  Currently available software solutions either impose configuration limitations, require extreme degrees of expertise, or extensive configuration time to use. A new system design, focusing on simplicity, ease of use, and live interactive spatialization is described. Automation of array calibration and tuning is included to facilitate rapid deployment and configuration. Comparisons with other solutions show favorability in terms of complexity, depth of control, and required features.  }
}

@inproceedings{aleslie2017,
  author = {Alexandra Murray-Leslie and Andrew Johnston},
  title = {The Liberation of the Feet: Demaking the High Heeled Shoe For Theatrical Audio-Visual Expression},
  pages = {296--301},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176258},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0057.pdf},
  abstract = {This paper describes a series of fashionable sounding shoe and foot based appendages made between 2007-2017. The research attempts to demake the physical high-heeled shoe through the iterative design and fabrication of new foot based musical instruments. This process of demaking also changes the usual purpose of shoes and associated stereotypes of high heeled shoe wear. Through turning high heeled shoes into wearable musical instruments for theatrical audio visual expressivity we question why so many musical instruments are made for the hands and not the feet? With this creative work we explore ways to redress the imbalance and consider what a genuinely &#8220;foot based&#8221; expressivity could be.  }
}

@inproceedings{crose2017,
  author = {Christiana Rose},
  title = {SALTO: A System for Musical Expression in the Aerial Arts},
  pages = {302--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176260},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0058.pdf},
  abstract = {Wearable sensor technology and aerial dance movement can be integrated to provide a new performance practice and perspective on interactive kinesonic composition. SALTO (Sonic Aerialist eLecTrOacoustic system), is a system which allows for the creation of collaborative works between electroacoustic composer and aerial choreographer. The system incorporates aerial dance trapeze movement, sensors, digital synthesis, and electroacoustic composition. In SALTO, the Max software programming environment employs parameters and mapping techniques for translating the performer's movement and internal experience into sound. Splinter (2016), a work for aerial choreographer/performer and the SALTO system, highlights the expressive qualities of the system in a performance setting.}
}

@inproceedings{mbaalman2017,
  author = {Marije Baalman},
  title = {Wireless Sensing for Artistic Applications, a Reflection on Sense/Stage to Motivate the Design of the Next Stage},
  pages = {307--312},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176262},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0059.pdf},
  abstract = {Academic research projects focusing on wireless sensor networks rarely live on after the funded research project has ended. In contrast, the Sense/Stage project has evolved over the past 6 years outside of an academic context and has been used in a multitude of artistic projects. This paper presents how the project has developed, the diversity of the projects that have been made with the technology, feedback from users on the system and an outline for the design of a successor to the current system.  }
}

@inproceedings{ibukvic2017,
  author = {Ivica Bukvic and Spencer Lee},
  title = {Glasstra: Exploring the Use of an Inconspicuous Head Mounted Display in a Live Technology-Mediated Music Performance},
  pages = {313--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176264},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0060.pdf},
  abstract = {The following paper explores the Inconspicuous Head-Mounted Display within the context of a live technology-mediated music performance. For this purpose in 2014 the authors have developed Glasstra, an Android/Google Glass networked display designed to project real-time orchestra status to the conductor, with the primary goal of minimizing the on-stage technology footprint and with it audience's potential distraction with technology. In preparation for its deployment in a real-world performance setting the team conducted a user study aimed to define relevant constraints of the Google Glass display. Based on the observed data, a conductor part from an existing laptop orchestra piece was retrofitted, thereby replacing the laptop with a Google Glass running Glasstra and a similarly inconspicuous forearm-mounted Wiimote controller. Below we present findings from the user study that have informed the design of the visual display, as well as multi-perspective observations from a series of real-world performances, including the designer, user, and the audience. We use findings to offer a new hypothesis, an inverse uncanny valley or what we refer to as uncanny mountain pertaining to audience's potential distraction with the technology within the context of a live technology-mediated music performance as a function of minimizing on-stage technological footprint.}
}

@inproceedings{sbarton2017,
  author = {Scott Barton and Ethan Prihar and Paulo Carvalho},
  title = {Cyther: a Human-playable, Self-tuning Robotic Zither},
  pages = {319--324},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176266},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0061.pdf},
  abstract = {Human-robot musical interaction typically consists of independent, physically-separated agents. We developed Cyther --- a human-playable, self-tuning robotic zither &#8211; to allow a human and a robot to interact cooperatively through the same physical medium to generate music. The resultant co- dependence creates new responsibilities, roles, and expressive possibilities for human musicians. We describe some of these possibilities in the context of both technical features and artistic implementations of the system.}
}

@inproceedings{bliang2017,
  author = {Beici Liang and György Fazekas and Andrew McPherson and Mark Sandler},
  title = {Piano Pedaller: A Measurement System for Classification and Visualisation of Piano Pedalling Techniques},
  pages = {325--329},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176268},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0062.pdf},
  abstract = {This paper presents the results of a study of piano pedalling techniques on the sustain pedal using a newly designed measurement system named Piano Pedaller. The system is comprised of an optical sensor mounted in the piano pedal bearing block and an embedded platform for recording audio and sensor data.  This enables recording the pedalling gesture of real players and the piano sound under normal playing conditions. Using the gesture data collected from the system, the task of classifying these data by pedalling technique was undertaken using a Support Vector Machine (SVM). Results can be visualised in an audio based score following application to show pedalling together with the player's position in the score.}
}

@inproceedings{jlong2017,
  author = {Jason Long and Jim Murphy and Dale A. Carnegie and Ajay Kapur},
  title = {A Closed-Loop Control System for Robotic Hi-hats},
  pages = {330--335},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176272},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0063.pdf},
  abstract = {While most musical robots that are capable of playing the drum kit utilise a relatively simple striking motion, the hi-hat, with the additional degree of motion provided by its pedal, requires more involved control strategies in order to produce expressive performances on the instrument. A robotic hi-hat should be able to control not only the striking timing and velocity to a high degree of precision, but also dynamically control the position of the two cymbals in a way that is consistent, reproducible and intuitive for composers and other musicians to use.  This paper describes the creation of a new, multifaceted hi-hat control system that utilises a closed-loop distance sensing and calibration mechanism in conjunction with an embedded musical information retrieval system to continuously calibrate the hi-hat's action both before and during a musical performance. This is achieved by combining existing musical robotic devices with a newly created linear actuation mechanism, custom amplification, acquisition and DSP hardware, and embedded software algorithms.  This new approach allows musicians to create expressive and reproducible musical performances with the instrument using consistent musical parameters, and the self-calibrating nature of the instrument lets users focus on creating music instead of maintaining equipment. }
}

@inproceedings{skountouras2017,
  author = {Stratos Kountouras and Ioannis Zannos},
  title = {Gestus: Teaching Soundscape Composition and Performance with a Tangible Interface},
  pages = {336--341},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176274},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0064.pdf},
  abstract = {Tangible user interfaces empower artists, boost their creative expression and enhance performing art. However, most of them are designed to work with a set of rules, many of which require advanced skills and target users above a certain age. Here we present a comparative and quantitative study of using TUIs as an alternative teaching tool in experimenting with and creating soundscapes with children. We describe an informal interactive workshop involving schoolchildren. We focus on the development of playful uses of technology to help children empirically understand audio feature extraction basic techniques. We promote tangible interaction as an alternative learning method in the creation of synthetic soundscape based on sounds recorded in a natural outdoor environment as main sources of sound. We investigate how schoolchildren perceive natural sources of sound and explore practices that reuse prerecorded material through a tangible interactive controller. We discuss the potential benefits of using TUIs as an alternative empirical method for tangible learning and interaction design, and its impact on encouraging and motivating creativity in children. We summarize our findings and review children's biehavioural indicators of engagement and enjoyment in order to provide insight to the design of TUIs based on user experience.}
}

@inproceedings{htez2017,
  author = {Hazar Emre Tez and Nick Bryan-Kinns},
  title = {Exploring the Effect of Interface Constraints on Live Collaborative Music Improvisation},
  pages = {342--347},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176276},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0065.pdf},
  abstract = {This research investigates how applying interaction constraints to digital music instruments (DMIs) affects the way that experienced music performers collaborate and find creative ways to make live improvised music on stage. The constraints are applied in two forms: i) Physically implemented on the instruments themselves, and ii) hidden rules that are defined on a network between the instruments and triggered depending on the musical actions of the performers. Six experienced musicians were recruited for a user study which involved rehearsal and performance. Performers were given deliberately constrained instruments containing a touch sensor, speaker, battery and an embedded computer. Results of the study show that whilst constraints can lead to more structured improvisation, the resultant music may not fit with performers' true intentions. It was also found that when external musical material is introduced to guide the performers into a collective convergence, it is likely to be ignored because it was perceived by performers as being out of context.}
}

@inproceedings{iwicaksono2017,
  author = {Irmandy Wicaksono and Joseph Paradiso},
  title = {FabricKeyboard: Multimodal Textile Sensate Media as an Expressive and Deformable Musical Interface},
  pages = {348--353},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176278},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0066.pdf},
  abstract = {This paper presents FabricKeyboard: a novel deformable keyboard interface based on a multi-modal fabric sensate surface. Multi-layer fabric sensors that detect touch, proximity, electric field, pressure, and stretch are machine-sewn in a keyboard pattern on a stretchable substrate. The result is a fabric-based musical controller that combines both the discrete controls of a keyboard and various continuous controls from the embedded fabric sensors. This enables unique tactile experiences and new interactions both with physical and non-contact gestures: physical by pressing, pulling, stretching, and twisting the keys or the fabric and non-contact by hovering and waving towards/against the keyboard and an electromagnetic source. We have also developed additional fabric-based modular interfaces such as a ribbon-controller and trackpad, allowing performers to add more expressive and continuous controls. This paper will discuss implementation strategies for our system-on-textile, fabric-based sensor developments, as well as sensor-computer interfacing and musical mapping examples of this multi-modal and expressive fabric keyboard.  }
}

@inproceedings{kkonovalovs2017,
  author = {Kristians Konovalovs and Jelizaveta Zovnercuka and Ali Adjorlu and Daniel Overholt},
  title = {A Wearable Foot-mounted / Instrument-mounted Effect Controller: Design and Evaluation},
  pages = {354--357},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176280},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0067.pdf},
  abstract = {This paper explores a new interaction possibility for increasing performer freedom via a foot-mounted wearable, and an instrument-mounted device that maintain stomp-box styles of interactivity, but without the restrictions normally associated with the original design of guitar effect pedals. The classic foot activated effect pedals that are used to alter the sound of the instrument are stationary, forcing the performer to return to the same location in order to interact with the pedals. This paper presents a new design that enables the performer to interact with the effect pedals anywhere on the stage. By designing a foot\&instrument-mounted effect controller, we kept the strongest part of the classical pedal design, while allowing the activation of the effect at any location on the stage. The usability of the device has been tested on thirty experienced guitar players. Their performance has been recorded and compared, and their opinion has been investigated through questionnaire and interview. The results of the experiment showed that, in theory, foot\&instrument-mounted effect controller can replace standard effect pedals and at the same time provide more mobility on a stage. }
}

@inproceedings{hchang2017,
  author = {Herbert Ho-Chun Chang and Lloyd May and Spencer Topel},
  title = {Nonlinear Acoustic Synthesis in Augmented Musical Instruments},
  pages = {358--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176282},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0068.pdf},
  abstract = {This paper discusses nonlinear acoustic synthesis in augmented musical instruments via acoustic transduction. Our work expands previous investigations into acoustic amplitude modulation, offering new prototypes that produce intermodulation in several instrumental contexts. Our results show nonlinear intermodulation distortion can be generated and controlled in electromagnetically driven acoustic interfaces that can be deployed in acoustic instruments through augmentation, thus extending the nonlinear acoustic synthesis to a broader range of sonic applications.}
}

@inproceedings{ghajdu2017,
  author = {Georg Hajdu and Benedict Carey and Goran Lazarevic and Eckhard Weymann},
  title = {From Atmosphere to Intervention: The circular dynamic of installations in hospital waiting areas},
  pages = {364--369},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176284},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0069.pdf},
  abstract = {This paper is a description of a pilot project conducted at the Hamburg University of Music and Drama (HfMT) during the academic year 2015-16. In this project we have addressed how interventions via interactive, generative music systems may contribute to the improvement of the atmosphere and thus to the well-being of patients in hospital waiting areas. The project was conducted by both the students of the music therapy and multimedia composition programs and has thus offered rare insights into the dynamic of such undertakings covering both the therapeutic underpinnings, as well as the technical means required to achieve a particular result.  DJster, the engine we used for the generative processes is based on Clarence Barlow's probabilistic algorithms. Equipped with the proper periphery (sensors, sound modules and spatializers), we looked at three different scenarios, each requiring specific musical and technological solutions. The pilot was concluded by a symposium in 2017 and the development of a prototype system. The symposium yielded a diagram detailing the circular dynamic of the factors involved in this particular project, while the prototype was demoed in 2016 at the HfMT facilities. The system will be installed permanently at the University Medical Center Hamburg-Eppendorf (UKE) in June 2017.}
}

@inproceedings{dbrown2017,
  author = {Dom Brown and Chris Nash and Tom Mitchell},
  title = {A User Experience Review of Music Interaction Evaluations},
  pages = {370--375},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176286},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0070.pdf},
  abstract = {The need for thorough evaluations is an emerging area of interest and importance in music interaction research. As a large degree of DMI evaluation is concerned with exploring the subjective experience: ergonomics, action-sound mappings and control intimacy; User Experience (UX) methods are increasingly being utilised to analyse an individual's experience of new musical instruments, from which we can extract meaningful, robust findings and subsequently generalised and useful recommendations. However, many music interaction evaluations remain informal. In this paper, we provide a meta-review of 132 papers from the 2014 -- 2016 proceedings of the NIME, SMC and ICMC conferences to collate the aspects of UX research that are already present in music interaction literature, and to highlight methods from UX's widening field of research that have not yet been explored. Our findings show that usability and aesthetics are the primary focus of evaluations in music interaction research, and other important components of the user experience such as enchantment, motivation and frustration are frequently if not always overlooked. We argue that these factors are prime areas for future research in the field and their consideration in design and evaluation could lead to a better understanding of NIMEs and other computer music technology.}
}

@inproceedings{wsiegel2017,
  author = {Wayne Siegel},
  title = {Conducting Sound in Space},
  pages = {376--380},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176288},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0071.pdf},
  abstract = {This paper discusses control of multichannel sound diffusion by means of motion-tracking hardware and software within the context of a live performance. The idea developed from the author's previous use of motion-tracking technology in his own artistic practice as a composer and performer. Various motion tracking systems were considered, experiments were conducted with three sound diffusion setups at three venues and a new composition for solo performer and motion-tracking system took form.}
}

@inproceedings{ssalazar:2017a,
  author = {Spencer Salazar and Sarah Reid and Daniel McNamara},
  title = {The Fragment String},
  pages = {381--386},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176290},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0072.pdf},
  abstract = {The Fragment String is a new digital musical instrument designed to reinterpret and reflect upon the sounds of the instruments it is performed in collaboration with. At its core, it samples an input audio signal and allows the performer to replay these samples through a granular resynthesizer. Normally the Fragment String samples an acoustic instrument that accompanies it, but in the absence of this input it will amplify the ambient environment and electronic noise of the input audio path to audible levels and sample these. This ability to leverage both structural, tonal sound and unstructured noise provide the instrument with multiple dimensions of musical expressivity. The relative magnitude of the physical gestures required to manipulate the instrument and control the sound also engage an audience in its performance. This straightforward yet expressive design has lent the Fragment String to a variety of performance techniques and settings. These are explored through case studies in a five year history of Fragment String-based compositions and performances, illustrating the strengths and limitations of these interactions and their sonic output.  }
}

@inproceedings{sjong2017,
  author = {Staas de Jong},
  title = {Ghostfinger: a novel platform for fully computational fingertip controllers},
  pages = {387--392},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176292},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0073.pdf},
  abstract = {We present Ghostfinger, a technology for highly dynamic up/down fingertip haptics and control. The overall user experience offered by the technology can be described as that of tangibly and audibly interacting with a small hologram.  &#160; More specifically, Ghostfinger implements automatic visualization of the dynamic instantiation/parametrization of algorithmic primitives that together determine the current haptic conditions for fingertip action. Some aspects of this visualization are visuospatial: A floating see-through cursor provides real-time, to-scale display of the fingerpad transducer, as it is being moved by the user.  Simultaneously, each haptic primitive instance is represented by a floating block shape, type-colored, variably transparent, and possibly overlapping with other such block shapes. Further aspects of visualization are symbolic: Each instance is also represented by a type symbol, lighting up within a grid if the instance is providing output to the user.  &#160; We discuss the system's user interface, programming interface, and potential applications. This from a general perspective that articulates and emphasizes the uniquely enabling role of the principle of computation in the implementation of new forms of instrumental control of musical sound. Beyond the currently presented technology, this also reflects more broadly on the role of Digital Musical Instruments (DMIs) in NIME.}
}

@inproceedings{jarmitage2017,
  author = {Jack Armitage and Fabio Morreale and Andrew McPherson},
  title = {The finer the musician, the smaller the details: NIMEcraft under the microscope},
  pages = {393--398},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176294},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0074.pdf},
  abstract = {Many digital musical instrument design frameworks have been proposed that are well suited for analysis and comparison. However, not all provide applicable design suggestions, especially where subtle but important details are concerned. Using traditional lutherie as a model, we conducted a series of interviews to explore how violin makers ``go beyond the obvious'', and how players perceive and describe subtle details of instrumental quality. We find that lutherie frameworks provide clear design methods and have substantial empirical backing, but are not enough to make a fine violin. Success comes after acquiring sufficient tacit knowledge, which enables detailed craft through subjective, empirical methods. Testing instruments for subtle qualities was suggested to be a different skill to playing. Whilst players are able to identify some specific details about instrumental quality by comparison, these are often not actionable, and important aspects of ``sound and feeling'' are much more difficult to describe. In the DMI domain, we introduce NIMEcraft to describe subtle differences between otherwise identical instruments and their underlying design processes, and consider how to improve the dissemination of NIMEcraft.}
}

@inproceedings{smehes2017,
  author = {Sandor Mehes and Maarten van Walstijn and Paul Stapleton},
  title = {Virtual-Acoustic Instrument Design: Exploring the Parameter Space of a String-Plate Model},
  pages = {399--403},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176296},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0075.pdf},
  abstract = {Exploration is an intrinsic element of designing and engaging with acoustic as well as digital musical instruments. This paper reports on the ongoing development of a virtual-acoustic instrument based on a physical model of a string coupled nonlinearly to a plate. The performer drives the model by tactile interaction with a string-board controller fitted with piezo-electric sensors.  The string-plate model is formulated in a way that prioritises its parametric explorability. Where the roles of creating performance gestures and designing instruments are traditionally separated, such a design provides a continuum across these domains. The string-plate model, its real-time implementation, and the control interface are described, and the system is preliminarily evaluated through informal observations of how musicians engage with the system.}
}

@inproceedings{nbouillot2017,
  author = {Nicolas Bouillot and Zack Settel and Michal Seta},
  title = {SATIE: a live and scalable 3D audio scene rendering environment for large multi-channel loudspeaker configurations},
  pages = {404--409},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176298},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0076.pdf},
  abstract = {Recent advances in computing offer the possibility to scale real-time 3D virtual audio scenes to include hundreds of simultaneous sound sources, rendered in realtime, for large numbers of audio outputs. Our Spatial Audio Toolkit for Immersive Environments (SATIE), allows us to render these dense audio scenes to large multi-channel (e.g. 32 or more) loudspeaker systems, in realtime and controlled from external software such as 3D scenegraph software.  As we describe here, SATIE is designed for improved scalability: minimum dependency between nodes in the audio DSP graph for parallel audio computation, controlling sound objects by groups and load balancing computation of geometry that allow to reduce the number of messages for controlling simultaneously a high number of sound sources. The paper presents SATIE along with example use case scenarios. Our initial work demonstrates SATIE's flexibility, and has provided us with novel sonic sensations such as ``audio depth of field'' and real-time sound swarming.}
}

@inproceedings{hscurto2017,
  author = {Hugo Scurto and Frédéric Bevilacqua and Jules Françoise},
  title = {Shaping and Exploring Interactive Motion-Sound Mappings Using Online Clustering Techniques},
  pages = {410--415},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176270},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0077.pdf},
  abstract = {Machine learning tools for designing motion-sound relationships often rely on a two-phase iterative process, where users must alternate between designing gestures and performing mappings. We present a first prototype of a user adaptable tool that aims at merging these design and performance steps into one fully interactive experience. It is based on an online learning implementation of a Gaussian Mixture Model supporting real-time adaptation to user movement and generation of sound parameters. To allow both fine-tune modification tasks and open-ended improvisational practices, we designed two interaction modes that either let users shape, or guide interactive motion-sound mappings. Considering an improvisational use case, we propose two example musical applications to illustrate how our tool might support various forms of corporeal engagement with sound, and inspire further perspectives for machine learning-mediated embodied musical expression.}
}

@inproceedings{kbhumber2017,
  author = {Kiran Bhumber and Bob Pritchard and Kitty Rodé},
  title = {A Responsive User Body Suit (RUBS)},
  pages = {416--419},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176300},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0078.pdf},
  abstract = {We describe the Responsive User Body Suit (RUBS), a tactile instrument worn by performers that allows the generation and manipulation of audio output using touch triggers. The RUBS system is a responsive interface between organic touch and electronic audio, intimately located on the performer's body. This system offers an entry point into a more intuitive method of music performance. A short overview of body instrument philosophy and related work is followed by the development and implementation process of the RUBS as both an interface and performance instrument. Lastly, observations, design challenges and future goals are discussed.}
}

@inproceedings{mhojlund2017,
  author = {Marie Højlund and Morten Riis and Daniel Rothmann and Jonas Kirkegaard},
  title = {Applying the EBU R128 Loudness Standard in live-streaming sound sculptures},
  pages = {420--425},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176354},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0079.pdf},
  abstract = {This paper describes the development of a loudness-based compressor for live audio streams. The need for this device arose while developing the public sound art project The Overheard, which involves mixing together several live audio streams through a web based mixing interface. In order to preserve a natural sounding dynamic image from the varying sound sources that can be played back under varying conditions, an adaptation of the EBU R128 loudness measurement recommendation, originally developed for levelling non-real-time broadcast material, has been applied. The paper describes the Pure Data implementation and the necessary compromises enforced by the live streaming condition. Lastly observations regarding design challenges, related application areas and future goals are presented.  }
}

@inproceedings{eberdahl2017,
  author = {Edgar Berdahl and Matthew Blessing and Matthew Williams and Pacco Tan and Brygg Ullmer and Jesse Allison},
  title = {Spatial Audio Approaches for Embedded Sound Art Installations with Loudspeaker Line Arrays},
  pages = {426--430},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176302},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0080.pdf},
  abstract = {The concept of embedded acoustic systems for diffusing spatial audio is considered.  This paradigm is enabled by advancements in floating-point hardware on inexpensive embedded Linux systems. Examples are presented using line array configurations for electroacoustic music and for making interactive kiosk and poster systems.}
}

@inproceedings{fkeenan2017,
  author = {Fiona Keenan and Sandra Pauletto},
  title = {Design and Evaluation of a Digital Theatre Wind Machine},
  pages = {431--435},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176304},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0081.pdf},
  abstract = {This paper presents the next stage of an investigation into the potential of historical theatre sound effects as a resource for Sonic Interaction Design (SID). An acoustic theatre wind machine was constructed, and a digital physical modelling-based version of this specific machine was programmed using the Sound Designer's Toolkit (SDT) in Max/MSP. The acoustic wind machine was fitted with 3D printed gearing to mechanically drive an optical encoder and control the digital synthesis engine in real time. The design of this system was informed by an initial comparison between the acoustic wind machine and the first iteration of its digital counterpart. To explore the main acoustic parameters and the sonic range of the acoustic and digital wind machines in operation, three simple and distinct rotational gestures were performed, with the resulting sounds recorded simultaneously, facilitating an analysis of the real-time performance of both sources. The results are reported, with an outline of future work. }
}

@inproceedings{ihattwick2017,
  author = {Ian Hattwick and Marcelo M. Wanderley},
  title = {Design of Hardware Systems for Professional Artistic Applications},
  pages = {436--441},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176306},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0082.pdf},
  abstract = {In this paper we present a discussion of the development of hardware systems in collaboration with professional artists, a context which presents both challenges and opportunities for researchers interested in the uses of technology in artistic practice. The establishment of design specifications within these contexts can be challenging, especially as they are likely to change during the development process. In order to assist in the consideration of the complete set of design specifications, we identify seven aspects of hardware design relevant to our applications: function, aesthetics, support for artistic creation, system architecture, manufacturing, robustness, and reusability.  Examples drawn from our previous work are used to illustrate the characteristics of interdependency and temporality, and form the basis of case studies investigating support for artistic creation and reusability. We argue that the consideration of these design aspects at appropriate times within the development process may facilitate the ability of hardware systems to support continued use in professional applications.}
}

@inproceedings{ajensenius2017,
  author = {Alexander Refsum Jensenius and Victor Gonzalez Sanchez and Agata Zelechowska and Kari Anne Vadstensvik Bjerkestrand},
  title = {Exploring the Myo controller for sonic microinteraction},
  pages = {442--445},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176308},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0083.pdf},
  abstract = {This paper explores sonic microinteraction using muscle sensing through the Myo armband. The first part presents results from a small series of experiments aimed at finding the baseline micromotion and muscle activation data of people being at rest or performing short/small actions. The second part presents the prototype instrument MicroMyo, built around the concept of making sound with little motion. The instrument plays with the convention that inputting more energy into an instrument results in more sound. MicroMyo, on the other hand, is built so that the less you move, the more it sounds. Our user study shows that while such an "inverse instrument" may seem puzzling at first, it also opens a space for interesting musical interactions. }
}

@inproceedings{jtilbian2017,
  author = {Joseph Tilbian and Andres Cabrera},
  title = {Stride for Interactive Musical Instrument Design},
  pages = {446--449},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176310},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0084.pdf},
  abstract = {Stride is a language tailored for designing new digital musical instruments and interfaces. Stride enables designers to fine tune the sound and the interactivity of the instruments they wish to create. Stride code provides a high-level description of processes in a platform agnostic manner. The syntax used to define these processes can also be used to define low-level signal processing algorithms.  Unlike other domain-specific languages for sound synthesis and audio processing, Stride can generate optimized code that can run on any supported hardware platform. The generated code can be compiled to run on a full featured operating system or bare metal on embedded devices. Stride goes further and enables a designer to consolidate various supported hardware and software platforms, define the communication between them, and target them as a single heterogeneous system.}
}

@inproceedings{jfernandez2017,
  author = {José Miguel Fernandez and Thomas Köppel and Nina Verstraete and Grégoire Lorieux and Alexander Vert and Philippe Spiesser},
  title = {GeKiPe, a gesture-based interface for audiovisual performance},
  pages = {450--455},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176312},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0085.pdf},
  abstract = {We present here GeKiPe, a gestural interface for musical expression, combining images and sounds, generated and controlled in real time by a performer. GeKiPe is developed as part of a creation project, exploring the control of virtual instruments through the analysis of gestures specific to instrumentalists, and to percussionists in particular. GeKiPe was used for the creation of a collaborative stage performance (Sculpt), in which the musician and their movements are captured by different methods (infrared Kinect cameras and gesture-sensors on controller gloves). The use of GeKiPe as an alternate sound and image controller allowed us to combine body movement, musical gestures and audiovisual expressions to create challenging collaborative performances.}
}

@inproceedings{jlarsen:2017a,
  author = {Jeppe Larsen and Hendrik Knoche},
  title = {Hear you later alligator: How delayed auditory feedback affects non-musically trained people's strumming},
  pages = {456--459},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176314},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0086.pdf},
  abstract = {Many musical instruments exhibit an inherent latency or delayed auditory feedback (DAF) between actuator activation and the occurrence of sound.  We investigated how DAF (73ms and 250ms) affects musically trained (MT) and non-musically trained (NMT) people's ability to synchronize the audible strum of an actuated guitar to a metronome at 60bpm and 120bpm. The long DAF matched a subdivision of the overall tempo. We compared their performance using two different input devices with feedback before or on activation. While 250ms DAF hardly affected musically trained participants, non-musically trained participants' performance declined substantially both in mean synchronization error and its spread. Neither tempo nor input devices affected performance.}
}

@inproceedings{mmulshine2017,
  author = {Michael Mulshine and Jeff Snyder},
  title = {OOPS: An Audio Synthesis Library in C for Embedded (and Other) Applications},
  pages = {460--463},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176316},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0087.pdf},
  abstract = {This paper introduces an audio synthesis library written in C with "object oriented" programming principles in mind. We call it OOPS: Object-Oriented Programming Sound, or, "Oops, it's not quite Object-Oriented Programming in C". The library consists of several UGens (audio components) and a framework to manage these components. The design emphases of the library are efficiency and organizational simplicity, with particular attention to the needs of embedded systems audio development.  }
}

@inproceedings{mkallionpaa2017,
  author = {Maria Kallionpää and Chris Greenhalgh and Adrian Hazzard and David M. Weigl and Kevin R. Page and Steve Benford},
  title = {Composing and Realising a Game-like Performance for Disklavier and Electronics},
  pages = {464--469},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176318},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0088.pdf},
  abstract = {&#8220;Climb!&#8221; is a musical composition that combines the ideas of a classical virtuoso piece and a computer game. We present a case study of the composition process and realization of &#8220;Climb!&#8221;, written for Disklavier and a digital interactive engine, which was co-developed together with the musical score. Specifically, the engine combines a system for recognising and responding to musical trigger phrases along with a dynamic digital score renderer. This tool chain allows for the composer's original scoring to include notational elements such as trigger phrases to be automatically extracted to auto-configure the engine for live performance. We reflect holistically on the development process to date and highlight the emerging challenges and opportunities. For example, this includes the potential for further developing the workflow around the scoring process and the ways in which support for musical triggers has shaped the compositional approach.}
}

@inproceedings{tmagnusson2017,
  author = {Thor Magnusson},
  title = {Contextualizing Musical Organics: An Ad-hoc Organological Classification Approach},
  pages = {470--475},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176320},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0089.pdf},
  abstract = {New digital musical instruments are difficult for organologists to deal with, due to their heterogeneous origins, interdisciplinary science, and fluid, open-ended nature. NIMEs are studied from a range of disciplines, such as musicology, engineering, human-computer interaction, psychology, design, and performance studies. Attempts to continue traditional organology classifications for electronic and digital instruments have been made, but with unsatisfactory results. This paper raises the problem of tree-like classifications of digital instruments, proposing an alternative approach: musical organics .  Musical organics is a philosophical attempt to tackle the problems inherent in the organological classification of digital instruments. Shifting the emphasis from hand-coded classification to information retrieval supported search and clustering, an open and distributed system that anyone can contribute to is proposed. In order to show how such a system could incorporate third-party additions, the paper also presents an organological ontogenesis of three innovative musical instruments: the saxophone, the Minimoog, and the Reactable.  This micro-analysis of innovation in the field of musical instruments can help forming a framework for the study of how instruments are adopted in musical culture.}
}

@inproceedings{sfasciani2017,
  author = {Stefano Fasciani},
  title = {Physical Audio Digital Filters},
  pages = {476--480},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176322},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0090.pdf},
  abstract = {We propose an approach to insert physical objects in audio digital signal processing chains, filtering the sound with the acoustic impulse response of any solid measured in real-time. We model physical objects as a linear time-invariant system, which is used as an audio filter. By interacting with the object or with the measuring hardware we can dynamically modify the characteristics of the filter. The impulse response is obtained correlating a noise signal injected in the object through an acoustic actuator with the signal received from an acoustic sensor placed on the object. We also present an efficient multichannel implementation of the system, which enables further creative applications beyond audio filtering, including tangible signal patching and sound spatialization.}
}

@inproceedings{btaylor2017,
  author = {Benjamin Taylor},
  title = {A History of the Audience as a Speaker Array},
  pages = {481--486},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176324},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0091.pdf},
  abstract = {Distributed music as a performance practice has seen significant growth over the past decade. This paper surveys the development of the genre, documenting important precedents, peripheral influences, and core works. We additionally discuss common modes of implementation in the genre and contrast these approaches and their motivations.}
}

@inproceedings{togata2017,
  author = {Takumi Ogata and Gil Weinberg},
  title = {Robotically Augmented Electric Guitar for Shared Control},
  pages = {487--488},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176326},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0092.pdf},
  abstract = {This paper is about a novel robotic guitar that establishes shared control between human performers and mechanical actuators.  Unlike other mechatronic guitar instruments that perform pre-programmed music automatically, this guitar allows the human and actuators to produce sounds jointly; there exists a distributed control between the human and robotic components. The interaction allows human performers to have full control over the melodic, harmonic, and expressive elements of the instrument while mechanical actuators excite and dampen the string with a rhythmic pattern.  Guitarists can still access the fretboard without the physical interference of a mechatronic system, so they can play melodies and chords as well as perform bends, slides, vibrato, and other expressive techniques. Leveraging the capabilities of mechanical actuators, the mechanized hammers can output complex rhythms and speeds not attainable by humans. Furthermore, the rhythmic patterns can be algorithmically or stochastically generated by the hammer, which supports real-time interactive improvising.}
}

@inproceedings{bneill2017,
  author = {Ben Neill},
  title = {The Mutantrumpet},
  pages = {489--490},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176328},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0093.pdf},
  abstract = {Ben Neill will demonstrate the mutantrumpet, a hybrid electro-acoustic instrument. The capabilities of the mutantrumpet are designed to erase the boundaries between acoustic and electronic musical creation and performance. It is both an expanded acoustic instrument and an electronic controller capable of interacting with audio and video simultaneously. The demonstration will explore the multi-faceted possibilities that are offered by the mutantrumpet in several brief, wide ranging musical examples composed and improvised by Neill.  Interactive video performance techniques and collaborations will be integrated into the excerpts. The aesthetics of live intermedia performance will be discussed along with a technical overview of the interface and associated software applications Junxion and RoSa from STEIM, Amsterdam.  Reflections on the development of a virtuosic performance technique with a hybrid instrument and influences from collaborators Robert Moog, David Behrman, Ralph Abraham, DJ Spooky and others will be included in the presentation.}
}

@inproceedings{ssmallwood2017,
  author = {Scott Smallwood},
  title = {Locus Sono: A Listening Game for NIME},
  pages = {491--492},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176330},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0094.pdf},
  abstract = {This paper/poster describes the development of an experimental listening game called Locus Sono; a 3D audio puzzle game where listening and exploration are the key forms of interaction.  The game was developed by a motivation to create an interactive audio environment in which sound is the key to solving in-game puzzles.  This work is a prototype for a larger planned work and illustrates a first step in a more complex audio gaming scenario, which will also be partially described in this short paper}
}

@inproceedings{rpolfreman2017,
  author = {Richard Polfreman and Benjamin Oliver},
  title = {Rubik's Cube, Music's Cube},
  pages = {493--494},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176332},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0095.pdf},
  abstract = {2017 marks the 40th anniversary of the Rubik's Cube (under its original name the Magic Cube). This paper-demonstration describes explorations of the cube as a performance controller for music. The pattern of colors on a face of the cube is detected via USB video camera and supplemented by EMG data from the performer to model the performer's interaction with the cube. This system was trialed in a variety of audio scenarios and deployed in the composition &#8220;Rubik's Study No. 1&#8221;, a work based on solving the cube with audible connections to 1980's pop culture. The cube was found to be an engaging musical controller, with further potential to be explored.}
}

@inproceedings{cmartin2017,
  author = {Charles Martin and Jim Torresen},
  title = {MicroJam: An App for Sharing Tiny Touch-Screen Performances},
  pages = {495--496},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176334},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0096.pdf},
  abstract = {MicroJam is a mobile app for sharing tiny touch-screen performances. Mobile applications that streamline creativity and social interaction have enabled a very broad audience to develop their own creative practices. While these apps have been very successful in visual arts (particularly photography), the idea of social music-making has not had such a broad impact. MicroJam includes several novel performance concepts intended to engage the casual music maker and inspired by current trends in social creativity support tools. Touch-screen performances are limited to five seconds, instrument settings are posed as sonic ``filters'', and past performances are arranged as a timeline with replies and layers. These features of MicroJam encourage users not only to perform music more frequently, but to engage with others in impromptu ensemble music making.}
}

@inproceedings{rnakagawa2017,
  author = {Ryu Nakagawa and Shotaro Hirata},
  title = {AEVE: An Audiovisual Experience Using VRHMD and EEG},
  pages = {497--498},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176336},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0097.pdf},
  abstract = {The AEVE provides for a brain-computer-interface (BCI) controlled audiovisual experience, presented through a virtual reality head-mounted display (VRHMD). We have developed an audiovisual art piece where progression through 3 sections and 1 extra section occurs using an &#8220;Attention&#8221; value derived from the Electroencephalography (EEG) data. The only interaction in this work is perspective that is participant's view, and the EEG data. However, we believe the simple interaction amplifies the participant's feeling of immersion. Through the narrative of the work and the simple interaction, we attempt to connect some concepts such as audiovisual experience, virtual reality (VR), BCI, grid, consciousness, memory, universe, etc. in a minimal way.}
}

@inproceedings{rcadiz2017,
  author = {Rodrigo Cadiz and Alvaro Sylleros},
  title = {Arcontinuo: the Instrument of Change},
  pages = {499--500},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176338},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0098.pdf},
  abstract = {The Arcontinuo is an electronic musical instrument designed from a perspective based in the study of their potential users and their interaction with existing musical interfaces. Arcontinuo aims to change the way electronic music is performed, as it is capable of incorporating natural and ergonomic human gestures, allowing the musician to engage with the instrument and as a result, enhance the connection with the audience. Arcontinuo challenges the notion of what a musical gesture is and goes against traditional ways of performing music, by proposing a concept that we call smart playing mapping, as a way of achieving a better and more meaningful performance.}
}

@inproceedings{rblazey2017,
  author = {Rob Blazey},
  title = {Kalimbo: an Extended Thumb Piano and Minimal Control Interface},
  pages = {501--502},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176340},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0099.pdf},
  abstract = {Kalimbo is an extended kalimba, built from repurposed materials and fitted with sensors that enable it to function as a reductionist control interface through physical gestures and capacitive sensing.  The work demonstrates an attempt to apply theories and techniques from visual collage art to the concept of musical performance ecologies. The body of the instrument emerged from material-led making, and the disparate elements of a particular musical performance ecology (acoustic instrument, audio effects, samples, synthesis and controls) are juxtaposed and unified into one coherent whole. As such, Kalimbo demonstrates how visual arts, in particular collage, can inform the design and creation of new musical instruments, interfaces and streamlined performance ecologies.}
}

@inproceedings{jtilbian:2017a,
  author = {Joseph Tilbian and Andres Cabrera and Steffen Martin and Lukasz Olczyk},
  title = {Stride on Saturn M7 for Interactive Musical Instrument Design},
  pages = {503--504},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176342},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0100.pdf},
  abstract = {This demonstration introduces the Stride programming language, the Stride IDE, and the Saturn M7 embedded audio development board. Stride is a declarative and reactive domain specific programming language for real-time sound synthesis, processing, and interaction design. The Stride IDE is a cross-platform integrated development environment for Stride. Saturn M7 is an embedded audio development board by Okra Engineering, designed around an ARM Cortex-M7 processor based microcontroller.  It targets high-end multi-channel audio processing and synthesis with very low latency and power consumption. The microcontroller has a rich set of audio and communication peripherals, capable of performing complex real-time DSP tasks with double precision floating point accuracy.  This demonstration will showcase specific features of the Stride language, which facilitates the design of new interactive musical instruments. The Stride IDE will be used to compose Stride code and generate code for the Saturn M7 board.  The various hardware capabilities of the Saturn M7 board will also be presented.}
}

@inproceedings{tkitahara2017,
  author = {Tetsuro Kitahara and Sergio Giraldo and Rafael Ramírez},
  title = {JamSketch: A Drawing-based Real-time Evolutionary Improvisation Support System},
  pages = {505--506},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176344},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0101.pdf},
  abstract = {In this paper, we present JamSketch, a real-time improvisation support system which automatically generates melodies according to melodic outlines drawn by the users. The system generates the improvised melodies based on (1) an outline sketched by the user using a mouse or a touch screen, (2) a genetic algorithm based on a dataset of existing music pieces as well as musical knowledge, and (3) an expressive performance model for timing and dynamic transformations. The aim of the system is to allow people with no prior musical knowledge to be able to enjoy playing music by improvising melodies in real time.}
}

@inproceedings{jharrison2017,
  author = {Jacob Harrison and Andrew McPherson},
  title = {An Adapted Bass Guitar for One-Handed Playing},
  pages = {507--508},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176346},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0102.pdf},
  abstract = {We present an attachment for the bass guitar which allows MIDI-controlled actuated fretting. This adapted instrument is presented as a potential method of augmenting the bass guitar for those with upper-limb disabilities. We conducted an online survey of 48 bassists in order to highlight the most important aspects of bass playing. We found that timbral and dynamic features related to the plucking hand were most important to the survey respondents. We designed an actuated fretting mechanism to replace the role of the fretting hand in order to preserve plucking hand techniques. We then conducted a performance study in which experienced bassists prepared and performed an accompaniment to a backing track with the adapted bass. The performances highlighted ways in which adapting a fretted string instrument in this way impacts plucking hand technique. }
}

@inproceedings{kcybulski2017,
  author = {Krzysztof Cybulski},
  title = {Feedboxes},
  pages = {509--510},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176348},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0103.pdf},
  abstract = {Feedboxes are interactive sound objects that generate rhythmic and harmonic patterns. Their purpose is to create intuitive tools for live improvisation, without the need for using computer with midi controller or fixed playback. Their only means of communication is sound --- they "listen" with the microphone and "speak" with the speaker, thus interaction with Feedboxes is very similar to playing with real musicians. The boxes could be used together with any instrument, or on their own &#8211; in this case they create a feedback loop by listening and responding to each other, creating ever-changing rhythmic structures.  Feedboxes react to incoming sounds in simple, predefined manner. Yet, when used together, their behaviour may become quite complex. Each of two boxes has its own sound and set of simple rules.}
}

@inproceedings{sglickman2017,
  author = {Seth Glickman and Byunghwan Lee and Fu Yen Hsiao and Shantanu Das},
  title = {Music Everywhere --- Augmented Reality Piano Improvisation Learning System},
  pages = {511--512},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176350},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0104.pdf},
  abstract = {This paper describes the design and implementation of an augmented reality (AR) piano learning tool that uses a Microsoft HoloLens and a MIDI-over-Bluetooth-enabled electric piano. The tool presents a unique visual interface&#8212;a &#8220;mirror key overlay&#8221; approach&#8212;fitted for the AR environment, and opens up the possibility of on-instrument learning experiences. The curriculum focuses on teaching improvisation in blues, rock, jazz and classical genres. Users at the piano engage with interactive lessons, watch virtual hand demonstrations, see and hear example improvisations, and play their own solos and accompaniment along with AR-projected virtual musicians. The tool aims to be entertaining yet also effective in teaching core musical concepts.}
}

@inproceedings{jbender2017,
  author = {Juan Bender and Gabriel Lecup and Sergio Fernandez},
  title = {Song Kernel --- Explorations in Intuitive Use of Harmony},
  pages = {513--514},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2017},
  publisher = {Aalborg University Copenhagen},
  address = {Copenhagen, Denmark},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176352},
  url = {http://www.nime.org/proceedings/2017/nime2017_paper0105.pdf},
  abstract = {Song Kernel is a chord-and-note harmonizing musical input interface applicable to electronic instruments in both hardware and software format. It enables to play chords and melodies while visualizing harmonic functions of chords within a scale of western music in one single static pattern.  It provides amateur musicians, as well as people with no experience in playing music, a graphic and intuitive way to play songs, manage harmonic structures and identify composition patterns.  }
}

