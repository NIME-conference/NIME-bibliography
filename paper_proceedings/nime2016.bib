@inproceedings{Sheffield2016,
  author = {Eric Sheffield and Edgar Berdahl and Andrew Pfalz},
  title = {The Haptic Capstans: Rotational Force Feedback for Music using a FireFader Derivative Device},
  pages = {1--2},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176002},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper00012.pdf},
  abstract = {The Haptic Capstans are two rotational force-feedback knobs
circumscribed by eye-catching LED rings. In this work, the Haptic Capstans are
programmed using physical models in order to experiment with audio-visual-haptic
interactions for music applications.}
}

@inproceedings{Long2016,
  author = {Jason Long and Dale Carnegie and Ajay Kapur},
  title = {The Closed-Loop Robotic Glockenspiel: Improving Musical Robots with Embedded Musical Information Retrieval},
  pages = {2--7},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.3964607},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0002.pdf},
  abstract = {Musical robots provide artists and musicians with the ability to realise complex new musical ideas in real acoustic space. However, most musical robots are created with open-loop control systems, many of which require time consuming calibration and do not reach the level of reliability of other electronic musical instruments such as synthesizers. This paper outlines the construction of a new robotic musical instrument, the Closed-Loop Robotic Glockenspiel, and discusses the improved robustness, usability and expressive capabilities that closed-loop control systems and embedded musical information retrieval processes can afford robotic musical instruments. The hardware design of the instrument is described along with the firmware of the embedded MIR system. The result is a new desktop robotic musical instrument that is capable of continuous unaided re-calibration, is as simple to use as more traditional hardware electronic sound-sources and provides musicians with new expressive capabilities. }
}

@inproceedings{Carey2016a,
  author = {Benedict Carey},
  title = {SpectraScore VR: Networkable virtual reality software tools for real-time composition and performance},
  pages = {3--4},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176004},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper00022.pdf},
  abstract = {This paper describes a package of modular tools developed for use
with virtual reality peripherals to allow for music composition, performance and
viewing in `real-time' across networks within a spectralist paradigm.
The central tool is SpectraScore, a Max/MSP abstraction for analysing audio
signals and ranking the resultant partials according to their harmonic pitch
class profiles. This data triggers the generation of objects in a virtual world
based on the `topography' of the source sound, which is experienced
by network clients via Google Cardboard headsets. They use their movements to
trigger audio in various microtonal tunings and incidentally generate scores.
These scores are transmitted to performers who improvise music from this notation
using Leap Motion Theremins, also in VR space. Finally, the performance is
broadcast via a web audio stream, which can be heard by the composer-audience in
the initial virtual world. The `real-time composers' and performers
are not required to have any prior knowledge of complex computer systems and
interact either using head position tracking, or with a Oculus Rift DK2 and a
Leap Motion Camera. }
}

@inproceedings{Chang2016,
  author = {Herbert H.C. Chang and Spencer Topel},
  title = {Electromagnetically Actuated Acoustic Amplitude Modulation Synthesis},
  pages = {8--13},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.3964599},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0003.pdf},
  abstract = {This paper discusses a new approach to acoustic amplitude modulation. Building on prior work with electromagnetic augmentation of acoustic instruments, we begin with a theory of operation model to describe the mechanical forces necessary to produce acoustic amplitude modulation synthesis. We then propose an implementation of our model as an instrumental prototype. The results illustrate that our acoustic amplitude modulation system produces controllable sideband components, and that synthesis generated from our corresponding numerical dynamic system model closely approximates the acoustic result of the physical system.}
}

@inproceedings{Berdahl2016,
  author = {Edgar Berdahl and Danny Holmes and Eric Sheffield},
  title = {Wireless Vibrotactile Tokens for Audio-Haptic Interaction with Touchscreen Interfaces},
  pages = {5--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1175984},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper00032.pdf},
  abstract = {New interfaces for vibrotactile interaction with touchscreens are
realized. An electromagnetic design for wireless actuation of 3D-printed
conductive tokens is analyzed. Example music interactions are implemented using
physical modeling paradigms, each investigated within the context of a particular
token that suggests a different interaction metaphor.}
}

@inproceedings{Baldwin2016,
  author = {Alex Baldwin and Troels Hammer and Edvinas Pechiulis and Peter Williams and Dan Overholt and Stefania Serafin},
  title = {Tromba Moderna: A Digitally Augmented Medieval Instrument},
  pages = {14--19},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.3964592},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0004.pdf},
  abstract = {An interactive museum exhibit of a digitally augmented medieval musical instrument, the tromba marina, is presented. The tromba marina is a curious single stringed instrument with a rattling bridge, from which a trumpet-like timbre is produced. The physical instrument was constructed as a replica of one found in Musikmuseet in Frederiksberg. The replicated instrument was augmented with a pickup, speakers and digital signal processing to create a more reliable, approachable and appropriate instrument for interactive display in the museum. We report on the evaluation of the instrument performed at the Danish museum of musical instruments.}
}

@inproceedings{Berg2016,
  author = {Henning Berg},
  title = {Tango: Software for Computer-Human Improvisation},
  pages = {7--8},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1175990},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper00042.pdf},
  abstract = {This demonstration describes Tango, software for
Computer-Human Improvisation developed for more than 25 years by Henning Berg.
Tango listens to an improvising musician, analyses what it hears
and plays musical responses which relate directly to the musical input.
If the improviser in turn reacts to these answers, a musical loop between the
human and the machine can emerge. The way input and reaction correlate and the
predictability of Tango's responses can be defined by the user via a setup
of improvising environments, called Rooms.
Real-time sampling with knowledge of the musical content behind the samples and
Midi-handling are unified via Tango's own monophonic audio-to-Midi, time
stretching and pitch shifting algorithms. Both audio and Midi can be used by
Tango's modules (e.g. Listeners, Players, Modifiers, Metronomes or Harmony)
for input and output.
A flexible real time control system allows for internal and external remote
control and scaling of most parameters.
The free software for Windows7 with all necessary folders, English and German
manuals, many example-Rooms and a few videos can be downloaded at
www.henning-berg.de.}
}

@inproceedings{McPherson2016,
  author = {Andrew McPherson and Robert Jack and Giulio Moro},
  title = {Action-Sound Latency: Are Our Tools Fast Enough?},
  pages = {20--25},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.3964611},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0005.pdf},
  abstract = {The importance of low and consistent latency in interactive music
systems is well-established. So how do commonly-used tools for creating digital
musical instruments and other tangible interfaces perform in terms of latency
from user action to sound output? This paper examines several common
configurations where a microcontroller (e.g. Arduino) or wireless device
communicates with computer-based sound generator (e.g. Max/MSP, Pd). We find
that, perhaps surprisingly, almost none of the tested configurations meet
generally-accepted guidelines for latency and jitter. To address this limitation,
the paper presents a new embedded platform, Bela, which is capable of complex
audio and sensor processing at submillisecond latency.}
}

@inproceedings{Berdahl2016a,
  author = {Edgar Berdahl and Andrew Pfalz and Stephen David Beck},
  title = {Very Slack Strings: A Physical Model and Its Use in the Composition Quartet for Strings},
  pages = {9--10},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1175988},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper00052.pdf},
  abstract = {Virtual ``slack'' strings are designed for and employed by the
Laptop Orchestra of Louisiana. These virtual strings are ``slack'' in the sense
that they can be very easily displaced, bent, tugged upon, etc. This enables
force-feedback control of widely ranging pitch glides, by as much as an octave or
more, simply by bending the virtual string. To realize a slack string design, a
virtual spring with a specific nonlinear characteristic curve is designed.
Violin, viola, and cello-scale models are tuned and employed by the Laptop
Orchestra of Louisiana in \emph{Quartet for Strings}.}
}

@inproceedings{Oda2016,
  author = {Reid Oda and Rebecca Fiebrink},
  title = {The Global Metronome: Absolute Tempo Sync For Networked Musical Performance},
  pages = {26--31},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176096},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0006.pdf},
  abstract = {At a time in the near future, many computers (including devices
such as smart-phones) will have system clocks that are synchronized to a high
degree (less than 1 ms of error). This will enable us to coordinate events across
unconnected devices with a degree of accuracy that was previously impossible. In
particular, high clock synchronization means that we can use these clocks to
synchronize tempo between humans or sequencers with little-to-no communication
between the devices. To facilitate this low-overhead tempo synchronization, we
propose the Global Metronome, which is a simple, computationally cheap method to
obtain absolute tempo synchronization. We present experimental results
demonstrating the effectiveness of using the Global Metronome and compare the
performance to MIDI clock sync, a common synchronization method. Finally, we
present an open source implementation of a Global Metronome server using a
GPS-connected Raspberry Pi that can be built for under $100.}
}

@inproceedings{Smallwood2016,
  author = {Scott Smallwood},
  title = {Coronium 3500: A Solarsonic Installation for Caramoor},
  pages = {32--35},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176127},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0007.pdf},
  abstract = {This paper describes the development, creation, and deployment of
a sound installation entitled Coronium 3500 (Lucie's Halo), commissioned by
the Caramoor Center for Music and the Arts. The piece, a 12-channel immersive
sound installation driven by solar power, was exhibited as part of the exhibition
In the Garden of Sonic Delights from June 7 to Nov. 4, 2014, and again for
similar duration in 2015. Herein I describe the aesthetic and technical details
of the piece and its ultimate deployment, as well as reflecting on the results
and the implications for future work.}
}

@inproceedings{Laurenzo2016,
  author = {Tomas Laurenzo},
  title = {5500: performance, control, and politics},
  pages = {36--40},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176058},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0008.pdf},
  abstract = {In the period between June 2014 and June 2015, at least 5,500
immigrants died trying to reach Europe from Africa while crossing the
Mediterranean Sea.
In this paper we present 5500, a piano performance that is a part of an on-going
project that investigates the incorporation of electrical muscle stimulation
(EMS) into musical performances, with a particular interest in the political
significance of the negotiation of control that arises.
5500 consists of a performance of Beethoven's Sonata Path\'{e}tique, where the
pianist's execution is disrupted using computer-controlled electrodes
which stimulate the muscles in his or her arms causing their involuntary
contractions and affecting the final musical result.}
}

@inproceedings{Johnson2016,
  author = {Bridget Johnson and Michael Norris and Ajay Kapur},
  title = {speaker.motion: A Mechatronic Loudspeaker System for Live Spatialisation},
  pages = {41--45},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176046},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0009.pdf},
  abstract = {This paper provides an overview of a new mechatronic loudspeaker
system: speaker.motion. The system affords automated positioning of a loudspeaker
in real-time in order to manipulate the spatial qualities of electronic music.
The paper gives a technical overview of how the system's hardware and
software were developed and the design criteria and methodology. There is
discussion of the unique features of the speaker.motion spatialisation system and
the methods of user interaction, as well as a look at the creative possibilities
that the loudspeakers afford. The creative affordances are explored through the
case study of two new pieces written for the speaker.motion system. It is hoped
that the speaker.motion system will afford composers and performers with a new
range of spatial aesthetics to use in spatial performances, and encourage
exploration of the acoustic properties of physical performance and installation
spaces in electronic music.}
}

@inproceedings{Nort2016,
  author = {Doug Van Nort and Ian Jarvis and Michael Palumbo},
  title = {Towards a Mappable Database of Emergent Gestural Meaning},
  pages = {46--50},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176092},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0010.pdf},
  abstract = {This paper presents our work towards a database of performance
activity that is grounded in an embodied view on meaning creation that crosses
sense modalities. Our system design is informed by the philosophical and
aesthestic intentions of the laboratory context within which it is designed,
focused on distribution of performance activity across temporal and spatial
dimensions, and expanded notions of the instrumental system as environmental
performative agent. We focus here on design decisions that result from this
overarching worldview on digitally-mediated performance.}
}

@inproceedings{Long2016a,
  author = {Jason Long and Ajay Kapur and Dale Carnegie},
  title = {An Analogue Interface for Musical Robots},
  pages = {51--54},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176072},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0011.pdf},
  abstract = {The majority of musical robotics performances, projects and
installations utilise microcontroller hardware to digitally interface the robotic
instruments with sequencer software and other musical controllers, often via a
personal computer. While in many ways digital interfacing offers considerable
power and flexibility, digital protocols, equipment and audio workstations often
tend to suggest particular music-making work-flows and have resolution and timing
limitations. This paper describes the creation of a hardware interface that
allows direct communication between analogue synthesizer equipment and simple
robotic musical instruments entirely in the analogue domain without the use of
computers, microcontrollers or software of any kind. Several newly created
musical robots of various designs are presented, together with a custom built
hardware interface with circuitry that enables analogue synthesizers to interface
with the robots without any digital intermediary. This enables novel methods of
musical expression, creates new music-making work-flows for composing and
improvising with musical robots and takes advantage of the low latency and
infinite resolution of analogue circuits.}
}

@inproceedings{Barrett2016,
  author = {Natasha Barrett and Alexander Refsum Jensenius},
  title = {The `Virtualmonium': an instrument for classical sound diffusion over a virtual loudspeaker orchestra},
  pages = {55--60},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175974},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0012.pdf},
  abstract = {Despite increasingly accessible and user-friendly multi-channel
compositional tools, many composers still choose stereo formats for their work,
where the compositional process is allied to diffusion performance over a
`classical' loudspeaker orchestra. Although such orchestras remain
common within UK institutions as well as in France, they are in decline in the
rest of the world. In contrast, permanent, high-density loudspeaker arrays are on
the rise, as is the practical application of 3-D audio technologies. Looking to
the future, we need to reconcile the performance of historical and new stereo
works, side-by-side native 3-D compositions. In anticipation of this growing
need, we have designed and tested a prototype `Virtualmonium'. The
Virtualmonium is an instrument for classical diffusion performance over an
acousmonium emulated in higher-order Ambisonics. It allows composers to
custom-design loudspeaker orchestra emulations for the performance of their
works, rehearse and refine performances off-site, and perform classical
repertoire alongside native 3-D formats in the same concert. This paper describes
the technical design of the Virtualmonium, assesses the success of the prototype
in some preliminary listening tests and concerts, and speculates how the
instrument can further composition and performance practice.}
}

@inproceedings{Arango2016,
  author = {Julian Jaramillo Arango and Daniel Mel\`{a}n Giraldo},
  title = {The Smartphone Ensemble. Exploring mobile computer mediation in collaborative musical performance},
  pages = {61--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175850},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0013.pdf},
  abstract = {This paper reports the goals, procedures and recent activities of
the Smartphone Ensemble, an academic group of musicians and designers exploring
mobile phones social mediation in musical contexts. The SE was created in the
Design and Creation program at the Caldas University in Manizales, Colombia and
includes six regular members. The group intends to enhance links among musicians,
and between the musicians and their audience, by leveraging the network
capabilities and mobility of smart phones, and exploring the expressivity of
urban space. Through the creation of pieces and interventions that are related to
urban experiences, the Smartphone Ensemble envisions alternatives to the standard
musical performance space. In this regard, the performances intend to be urban
interventions, not traditional concerts, they progress according to previously
defined tours around the city that the group embarks while playing}
}

@inproceedings{Hofmann2016,
  author = {Alex Hofmann and Vasileios Chatziioannou and Alexander Mayer and Harry Hartmann},
  title = {Development of Fibre Polymer Sensor {Reed}s for Saxophone and Clarinet},
  pages = {65--68},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176028},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0014.pdf},
  abstract = {Electronic pickup systems for acoustic instruments are often used
in popular and contemporary music performances because they allow amplification
and modification of a clean and direct signal. Strain gauge sensors on saxophone
and clarinet reeds have been shown to be a useful tool to gain insight into
tongue articulation during performance but also capture the reed vibrations. In
our previous design, we used a procedure with epoxy adhesive to glue the strain
gauge sensors to the flat side of the synthetic single reeds. The new design
integrates the sensor inside a synthetic reed, respectively between layers of
fibre polymer and wood. This allows an industrial production of sensor reeds.
Sensor reeds open up new possibilities to pick up woodwind instruments and to
analyse, to modify, and to amplify the signal. A signal-to-noise analysis of the
signals from both designs showed that a sensor, glued to the outside of the reed,
produced a cleaner signal.}
}

@inproceedings{Kapur2016,
  author = {Ajay Kapur and Jim Murphy and Michael Darling and Eric Heep and Bruce Lott and Ness Morris},
  title = {MalletOTon and the Modulets: Modular and Extensible Musical Robots},
  pages = {69--72},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176050},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0015.pdf},
  abstract = {This paper presents two new musical robot systems and an accompanying driver electronics array. These systems are designed to allow for modular extensibility and ease of use with different instrument systems. The first system discussed is MalletOTon, a mechatronic mallet instrument player that may be re-configured to play a number of different instruments. Secondly, the Modulet mechatronic noisemakers are presented. These instruments are discrete modules that may be installed throughout a space in a wide variety of configurations. In addition to presenting the aforementioned new instruments, the Novalis system is shown. Novalis is an open-ended driver system for mechatronic instruments, designed to afford rapid deployment and modularity. Where prior mechatronic instruments are often purpose-built, the robots and robot electronics presented in this paper may be re-deployed in a wide-ranging, diverse manner. Taken as a whole, the design practices discussed in this paper go toward establishing a paradigm of modular and extensible mechatronic instrument development.}
}

@inproceedings{Olson2016,
  author = {Ben Olson},
  title = {Transforming 8-Bit Video Games into Musical Interfaces via Reverse Engineering and Augmentation},
  pages = {73--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176100},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0016.pdf},
  abstract = {Video games and music have influenced each other since the
beginning of the consumer video game era. In particular the chiptune genre of
music uses sounds from 8-bit video games; these sounds have even found their way
into contemporary popular music. However, in this genre, game sounds are arranged
using conventional musical interfaces, meaning the games themselves (their
algorithms, design and interactivity) play no role in the creation of the music.
This paper describes a new way of creating music with 8-bit games, by reverse
engineering and augmenting them with run-time scripts. A new API, Emstrument, is
presented which allows these scripts to send MIDI to music production software.
The end result is game-derived musical interfaces any computer musician can use
with their existing workflow. This enhances prior work in repurposing games as
musical interfaces by allowing musicians to use the original games instead of
having to build new versions with added musical capabilities.
Several examples of both new musical instruments and dynamic interactive musical
compositions using Emstrument are presented, using iconic games from the 8-bit
era.}
}

@inproceedings{Cherston2016,
  author = {Juliana Cherston and Ewan Hill and Steven Goldfarb and Joseph Paradiso},
  title = {Musician and Mega-Machine: Compositions Driven by Real-Time Particle Collision Data from the ATLAS Detector},
  pages = {78--83},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176012},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0017.pdf},
  abstract = {We present a sonification platform for generating audio driven by
real-time particle collision data from the ATLAS experiment at CERN. This paper
provides a description of the data-to-audio mapping interfaces supported by the
project's composition tool as well as a preliminary evaluation of the platform's
evolution to meet the aesthetic needs of vastly distinct musical styles and
presentation venues. Our work has been conducted in collaboration with the ATLAS
Outreach team and is part of a broad vision to better harness real-time sensor
data as a canvas for artistic expression. Data-driven streaming audio can be
treated as a reimagined form of live radio for which composers craft the
instruments but real-time particle collisions pluck the strings.}
}

@inproceedings{Lind2016,
  author = {Anders Lind and Daniel Nyl\'{e}n},
  title = {Mapping Everyday Objects to Digital Materiality in The Wheel Quintet: Polytempic Music and Participatory Art},
  pages = {84--89},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176064},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0018.pdf},
  abstract = {Digitalization has enabled material decoupling of sound from the
physical devices traditionally used to conceive it. This paper reports an
artistic exploration of novel mappings between everyday objects and digital
sound. The Wheel Quintet---a novel musical instrument comprising four
bicycle wheels and a skateboard---was created using off-the-shelf
components and visual programming in Max/MSP. The use of everyday objects sought
to enable people to quickly master the instrument, regardless of their musical
backgrounds, and collectively create polytempic musical textures in a
participatory art context. Applying an action research approach, the paper
examines in detail two key cycles of planning, action, and analysis related to
the instrument, involving an interactive museum exhibition open to the public and
a concert hall performance conducted by an animated music notation system.
Drawing on insights from the study, the paper contributes new knowledge
concerning the creation and use of novel interfaces for music composition and
performance enabled by digitalization.}
}

@inproceedings{Balandra2016,
  author = {Alfonso Balandra and Hironori Mitake and Shoichi Hasegawa},
  title = {Haptic Music Player---Synthetic audio-tactile stimuli generation based on the notes' pitch and instruments' envelope mapping},
  pages = {90--95},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175968},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0019.pdf},
  abstract = {An entertainment environment to enrich music listening experience is purposed. This environment is composed of 3 modules: a MIDI player, a music animation and a haptic module that translates the notes played by one instrument into a resemblant vibration. To create the haptic vibration, the notes' relative pitch in the song are calculated, then these positions are mapped into the haptic signals' amplitude and frequency. Also, the envelope of the haptic signal is modified, by using an ADSR filter, to have the same envelope as the audio signal. To evaluate the perceived cross-modal similarity between users, two experiments were performed. In both, the users used the complete entertainment environment to rank the similarity between 3 different haptic signals, with triangular, square and analogue envelopes and 4 different instruments in a classical song. The first experiment was performed with the purposed amplitude and frequency technique, while the second experiment was performed with constant frequency and amplitude. Results, show different envelope user preferences. The square and triangular envelopes were preferred in the first experiment, while only analogue envelopes were preferred in the second. This suggests that the users' envelope perception was masked by the changes in amplitude and frequency.}
}

@inproceedings{Huberth2016,
  author = {Madeline Huberth and Chryssie Nanou},
  title = {Notation for {3D} Motion Tracking Controllers: A Gametrak Case Study},
  pages = {96--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176034},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0020.pdf},
  abstract = {Notation systems are used in almost all fields, especially for the communication and expression of ideas. This paper proposes and discusses a notation system for Gametrak-based computer music instruments. The notation system's design is informed both by Western music notation and dance notation, as well as common mappings used in laptop orchestras. It is designed to be sound-agnostic, primarily instructing the performer in their motions. While the discussion of such a notation system may be particularly timely due to the growing commercially-available 3D motion tracking controllers, the notation system may prove especially useful in the context of Gametrak and laptop orchestra, for which score-based representation can help clarify performer interaction and serve as a teaching tool in documenting prior work.}
}

@inproceedings{Cakmak2016,
  author = {Cem Cakmak and Anil Camci and Angus Forbes},
  title = {Networked Virtual Environments as Collaborative Music Spaces},
  pages = {106--111},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176002},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0022.pdf},
  abstract = {In this paper, we describe a novel multimedia system for networked
musical collaboration. Our system, called Monad, offers a 3D virtual environment
that can be shared by multiple participants to collaborate remotely on a musical
performance. With Monad, we explore how various features of this environment in
relation to game mechanics, network architecture, and audiovisual aesthetics can
be used to mitigate problems inherent to networked musical performance, such as
time delays, data loss, and reduced agency of users. Finally, we describe the
results of a series of qualitative user studies that illustrate the effectiveness
of some of our design decisions with two separate versions of Monad.}
}

@inproceedings{Becking2016,
  author = {Dominic Becking and Christine Steinmeier and Philipp Kroos},
  title = {Drum-Dance-Music-Machine: Construction of a Technical Toolset for Low-Threshold Access to Collaborative Musical Performance},
  pages = {112--117},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175980},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0023.pdf},
  abstract = {Most instruments traditionally used to teach music in early
education, like xylophones or flutes, encumber children with the additional
difficulty of an unfamiliar and unnatural interface. The most simple expressive
interaction, that even the smallest children use in order to make music, is
pounding at surfaces. Through the design of an instrument with a simple
interface, like a drum, but which produces a melodic sound, children can be
provided with an easy and intuitive means to produce consonance. This should then
be further complemented with information from analysis and interpretation of
childlike gestures and dance moves, reflecting their natural understanding
of musical structure and motion. Based on these assumptions we propose a modular
and reactive system for dynamic composition with accessible interfaces, divided
into distinct plugins usable in a standard digital audio workstation. This paper
describes our concept and how it can facilitate access to collaborative music
making for small children. A first prototypical implementation has been
designed and developed during the ongoing research project
Drum-Dance-Music-Machine (DDMM), a cooperation with the local social welfare
association AWO Hagen and the chair of musical education at the University of
Applied Sciences Bielefeld.}
}

@inproceedings{Leitman2016,
  author = {Sasha Leitman and John Granzow},
  title = {Music Maker: 3d Printing and Acoustics Curriculum},
  pages = {118--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176062},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0024.pdf},
  abstract = {Music Maker is a free online resource that provides files for 3D
printing woodwind and brass mouthpieces and tutorials for using those mouthpieces
to learn about acoustics and music. The mouthpieces are designed to fit into
standard plumbing and automobile parts that can be easily purchased at home
improvement and automotive stores. The result is a musical tool that can be used
as simply as a set of building blocks to bridge the gap between our increasingly
digital world of fabrication and the real-world materials that make up our daily
lives.
An increasing number of schools, libraries and community groups are purchasing 3D
printers but many are still struggling to create engaging and relevant curriculum
that ties into academic subjects. Making new musical instruments is a fantastic
way to learn about acoustics, physics and mathematics.}
}

@inproceedings{Sello2016,
  author = {Jacob T. Sello},
  title = {The Hexenkessel: A Hybrid Musical Instrument for Multimedia Performances},
  pages = {122--131},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176118},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0025.pdf},
  abstract = {This paper introduces the Hexenkessel---an augmented musical
instrument for interactive multimedia arts. The Hexenkessel is a classical
timpani with its drumhead acting as a tangible user interface for expressive
multimedia performances on stage.}
}

@inproceedings{Lnicode228hdeoja2016,
  author = {Otso L\''{a}hdeoja},
  title = {Active Acoustic Instruments for Electronic Chamber Music},
  pages = {132--136},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176054},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0027.pdf},
  abstract = {This paper presents an ongoing project for augmenting acoustic
instruments with active acoustics. Active acoustics are defined as audio-rate
vibration driven into the instruments physical structure, inducing air-borne
sound output. The instrument's acoustic sound is thus doubled by an
electronic soundscape radiating from the same source. The article is centered on
a case study on two guitars, one with hexaphonic sound capture and the other with
monophonic pickup. The article discusses the design, implementation, acoustics,
sound capture and processing of an active acoustic instrument, as well as
gestural control using the Leap Motion sensor. Extensions towards other
instruments are presented, in connection with related artistic projects and
`electronic chamber music' aesthetics.}
}

@inproceedings{Lynch2016,
  author = {Evan Lynch and Joseph Paradiso},
  title = {SensorChimes: Musical Mapping for Sensor Networks},
  pages = {137--142},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176074},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0028.pdf},
  abstract = {We present a composition framework that facilitates novel musical
mappings for large-scale distributed networks of environmental sensors. A library
of C-externals called ChainFlow for the graphical programming language Max/MSP
that provides an interface to real-time and historical data for large sensor
deployments was designed and implemented. This library along with spatialized
audio techniques were used to create immersive musical compositions which can be
presented on their own or complemented by a graphical 3D virtual world. Musical
works driven by a sensor network deployed in a wetland restoration project called
Tidmarsh are presented as case studies in augmented presence through musical
mapping.}
}

@inproceedings{Nakanishi2016,
  author = {Kyosuke Nakanishi and Paul Haimes and Tetsuaki Baba and Kumiko Kushiyama},
  title = {NAKANISYNTH: An Intuitive Freehand Drawing Waveform Synthesiser Application for iOS Devices},
  pages = {143--145},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176086},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0029.pdf},
  abstract = {NAKANISYNTH is a synthesiser application available on iOS devices
that provides a simple and intuitive interface, allowing users to produce sound
loops by freehand drawing sound waves and envelope curves. The interface provides
a simple way of interacting: the only input required involves drawing two
waveforms, meaning that users can easily produce various sounds intuitively
without the need for complex manipulation. The application's interface comprises
of an interchangeable ribbon and keyboard feature, plus two panels where users
can edit waveforms, allowing users to make sounds. This simple approach to the
interface means that it is easy for users to understand the relationship between
a waveform and the sound that it produces. }
}

@inproceedings{Vindriis2016,
  author = {Richard Vindriis and Dale Carnegie},
  title = {StrumBot---An Overview of a Strumming Guitar Robot},
  pages = {146--151},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176135},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0030.pdf},
  abstract = {StrumBot is a novel standalone six stringed robotic guitar
consisting of mechanisms designed to enable musical expressivity and minimise
acoustic noise. It is desirable for less than 60 dBA of noise at 1 m to be
emitted to allow StrumBot to play in intimate venues such as caf\'{e}s or
restaurants without loud motor noises detracting from the musical experience.
StrumBot improves upon previous RMI's by allowing additional expressive
opportunities for a composer to utilise. StrumBot can perform slides, vibrato,
muting techniques, pitch bends, pluck power variances, timbre control, complex
chords and fast strumming patterns.
A MIDI input allows commercial or custom controllers to operate StrumBot. Novel
note allocation algorithms were created to allow a single MIDI stream of notes to
be allocated across the six guitar strings.
Latency measurements from MIDI input to string pluck are as low as 40 ms for a
best case scenario strum, allowing StrumBot to accompany a live musician with
minimal audible delay.
A relay based loop switcher is incorporated, allowing StrumBot to activate
standard commercial guitar pedals based on a MIDI instruction. }
}

@inproceedings{Shaw2016,
  author = {Tim Shaw and Simon Bowen and John Bowers},
  title = {Unfoldings: Multiple Explorations of Sound and Space},
  pages = {152--157},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176122},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0031.pdf},
  abstract = {This paper describes a long term, collaborative project Sound
Spaces. Within this project we creatively investigated various environments and
built a collection of artworks in response to material gathered through a number
of practical field visits. Our responses were presented in numerous,
idiosyncratic ways and took shape through a number of concerted making
activities. The work was conducted both in and with the public, allowing
participants to inform the creative decisions made throughout the project as well
as experiencing the building of the artworks. Within this essay we report on our
process, presentation and offer alternative methods for collecting material and
presenting representations of space. We describe the many responses made during
our time and related these to research concerns relevant to the NIME community.
We conclude with our findings and, through the production of an annotated
portfolio, offer our main emerging themes as points of discussion. }
}

@inproceedings{Rieger2016,
  author = {Alexandra Rieger and Spencer Topel},
  title = {Driftwood: Redefining Sound Sculpture Controllers},
  pages = {158--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176110},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0032.pdf},
  abstract = {The Driftwood is a maneuverable sculptural instrument &
controller. Tactilely, it is a micro-terrain one can explore with the hands as
with the ears. Closed circuit sensors, moving wooden parts and Piezo microphones
are discussed in the design phase alongside background and musical implementation
concepts. Electronics and nature converge in this instrument harmoniously
referencing our changing world and environment. When engaging with the sonic
sculpture silent objects become audible and rest-wood is venerated. It is
revealed to the musician interacting with Driftwood that our actions intervene
directly with issues relating to sustainability and the amount of value we place
on the world we live in. Every scrap of wood was once a tree, Driftwood reminds
us of this in a multi-sensory playing experience. The Driftwood proposes a
reinterpretation of the process of music creation, awareness and expression.}
}

@inproceedings{Kleinberger2016,
  author = {Rebecca Kleinberger and Akito van Troyer},
  title = {Dooremi: a Doorway to Music},
  pages = {160--161},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176052},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0033.pdf},
  abstract = {The following paper documents the prototype of a musical door that
interactively plays sounds, melodies, and sound textures when in use. We took the
natural interactions people have with doors---grabbing and turning
the knob and pushing and puling motions---and turned them into
musical activities. The idea behind this project comes from the fact that the
activity of using a door is almost always accompanied by a sound that is
generally ignored by the user. We believe that this sound can be considered
musically rich and expressive because each door has specific sound
characteristics and each person makes it sound slightly different. By augmenting
the door to create an unexpected sound, this project encourages us to listen to
our daily lives with a musician's critical ear, and reminds us of the musicality
of our everyday activities.}
}

@inproceedings{Normark2016,
  author = {Normark, Carl J\''{u}rgen  and Peter Parnes and Robert Ek and Harald Andersson},
  title = {The extended clarinet},
  pages = {162--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176090},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0034.pdf},
  abstract = {This paper describes how a classical instrument, the clarinet, can
be extended with modern technology to create a new and easy to use augmented
instrument. The paper describes the design process, technical details and how a
musician can use the instrument. The clarinet bell is extended with sensor
technology in order to improve the ways the clarinet is traditionally played and
improve the performing artist's musical and performative expressions. New
ways of performing music with a clarinet also opens up for novel ways of
composing musical pieces. The design is iterated in two versions with improved
hardware and form factor where everything is packaged into the clarinet bell. The
clarinet uses electronics that wirelessly sends sensor data to a computer that
processes a live audio feed via the software MAX 7 and plays it back via
loudspeakers on the stage. The extended clarinet provides several ways of
transforming audio and also adds several ways of making performances more
visually interesting. It is shown that this way of using sensor technology in a
traditional musical instrument adds new dimensions to the performance and allows
creative persons to express themselves in new ways as well as giving the audience
an improved experience. }
}

@inproceedings{Nagashim2016,
  author = {Yoichi Nagashima},
  title = {Multi Rubbing Tactile Instrument},
  pages = {168--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176084},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0035.pdf},
  abstract = {This is a report of a novel tactile musical instrument. This
instrument is called Multi Rubbing Tactile Instrument (MRTI2015), using ten
pieces of PAW sensor produced by the RT corporation. Previous research was
focused on untouchable instruments, but this approach is fully tactile---rub
and touch. The ten PAW sensors are assigned on the surface of the egg-like
plastic case to fit the ten fingers grasping the instrument. The controller is
mbed (NucleoF401RE), and it communicates with the host PC via high speed serial
(115200bps) by an MIDI-like protocol. Inside the egg-like plastic case, this
instrument has eight blue-LEDs which are controlled by the host in order to
display the grasping nuances. The prototype of this instrument contains realtime
visualizing system with chaotic graphics by Open-GL. I will report on the
principle of the sensor, and details about realizing the new system.}
}

@inproceedings{Zhang2016,
  author = {Leshao Zhang and Yongmeng Wu and Mathieu Barthet},
  title = {A Web Application for Audience Participation in Live Music Performance: The Open Symphony Use Case},
  pages = {170--175},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176147},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0036.pdf},
  abstract = {This paper presents a web-based application enabling audiences to
collaboratively contribute to the creative process during live music
performances. The system aims at enhancing audience engagement and creating new
forms of live music experiences. Interaction between audience and performers is
made possible through a client/server architecture enabling bidirectional
communication of creative data. Audience members can vote for pre-determined
musical attributes using a smartphone-friendly and cross-platform web
application. The system gathers audience members' votes and provide feedback
through visualisations that can be tailored for specific needs. In order to
support multiple performers and large audiences, automatic audience-to-performer
groupings are handled by the application. The framework was applied to support
live interactive musical improvisations where creative roles are shared amongst
audience and performers (Open Symphony). Qualitative analyses of user surveys
highlighted very positive feedback related to themes such as engagement and
creativity and also identified further design challenges around audience sense of
control and latency.}
}

@inproceedings{CarvalhoJunior2016,
  author = {Antonio Deusany de Carvalho Junior and Sang Won Lee and Georg Essl},
  title = {Understanding Cloud Support for the Audience Participation Concert Performance of Crowd in C[loud]},
  pages = {176--181},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176008},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0037.pdf},
  abstract = {Cloud services allow musicians and developers to build audience
participation software with minimal network configuration for audience and no
need for server-side development. In this paper we discuss how a cloud service
supported the audience participation music performance, Crowd in C[loud], which
enables audience participation on a large scale using the audience audience's
smartphones.
We present the detail of the cloud service technology and an analysis of the
network transaction data regarding the performance.
This helps us to understand the nature of cloud-based audience participation
pieces based on the characteristics of a performance reality and provides cues
about the technology's scalability.}
}

@inproceedings{Wang2016,
  author = {Ge Wang},
  title = {Game Design for Expressive Mobile Music},
  pages = {182--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176141},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0038.pdf},
  abstract = {This article presents observations and strategies for designing
game-like elements for expressive mobile musical interactions. The designs of
several popular commercial mobile music instruments are discussed and compared,
along with the different ways they integrate musical information and game-like
elements. In particular, issues of designing goals, rules, and interactions are
balanced with articulating expressiveness. These experiences aim to invite and
engage users with game design while maintaining and encouraging open-ended
musical expression and exploration. A set of observations is derived, leading to
a broader design motivation and philosophy.}
}

@inproceedings{Banas2016,
  author = {Jan Banas and Razvan Paisa and Iakovos Vogiatzoglou and Francesco Grani and Stefania Serafin},
  title = {Design and evaluation of a gesture driven wave field synthesis auditory game},
  pages = {188--193},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175972},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0039.pdf},
  abstract = {An auditory game has been developed as a part of research in
Wavefield Synthesis. In order to design and implement this game, a number of
technologies have been incorporated in the development process. By pairing motion
capture with a WiiMote new dimension of movement input was achieved.
We present an evaluation study where the game was assessed.}
}

@inproceedings{Baytas2016,
  author = {Baytas, Mehmet Aydin and Tilbe Goksun and Oguzhan Ozcan},
  title = {The Perception of Live-sequenced Electronic Music via Hearing and Sight},
  pages = {194--199},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175978},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0040.pdf},
  abstract = {In this paper, we investigate how watching a live-sequenced
electronic music performance, compared to merely hearing the music, contributes
to spectators' experiences of tension. We also explore the role of the
performers' effective and ancillary gestures in conveying tension, when they can
be seen. To this end, we conducted an experiment where 30 participants heard,
saw, or both heard and saw a live-sequenced techno music performance recording
while they produced continuous judgments on their experience of tension. Eye
tracking data was also recorded from participants who saw the visuals, to reveal
aspects of the performance that influenced their tension judgments. We analysed
the data to explore how auditory and visual components and the performer's
movements contribute to spectators' experience of tension. Our results show that
their perception of emotional intensity is consistent across hearing and sight,
suggesting that gestures in live-sequencing can be a medium
for expressive performance.}
}

@inproceedings{Bin2016,
  author = {S. Astrid Bin and Nick Bryan-Kinns and Andrew P. McPherson},
  title = {Skip the Pre-Concert Demo: How Technical Familiarity and Musical Style Affect Audience Response},
  pages = {200--205},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175994},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0041.pdf},
  abstract = {This paper explores the roles of technical and musical familiarity
in shaping audience response to digital musical instrument (DMI) performances. In
an audience study conducted during an evening concert, we examined two primary
questions: first, whether a deeper understanding of how a DMI works increases an
audience's enjoyment and interest in the performance; and second, given the same
DMI and same performer, whether playing in a conventional (vernacular) versus an
experimental musical style affects an audience's response. We held a concert in
which two DMI creator-performers each played two pieces in differing styles.
Before the concert, each half the 64-person audience was given a technical
explanation of one of the instruments. Results showed that receiving an
explanation increased the reported understanding of that instrument, but had no
effect on either the reported level of interest or enjoyment. On the other hand,
performances in experimental versus conventional style on the same instrument
received widely divergent audience responses. We discuss implications of these
findings for DMI design.}
}

@inproceedings{Wu2016,
  author = {Jiayue Cecilia Wu and Madeline Huberth and Yoo Hsiu Yeh and Matt Wright},
  title = {Evaluating the Audience's Perception of Real-time Gestural Control and Mapping Mechanisms in Electroacoustic Vocal Performance},
  pages = {206--211},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176143},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0042.pdf},
  abstract = {This paper presents an empirical evaluation of a digital music
instrument (DMI) for electroacoustic vocal performance, the Tibetan Singing
Prayer Wheel (TSPW). Specifically, we study audience preference for the way it
maps horizontal spinning gestures to vocal processing parameters. We filmed six
songs with the singer using the TSPW, and created two alternative soundtracks for
each song: one desynchronized, and one with the mapping inverted. Participants
viewed all six songs with either the original or desynchronized soundtrack
(Experiment 1), or either the original or inverted-mapping soundtrack (Experiment
2). Participants were asked several questions via questionnaire after each song.
Overall, they reported higher engagement and preference for the original
versions, suggesting that audiences of the TSPW prefer more highly synchronized
performances, as well as more intuitive mappings, though level of perceived
expression of the performer only significantly differed in Experiment 1. Further,
we believe that our experimental methods contribute to how DMIs can be evaluated
from the audience's (a recently noted underrepresented stakeholder)
perspective.}
}

@inproceedings{Lee2016,
  author = {Sang Won Lee and Georg Essl and Mari Martinez},
  title = {Live Writing : Writing as a Real-time Audiovisual Performance},
  pages = {212--217},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176060},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0043.pdf},
  abstract = {This paper suggests a novel form of audiovisual performance --- live writing --- that transforms creative writing into a real-time performing art. The process of typing a poem on the fly is captured and augmented to create an audiovisual performance that establishes natural links among the components of typing gestures, the poem being written on the fly, and audiovisual artifacts. Live writing draws upon ideas from the tradition of live coding in which the process of programming is revealed to the audience in real-time. This paper discusses the motivation behind the idea, interaction schemes and a performance interface for such a performance practice. Our live writing performance system is enabled by a custom text editor, writing-sound mapping strategies of our choice, a poem-sonification, and temporal typography. We describe two live writing performances that take different approaches as they vary the degree of composition and improvisation in writing.}
}

@inproceedings{Jathal2016,
  author = {Kunal Jathal and Tae-Hong Park},
  title = {The HandSolo: A Hand Drum Controller for Natural Rhythm Entry and Production},
  pages = {218--223},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176042},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0044.pdf},
  abstract = {The majority of electronic percussion controllers on the market
today are based on location-oriented striking techniques, resulting in a finger
drumming interaction paradigm, that is both fundamentally eclectic as well as
imposingly contr. The few controllers that allow hand-drumming
techniques also invariably conform to region-based triggering design, or, in
trade-off for expressivity, end up excluding hardware connectivity options that
are vital to the context of the modern electronic rhythm producer. The HandSolo
is a timbre-based drum controller that allows the use of natural, hand-drumming
strokes, whilst offering the same end-goal functionality that percussion
controller users have come to expect over the past decade.}
}

@inproceedings{Nash2016,
  author = {Chris Nash},
  title = {The 'E' in QWERTY: Musical Expression with Old Computer Interfaces},
  pages = {224--229},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176088},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0045.pdf},
  abstract = {This paper presents a development of the ubiquitous computer
keyboard to capture velocity and other continuous musical properties, in order to
support more expressive interaction with music software. Building on existing
`virtual piano' utilities, the device is designed to provide a richer
mechanism for note entry within predominantly non-realtime editing tasks, in
applications where keyboard interaction is a central component of the user
experience (score editors, sequencers, DAWs, trackers, live coding), and in which
users draw on virtuosities in both music and computing.
In the keyboard, additional hardware combines existing scan code (key press)
data with accelerometer readings to create a secondary USB device, using the same
cable but visible to software as a separate USB MIDI device aside existing USB
HID functionality. This paper presents and evaluates an initial prototype,
developed using an Arduino board and inexpensive sensors, and discusses design
considerations and test findings in musical applications, drawing on user studies
of keyboard-mediated music interaction. Without challenging more established (and
expensive) performance devices; significant benefits are demonstrated in
notation-mediated interaction, where the user's focus rests with
software.}
}

@inproceedings{Greenhill2016,
  author = {Stewart Greenhill and Cathie Travers},
  title = {Focal : An Eye-Tracking Musical Expression Controller},
  pages = {230--235},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176022},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0046.pdf},
  abstract = {We present Focal, an eye-tracking musical expression controller
which allows
hands-free control over audio effects and synthesis parameters during
peformance. A see-through head-mounted display projects virtual dials and
switches into the visual field. The performer controls these with a single
expression pedal, switching context by glancing at the object they wish to
control. This simple interface allows for minimal physical disturbance to the
instrumental musician, whilst enabling the control of many parameters otherwise
only achievable with multiple foot pedalboards. We describe the development of
the system, including the construction of the eye-tracking display, and the
design of the musical interface. We also present a comparison of a performance
between Focal and conventional controllers. }
}

@inproceedings{Meacham2016,
  author = {Aidan Meacham and Sanjay Kannan and Ge Wang},
  title = {The Laptop Accordion},
  pages = {236--240},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176078},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0047.pdf},
  abstract = {The `Laptop Accordion' co-opts the commodity laptop
computer
to craft an expressive, whimsical accordion-like instrument.
It utilizes the opening and closing of the laptop
screen as a physical metaphor for accordion bellows, and the
laptop keyboard as musical buttonboard. Motion is tracked
using the laptop camera via optical flow and mapped to continuous
control over dynamics, while the sound is generated
in real-time. The instrument uses both skeuomorphic and
abstract onscreen graphics which further reference the core
mechanics of `squeezebox' instruments. The laptop accordion
provides several game modes, while overall offering an
unconventional aesthetic experience in music making.}
}

@inproceedings{Jakobsen2016,
  author = {Kasper buhl Jakobsen and Marianne Graves Petersen and Majken Kirkegaard Rasmussen and Jens Emil Groenbaek and jakob winge and jeppe stougaard},
  title = {Hitmachine: Collective Musical Expressivity for Novices},
  pages = {241--246},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176038},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0048.pdf},
  abstract = {This paper presents a novel platform for expressive music making
called Hitmachine. Hitmachine lets you build and play your own musical
instruments from Legos and sensors and is aimed towards empowering everyone to
engage in rich music making despite of prior musical experience. The paper
presents findings from a 4-day workshop where more that 150 children from ages
3-13 built and played their own musical instruments. The children used different
sensors for playing and performed with their instruments on stage. The findings
show how age influenced the children's musical understanding and
expressivity, and gives insight into important aspects to consider when designing
for expressive music for novices.}
}

@inproceedings{Michon2016,
  author = {Romain Michon and Julius Orion Iii Smith and Matthew Wright and Chris Chafe},
  title = {Augmenting the iPad: the BladeAxe},
  pages = {247--252},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176080},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0049.pdf},
  abstract = {In this paper, we present the BladeAxe: an iPad-based musical
instrument leveraging the concepts of augmented mobile device and hybrid
physical model controller. By being almost fully standalone, it can be used
easily on stage in the frame of a live performance by simply plugging it to a
traditional guitar amplifier or to any sound system. Its acoustical plucking
system provides the performer with an extended expressive potential compared to a
standard controller.
After presenting an intermediate version of the BladeAxe, we'll describe
our final design. We will also introduce a similar instrument: the PlateAxe.}
}

@inproceedings{Hnicode233on-Morissette2016,
  author = {Barah H\'{e}on-Morissette},
  title = {Transdisciplinary Methodology: from Theory to the Stage, Creation for the {SIC}MAP},
  pages = {253--258},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176024},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0050.pdf},
  abstract = {The author's artistic practice as a composer and performer is
transdisciplinary. The body as a vector associated with sound, gesture, video,
physical space, and technological space, constitute the six founding elements.
They give rise to works between music and dance, between musical theater and
multimedia works leading to a new hybrid performative practice. These works are
realized using a motion capture system by computer vision, SICMAP (Syst\'{e}me
Interactif de Captation du Mouvement en Art Performatif --- Interactive
Motion Capture System For The Performative Arts). In this paper, the author
situates her artistic practice founded by the three pillars of transdisciplinary
research methodology. The path taken by the performer-creator, leading to the
conception of the SICMAP, is then explained through a reflection on the
`dream instrument'. Followed by a technical description, the SICMAP
is contextualized using theoretical models: the instrumental continuum and energy
continuum, the `dream instrument' and the typology of the
instrumental gesture. Initiated by the SICMAP, these are then applied to a new
paradigm the gesture-sound space and subsequently put into practice through the
creation of the work From Infinity To Within.}
}

@inproceedings{Xiao2016,
  author = {Xiao Xiao and Donald Derek Haddad and Thomas Sanchez and Akito van Troyer and R\'{e}becca Kleinberger and Penny Webb and Joe Paradiso and Tod Machover and Hiroshi Ishii},
  title = {Kin\'{e}phone: Exploring the Musical Potential of an Actuated Pin-Based Shape Display},
  pages = {259--264},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176145},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0051.pdf},
  abstract = {This paper explores how an actuated pin-based shape display may
serve as a platform on which to build musical instruments and controllers. We
designed and prototyped three new instruments that use the shape display not only
as an input device, but also as a source of acoustic sound. These cover a range
of interaction paradigms to generate ambient textures, polyrhythms, and melodies.
This paper first presents existing work from which we drew interactions and
metaphors for our designs. We then introduce each of our instruments and the
back-end software we used to prototype them. Finally, we offer reflections on
some central themes of NIME, including the relationship between musician and
machine.}
}

@inproceedings{Waite2016,
  author = {Si Waite},
  title = {Church Belles: An Interactive System and Composition Using Real-World Metaphors},
  pages = {265--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176139},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0052.pdf},
  abstract = {This paper presents a brief review of current literature detailing
some of the issues and trends in composition and performance with interactive
music systems. Of particular interest is how musicians interact with a separate
machine entity that exercises agency over the creative process. The use of
real-world metaphors as a strategy for increasing audience engagement is also
discussed.
The composition and system Church Belles is presented, analyzed and evaluated in
terms of its architecture, how it relates to existing studies of musician-machine
creative interaction and how the use of a real-world metaphor can promote
audience perceptions of liveness. This develops previous NIME work by offering a
detailed case study of the development process of both a system and a piece for
popular, non-improvisational vocal/guitar music.}
}

@inproceedings{Olowe2016,
  author = {Ireti Olowe and Giulio Moro and Mathieu Barthet},
  title = {residUUm: user mapping and performance strategies for multilayered live audiovisual generation},
  pages = {271--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176098},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0053.pdf},
  abstract = {We propose residUUm, an audiovisual performance tool that uses
sonification to orchestrate a particle system of shapes, as an attempt to build
an audiovisual user interface in which all the actions of a performer on a laptop
are intended to be explicitly interpreted by the audience. We propose two
approaches to performing with residUUm and discuss the methods utilized to
fulfill the promise of audience-visible interaction: mapping and performance
strategies applied to express audiovisual interactions with multilayered
sound-image relationships. The system received positive feedback from 34 audience
participants on aspects such as aesthetics and audiovisual integration, and we
identified further design challenges around performance clarity and strategy. We
discuss residUUm's development objectives, modes of interaction and the impact of
an audience-visible interface on the performer and observer. }
}

@inproceedings{Bhumber2016,
  author = {Kirandeep Bhumber and Nancy Lee and Brian Topp},
  title = {Pendula: An Interactive Swing Installation and Performance Environment},
  pages = {277--285},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175992},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0054.pdf},
  abstract = {This paper describes the processes involved in developing Pendula,
a performance environment and interactive installation using swings, interactive
video, and audio. A presentation of the project is described using three swings.
Gyroscopic and accelerometer data were used in each of the setups to control
audio and visual parameters.The installation was presented as both an interactive
environment and as a performance instrument, with multiple public performances.
Construction of the physical devices used, circuits built, and software created
is covered in this paper, along with a discussion of problems and their solutions
encountered during the development of Pendula.}
}

@inproceedings{Dabin2016,
  author = {Matthew Dabin and Terumi Narushima and Stephen Beirne and Christian Ritz and Kraig Grady},
  title = {{3D} Modelling and Printing of Microtonal Flutes},
  pages = {286--290},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176014},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0056.pdf},
  abstract = {This project explores the potential for 3D modelling and printing
to create customised flutes that can play music in a variety of microtonal
scales. One of the challenges in the field of microtonality is that conventional
musical instruments are inadequate for realising the abundance of theoretical
tunings that musicians wish to investigate. This paper focuses on the development
of two types of flutes, the recorder and transverse flute, with interchangeable
mouthpieces. These flutes are designed to play subharmonic microtonal scales. The
discussion provides an overview of the design and implementation process,
including calculation methods for acoustic modelling and 3D printing
technologies, as well as an evaluation of some of the difficulties encountered.
Results from our 3D printed flutes suggest that whilst further refinements are
necessary in our designs, 3D modelling and printing techniques offer new and
valuable methods for the design and production of customised musical instruments.
The long term goal of this project is to create a system in which users can
specify the tuning of their instrument to generate a 3D model and have it printed
on demand. }
}

@inproceedings{Hofmann2016a,
  author = {Alex Hofmann and Bernt Waerstad and Kristoffer Koch},
  title = {Csound Instruments On Stage},
  pages = {291--294},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176030},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0057.pdf},
  abstract = {Low cost, credit card size computers like the Raspberry Pi allow
musicians to experiment with building software-based standalone musical
instruments. The COSMO Project aims to provide an easy-to-use hardware and
software framework to build Csound based instruments as hardware devices. Inside
the instrument, the Csound software is running on a Raspberry Pi computer,
connected to a custom designed interface board (COSMO-HAT) that allows to connect
potentiometers, switches, LED's, and sensors. A classic stomp box design is used
to demonstrate how Csound can be brought on stage as a stand-alone hardware
effect instrument.}
}

@inproceedings{Resch2016,
  author = {Thomas Resch and Stefan Bilbao},
  title = {Controlling complex virtuel instruments---A setup with note~ for Max and prepared piano sound synthesis},
  pages = {295--299},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176108},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0058.pdf},
  abstract = {This paper describes a setup for embedding complex virtual
instruments such as a physical model of the prepared piano sound synthesis in the
sequencing library note~ for Max. Based on the requirements of contemporary music
and media arts, note~ introduces computer-aided composition techniques and
graphical user interfaces for sequencing and editing into the real time world of
Max/MSP. A piano roll, a microtonal musical score and the capability to attach
floating-point lists of (theoretically) arbitrary length to a single note-on
event, enables artists to play, edit and record compound sound synthesis with the
necessary precision.}
}

@inproceedings{Brown2016,
  author = {Dom Brown and Nathan Renney and Adam Stark and Chris Nash and Tom Mitchell},
  title = {Leimu: Gloveless Music Interaction Using a Wrist Mounted Leap Motion},
  pages = {300--304},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176000},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0059.pdf},
  abstract = {Camera-based motion tracking has become a popular enabling
technology for gestural human-computer interaction. However, the approach suffers
from several limitations which have been shown to be particularly problematic
when employed within musical contexts. This paper presents Leimu, a wrist mount
that couples a Leap Motion optical sensor with an inertial measurement unit to
combine the benefits of wearable and camera-based motion tracking. Leimu is
designed, developed and then evaluated using discourse and statistical analysis
methods. The results indicate that the Leimu is an effective interface for
gestural music interaction and offers improved tracking precision over Leap
Motion positioned on a table top. }
}

@inproceedings{Gnicode243mez2016,
  author = {Esteban G\'{o}mez and Javier Jaimovich},
  title = {Designing a Flexible Workflow for Complex Real-Time Interactive Performances},
  pages = {305--309},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176018},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0060.pdf},
  abstract = {This paper presents the design of a Max/MSP flexible workflow
framework built for complex real-time interactive performances. This system was
developed for Emovere, an interdisciplinary piece for dance, biosignals, sound
and visuals, yet it was conceived to accommodate interactive performances of
different nature and of heterogeneous technical requirements, which we believe to
represent a common underlying structure among these.
The work presented in this document proposes a framework that takes care of the
signal input/output stages, as well as storing and recalling presets and scenes,
thus allowing the user to focus on the programming of interaction models and
sound synthesis or sound processing. Results are presented with Emovere as an
example case, discussing the advantages and further challenges that this
framework offers for other performance scenarios.}
}

@inproceedings{Volioti2016,
  author = {Christina Volioti and Sotiris Manitsaris and Eleni Katsouli and Athanasios Manitsaris},
  title = {x2Gesture: how machines could learn expressive gesture variations of expert musicians},
  pages = {310--315},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176137},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0061.pdf},
  abstract = {There is a growing interest in `unlocking' the motor
skills of expert musicians. Motivated by this need, the main objective of this
paper is to present a new way of modeling expressive gesture variations in
musical performance. For this purpose, the 3D gesture recognition engine
`x2Gesture' (eXpert eXpressive Gesture) has been developed, inspired
by the Gesture Variation Follower, which is initially designed and developed at
IRCAM in Paris and then extended at Goldsmiths College in London. x2Gesture
supports both learning of musical gestures and live performing, through gesture
sonification, as a unified user experience. The deeper understanding of the
expressive gestural variations permits to define the confidence bounds of the
expert's gestures, which are used during the decoding phase of the
recognition. The first experiments show promising results in terms of recognition
accuracy and temporal alignment between template and performed gesture, which
leads to a better fluidity and immediacy and thus gesture sonification. }
}

@inproceedings{Jaimovich2016,
  author = {Javier Jaimovich},
  title = {Emovere: Designing Sound Interactions for Biosignals and Dancers},
  pages = {316--320},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176036},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0062.pdf},
  abstract = {This paper presents the work developed for Emovere: an interactive real-time interdisciplinary performance that measures physiological signals from dancers to drive a piece that explores and reflects around the biology of emotion. This document focuses on the design of a series of interaction modes and materials that were developed for this performance, and are believed to be a contribution for the creation of artistic projects that work with dancers and physiological signals. The paper introduces the motivation and theoretical framework behind this project, to then deliver a detailed description and analysis of four different interaction modes built to drive this performance using electromyography and electrocardiography. Readers will find a discussion of the results obtained with these designs, as well as comments on future work.}
}

@inproceedings{Snicode248derberg2016,
  author = {S\''{o}derberg, Ene Alicia and Odgaard, Rasmus Emil and Sarah Bitsch and Oliver H\''{o}eg-Jensen and Christensen, Nikolaj Schildt and Poulsen, S\''{o}ren Dahl and Steven Gelineck},
  title = {Music Aid---Towards a Collaborative Experience for Deaf and Hearing People in Creating Music},
  pages = {321--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176112},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0063.pdf},
  abstract = {This paper explores the possibility of breaking the barrier
between deaf and hearing people when it comes to the subject of making music.
Suggestions on how deaf and hearing people can collaborate in creating music
together, are presented. The conducted research will focus on deaf people with a
general interest in music as well as hearing musicians as target groups. Through
reviewing different related research areas, it is found that visualization of
sound along with a haptic feedback can help deaf people interpret and interact
with music. With this in mind, three variations of a collaborative user interface
are presented, in which deaf and hearing people are meant to collaborate in
creating short beats and melody sequences. Through evaluating the three
prototypes, with two deaf people and two hearing musicians, it is found that the
target groups can collaborate to some extent in creating beats. However, in order
for the target groups to create melodic sequences together in a satisfactory
manner, more detailed visualization and distributed haptic output is necessary,
mostly due to the fact that the deaf test participants struggle in distinguishing
between higher pitch and timbre. }
}

@inproceedings{Larsen2016,
  author = {Jeppe Veirum Larsen and Dan Overholt and Thomas B. Moeslund},
  title = {The Prospects of Musical Instruments For People with Physical Disabilities},
  pages = {327--331},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176056},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0064.pdf},
  abstract = {Many forms of enabling technologies exist today. While
technologies aimed at enabling basic tasks in everyday life (locomotion, eating,
etc.) are more common, musical instruments for people with disabilities can
provide a chance for emotional enjoyment, as well as improve physical conditions
through therapeutic use. The field of musical instruments for people with
physical disabilities, however, is still an emerging area of research. In this
article, we look at the current state of developments, including a survey of
custom designed instruments, augmentations / modifications of existing
instruments, music-supported therapy, and recent trends in the area. The overview
is extrapolated to look at where the research is headed, providing insights for
potential future work.}
}

@inproceedings{Benson2016,
  author = {Christopher Benson and Bill Manaris and Seth Stoudenmier and Timothy Ward},
  title = {SoundMorpheus: A Myoelectric-Sensor Based Interface for Sound Spatialization and Shaping},
  pages = {332--337},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175982},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0065.pdf},
  abstract = {We present an innovative sound spatialization and shaping
interface, called SoundMorpheus, which allows the placement of sounds in space,
as well as the altering of sound characteristics, via arm movements that resemble
those of a conductor. The interface displays sounds (or their attributes) to the
user, who reaches for them with one or both hands, grabs them, and gently or
forcefully sends them around in space, in a 360$^{\circ}$ circle. The system
combines MIDI and traditional instruments with one or more myoelectric sensors.
These components may be physically collocated or distributed in various locales
connected via the Internet. This system also supports the performance of
acousmatic and electronic music, enabling performances where the traditionally
central mixing board, need not be touched at all (or minimally touched for
calibration). Finally, the system may facilitate the recording of a visual score
of a performance, which can be stored for later playback and additional
manipulation. We present three projects that utilize SoundMorpheus and
demonstrate its capabilities and potential.}
}

@inproceedings{Ozdemir2016,
  author = {Gorkem Ozdemir and Anil Camci and Angus Forbes},
  title = {PORTAL: An Audiovisual Laser Performance System},
  pages = {338--343},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176102},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0066.pdf},
  abstract = {PORTAL is an interactive performance tool that uses a laser
projector to visualize computer-generated audio signals. In this paper, we first
offer an overview of earlier work on audiovisual and laser art that inspired the
current project. We then discuss our own implementation, focusing not only on the
technical issues related to the use of a laser projector in an artistic context,
but also on the aesthetic considerations in dealing with the translation of
sounds into visuals, and vice versa. We provide detailed descriptions of our
hardware implementation, our software system, and its desktop and mobile
interfaces, which are made available online. Finally, we offer the results of a
user study we conducted in the form of an interactive online survey on audience
perception of the relationship between analogous sounds and visuals, which was
explored as part of our performance practice.}
}

@inproceedings{Lindell2016,
  author = {Rikard Lindell and Koray Tahiroglu and Morten Riis and Jennie Schaeffer},
  title = {Materiality for Musical Expressions: an Approach to Interdisciplinary Syllabus Development for NIME},
  pages = {344--349},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176066},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0067.pdf},
  abstract = {We organised an elven day intense course in materiality for
musical expressions to explore underlying principles of New Interfaces for
Musical Expression (NIME) in higher education. We grounded the course in
different aspects of materiality and gathered interdisciplinary student teams
from three Nordic universities. Electronic music instrument makers participated
in providing the course. In eleven days the students designed and built
interfaces for musical expressions, composed a piece, and performed at the
Norberg electronic music festival. The students explored the relationship
between technology and possible musical expression with a strong connection to
culture and place. The emphasis on performance provided closure and motivated
teams to move forward in their design and artistic processes. On the basis of the
course we discuss an interdisciplinary NIME course syllabus, and we infer that it
benefits from grounding in materiality and in the place with a strong reference
to culture.}
}

@inproceedings{Gimenes2016,
  author = {Marcelo Gimenes and Pierre-Emmanuel Largeron and Eduardo Miranda},
  title = {Frontiers: Expanding Musical Imagination With Audience Participation},
  pages = {350--354},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176020},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0068.pdf},
  abstract = {This paper introduces Performance Without Borders and Embodied
iSound, two sound installations performed at the 2016 Peninsula Arts Contemporary
Music Festival at Plymouth University. Sharing in common the use of smartphones
to afford real-time audience participation, two bespoke distributed computer
systems (Sherwell and Levinsky Music, respectively). Whilst the first one
implements a cloud-based voting system, the second implements movement tracking
and iBeacon-based indoor-positioning to control the choice of soundtracks, audio
synthesis, and surround sound positioning, among other parameters. The general
concepts of the installations, in particular design and interactive possibilities
afforded by the computer systems are presented.}
}

@inproceedings{Schlei2016,
  author = {Kevin Schlei and Chris Burns and Aidan Menuge},
  title = {PourOver: A Sensor-Driven Generative Music Platform},
  pages = {355--358},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176114},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0069.pdf},
  abstract = {The PourOver Sensor Framework is an open iOS framework designed to
connect iOS control sources (hardware sensors, user input, custom algorithms) to
an audio graph's parameters. The design of the framework, motivation, and use
cases are discussed. The framework is demonstrated in an end-user friendly iOS
app PourOver, in which users can run Pd patches with easy access to hardware
sensors and iOS APIs.}
}

@inproceedings{Hindle2016,
  author = {Abram Hindle},
  title = {Hacking NIMEs},
  pages = {359--364},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176026},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0070.pdf},
  abstract = { NIMEs typically focus on novelty but the cost of novelty is
often to
ignore other non-functional requirements and concerns such as
usability or security. Digital security has probably not been a
concern for performers due to the duration of their performances and
lack of disrespectful hackers, known as crackers, in attendance
carrying the appropriate equipment and software necessary to hack a
performance. Yet many modern NIMEs could be hacked from smart-phones
in the audience. The lack of security hardening makes NIMEs an easy
target --- but a question arises: if hacking can interrupt or modify
a performance couldn't hacking itself also be performance? Thus
would music hacking, live-hacking, be similar to live-coding? In
this paper we discuss how NIMEs are in danger of being hacked, and
yet how hacking can be an act of performance too.}
}

@inproceedings{Jordnicode2252016,
  author = {Sergi Jord\`{a} and Daniel G\'{o}mez-Mar\'{i}n and \'{A}ngel Faraldo and Perfecto Herrera},
  title = {Drumming with style: From user needs to a working prototype},
  pages = {365--370},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176048},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0071.pdf},
  abstract = {This paper presents a generative drumming agent built from the results of an extensive survey carried out with electronic music producers, in two phases. Following the techniques of user-centered interaction design, an international group of beat producers was reviewed on the possibility of using AI algorithms to help them in the beat production workflow. The analyzed results of these tests were used as design requirements for constructing a system that would indeed perform some tasks alongside the producer. The first results of this working prototype are presented with a description of the system. The prototype is a stylistic drum generator that creates new rhythmic patterns after being trained with a collection of drum tracks. Further stages of development and potential algorithms are discussed.}
}

@inproceedings{Bown2016,
  author = {Oliver Bown and Sam Ferguson},
  title = {A Musical Game of Bowls Using the DIADs},
  pages = {371--372},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1175998},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0072.pdf},
  abstract = {We describe a project in which a game of lawn bowls was recreated
using Distributed Interactive Audio Devices (DIADs), to create an interactive
musical experience in the form of a game. This paper details the design of the
underlying digital music system, some of the compositional and design
considerations, and the technical challenges involved. We discuss future
directions for our system and compositional method.}
}

@inproceedings{Eyes2016,
  author = {Benjamin James Eyes and Laurits Esben Jongejan},
  title = {How to Stop Sound: Creating a light instrument and `Interruption' a piece for the Mimerlaven, Norberg Festival 2015.},
  pages = {373--374},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176016},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0073.pdf},
  abstract = {During an electronic music performance it is common to see light
and sound interacting electronically in many different ways. From sound and light
shows, whereby light reacts to sound, or generated visuals are projected onto a
screen behind the performer. However we asked the question what if we could
convert sound to light and back again and control sound with light? Inspired by
the huge acoustic of the Mimerlaven at Norberg festival we built a `light
instrument' that allowed us to interrupt and disrupt sound using light
forming the basis of our piece `Interruption'.}
}

@inproceedings{Hope2016,
  author = {Cat Hope and Stuart James and Aaron Wyatt},
  title = {Headline grabs for music: The development of the iPad score generator for `Loaded (NSFW)'},
  pages = {375--376},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Demonstrations},
  doi = {10.5281/zenodo.1176032},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0074.pdf},
  abstract = {This paper-demonstration provides an overview of an generative
music score adapted for the iPad by the Decibel new music ensemble. The original
score `Loaded (NSFW)' (2015) is by Western Australian composer Laura
Jane Lowther, and is scored for ensemble and electronics, commissioned for a
performance in April 2015 at the Perth Institute of Contemporary Arts. It engages
and develops the Decibel Score Player application, a score reader and generator
for the iPad as a tool for displaying an interactive score that requires
performers to react to news headlines through musical means. The paper will
introduce the concept for the player, how it was developed, and how it was used
in the premiere performance. The associated demonstration shows how the score
appears on the iPads. }
}

@inproceedings{Carey2016,
  author = {Benjamin Carey and Andrew Johnston},
  title = {Reflection On Action in NIME Research: Two Complementary Perspectives},
  pages = {377--382},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176006},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0075.pdf},
  abstract = {This paper discusses practice-based research in the context of
live performance with interactive systems. We focus on two approaches, both of
which are concerned with documenting, examining and reflecting on the real-world
behaviours and experiences of people and artefacts involved in the creation of
new works. The first approach is primarily based on reflections by an individual
performer/developer (auto-ethnography) and the second on interviews and
observations. The rationales for both approaches are presented along with
findings from research which applied them in order to illustrate and explore the
characteristics of both. Challenges, including the difficulty of balancing
rigour and relevance and the risks of negatively impacting on creative practices
are articulated, as are the potential benefits.}
}

@inproceedings{Nuannicode225in2016,
  author = {C\`{a}rthach \'{O} Nuan\`{a}in and Sergi Jord\`{a} and Perfecto Herrera},
  title = {An Interactive Software Instrument for Real-time Rhythmic Concatenative Synthesis},
  pages = {383--387},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176094},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0076.pdf},
  abstract = {In this paper we describe an approach for generating and
visualising new rhythmic patterns from existing audio in real-time using
concatenative synthesis. We introduce a graph-based model enabling novel
visualisation and manipulation of new patterns that mimics the rhythmic and
timbral character of an existing target seed pattern using a separate database of
palette sounds. Our approach is described, reporting on those features that may
be useful in describing units of sound related to rhythm and how they might then
be projected into two-dimensional space for visualisation using reduction
techniques and clustering. We conclude the paper with our qualitative appraisal
of using the interface and outline scope for future work.}
}

@inproceedings{Milne2016,
  author = {Andrew J. Milne and Steffen A. Herff and David Bulger and William A. Sethares and Roger T. Dean},
  title = {XronoMorph: Algorithmic Generation of Perfectly Balanced and Well-Formed Rhythms},
  pages = {388--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176082},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0077.pdf},
  abstract = {We present an application XronoMorph for the
algorithmic generation of rhythms in the context of creative composition and
performance, and of musical analysis and education. XronoMorph makes use of
visual and geometrical conceptualizations of rhythms, and allows the user to
smoothly morph between rhythms. Sonification of the user generated geometrical
constructs is possible using a built-in sampler, VST and AU plugins, or
standalone synthesizers via MIDI. The algorithms are based on two underlying
mathematical principles: perfect balance and well-formedness, both of which can
be derived from coefficients of the discrete Fourier transform of the rhythm. The
mathematical background, musical implications, and their implementation in the
software are discussed.}
}

@inproceedings{Vickery2016,
  author = {Lindsay Vickery},
  title = {Rhizomatic approaches to screen-based music notation},
  pages = {394--400},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176133},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0078.pdf},
  abstract = {The rhizome concept explored by Deleuze and Guatarri has had an
important influence on formal thinking in music and new media. This paper
explores the development of rhizomatic musical scores that are arranged
cartographically with nodal points allowing for alternate pathways to be
traversed. The challenges of pre-digital exemplars of rhizomatic structure are
discussed. It follows the development of concepts and technology used in the
creation of five works by the author Ubahn c. 1985: the Rosenberg Variations
[2012], The Last Years [2012], Sacrificial Zones [2014], detritus [2015] and
trash vortex [2015]. The paper discusses the potential for the evolution of novel
formal structures using rhizomatic structures. }
}

@inproceedings{James2016,
  author = {Stuart James},
  title = {A Multi-Point {2D} Interface: Audio-Rate Signals for Controlling Complex Multi-Parametric Sound Synthesis},
  pages = {401--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176040},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0079.pdf},
  abstract = {This paper documents a method of controlling complex sound
synthesis processes such as granular synthesis, additive synthesis, timbre
morphology, swarm-based spatialisation, spectral spatialisation, and timbre
spatialisation via a multi-parametric 2D interface. This paper evaluates the use
of audio-rate control signals for sound synthesis, and discussing approaches to
de-interleaving, synchronization, and mapping. The paper also outlines a number
of ways of extending the expressivity of such a control interface by coupling
this with another 2D multi-parametric nodes interface and audio-rate 2D table
lookup. The paper proceeds to review methods of navigating multi-parameter sets
via interpolation and transformation. Some case studies are finally discussed in
the paper. The author has used this method to control complex sound synthesis
processes that require control data for more that a thousand parameters.}
}

@inproceedings{Schlienger2016,
  author = {Dominik Schlienger},
  title = {Acoustic Localisation for Spatial Reproduction of Moving Sound Source: Application Scenarios \& Proof of Concept},
  pages = {407--412},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176116},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0080.pdf},
  abstract = {Despite the near ubiquitous availability of interfaces for spatial
interaction, standard audio spatialisation technology makes very little use of
it. In fact, we find that audio technology often impedes spatial interaction: In
the workshop on music, space and interaction we thus developed the idea of a
real-time panning whereby a moving sound source is reproduced as a virtual source
on a panning trajectory. We define a series of application scenarios where we
describe in detail what functionality is required to inform an implementation. In
our earlier work we showed that Acoustic Localisation (AL) potentially can
provide a pervasive technique for spatially interactive audio applications.
Playing through the application scenarios with AL in mind provides interesting
approaches. For one scenario we show an example implementation as proof of
concept.}
}

@inproceedings{Soraghan2016,
  author = {Sean Soraghan and Alain Renaud and Ben Supper},
  title = {Towards a perceptual framework for interface design in digital environments for timbre manipulation},
  pages = {413--418},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176129},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0081.pdf},
  abstract = {Many commercial software applications for timbre creation and
manipulation feature an engineering-focused, parametric layout. This paper argues
the case for a perceptually motivated approach to interface design in such tools.
`Perceptually motivated' in this context refers to the use of common semantic
timbre descriptors to influence the digital representation of timbre. A review is
given of existing research into semantic descriptors of timbre, as well as
corresponding acoustic features of timbre. Discussion is also given on existing
interface design techniques. The perceptually motivated approach to interface
design is demonstrated using an example system, which makes use of perceptually
relevant mappings from acoustic timbre features to semantic timbre descriptors
and visualises sounds as physical objects.}
}

@inproceedings{Reid2016,
  author = {Sarah Reid and Ryan Gaston and Colin Honigman and Ajay Kapur},
  title = {Minimally Invasive Gesture Sensing Interface (MIGSI) for Trumpet},
  pages = {419--424},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176106},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0082.pdf},
  abstract = {This paper describes the design of a Minimally Invasive Gesture
Sensing Interface (MIGSI) for trumpet. The interface attaches effortlessly to any
B-flat or C trumpet and requires no permanent modifications to the
host-instrument. It was designed first and foremost with accessibility in
mind an approach that is uncommon in augmented instrument design and
seeks to strike a balance between minimal design and robust control. MIGSI uses
sensor technology to capture gestural data such as valve displacement, hand
tension, and instrument position, to offer extended control and expressivity to
trumpet players. Several streams of continuous data are transmitted wirelessly
from MIGSI to the receiving computer, where MIGSI Mapping application (a simple
graphical user interface) parses the incoming data into individually accessible
variables. It is our hope that MIGSI will be adopted by trumpet players and
composers, and that over time a new body of repertoire for the augmented trumpet
will emerge.}
}

@inproceedings{Paine2016,
  author = {Garth Paine},
  title = {Now},
  pages = {425--426},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176104},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0083.pdf},
  abstract = {The question of sound as an experience of now, as a conduit to the
quality of our belonging to the present, is challenging. Yet it is a crucial
issue in discussions about ecological listening. I have come to think of sound as
a viscous material, a vibrating field of energy that has texture and density and
a physicality that is unlike most other media.
Now suggests a desire of becoming present in the resonating sound field of our
immediate environment. The energy in the field constantly modulates and drifts. I
draw on voices and forces from the natural environment, humans and machines. The
work seeks to draw the listeners into an inner space in which they can be both
present and aware of their sonic environment and become immersed in it. Now is
partly inspired by Samuel Beckett's novel Watt, specifically Watt's
mysterious journey into to the unknown.}
}

@inproceedings{Shapiro2016,
  author = {R. Benjamin Shapiro and Rebecca Fiebrink and Matthew Ahrens and Annie Kelly},
  title = {BlockyTalky: A Physical and Distributed Computer Music Toolkit for Kids},
  pages = {427--432},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176120},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0084.pdf},
  abstract = {NIME research realizes a vision of performance by means of
computational expression, linking body and space to sound and imagery through
eclectic forms of sensing and interaction. This vision could dramatically impact
computer science education, simultaneously modernizing the field and drawing in
diverse new participants. We describe our work creating a NIME-inspired computer
music toolkit for kids called BlockyTalky; the toolkit enables users to create
networks of sensing devices and synthesizers. We offer findings from our research
on student learning through programming and performance. We conclude by
suggesting a number of future directions for NIME researchers interested in
education.}
}

@inproceedings{Bowers2016,
  author = {John Bowers and John Richards and Tim Shaw and Jim Frieze and Ben Freeth and Sam Topley and Neal Spowage and Steve Jones and Amit Patel and Li Rui},
  title = {One Knob To Rule Them All: Reductionist Interfaces for Expansionist Research},
  pages = {433--438},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1175996},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0085.pdf},
  abstract = {This paper describes an instance of what we call `curated research', a concerted thinking, making and performance activity between two research teams with a dedicated interest in the creation of experimental musical instruments and the development of new performance practices. Our work builds theoretically upon critical work in philosophy, anthropology and aesthetics, and practically upon previous explorations of strategies for facilitating rapid, collaborative, publicly-oriented making in artistic settings. We explored an orientation to making which promoted the creation of a family of instruments and performance environments that were responses to the self-consciously provocative theme of `One Knob To Rule Them All'. A variety of design issues were explored including: mapping, physicality, the question of control in interface design, reductionist aesthetics and design strategies, and questions of gender and power in musical culture. We discuss not only the technologies which were made but also reflect on the value of such concerted, provocatively thematised, collective making activities for addressing foundational design issues. As such, our work is intended not just as a technical and practical contribution to NIME but also a reflective provocation into how we conduct research itself in a curated critical manner.}
}

@inproceedings{Jensenius2016,
  author = {Alexander Refsum Jensenius and Michael J. Lyons},
  title = {Trends at NIME---Reflections on Editing A NIME Reader},
  pages = {439--443},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176044},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0086.pdf},
  abstract = {This paper provides an overview of the process of editing the
forthcoming anthology A NIME Reader---Fifteen years of New Interfaces for
Musical Expression. The selection process is presented, and we reflect on some
of the trends we have observed in re-discovering the collection of more than 1200
NIME papers published throughout the 15 yearlong history of the conference. An
anthology is necessarily selective, and ours is no exception. As we present in
this paper, the aim has been to represent the wide range of artistic,
scientific, and technological approaches that characterize the NIME conference.
The anthology also includes critical discourse, and through acknowledgment of the
strengths and weaknesses of the NIME community, we propose activities which could
further diversify and strengthen the field.}
}

@inproceedings{Tahironicode287lu2016,
  author = {Koray Tahiroglu and Juan Carlos Vasquez and Johan Kildal},
  title = {Non-intrusive Counter-actions: Maintaining Progressively Engaging Interactions for Music Performance},
  pages = {444--449},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  volume = {16},
  year = {2016},
  publisher = {Queensland Conservatorium Griffith University},
  address = {Brisbane, Australia},
  isbn = {978-1-925455-13-7},
  issn = {2220-4806},
  track = {Papers},
  doi = {10.5281/zenodo.1176131},
  url = {http://www.nime.org/proceedings/2016/nime2016_paper0087.pdf},
  abstract = {In this paper we present the new development of a semi-autonomous
response module for the NOISA system. NOISA is an interactive music system that
predicts performer's engagement levels, learns from the performer, decides what
to do and does it at the right moment. As an improvement for the above, we
implemented real-time adaptive features that respond to a detailed monitoring of
the performer's engagement and to overall sonic space, while evaluating the
impact of its actions. Through these new features, the response module produces
meaningful and non-intrusive counter actions, attempting to deepen and maintain
the performer's engagement in musical interaction. In a formative study we
compared our designed response module against a random control system of events,
in which the former performed consistently better than the latter.}
}

