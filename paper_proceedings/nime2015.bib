@inproceedings{ckorda2015,
  author = {Chris Korda},
  title = {ChordEase: A {MIDI} remapper for intuitive performance of non-modal music},
  pages = {322--324},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179110},
  url = {http://www.nime.org/proceedings/2015/nime2015_103.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/103/0103-file1.avi},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/103/0103-file2.avi},
  abstract = {Improvising to non-modal chord progressions such as those found in jazz necessitates switching between the different scales implied by each chord. This work attempted to simplify improvisation by delegating the process of switching scales to a computer. An open-source software MIDI remapper called ChordEase was developed that dynamically alters the pitch of notes, in order to fit them to the chord scales of a predetermined song. ChordEase modifies the behavior of ordinary MIDI instruments, giving them new interfaces that permit non-modal music to be approached as if it were modal. Multiple instruments can be remapped simultaneously, using a variety of mapping functions, each optimized for a particular musical role. Harmonization and orchestration can also be automated. By facilitating the selection of scale tones, ChordEase enables performers to focus on other aspects of improvisation, and thus creates new possibilities for musical expression.}
}

@inproceedings{makbari2015,
  author = {Mohammad Akbari and Howard Cheng},
  title = {claVision: Visual Automatic Piano Music Transcription},
  pages = {313--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179002},
  url = {http://www.nime.org/proceedings/2015/nime2015_105.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/105/0105-file1.avi},
  abstract = {One important problem in Musical Information Retrieval is Automatic Music Transcription, which is an automated conversion process from played music to a symbolic notation such as sheet music. Since the accuracy of previous audio-based transcription systems is not satisfactory, we propose an innovative visual-based automatic music transcription system named claVision to perform piano music transcription. Instead of processing the music audio, the system performs the transcription only from the video performance captured by a camera mounted over the piano keyboard. claVision can be used as a transcription tool, but it also has other applications such as music education. The claVision software has a very high accuracy (over 95%) and a very low latency in real-time music transcription, even under different illumination conditions.}
}

@inproceedings{jschacher2015,
  author = {{Jan C.} Schacher and Chikashi Miyama and Daniel Bisig},
  title = {Gestural Electronic Music using Machine Learning as Generative Device},
  pages = {347--350},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179172},
  url = {http://www.nime.org/proceedings/2015/nime2015_117.pdf},
  abstract = {When performing with gestural devices in combination with machine learning techniques, a mode of high-level interaction can be achieved. The methods of machine learning and pattern recognition can be re-appropriated to serve as a generative principle. The goal is not classification but reaction from the system in an interactive and autonomous manner. This investigation looks at how machine learning algorithms fit generative purposes and what independent behaviours they enable. To this end we describe artistic and technical developments made to leverage existing machine learning algorithms as generative devices and discuss their relevance to the field of gestural interaction.}
}

@inproceedings{spapetti2015,
  author = {Stefano Papetti and S\'ebastien Schiesser and Martin Fr\''ohlich},
  title = {Multi-point vibrotactile feedback for an expressive musical interface},
  pages = {235--240},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179152},
  url = {http://www.nime.org/proceedings/2015/nime2015_118.pdf},
  abstract = {This paper describes the design of a hardware/software system for rendering multi-point, localized vibrotactile feedback in a multi-touch musical interface. A prototype was developed, based on the Madrona Labs Soundplane, which was chosen for it provides easy access to multi-touch data, including force, and its easily expandable layered construction. The proposed solution makes use of several piezo actuator discs, densely arranged in a honeycomb pattern on a thin PCB layer. Based on off-the-shelf components, custom amplifying and routing electronics were designed to drive each piezo element with standard audio signals. Features, as well as electronic and mechanical issues of the current prototype are discussed.}
}

@inproceedings{dramsay2015,
  author = {David Ramsay and Joseph Paradiso},
  title = {GroupLoop: A Collaborative, Network-Enabled Audio Feedback Instrument},
  pages = {251--254},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179158},
  url = {http://www.nime.org/proceedings/2015/nime2015_119.pdf},
  abstract = {GroupLoop is a browser-based, collaborative audio feedback control system for musical performance. GroupLoop users send their microphone stream to other participants while simultaneously controlling the mix of other users' streams played through their speakers. Collaborations among users can yield complex feedback loops where feedback paths overlap and interact. Users are able to shape the feedback sounds in real-time by adjusting delay, EQ, and gain, as well as manipulating the acoustics of their portion of the audio feedback path. This paper outlines the basic principles underlying Grouploop, describes its design and feature-set, and discusses observations of GroupLoop in performances. It concludes with a look at future research and refinement. }
}

@inproceedings{kyamamoto2015,
  author = {Kazuhiko Yamamoto and Takeo Igarashi},
  title = {LiVo: Sing a Song with a Vowel Keyboard},
  pages = {205--208},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181414},
  url = {http://www.nime.org/proceedings/2015/nime2015_120.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/120/0120-file1.mp4},
  abstract = {We propose a novel user interface that enables control of a singing voice synthesizer at a live improvisational performance. The user first registers the lyrics of a song with the system before performance, and the system builds a probabilistic model that models the possible jumps within the lyrics. During performance, the user simultaneously inputs the lyrics of a song with the left hand using a vowel keyboard and the melodies with the right hand using a standard musical keyboard. Our system searches for a portion of the registered lyrics whose vowel sequence matches the current user input using the probabilistic model, and sends the matched lyrics to the singing voice synthesizer. The vowel input keys are mapped onto a standard musical keyboard, enabling experienced keyboard players to learn the system from a standard musical score. We examine the feasibility of the system through a series of evaluations and user studies. }
}

@inproceedings{ktahiroglu2015,
  author = {Koray Tahiroglu and Thomas Svedstr\''om and Valtteri Wikstr\''om},
  title = {Musical Engagement that is Predicated on Intentional Activity of the Performer with NOISA Instruments},
  pages = {132--135},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179182},
  url = {http://www.nime.org/proceedings/2015/nime2015_121.pdf},
  abstract = {This paper presents our current research in which we study the notion of performer engagement within the variance and diversities of the intentional activities of the performer in musical interaction. We introduce a user-test study with the aim to evaluate our system's engagement prediction capability and to understand in detail the system's response behaviour. The quantitative results indicate that our system recognises and monitors performer's engagement successfully, although we found that the system's response to maintain and deepen the performer's engagement is perceived differently among participants. The results reported in this paper can be used to inform the design of interactive systems that enhance the quality of performer's engagement in musical interaction with new interfaces.}
}

@inproceedings{jlong2015,
  author = {Jason Long and Jim Murphy and Ajay Kapur and Dale Carnegie},
  title = {A Methodology for Evaluating Robotic Striking Mechanisms for Musical Contexts},
  pages = {404--407},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179120},
  url = {http://www.nime.org/proceedings/2015/nime2015_130.pdf},
  abstract = {This paper presents a methodology for evaluating the performance of several types of striking mechanism commonly utilized in musical robotic percussion systems. The goal is to take steps towards standardizing methods of comparing the attributes of a range of devices to inform their design and application in various musical situations. A system for testing the latency, consistency, loudness and striking speed of these mechanisms is described and the methods are demonstrated by subjecting several new robotic percussion mechanisms to these tests. An analysis of the results of the evaluation is also presented and the advantages and disadvantages of each of the types of mechanism in various musical contexts is discussed.}
}

@inproceedings{skemper2015,
  author = {Troy Rogers and Steven Kemper and Scott Barton},
  title = {MARIE: Monochord-Aerophone Robotic Instrument Ensemble},
  pages = {408--411},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179166},
  url = {http://www.nime.org/proceedings/2015/nime2015_141.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/141/0141-file1.mov},
  abstract = {The Modular Electro-Acoustic Robotic Instrument System (MEARIS) represents a new type of hybrid electroacoustic-electromechanical instrument model. Monochord-Aerophone Robotic Instrument Ensemble (MARIE), the first realization of a MEARIS, is a set of interconnected monochord and cylindrical aerophone robotic musical instruments created by Expressive Machines Musical Instruments (EMMI). MARIE comprises one or more matched pairs of Automatic Monochord Instruments (AMI) and Cylindrical Aerophone Robotic Instruments (CARI). Each AMI and CARI is a self-contained, independently operable robotic instrument with an acoustic element, a control system that enables automated manipulation of this element, and an audio system that includes input and output transducers coupled to the acoustic element. Each AMI-CARI pair can also operate as an interconnected hybrid instrument, allowing for effects that have heretofore been the domain of physical modeling technologies, such as a plucked air column or blown string. Since its creation, MARIE has toured widely, performed with dozens of human instrumentalists, and has been utilized by nine composers in the realization of more than twenty new musical works.}
}

@inproceedings{jharriman2015,
  author = {Jiffer Harriman},
  title = {Pd Poems and Teaching Tools},
  pages = {331--334},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179074},
  url = {http://www.nime.org/proceedings/2015/nime2015_145.pdf},
  abstract = {Music offers an intriguing context to engage children in electronics, programming and more. Over the last year we been developing a hardware and software toolkit for music called modular-muse. Here we describe the design and goals for these tools and how they have been used in different settings to introduce children to concepts of interaction design for music and sound design. Two exploratory studies which used modular-muse are described here with different approaches; a two day build your own instrument workshop where participants learned how to use both hardware and software concurrently to control synthesized sounds and trigger solenoids, and a middle school music classroom where the focus was only on programming for sound synthesis using the modular-muse Pd library. During the second study, a project called Pd Poems, a teaching progression emerged we call Build-Play-Share-Focus which is also described. }
}

@inproceedings{rhayward2015,
  author = {Robin Hayward},
  title = {The Hayward Tuning Vine: an interface for Just Intonation},
  pages = {209--214},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179084},
  url = {http://www.nime.org/proceedings/2015/nime2015_146.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/146/0146-file1.mov},
  abstract = {The Hayward Tuning Vine is a software interface for exploring the system of microtonal tuning known as Just Intonation. Based ultimately on prime number relationships, harmonic space in Just Intonation is inherently multidimensional, with each prime number tracing a unique path in space. Taking this multidimensionality as its point of departure, the Tuning Vine interface assigns a unique angle and colour to each prime number, along with aligning melodic pitch height to vertical height on the computer screen. These features allow direct and intuitive interaction with Just Intonation. The inclusion of a transposition function along each prime number axis also enables potentially unlimited exploration of harmonic space within prime limit 23. Currently available as desktop software, a prototype for a hardware version has also been constructed, and future tablet app and hardware versions of the Tuning Vine are planned that will allow tangible as well as audiovisual interaction with microtonal harmonic space.}
}

@inproceedings{mkrzyzaniak2015,
  author = {Michael Krzyzaniak and Garth Paine},
  title = {Realtime Classification of Hand-Drum Strokes},
  pages = {400--403},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179112},
  url = {http://www.nime.org/proceedings/2015/nime2015_147.pdf},
  abstract = {Herein is presented a method of classifying hand-drum strokes in real-time by analyzing 50 milliseconds of audio signal as recorded by a contact-mic affixed to the body of the instrument. The classifier performs with an average accuracy of about 95% across several experiments on archetypical strokes, and 89% on uncontrived playing. A complete ANSI C implementation for OSX and Linux is available on the author's website.}
}

@inproceedings{rvanrooyen2015,
  author = {Robert Van Rooyen and Andrew Schloss and George Tzanetakis},
  title = {Snare Drum Motion Capture Dataset},
  pages = {329--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179168},
  url = {http://www.nime.org/proceedings/2015/nime2015_148.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/148/0148-file1.mp4},
  abstract = {Comparative studies require a baseline reference and a documented process to capture new subject data. This paper combined with its principal reference [1] presents a definitive dataset in the context of snare drum performances along with a procedure for data acquisition, and a methodology for quantitative analysis. The dataset contains video, audio, and discrete two dimensional motion data for forty standardized percussive rudiments.}
}

@inproceedings{rbhandari2015,
  author = {Rhushabh Bhandari and Avinash Parnandi and Eva Shipp and Beena Ahmed and Ricardo Gutierrez-Osuna},
  title = {Music-based respiratory biofeedback in visually-demanding tasks},
  pages = {78--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179030},
  url = {http://www.nime.org/proceedings/2015/nime2015_149.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/149/0149-file1.mp4},
  abstract = {Biofeedback tools generally use visualizations to display physiological information to the user. As such, these tools are incompatible with visually demanding tasks such as driving. While auditory or haptic biofeedback may be used in these cases, the additional sensory channels can increase workload or act as a nuisance to the user. A number of studies, however, have shown that music can improve mood and concentration, while also reduce aggression and boredom. Here, we propose an intervention that combines the benefits of biofeedback and music to help users regulate their stress response while performing a visual task (driving a car simulator). Our approach encourages slow breathing by adjusting the quality of the music in response to the user's breathing rate. We evaluate the intervention on a 2$\times$2 design with music and auditory biofeedback as independent variables. Our results indicate that our music-biofeedback intervention leads to lower arousal (reduced electrodermal activity and increased heart rate variability) than music alone, auditory biofeedback alone and a control condition. }
}

@inproceedings{mmyllykoski2015,
  author = {Mikko Myllykoski and Kai Tuuri and Esa Viirret and Jukka Louhivuori and Antti Peltomaa and Janne Kek\''al\''ainen},
  title = {Prototyping hand-based wearable music education technology},
  pages = {182--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179144},
  url = {http://www.nime.org/proceedings/2015/nime2015_151.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/151/0151-file1.m4v},
  abstract = {This paper discusses perspectives for conceptualizing and developing hand-based wearable musical interface. Previous implementations of such interfaces have not been targeted for music pedagogical use. We propose principles for pedagogically oriented `musical hand' and outline its development through the process of prototyping, which involves a variety of methods. The current functional prototype, a touch-based musical glove, is presented. }
}

@inproceedings{jharrimanb2015,
  author = {Jiffer Harriman},
  title = {Feedback Lapsteel : Exploring Tactile Transducers As String Actuators},
  pages = {178--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179076},
  url = {http://www.nime.org/proceedings/2015/nime2015_152.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/152/0152-file1.mp4},
  abstract = {The Feedback Lap Steel is an actuated instrument which makes use of mechanical vibration of the instruments bridge to excite the strings. A custom bridge mounted directly to a tactile transducer enables the strings to be driven with any audio signal from a standard audio amplifier. The instrument can be played as a traditional lap steel guitar without any changes to playing technique as well as be used to create new sounds which blur the line between acoustic and electronic through a combination of acoustic and computer generated and controlled sounds. This introduces a new approach to string actuation using commonly available parts. This demonstration paper details the construction, uses and lessons learned in the making of the Feedback Lap Steel guitar.}
}

@inproceedings{rmichon2015,
  author = {Romain Michon and {Julius Orion} {Smith III} and Yann Orlarey},
  title = {MobileFaust: a Set of Tools to Make Musical Mobile Applications with the Faust Programming Language},
  pages = {396--399},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179140},
  url = {http://www.nime.org/proceedings/2015/nime2015_153.pdf},
  abstract = {This work presents a series of tools to turn Faust code into various elements ranging from fully functional applications to multi-platform libraries for real time audio signal processing on iOS and Android. Technical details about their use and function are provided along with audio latency and performance comparisons, and examples of applications.}
}

@inproceedings{amercer-taylor2015,
  author = {Andrew Mercer-Taylor and Jaan Altosaar},
  title = {Sonification of Fish Movement Using Pitch Mesh Pairs},
  pages = {28--29},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179138},
  url = {http://www.nime.org/proceedings/2015/nime2015_155.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/155/0155-file1.mp4},
  abstract = {On a traditional keyboard, the actions required to play a consonant chord progression must be quite precise; accidentally strike a neighboring key, and a pleasant sonority is likely to become a jarring one. Inspired by the Tonnetz (a tonal diagram), we present a new layout of pitches defined using low-level harmonic notions. We demonstrate the potential of our system by mapping the random movements of fish in an aquarium to this layout. Qualitatively, we find that this captures the intuition behind mapping motion to sound. Similarly moving fish produce consonant chords, while fish moving in non-unison produce dissonant chords. We introduce an open source MATLAB library implementing these techniques, which can be used for sonifying multimodal streaming data. }
}

@inproceedings{klin2015,
  author = {Hans Anderson and Kin Wah Edward Lin and Natalie Agus and Simon Lui},
  title = {Major Thirds: A Better Way to Tune Your iPad},
  pages = {365--368},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179006},
  url = {http://www.nime.org/proceedings/2015/nime2015_157.pdf},
  abstract = {Many new melodic instruments use a touch sensitive surface with notes arranged in a two-dimensional grid. Most of these arrange notes in chromatic half-steps along the horizontal axis and in intervals of fourths along the vertical axis. Although many alternatives exist, this arrangement, which resembles that of a bass guitar, is quickly becoming the de facto standard. In this study we present experimental evidence that grid based instruments are significantly easier to play when we tune adjacent rows in Major thirds rather than fourths. We have developed a grid-based instrument as an iPad app that has sold 8,000 units since 2012. To test our proposed alternative tuning, we taught a group twenty new users to play basic chords on our app, using both the standard tuning and our proposed alternative. Our results show that the Major thirds tuning is much easier to learn, even for users that have previous experience playing guitar.}
}

@inproceedings{jaltosaar2015,
  author = {Ethan Benjamin and Jaan Altosaar},
  title = {MusicMapper: Interactive {2D} representations of music samples for in-browser remixing and exploration},
  pages = {325--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179018},
  url = {http://www.nime.org/proceedings/2015/nime2015_161.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/161/0161-file1.mp4},
  abstract = {Much of the challenge and appeal in remixing music comes from manipulating samples. Typically, identifying distinct samples of a song requires expertise in music production software. Additionally, it is difficult to visualize similarities and differences between all samples of a song simultaneously and use this to select samples. MusicMapper is a web application that allows nonexpert users to find and visualize distinctive samples from a song without any manual intervention, and enables remixing by having users play back clusterings of such samples. This is accomplished by splitting audio from the Soundcloud API into appropriately-sized spectrograms, and applying the t-SNE algorithm to visualize these spectrograms in two dimensions. Next, we apply k-means to guide the user's eye toward related clusters and set $k=26$ to enable playback of the clusters by pressing keys on an ordinary keyboard. We present the source code (https://github.com/fatsmcgee/MusicMappr) and a demo video (http://youtu.be/mvD6e1uiO8k) of the MusicMapper web application that can be run in most modern browsers.}
}

@inproceedings{jjaimovich2015,
  author = {Javier Jaimovich and {R. Benjamin} Knapp},
  title = {Creating Biosignal Algorithms for Musical Applications from an Extensive Physiological Database},
  pages = {1--4},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179096},
  url = {http://www.nime.org/proceedings/2015/nime2015_163.pdf},
  abstract = {Previously the design of algorithms and parameter calibration for biosignal music performances has been based on testing with a small number of individuals --- in fact usually the performer themselves. This paper uses the data collected from over 4000 people to begin to create a truly robust set of algorithms for heart rate and electrodermal activity measures, as well as the understanding of how the calibration of these vary by individual.}
}

@inproceedings{bknichel2015,
  author = {Benjamin Knichel and Holger Reckter and Peter Kiefer},
  title = {resonate -- a social musical installation which integrates tangible multiuser interaction},
  pages = {111--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179108},
  url = {http://www.nime.org/proceedings/2015/nime2015_164.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/164/0164-file1.mp4},
  abstract = {Resonate was a musical installation created with focus on interactivity and collaboration. In this paper we will focus on the design-process and the different steps involved. We describe and discuss the methods to create, synchronize and combine the aspects of space, object, music and interaction for the development of resonate. The realized space-filling tangible installation allowed visitors to interact with different interaction objects and change therefore the musical expression as well as the visual response and aesthetic. After a non-formal quality evaluation of this installation we changed some aspects which resulted in a more refined version which we will also discuss here. }
}

@inproceedings{gdublon2015,
  author = {Rebecca Kleinberger and Gershon Dublon and {Joseph A.} Paradiso and Tod Machover},
  title = {PHOX Ears: A Parabolic, Head-mounted, Orientable, eXtrasensory Listening Device},
  pages = {30--31},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179106},
  url = {http://www.nime.org/proceedings/2015/nime2015_165.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/165/0165-file1.mp4},
  abstract = {The Electronic Fox Ears helmet is a listening device that changes its wearer's experience of hearing. A pair of head-mounted, independently articulated parabolic microphones and built-in bone conduction transducers allow the wearer to sharply direct their attention to faraway sound sources. Joysticks in each hand control the orientations of the microphones, which are mounted on servo gimbals for precise targeting. Paired with a mobile device, the helmet can function as a specialized, wearable field recording platform. Field recording and ambient sound have long been a part of electronic music; our device extends these practices by drawing on a tradition of wearable technologies and prosthetic art that blur the boundaries of human perception. }
}

@inproceedings{pdahlstedt2015,
  author = {Palle Dahlstedt},
  title = {Mapping Strategies and Sound Engine Design for an Augmented Hybrid Piano},
  pages = {271--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179046},
  url = {http://www.nime.org/proceedings/2015/nime2015_170.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/170/0170-file1.zip},
  abstract = {Based on a combination of novel mapping techniques and carefully designed sound engines, I present an augmented hybrid piano specifically designed for improvisation. The mapping technique, originally developed for other control interfaces but here adapted to the piano keyboard, is based on a dynamic vectorization of control parameters, allowing both wild sonic exploration and minute intimate expression. The original piano sound is used as the sole sound source, subjected to processing techniques such as virtual resonance strings, dynamic buffer shuffling, and acoustic and virtual feedback. Thanks to speaker and microphone placement, the acoustic and processed sounds interact in both directions and blend into one new instrument. This also allows for unorthodox playing (knocking, plucking, shouting). Processing parameters are controlled from the keyboard playing alone, allowing intuitive control of complex processing by ear, integrating expressive musical playing with sonic exploration. The instrument is not random, but somewhat unpredictable. This feeds into the improvisation, defining a particular idiomatics of the instruments. Hence, the instrument itself is an essential part of the musical work. Performances include concerts in UK, Japan, Singapore, Australia and Sweden, in solos and ensembles, performed by several pianists. Variations of this hybrid instrument for digital keyboards are also presented.}
}

@inproceedings{pdahlstedtb2015,
  author = {Palle Dahlstedt and Per Anders Nilsson and Gino Robair},
  title = {The Bucket System --- A computer mediated signalling system for group improvisation},
  pages = {317--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179048},
  url = {http://www.nime.org/proceedings/2015/nime2015_171.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/171/0171-file1.mp4},
  abstract = {The Bucket System is a new system for computer-mediated ensemble improvisation, designed by improvisers for improvisers. Coming from a tradition of structured free ensemble improvisation practices (comprovisation), influenced by post-WW2 experimental music practices, it is a signaling system implemented with a set of McMillen QuNeo controllers as input and output interfaces, powered by custom software. It allows for a new kind of on-stage compositional/improvisation interaction.}
}

@inproceedings{salexander-adams2015,
  author = {Simon Alexander-Adams and Michael Gurevich},
  title = {A Flexible Platform for Tangible Graphic Scores},
  pages = {174--175},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179004},
  url = {http://www.nime.org/proceedings/2015/nime2015_172.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/172/0172-file1.mov},
  abstract = {This paper outlines the development of a versatile platform for the performance and composition of tangible graphic scores, providing technical details of the hardware and software design. The system is conceived as a touch surface facilitating modular textured plates, coupled with corresponding visual feedback.}
}

@inproceedings{rvanrooyenb2015,
  author = {Robert {Van Rooyen} and George Tzanetakis},
  title = {Pragmatic Drum Motion Capture System},
  pages = {339--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181400},
  url = {http://www.nime.org/proceedings/2015/nime2015_173.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/173/0173-file1.mp4},
  abstract = {The ability to acquire and analyze a percussion performance in an efficient, affordable, and non-invasive manner has been made possible by a unique composite of off-the-shelf products. Through various methods of calibration and analysis, human motion as imparted on a striking implement can be tracked and correlated with traditional audio data in order to compare performances. Ultimately, conclusions can be drawn that drive pedagogical studies as well as advances in musical robots.}
}

@inproceedings{qyang2015,
  author = {Qi Yang and Georg Essl},
  title = {Representation-Plurality in Multi-Touch Mobile Visual Programming for Music},
  pages = {369--373},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181416},
  url = {http://www.nime.org/proceedings/2015/nime2015_177.pdf},
  abstract = {Multi-touch mobile devices provide a fresh paradigm for interactions, as well as a platform for building rich musical applications. This paper presents a multi-touch mobile programming environment that supports the exploration of different representations in visual programming for music and audio interfaces. Using a common flow-based visual programming vocabulary, we implemented a system based on the urMus platform that explores three types of touch-based interaction representations: a text-based menu representation, a graphical icon-based representation, and a novel multi-touch gesture-based representation. We illustrated their use on interface design for musical controllers.}
}

@inproceedings{ajensenius2015,
  author = {Jensenius, Alexander Refsum},
  title = {Microinteraction in Music/Dance Performance},
  pages = {16--19},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179100},
  url = {http://www.nime.org/proceedings/2015/nime2015_178.pdf},
  abstract = {This paper presents the scientific-artistic project Sverm, which has focused on the use of micromotion and microsound in artistic practice. Starting from standing still in silence, the artists involved have developed conceptual and experiential knowledge of microactions, microsounds and the possibilities of microinteracting with light and sound.}
}

@inproceedings{ajenseniusb2015,
  author = {Nymoen, Kristian and Haugen, Mari Romarheim and Jensenius, Alexander Refsum},
  title = {MuMYO --- Evaluating and Exploring the MYO Armband for Musical Interaction},
  pages = {215--218},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179150},
  url = {http://www.nime.org/proceedings/2015/nime2015_179.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/179/0179-file1.mov},
  abstract = {The MYO armband from Thalmic Labs is a complete and wireless motion and muscle sensing platform. This paper evaluates the armband's sensors and its potential for NIME applications. This is followed by a presentation of the prototype instrument MuMYO. We conclude that, despite some shortcomings, the armband has potential of becoming a new ``standard'' controller in the NIME community.}
}

@inproceedings{esheffieldb2015,
  author = {Eric Sheffield and Michael Gurevich},
  title = {Distributed Mechanical Actuation of Percussion Instruments},
  pages = {11--15},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179176},
  url = {http://www.nime.org/proceedings/2015/nime2015_183.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/183/DistributedActuationDemo.mp4},
  abstract = {This paper describes a system for interactive mechanically actuated percussion. Design principles regarding seamless control and retention of natural acoustic properties are established. Performance patterns on a preliminary version are examined, including the potential for cooperative and distributed performance.}
}

@inproceedings{jhe2015,
  author = {Jingyin He and Ajay Kapur and Dale Carnegie},
  title = {Developing A Physical Gesture Acquisition System for Guqin Performance},
  pages = {187--190},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179088},
  url = {http://www.nime.org/proceedings/2015/nime2015_184.pdf},
  abstract = {Motion-based musical interfaces are ubiquitous. With the plethora of sensing solutions and the possibility of developing custom designs, it is important that the new musical interface has the capability to perform any number of tasks. This paper presents the theoretical framework for defining, designing, and evaluation process of a physical gesture acquisition for Guqin performance. The framework is based on an iterative design process, and draws upon the knowledge in Guqin performance to develop a system to determine the interaction between a Guqin player and the computer. This paper emphasizes the definition, conception, and evaluation of the acquisition system.}
}

@inproceedings{rgrahamb2015,
  author = {Richard Graham and John Harding},
  title = {SEPTAR: Audio Breakout Design for Multichannel Guitar},
  pages = {241--244},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179070},
  url = {http://www.nime.org/proceedings/2015/nime2015_187.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/187/0187-file1.wav},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/187/0187-file2.wav},
  abstract = {Multichannel (or divided) audio pickups are becoming increasingly ubiquitous in electric guitar and computer music communities. These systems allow performers to access signals for each string of their instrument independently and concurrently in real-time creative practice. This paper presents an open-source audio breakout circuit that provides independent audio outputs per string of any chordophone (stringed instrument) that is fitted with a multichannel audio pickup system. The following sections include a brief historical contextualization and discussion on the significance of multichannel audio technology in instrumental guitar music, an overview of our proposed impedance matching circuit for piezoelectric-based audio pickups, and a presentation of a new open-source PCB design (SEPTAR V2) that includes a mountable 13-pin DIN connection to improve compatibility with commercial multichannel pickup systems. This paper will also include a short summary of the potential creative applications and perceptual implications of this multichannel technology when used in creative practice.}
}

@inproceedings{fberthaut2015,
  author = {Florent Berthaut and Diego Martinez and Martin Hachet and Sriram Subramanian},
  title = {Reflets: Combining and Revealing Spaces for Musical Performances},
  pages = {116--120},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179028},
  url = {http://www.nime.org/proceedings/2015/nime2015_190.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/190/0190-file1.mp4},
  abstract = {We present Reflets, a mixed-reality environment for musical performances that allows for freely displaying virtual content on stage, such as 3D virtual musical interfaces or visual augmentations of instruments and performers. It relies on spectators and performers revealing virtual objects by slicing through them with body parts or objects, and on planar slightly reflective transparent panels that combine the stage and audience spaces. In this paper, we describe the approach and implementation challenges of Reflets. We then demonstrate that it matches the requirements of musical performances. It allows for placing virtual content anywhere on large stages, even overlapping with physical elements and provides a consistent rendering of this content for large numbers of spectators. It also preserves non-verbal communication between the audience and the performers, and is inherently engaging for the spectators. We finally show that Reflets opens musical performance opportunities such as augmented interaction between musicians and novel techniques for 3D sound shapes manipulation.}
}

@inproceedings{slui2015,
  author = {Simon Lui},
  title = {Generate expressive music from picture with a handmade multi-touch music table},
  pages = {374--377},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179122},
  url = {http://www.nime.org/proceedings/2015/nime2015_191.pdf},
  abstract = {The multi-touch music table is a novel tabletop tangible interface for expressive musical performance. User touches the picture projected on the table glass surface to perform music. User can click, drag or use various multi-touch gestures with fingers to perform music expressively. The picture color, luminosity, size, finger gesture and pressure determine the music output. The table detects up to 10 finger touches with their touch pressure. We use a glass, a wood stand, a mini projector, a web camera and a computer to construct this music table. Hence this table is highly customizable. The table generates music via a re-interpretation of the artistic components of pictures. It is a cross modal inspiration of music from visual art on a tangible interface. }
}

@inproceedings{swaite2015,
  author = {Si Waite},
  title = {Reimagining the Computer Keyboard as a Musical Interface},
  pages = {168--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179192},
  url = {http://www.nime.org/proceedings/2015/nime2015_193.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/193/0193-file1.mov},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/193/0193-file2.mp4},
  abstract = {This paper discusses the use of typed text as a real-time input for interactive performance systems. A brief review of the literature discusses text-based generative systems, links between typing and playing percussion instruments and the use of typing gestures in contemporary performance practice. The paper then documents the author's audio-visual system that is driven by the typing of text/lyrics in real-time. It is argued that the system promotes the sensation of liveness through clear, perceptible links between the performer's gestures, the system's audio outputs and the its visual outputs. The system also provides a novel approach to the use of generative techniques in the composition and live performance of songs. Future developments would include the use of dynamic text effects linked to sound generation and greater interaction between human performer and the visuals. }
}

@inproceedings{mhirabayashi2015,
  author = {Masami Hirabayashi and Kazuomi Eshima},
  title = {Sense of Space: The Audience Participation Music Performance with High-Frequency Sound ID},
  pages = {58--60},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179092},
  url = {http://www.nime.org/proceedings/2015/nime2015_195.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/195/0195-file1.mp4},
  abstract = {We performed the musical work ``Sense of Space'' which uses the sound ID with high frequency sound DTMF. The IDs are embedded into the music, audiences' smartphones and tablets at the venue reacted to the IDs and then they play music pieces. We considered the possibility for novel music experiences brought about through the participation of audiences and spreading sound at the music venue.}
}

@inproceedings{tshaw2015,
  author = {Tim Shaw and S\'ebastien Piquemal and John Bowers},
  title = {Fields: An Exploration into the use of Mobile Devices as a Medium for Sound Diffusion},
  pages = {281--284},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179174},
  url = {http://www.nime.org/proceedings/2015/nime2015_196.pdf},
  abstract = {In this paper we present Fields, a sound diffusion performance implemented with web technologies that run on the mobile devices of audience members. Both a technical system and bespoke composition, Fields allows for a range of sonic diffusions to occur, and therefore has the potential to open up new paradigms for spatialised music and media performances. The project introduces how handheld technology used as a collective array of speakers controlled live by a centralized performer can create alternative types of participation within musical performance. Fields not only offers a new technological approach to sound diffusion, it also provides an alternative way for audiences to participate in live events, and opens up unique forms of engagement within sonic media contexts. }
}

@inproceedings{rdannenberg2015,
  author = {Dan Ringwalt and Roger Dannenberg and Andrew Russell},
  title = {Optical Music Recognition for Interactive Score Display},
  pages = {95--98},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179162},
  url = {http://www.nime.org/proceedings/2015/nime2015_198.pdf},
  abstract = {Optical music recognition (OMR) is the task of recognizing images of musical scores. In this paper, improved algorithms for the fi
rst steps of optical music recognition were developed, which facilitated bulk annotation of scanned scores for use in an interactive score display system. Creating an initial annotation by OMR and verifying by hand substantially reduced the manual eff
ort required to process scanned scores to be used in a live performance setting.}
}

@inproceedings{amomeni2015,
  author = {Ali Momeni},
  title = {Caress: An Electro-acoustic Percussive Instrument for Caressing Sounds},
  pages = {245--250},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179142},
  url = {http://www.nime.org/proceedings/2015/nime2015_199.pdf},
  abstract = {This paper documents the development of Caress, an electroacoustic percussive instrument that blends drumming and audio synthesis in a small and portable form factor. Caress is an octophonic miniature drum-set for the fingertips that employs eight acoustically isolated piezo-microphones, coupled with eight independent signal chains that excite a unique resonance model with audio from the piezos. The hardware is designed to be robust and quickly reproducible (parametric design and machine fabrication), while the software aims to be light-weight (low-CPU requirements) and portable (multiple platforms, multiple computing architectures). Above all, the instrument aims for the level of control intimacy and tactile expressivity achieved by traditional acoustic percussive instruments, while leveraging real-time software synthesis and control to expand the sonic palette. This instrument as well as this document are dedicated to the memory of the late David Wessel, pioneering composer, performer, researcher, mentor and all-around Yoda of electroacoustic music. }
}

@inproceedings{rdannenbergb2015,
  author = {Roger Dannenberg and Andrew Russell},
  title = {Arrangements: Flexibly Adapting Music Data for Live Performance},
  pages = {315--316},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179050},
  url = {http://www.nime.org/proceedings/2015/nime2015_200.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/200/0200-file1.mp4},
  abstract = {Human-Computer Music Performance for popular music -- where musical structure is important, but where musicians often decide on the spur of the moment exactly what the musical form will be -- presents many challenges to make computer systems that are flexible and adaptable to human musicians. One particular challenge is that humans easily follow scores and chord charts, adapt these to new performance plans, and understand media locations in musical terms (beats and measures), while computer music systems often use rigid and even numerical representations that are difficult to work with. We present new formalisms and representations, and a corresponding implementation, where musical material in various media is synchronized, where musicians can quickly alter the performance order by specifying (re-)arrangements of the material, and where interfaces are supported in a natural way by music notation.}
}

@inproceedings{amomenib2015,
  author = {Jamie Bullock and Ali Momeni},
  title = {ml.lib: Robust, Cross-platform, Open-source Machine Learning for Max and Pure Data},
  pages = {265--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179038},
  url = {http://www.nime.org/proceedings/2015/nime2015_201.pdf},
  abstract = {This paper documents the development of ml.lib: a set of open-source tools designed for employing a wide range of machine learning techniques within two popular real-time programming environments, namely Max and Pure Data. ml.lib is a cross-platform, lightweight wrapper around Nick Gillian's Gesture Recognition Toolkit, a C++ library that includes a wide range of data processing and machine learning techniques. ml.lib adapts these techniques for real-time use within popular data-flow IDEs, allowing instrument designers and performers to integrate robust learning, classification and mapping approaches within their existing workflows. ml.lib has been carefully de-signed to allow users to experiment with and incorporate ma-chine learning techniques within an interactive arts context with minimal prior knowledge. A simple, logical and consistent, scalable interface has been provided across over sixteen exter-nals in order to maximize learnability and discoverability. A focus on portability and maintainability has enabled ml.lib to support a range of computing architectures---including ARM---and operating systems such as Mac OS, GNU/Linux and Win-dows, making it the most comprehensive machine learning implementation available for Max and Pure Data.}
}

@inproceedings{rdannenbergc2015,
  author = {Guangyu Xia and Roger Dannenberg},
  title = {Duet Interaction: Learning Musicianship for Automatic Accompaniment},
  pages = {259--264},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179198},
  url = {http://www.nime.org/proceedings/2015/nime2015_202.pdf},
  abstract = {Computer music systems can interact with humans at different levels, including scores, phrases, notes, beats, and gestures. However, most current systems lack basic musicianship skills. As a consequence, the results of human-computer interaction are often far less musical than the interaction between human musicians. In this paper, we explore the possibility of learning some basic music performance skills from rehearsal data. In particular, we consider the piano duet scenario where two musicians expressively interact with each other. Our work extends previous automatic accompaniment systems. We have built an artificial pianist that can automatically improve its ability to sense and coordinate with a human pianist, learning from rehearsal experience. We describe different machine learning algorithms to learn musicianship for duet interaction, explore the properties of the learned models, such as dominant features, limits of validity, and minimal training size, and claim that a more human-like interaction is achieved.}
}

@inproceedings{jleonard2015,
  author = {James Leonard and Claude Cadoz},
  title = {Physical Modelling Concepts for a Collection of Multisensory Virtual Musical Instruments},
  pages = {150--155},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179116},
  url = {http://www.nime.org/proceedings/2015/nime2015_203.pdf},
  abstract = {This paper discusses how haptic devices and physical modelling can be employed to design and simulate multisensory virtual musical instruments, providing the musician with joint audio, visual and haptic feedback. After briefly reviewing some of the main use-cases of haptics in Computer Music, we present GENESIS-RT, a software and hardware platform dedicated to the design and real-time haptic playing of virtual musical instruments using mass-interaction physical modelling. We discuss our approach and report on advancements in modelling various instrument categories instruments, including physical models of percussion, plucked and bowed instruments. Finally, we comment on the constraints, challenges and new possibilities opened by modelling haptic virtual instruments with our platform, and discuss common points and differences in regards to classical Digital Musical Instruments. }
}

@inproceedings{jvilleneuve2015,
  author = {J\'er\^ome Villeneuve and Claude Cadoz and Nicolas Castagn\'e},
  title = {Visual Representation in GENESIS as a tool for Physical Modeling, Sound Synthesis and Musical Composition},
  pages = {195--200},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179190},
  url = {http://www.nime.org/proceedings/2015/nime2015_204.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/204/0204-file1.mov},
  abstract = {The motivation of this paper is to highlight the importance of visual representations for artists when modeling and simulating mass-interaction physical networks in the context of sound synthesis and musical composition. GENESIS is a musician-oriented software environment for sound synthesis and musical composition. However, despite this orientation, a substantial amount of effort has been put into building a rich variety of tools based on static or dynamic visual representations of models and of abstractions of their properties. After a quick survey of these tools, we will illustrate the significant role they play in the creative process involved when going from a musical idea and exploration to the production of a complete musical piece. To that aim, our analysis will rely on the work and practice of several artists having used GENESIS in various ways.}
}

@inproceedings{fcalegario2015,
  author = {Jer\^onimo Barbosa and Filipe Calegario and Jo\~ao Tragtenberg and Giordano Cabral and Geber Ramalho and {Marcelo M.} Wanderley},
  title = {Designing {DMI}s for Popular Music in the {Brazil}ian Northeast: Lessons Learned},
  pages = {277--280},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179008},
  url = {http://www.nime.org/proceedings/2015/nime2015_207.pdf},
  abstract = {Regarding the design of new DMIs, it is possible to fit the majority of projects into two main cases: those developed by the academic research centers, which focus on North American and European contemporary classical and experimental music; and the DIY projects, in which the luthier also plays the roles of performer and/or composer. In both cases, the design process is not focused on creating DMIs for a community with a particular culture --- with established instruments, repertoire and playing styles --- outside European and North American traditions. This challenge motivated our research. In this paper, we discuss lessons learned during an one-year project called Batebit. Our approach was based on Design Thinking methodology, comprising cycles of inspiration, ideation and implementation. It resulted in two new DMIs developed collaboratively with musicians from the Brazilian Northeast.}
}

@inproceedings{dmenzies2015,
  author = {Duncan Menzies and Andrew McPherson},
  title = {Highland Piping Ornament Recognition Using Dynamic Time Warping},
  pages = {50--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179136},
  url = {http://www.nime.org/proceedings/2015/nime2015_208.pdf},
  abstract = {This work uses a custom-built digital bagpipe chanter interface to assist in the process of learning the Great Highland Bagpipe (GHB). In this paper, a new algorithm is presented for the automatic recognition and evaluation of the various ornamentation techniques that are a central aspect of traditional Highland bagpipe music. The algorithm is evaluated alongside a previously published approach, and is shown to provide a significant improvement in performance. The ornament detection facility forms part of a complete hardware and software system for use in both tuition and solo practice situations, allowing details of ornamentation errors made by the player to be provided as visual and textual feedback. The system also incorporates new functionality for the identification and description of GHB fingering errors.}
}

@inproceedings{aflo2015,
  author = {{Asbj\o rn Blokkum} Fl\o and Hans Wilmers},
  title = {Doppelg{\''a}nger: A solenoid-based large scale sound installation.},
  pages = {61--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179060},
  url = {http://www.nime.org/proceedings/2015/nime2015_212.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/212/0212-file1.mp4},
  abstract = {This paper presents the sound art installation Doppelg{\''a}nger. In Doppelg{\''a}nger, we combine an artistic concept on a large scale with a high degree of control over timbre and dynamics. This puts great demands on the technical aspects of the work. The installation consists of seven 3.5 meters-tall objects weighing a total of 1500 kilos. Doppelg{\''a}nger transfers one soundscape into another using audio analysis, mapping, and computer-controlled acoustic sound objects. The technical realization is based on hammer mechanisms actuated by powerful solenoids, driven by a network of Arduino boards with high power PWM outputs, and a Max-patch running audio analysis and mapping. We look into the special requirements in mechanics for large-scale projects. Great care has been taken in the technical design to ensure that the resulting work is scalable both in numbers of elements and in physical dimensions. This makes our findings easily applicable to other projects of a similar nature.}
}

@inproceedings{ahazzard2015,
  author = {Adrian Hazzard and Steve Benford and Alan Chamberlain and Chris Greenhalgh},
  title = {Considering musical structure in location-based experiences},
  pages = {378--381},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179086},
  url = {http://www.nime.org/proceedings/2015/nime2015_214.pdf},
  abstract = {Locative music experiences are often non-linear and as such they are co-created, as the final arrangement of the music heard is guided by the movements of the user. We note an absence of principles and guidelines regarding how composers should approach the structuring of such locative soundtracks. For instance, how does one compose for a non-linear, indeterminate experience using linear pre-composed placed sounds, where fixed musical time is situated into the indeterminate time of the user's experience? Furthermore, how does one create a soundtrack that is suitable for the location, but also functions as a coherent musical structure? We explore these questions by analyzing an existing `placed sound' work from a traditional music theory perspective and in doing so reveal some structural principals from `fixed' musical forms can also support the composition of contemporary locative music experiences.}
}

@inproceedings{btome2015,
  author = {Basheer Tome and {Donald Derek} Haddad and Tod Machover and Joseph Paradiso},
  title = {MMODM: Massively Multipler Online Drum Machine},
  pages = {285--288},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179184},
  url = {http://www.nime.org/proceedings/2015/nime2015_215.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/215/0215-file1.mp4},
  abstract = {Twitter has provided a social platform for everyone to enter the previously exclusive world of the internet, enriching this online social tapestry with cultural diversity and enabling revolutions. We believe this same tool can be used to also change the world of music creation. Thus we present MMODM, an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments using plain text tweets on their favorite device, in real-time. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.}
}

@inproceedings{nbarrett2015,
  author = {Natasha Barrett},
  title = {Creating tangible spatial-musical images from physical performance gestures},
  pages = {191--194},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179014},
  url = {http://www.nime.org/proceedings/2015/nime2015_216.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/216/0216-file1.zip},
  abstract = {Electroacoustic music has a longstanding relationship with gesture and space. This paper marks the start of a project investigating acousmatic spatial imagery, real gestural behaviour and ultimately the formation of tangible acousmatic images. These concepts are explored experimentally using motion tracking in a source-sound recording context, interactive parameter-mapping sonification in three-dimensional high-order ambisonics, composition and performance. The spatio-musical role of physical actions in relation to instrument excitation is used as a point of departure for embodying physical spatial gestures in the creative process. The work draws on how imagery for music is closely linked with imagery for music-related actions.}
}

@inproceedings{jharrimanc2015,
  author = {Jiffer Harriman},
  title = {Start 'em Young: Digital Music Instrument for Education},
  pages = {70--73},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179078},
  url = {http://www.nime.org/proceedings/2015/nime2015_218.pdf},
  abstract = {Designing and building Digital Music Instruments (DMIs) is a promising context to engage children in technology design with parallels to hands on and project based learning educational approaches. Looking at tools and approaches used in STEM education we find much in common with the tools and approaches used in the creation of DMIs as well as opportunities for future development, in particular the use of scaffolded software and hardware toolkits. Current approaches to teaching and designing DMIs within the community suggest fruitful ideas for engaging novices in authentic design activities. Hardware toolkits and programming approaches are considered to identify productive approaches to teach technology design through building DMIs.}
}

@inproceedings{dcazzani2015,
  author = {Dario Cazzani},
  title = {Posture Identification of Musicians Using Non-Intrusive Low-Cost Resistive Pressure Sensors},
  pages = {54--57},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179042},
  url = {http://www.nime.org/proceedings/2015/nime2015_220.pdf},
  abstract = {The following paper documents the creation of a prototype of shoe-soles designed to detect various postures of standing musicians using non-intrusive pressure sensors. In order to do so, flexible algorithms were designed with the capacity of working even with an imprecise placement of the sensors. This makes it easy and accessible for all potential users. At least 4 sensors are required: 2 for the front and 2 for the back; this prototype uses 6. The sensors are rather inexpensive, widening the economic availability. For each individual musician, the algorithms are capable of ``personalising'' postures in order to create consistent evaluations; the results of which may be, but are not limited to: new musical interfaces, educational analysis of technique, or music controllers. In building a prototype for the algorithms, data was acquired by wiring the sensors to a data-logger. The algorithms and tests were implemented using MATLAB. After designing the algorithms, various tests were run in order to prove their reliability. These determined that indeed the algorithms work to a sufficient degree of certainty, allowing for a reliable classification of a musician's posture or position.}
}

@inproceedings{roda2015,
  author = {Zeyu Jin and Reid Oda and Adam Finkelstein and Rebecca Fiebrink},
  title = {MalLo: A Distributed Synchronized Musical Instrument Designed For Internet Performance},
  pages = {293--298},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179102},
  url = {http://www.nime.org/proceedings/2015/nime2015_223.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/223/0223-file1.mp4},
  abstract = {The Internet holds a lot of potential as a music listening, collaboration, and performance space. It has become commonplace to stream music and video of musical performance over the web. However, the goal of playing rhythmically synchronized music over long distances has remained elusive due to the latency inherent in networked communication. The farther apart two artists are from one another, the greater the delay. Furthermore, latency times can change abruptly with no warning. In this paper, we demonstrate that it is possible to create a distributed, synchronized musical instrument that allows performers to play together over long distances, despite latency. We describe one such instrument, MalLo, which combats latency by predicting a musician's action before it is completed. MalLo sends information about a predicted musical note over the Internet before it is played, and synthesizes this note at a collaborator's location at nearly the same moment it is played by the performer. MalLo also protects against latency spikes by sending the prediction data across multiple network paths, with the intention of routing around latency. }
}

@inproceedings{lhayes2015,
  author = {Lauren Hayes},
  title = {Enacting Musical Worlds: Common Approaches to using NIMEs within both Performance and Person-Centred Arts Practices},
  pages = {299--302},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179082},
  url = {http://www.nime.org/proceedings/2015/nime2015_227.pdf},
  abstract = {Live music making can be understood as an enactive process, whereby musical experiences are created through human action. This suggests that musical worlds coevolve with their agents through repeated sensorimotor interactions with the environment (where the music is being created), and at the same time cannot be separated from their sociocultural contexts. This paper investigates this claim by exploring ways in which technology, physiology, and context are bound up within two different musical scenarios: live electronic musical performance; and person-centred arts applications of NIMEs. In this paper I outline an ethnographic and phenomenological enquiry into my experiences as both a performer of live electronic and electro-instrumental music, as well as my extensive background in working with new technologies in various therapeutic and person-centred artistic situations. This is in order to explore the sociocultural and technological contexts in which these activities take place. I propose that by understanding creative musical participation as a highly contextualised practice, we may discover that the greatest impact of rapidly developing technological resources is their ability to afford richly diverse, personalised, and embodied forms of music making. I argue that this is applicable over a wide range of musical communities.}
}

@inproceedings{ncorreia2015,
  author = {{Nuno N.} Correia and Atau Tanaka},
  title = {Prototyping Audiovisual Performance Tools: A Hackathon Approach},
  pages = {319--321},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179044},
  url = {http://www.nime.org/proceedings/2015/nime2015_230.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/230/0230-file1.mp4},
  abstract = {We present a user-centered approach for prototyping tools for performance with procedural sound and graphics, based on a hackathon. We also present the resulting prototypes. These prototypes respond to a challenge originating from earlier stages of the research: to combine ease-of-use with expressiveness and visibility of interaction in tools for audiovisual performance. We aimed to convert sketches, resulting from an earlier brainstorming session, into functional prototypes in a short period of time. The outcomes include open-source software base released online. The conclusions reflect on the methodology adopted and the effectiveness of the prototypes.}
}

@inproceedings{pbennett2015,
  author = {Peter Bennett and Jarrod Knibbe and Florent Berthaut and Kirsten Cater},
  title = {Resonant Bits: Controlling Digital Musical Instruments with Resonance and the Ideomotor Effect},
  pages = {176--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179020},
  url = {http://www.nime.org/proceedings/2015/nime2015_235.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/235/0235-file1.mp4},
  abstract = {Resonant Bits proposes giving digital information resonant dynamic properties, requiring skill and concerted effort for interaction. This paper applies resonant interaction to musical control, exploring musical instruments that are controlled through both purposeful and subconscious resonance. We detail three exploratory prototypes, the first two illustrating the use of resonant gestures and the third focusing on the detection and use of the ideomotor (subconscious micro-movement) effect.}
}

@inproceedings{adecarvalhojunior2015,
  author = {de Carvalho Junior, Antonio Deusany},
  title = {Indoor localization during installations using {WiFi}},
  pages = {40--41},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179052},
  url = {http://www.nime.org/proceedings/2015/nime2015_237.pdf},
  abstract = {The position of a participant during an installation is a valuable data. One may want to start some sample when someone cross a line or stop the music automatically whenever there is nobody inside the main area. GPS is a good solution for localization, but it usually loses its capabilities inside buildings. This paper discuss the use of Wi-Fi signal strength during an installation as an alternative to GPS.}
}

@inproceedings{cmartin2015,
  author = {Charles Martin and Henry Gardner and Ben Swift},
  title = {Tracking Ensemble Performance on Touch-Screens with Gesture Classification and Transition Matrices},
  pages = {359--364},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179130},
  url = {http://www.nime.org/proceedings/2015/nime2015_242.pdf},
  abstract = {We present and evaluate a novel interface for tracking ensemble performances on touch-screens. The system uses a Random Forest classifier to extract touch-screen gestures and transition matrix statistics. It analyses the resulting gesture-state sequences across an ensemble of performers. A series of specially designed iPad apps respond to this real-time analysis of free-form gestural performances with calculated modifications to their musical interfaces. We describe our system and evaluate it through cross-validation and profiling as well as concert experience.}
}

@inproceedings{byuksel2015,
  author = {{Beste Filiz} Yuksel and Daniel Afergan and Evan Peck and Garth Griffin and Lane Harrison and Nick Chen and Remco Chang and Robert Jacob},
  title = {BRAAHMS: A Novel Adaptive Musical Interface Based on Users' Cognitive State},
  pages = {136--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181418},
  url = {http://www.nime.org/proceedings/2015/nime2015_243.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/243/0243-file1.mp4},
  abstract = {We present a novel brain-computer interface (BCI) integrated with a musical instrument that adapts implicitly (with no extra effort from user) to users' changing cognitive state during musical improvisation. Most previous musical BCI systems use either a mapping of brainwaves to create audio signals or use explicit brain signals to control some aspect of the music. Such systems do not take advantage of higher level semantically meaningful brain data which could be used in adaptive systems or without detracting from the attention of the user. We present a new type of real-time BCI that assists users in musical improvisation by adapting to users' measured cognitive workload implicitly. Our system advances the state of the art in this area in three ways: 1) We demonstrate that cognitive workload can be classified in real-time while users play the piano using functional near-infrared spectroscopy. 2) We build a real-time, implicit system using this brain signal that musically adapts to what users are playing. 3) We demonstrate that users prefer this novel musical instrument over other conditions and report that they feel more creative.}
}

@inproceedings{ahindle2015,
  author = {Abram Hindle},
  title = {Orchestrating Your Cloud Orchestra},
  pages = {121--125},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179090},
  url = {http://www.nime.org/proceedings/2015/nime2015_244.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/244/0244-file1.mp4},
  abstract = { Cloud computing potentially ushers in a new era of computer music performance with exceptionally large computer music instruments consisting of 10s to 100s of virtual machines which we propose to call a `cloud-orchestra'. Cloud computing allows for the rapid provisioning of resources, but to deploy such a complicated and interconnected network of software synthesizers in the cloud requires a lot of manual work, system administration knowledge, and developer/operator skills. This is a barrier to computer musicians whose goal is to produce and perform music, and not to administer 100s of computers. This work discusses the issues facing cloud-orchestra deployment and offers an abstract solution and a concrete implementation. The abstract solution is to generate cloud-orchestra deployment plans by allowing computer musicians to model their network of synthesizers and to describe their resources. A model optimizer will compute near-optimal deployment plans to synchronize, deploy, and orchestrate the start-up of a complex network of synthesizers deployed to many computers. This model driven development approach frees computer musicians from much of the hassle of deployment and allocation. Computer musicians can focus on the configuration of musical components and leave the resource allocation up to the modelling software to optimize.}
}

@inproceedings{apiepenbrink2015,
  author = {Andrew Piepenbrink and Matthew Wright},
  title = {The Bistable Resonator Cymbal: An Actuated Acoustic Instrument Displaying Physical Audio Effects},
  pages = {227--230},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179154},
  url = {http://www.nime.org/proceedings/2015/nime2015_245.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/245/0245-file1.mov},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/245/0245-file2.zip},
  abstract = {We present the Bistable Resonator Cymbal, a type of actuated acoustic instrument which augments a conventional cymbal with feedback-induced resonance. The system largely employs standard, commercially-available sound reinforcement and signal processing hardware and software, and no permanent modifications to the cymbal are needed. Several types of cymbals may be used, each capable of producing a number of physical audio effects. Cymbal acoustics, implementation, stability issues, interaction behavior, and sonic results are discussed.}
}

@inproceedings{abergsland2015,
  author = {Andreas Bergsland and Robert Wechsler},
  title = {Composing Interactive Dance Pieces for the MotionComposer, a device for Persons with Disabilities},
  pages = {20--23},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179024},
  url = {http://www.nime.org/proceedings/2015/nime2015_246.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/246/0246-file2.mp4},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/246/La_Danse_II.mp4},
  urlsuppl3 = {http://www.nime.org/proceedings/2015/246/SongShanMountain-SD.mp4},
  abstract = {The authors have developed a new hardware/software device for persons with disabilities (the MotionComposer), and in the process created a number of interactive dance pieces for non-disabled professional dancers. The paper briefly describes the hardware and motion tracking software of the device before going into more detail concerning the mapping strategies and sound design applied to three interactive dance pieces. The paper concludes by discussing a particular philosophy championing transparency and intuitiveness (clear causality) in the interactive relationship, which the authors apply to both the device and to the pieces that came from it.}
}

@inproceedings{bmccloskey2015,
  author = {Brendan McCloskey and Brian Bridges and Frank Lyons},
  title = {Accessibility and dimensionalty: enhanced real-time creative independence for digital musicians with quadriplegic cerebral palsy},
  pages = {24--27},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179132},
  url = {http://www.nime.org/proceedings/2015/nime2015_250.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/250/0250-file1.zip},
  abstract = {Inclusive music activities for people with physical disabilities commonly emphasise facilitated processes, based both on constrained gestural capabilities, and on the simplicity of the available interfaces. Inclusive music processes employ consumer controllers, computer access tools and/or specialized digital musical instruments (DMIs). The first category reveals a design ethos identified by the authors as artefact multiplication -- many sliders, buttons, dials and menu layers; the latter types offer ergonomic accessibility through artefact magnification. We present a prototype DMI that eschews artefact multiplication in pursuit of enhanced real time creative independence. We reconceptualise the universal click-drag interaction model via a single sensor type, which affords both binary and continuous performance control. Accessibility is optimized via a familiar interaction model and through customized ergonomics, but it is the mapping strategy that emphasizes transparency and sophistication in the hierarchical correspondences between the available gesture dimensions and expressive musical cues. Through a participatory and progressive methodology we identify an ostensibly simple targeting gesture rich in dynamic and reliable features: (1) contact location; (2) contact duration; (3) momentary force; (4) continuous force, and; (5) dyad orientation. These features are mapped onto dynamic musical cues, most notably via new mappings for vibrato and arpeggio execution. }
}

@inproceedings{anath2015,
  author = {Ajit Nath and Samson Young},
  title = {VESBALL: A ball-shaped instrument for music therapy},
  pages = {387--391},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179146},
  url = {http://www.nime.org/proceedings/2015/nime2015_252.pdf},
  abstract = {In this paper the authors describe the VESBALL, which is a ball-shaped musical interface designed for group music therapy. Therapy sessions take the form of ``musical ensembles'' comprised of individuals with Autism Spectrum Disorder (ASD), typically led by one or more certified music therapists. VESBALL had been developed in close consultation with therapists, clients, and other stakeholders, and had undergone several phases of trials at a music therapy facility over a period of 6 months. VESBALL has an advantage over other related work in terms of its robustness, ease of operation and setup (for clients and therapists), sound source integration, and low cost of production. The authors hope VESBALL would positively impact the conditions of individuals with ASD, and pave way for new research in custom-designed NIME for communities with specific therapeutic needs.}
}

@inproceedings{swaloschek2015,
  author = {Simon Waloschek and Aristotelis Hadjakos},
  title = {Sensors on Stage: Conquering the Requirements of Artistic Experiments and Live Performances},
  pages = {351--354},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179194},
  url = {http://www.nime.org/proceedings/2015/nime2015_254.pdf},
  abstract = {With the rapid evolution of technology, sensor aided performances and installations have gained popularity. We identified a number of important criteria for stage usage and artistic experimentation. These are partially met by existing approaches, oftentimes trading off programmability for ease of use. We propose our new sensor interface SPINE-2 that presents a comprehensive solution to these stage requirements without that trade-off.}
}

@inproceedings{amcpherson2015,
  author = {Andrew McPherson and Victor Zappi},
  title = {Exposing the Scaffolding of Digital Instruments with Hardware-Software Feedback Loops},
  pages = {162--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179134},
  url = {http://www.nime.org/proceedings/2015/nime2015_258.pdf},
  abstract = {The implementation of digital musical instruments is often opaque to the performer. Even when the relationship between action and sound is readily understandable, the internal hardware or software operations that create that relationship may be inaccessible to scrutiny or modification. This paper presents a new approach to digital instrument design which lets the performer alter and subvert the instrument's internal operation through circuit-bending techniques. The approach uses low-latency feedback loops between software and analog hardware to expose the internal working of the instrument. Compared to the standard control voltage approach used on analog synths, alterations to the feedback loops produce distinctive and less predictable changes in behaviour with original artistic applications. This paper discusses the technical foundations of the approach, its roots in hacking and circuit bending, and case studies of its use in live performance with the D-Box hackable instrument.}
}

@inproceedings{arau2015,
  author = {Tommy Feldt and Sarah Freilich and Shaun Mendonsa and Daniel Molin and Andreas Rau},
  title = {Puff, Puff, Play: A Sip-And-Puff Remote Control for Music Playback},
  pages = {34--35},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179058},
  url = {http://www.nime.org/proceedings/2015/nime2015_260.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/260/0260-file1.mp4},
  abstract = {We introduce the Peripipe, a tangible remote control for a music player that comes in the shape of a wooden tobacco pipe. The design is based on breath control, using sips and puffs as control commands. An atmospheric pressure sensor in the Peripipe senses changes in the air pressure. Based on these changes, the pipe determines when the user performs a puff, double-puff, sip, double-sip or a long puff or long sip action, and wirelessly sends commands to a smartphone running the music player. Additionally, the Peripipe provides fumeovisual feedback, using color-illuminated smoke to display the system status. With the form factor, the materials used, the interaction through breath, and the ephemeral feedback we aim to emphasize the emotional component of listening to music that, in our eyes, is not very well reflected in traditional remote controls.}
}

@inproceedings{ndalessandro2015,
  author = {Nicolas d'Alessandro and Jo\''elle Tilmanne and Ambroise Moreau and Antonin Puleo},
  title = {AirPiano: A Multi-Touch Keyboard with Hovering Control},
  pages = {255--258},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1181434},
  url = {http://www.nime.org/proceedings/2015/nime2015_261.pdf},
  abstract = {In this paper, we describe the prototyping of two musical interfaces that use the LeapMotion camera in conjunction with two different touch surfaces: a Wacom tablet and a transparent PVC sheet. In the Wacom use case, the camera is between the hand and the surface. In the PVC use case, the camera is under the transparent sheet and tracks the hand through it. The aim of this research is to explore hovering motion surrounding the touch interaction on the surface and include properties of such motion in the musical interaction. We present our unifying software, called AirPiano, that discretises the 3D space into 'keys' and proposes several mapping strategies with the available dimensions. These control dimensions are mapped onto a modified HandSketch sound engine that achieves multitimbral pitch-synchronous point cloud granulation.}
}

@inproceedings{ahazzardb2015,
  author = {Steve Benford and Adrian Hazzard and Alan Chamberlain and Liming Xu},
  title = {Augmenting a Guitar with its Digital Footprint},
  pages = {303--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179016},
  url = {http://www.nime.org/proceedings/2015/nime2015_264.pdf},
  abstract = {We explore how to digitally augment musical instruments by connecting them to their social histories. We describe the use of Internet of Things technologies to connect an acoustic guitar to its digital footprint -- a record of how it was designed, built and played. We introduce the approach of crafting interactive decorative inlay into the body of an instrument that can then be scanned using mobile devices to reveal its digital footprint. We describe the design and construction of an augmented acoustic guitar called Carolan alongside activities to build its digital footprint through documented encounters with twenty-seven players in a variety of settings. We reveal the design challenge of mapping the different surfaces of the instrument to various facets of its footprint so as to afford appropriate experiences to players, audiences and technicians. We articulate an agenda for further research on the topic of connecting instruments to their social histories, including capturing and performing digital footprints and creating personalized and legacy experiences.}
}

@inproceedings{skestelli2015,
  author = {{Sair Sinan} Kestelli},
  title = {Motor Imagery: What Does It Offer for New Digital Musical Instruments?},
  pages = {107--110},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179104},
  url = {http://www.nime.org/proceedings/2015/nime2015_265.pdf},
  abstract = {There have been more interest and research towards multisensory aspects of sound as well as vision and movement, especially in the last two decades. An emerging research field related with multi-sensory research is 'motor imagery', which could be defined as the mental representation of a movement without actual production of muscle activity necessary for its execution. Emphasizing its close relationship and potential future use in new digital musical instruments (DMI) practice and reviewing literature, this paper will introduce fundamental concepts about motor imagery (MI), various methods of measuring MI in different configurations and summarize some important findings about MI in various studies. Following, it will discuss how this research area is related to DMI practice and propose potential uses of MI in this field. }
}

@inproceedings{hpurwins2015,
  author = {Mikkel J\''{o}rgensen and Aske Knudsen and Thomas Wilmot and Kasper Lund and Stefania Serafin and Hendrik Purwins},
  title = {A Mobile Music Museum Experience for Children},
  pages = {36--37},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178997},
  url = {http://www.nime.org/proceedings/2015/nime2015_267.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/267/0267-file1.mov},
  abstract = {An interactive music instrument museum experience for children of 10-12 years is presented. Equipped with tablet devices, the children are sent on a treasure hunt. Based on given sound samples, the participants have to identify the right musical instrument (harpsichord, double bass, viola) out of an instrument collection. As the right instrument is located, a challenge of playing an application on the tablet is initiated. This application is an interactive digital representation of the found instrument, mimicking some of its key playing techniques, using a simplified scrolling on screen musical notation. The musical performance of the participant is graded on a point scale. After completion of the challenge, the participants' performances of the three instruments are played back simultaneously, constituting a composition. A qualitative evaluation of the application in a focus group interview with school children revealed that the children were more engaged when playing with the interactive application than when only watching a music video.}
}

@inproceedings{tresch2015,
  author = {Thomas Resch},
  title = {RWA -- A Game Engine for Real World Audio Games},
  pages = {392--395},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179160},
  url = {http://www.nime.org/proceedings/2015/nime2015_269.pdf},
  abstract = {Audio guides and (interactive) sound walks have existed for decades. Even smartphone games taking place in the real world are no longer a novelty. But due to the lack of a sufficient middleware which fulfills the requirements for creating this software genre, artists, game developers and institutions such as museums are forced to implement rather similar functionality over and over again. This paper describes the basic principles of Real World Audio (RWA), an extendable audio game engine for targeting smartphone operating systems, which rolls out all functionality for the generation of the above-mentioned software genres. It combines the ability for building location-based audio walks and -guides with the components necessary for game development. Using either the smartphone sensors or an external sensor board for head tracking and gesture recognition, RWA allows developers to create audio walks, audio adventures and audio role playing games (RPG) outside in the real world.}
}

@inproceedings{ajense2015,
  author = {Arvid Jense and Hans Leeuw},
  title = {WamBam: A case study in design for an electronic musical instrument for severely intellectually disabled users},
  pages = {74--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179098},
  url = {http://www.nime.org/proceedings/2015/nime2015_270.pdf},
  abstract = {This paper looks at the design process of the WamBam; a self-contained electronic hand-drum meant for music therapy sessions with severely intellectually disabled clients. Using co-reflection with four musical therapists and literature research, design guidelines related to this specific user-group and context are formed. This leads to a concept of which the most relevant aspects are discussed, before describing the user studies. Finally, the plan for the redesign is discussed. The WamBam has unique possibilities to deal with the low motor skills and cognitive abilities of severely intellectually disabled users while music therapists benefit from the greater versatility and portability of this design compared to other musical instruments. A prototype was tested with twenty users. Participants proved to be triggered positively by the WamBam, but three limiting usability issues were found. These issues were used as the fundamentals for a second prototype. Music therapists confirm the value of the WamBam for their practice. }
}

@inproceedings{hlimerick2015,
  author = {Florent Berthaut and David Coyle and James Moore and Hannah Limerick},
  title = {Liveness Through the Lens of Agency and Causality},
  pages = {382--386},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179026},
  url = {http://www.nime.org/proceedings/2015/nime2015_272.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/272/0272-file1.mp4},
  abstract = {Liveness is a well-known problem with Digital Musical Instruments (DMIs). When used in performances, DMIs provide less visual information than acoustic instruments, preventing the audience from understanding how the musicians influence the music. In this paper, we look at this issue through the lens of causality. More specifically, we investigate the attribution of causality by an external observer to a performer, relying on the theory of apparent mental causation. We suggest that the perceived causality between a performer's gestures and the musical result is central to liveness. We present a framework for assessing attributed causality and agency to a performer, based on a psychological theory which suggests three criteria for inferred causality. These criteria then provide the basis of an experimental study investigating the effect of visual augmentations on audience's inferred causality. The results provide insights on how the visual component of performances with DMIs impacts the audience's causal inferences about the performer. In particular we show that visual augmentations help highlight the influence of the musician when parts of the music are automated, and help clarify complex mappings between gestures and sounds. Finally we discuss the potential wider implications for assessing liveness in the design of new musical interfaces.}
}

@inproceedings{dverdonk2015,
  author = {Dianne Verdonk},
  title = {Visible Excitation Methods: Energy and Expressiveness in Electronic Music Performance},
  pages = {42--43},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179188},
  url = {http://www.nime.org/proceedings/2015/nime2015_273.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/273/0273-file1.m4v},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/273/0273-file2.m4v},
  abstract = {In electronic music performance, a good relationship between what is visible and what is audible can contribute to a more succesful way of conveying thought or feeling. This connection can be enhanced by putting visible energy into an electronic interface or instrument. This paper discusses the advantages and implementations of visible excitation methods, and how these could reinforce the bridge between the performance of acoustic and electronic instruments concerning expressiveness.}
}

@inproceedings{snyder2015,
  author = {Jeff Snyder and Ryan Johns and Charles Avis and Gene Kogan and Axel Kilian},
  title = {Machine Yearning: An Industrial Robotic Arm as a Performance Instrument},
  pages = {184--186},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179180},
  url = {http://www.nime.org/proceedings/2015/nime2015_275.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/275/0275-file1.mp3},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/275/0275-file2.mp4},
  abstract = {This paper describes a project undertaken in the Spring of 2014 that sought to create an audio-visual performance using an industrial robotic arm. Some relevant examples of previous robotic art are discussed, and the design challenges posed by the unusual situation are explored. The resulting design solutions for the sound, robotic motion, and video projection mapping involved in the piece are explained, as well as the artistic reasoning behind those solutions. Where applicable, links to open source code developed for the project are provided.}
}

@inproceedings{eberdahl2015,
  author = {Edgar Berdahl and Denis Huber},
  title = {The Haptic Hand},
  pages = {303--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179022},
  url = {http://www.nime.org/proceedings/2015/nime2015_281.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/281/0281-file1.mov},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/281/0281-file2.mov},
  abstract = {The haptic hand is a greatly simplified robotic hand that is designed to mirror the human hand and provide haptic force feedback for applications in music. The fingers of the haptic hand device are laid out to align with four of the fingers of the human hand. A key is placed on each of the fingers so that a human hand can perform music by interacting with the keys. The haptic hand is distinguished from other haptic keyboards in the sense that each finger is meant to stay with a particular key. The haptic hand promotes unencumbered interaction with the keys. The user can easily position a finger over a key and press downward to activate it---the user does not need to insert his or her fingers into an unwieldy exoskeleton or set of thimbles. An example video demonstrates some musical ideas afforded by this open-source software and hardware project.}
}

@inproceedings{slee2015,
  author = {Lee, Sang Won and Georg Essl},
  title = {Web-Based Temporal Typography for Musical Expression and Performance},
  pages = {65--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179114},
  url = {http://www.nime.org/proceedings/2015/nime2015_283.pdf},
  abstract = {This paper introduces programmable text rendering that enables temporal typography in web browsers. Typing is seen not only as a dynamic but interactive process facilitating both scripted and live musical expression in various contexts such as audio-visual performance using keyboards and live coding visualization. With the programmable text animation , we turn plain text into a highly audiovisual medium and a musical interface which is visually expressive. We describe a concrete technical realization of the concept using Web Audio API, WebGL and GLSL shaders. We further show a number of examples that illustrate instances of the concept in various scenarios ranging from simple textual visualization to live coding environments. Lastly, we present an audiovisual music piece that involves live writing augmented by the visualization technique.}
}

@inproceedings{esheffield2015,
  author = {Eric Sheffield and Sile O'Modhrain and Michael Gould and Brent Gillespie},
  title = {The Pneumatic Practice Pad},
  pages = {231--234},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179178},
  url = {http://www.nime.org/proceedings/2015/nime2015_286.pdf},
  abstract = {The Pneumatic Practice Pad is a commercially available 10'' practice pad that has been modified to allow for tension changes in a matter of seconds using a small electric air pump. In this paper, we examine the rebound characteristics of the Pneumatic Practice Pad at various pressure presets and compare them to a sample of acoustic drums. We also review subjective feedback from participants in a playing test.}
}

@inproceedings{wmarley2015,
  author = {William Marley and Nicholas Ward},
  title = {Gestroviser: Toward Collaborative Agency in Digital Musical Instruments.},
  pages = {140--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179124},
  url = {http://www.nime.org/proceedings/2015/nime2015_287.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/287/0287-file1.mp4},
  abstract = {This paper describes a software extension to the Reactable entitled Gestroviser that was developed to explore musician machine collaboration at the control signal level. The system functions by sampling a performers input, processing or reshaping this sampled input, and then repeatedly replaying it. The degree to which the sampled control signal is processed during replay is adjustable in real-time by the manipulation of a continuous finger slider function. The reshaping algorithm uses stochastic methods commonly used for MIDI note generation from a provided dataset. The reshaped signal therefore varies in an unpredictable manner. In this way the Gestroviser is a device to capture, reshape and replay an instrumental gesture. We describe the result of initial user testing of the system and discuss possible further development.}
}

@inproceedings{kschlei2015,
  author = {Warren Enstr\''om and Josh Dennis and Brian Lynch and Kevin Schlei},
  title = {Musical Notation for Multi-Touch Interfaces},
  pages = {83--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179056},
  url = {http://www.nime.org/proceedings/2015/nime2015_289.pdf},
  abstract = {This paper explores the creation and testing of a new system for notating physical actions on a surface. This notation is conceptualized through the medium of, and initially tested on, multi-touch interfaces. Existing methods of notating movement are reviewed, followed by a detailed explanation of our notation. User trials were carried out in order to test how effective this notation was, the results of which be explained. An analysis of the collected data follows, as well as criticisms of the notation and testing process.}
}

@inproceedings{bbortz2015,
  author = {Brennon Bortz and Javier Jaimovich and {R. Benjamin} Knapp},
  title = {Emotion in Motion: A Reimagined Framework for Biomusical/Emotional Interaction},
  pages = {44--49},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179034},
  url = {http://www.nime.org/proceedings/2015/nime2015_291.pdf},
  abstract = {Our experiment, Emotion in Motion, has amassed the world's largest database of human physiology associated with emotion in response to the presentation of various selections of musical works. What began as a doctoral research study has grown to include the emotional responses to musical experience from over ten thousand participants across the world, from installations in Dublin, New York City, Norway, Singapore, the Philippines, and Taiwan. The most recent iteration of is currently underway in Taipei City, Taiwan. Preparation for this installation provided an opportunity to reimagine the architecture of , allowing for a wider range of potential applications than were originally possible with the original tools that drove the experiment. Now more than an experiment, is a framework for developing myriad emotional/musical/biomusical interactions with multiple co-located or remote participants. This paper describes the development of this open-source framework and includes discussion of its various components: hardware agnostic sensor inputs, refined physiological signal processing tools, and a public database of data collected during various instantiations of applications built on the framework. We also discuss our ongoing work with this tool, and provide the reader with other potential applications that they might realize in using .}
}

@inproceedings{hlin2015,
  author = {Hsin-Ming Lin and Chin-Ming Lin},
  title = {Harmonic Intonation Trainer: An Open Implementation in Pure Data},
  pages = {38--39},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179118},
  url = {http://www.nime.org/proceedings/2015/nime2015_300.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/300/0300-file1.mp4},
  abstract = {Pedagogical research demonstrates theories and practices of perception or production of melodic or harmonic ``intonation'', i.e. the realization of pitch accuracy. There are software and hardware to help students improve intonation. Those tools have various functions. Nevertheless, they still miss something which could benefit users very much. Even worse, they are not easy to be revised. Most importantly, there should be more amusing and engaging interaction between a tuning trainer and a user which is able to exchange roles of tuner and player. In this research, we implement an open-source program named ``Harmonic Intonation Trainer'' in Pure Data. It includes most of essential elements of a smart tuner. A user can tune his pitch while optionally hearing (through earphones) the target pitch and other harmonic intervals in respective octaves. Moreover, in its interactive accompanist mode, a user's input pitch serves as the reference frequency; the program follows his intonation to generate corresponding harmonic intervals. Additionally, user can straightforwardly edit all parameters and patches by Pure Data. Any adoption or revision is absolutely welcome. Finally, we will initiate another research to test and to inspect experimental results from student orchestras so that its future version is expected to be more sophisticated.}
}

@inproceedings{jbarbosa2015,
  author = {Jeronimo Barbosa and Joseph Malloch and Marcelo Wanderley and St\'ephane Huot},
  title = {What does 'Evaluation' mean for the NIME community?},
  pages = {156--161},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179010},
  url = {http://www.nime.org/proceedings/2015/nime2015_301.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/301/0301-file1.xlsx},
  abstract = {Evaluation has been suggested to be one of the main trends in current NIME research. However, the meaning of the term for the community may not be as clear as it seems. In order to explore this issue, we have analyzed all papers and posters published in the proceedings of the NIME conference from 2012 to 2014. For each publication that explicitly mentioned the term evaluation, we looked for: a) What targets and stakeholders were considered? b) What goals were set? c) What criteria were used? d) What methods were used? e) How long did the evaluation last? Results show different understandings of evaluation, with little consistency regarding the usage of the word. Surprisingly in some cases, not even basic information such as goal, criteria and methods were provided. In this paper, we attempt to provide an idea of what evaluation means for the NIME community, pushing the discussion towards how could we make a better use of evaluation on NIME design and what criteria should be used regarding each goal.}
}

@inproceedings{ihattwick2015,
  author = {Ian Hattwick and Marcelo Wanderley},
  title = {Interactive Lighting in the Pearl: Considerations and Implementation},
  pages = {201--204},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179080},
  url = {http://www.nime.org/proceedings/2015/nime2015_302.pdf},
  abstract = {The Pearl is a multi-modal computer interface initially conceived as an interactive prop for a multi-artistic theatrical performance. It is a spherical hand-held wireless controller embedded with various sensor technologies and interactive lighting. The lighting was a key conceptual component in the instrument's creation both as a theatrical prop and also as an interface for musical performance as it helps to address conceptual challenges and opportunities posed by the instrument's spherical form. This paper begins by providing a brief description of the Pearl and its use as a spherical instrument. We then discuss mapping the Pearl both to generate sound and control its interactive lighting, and identify different strategies for its use. Strategies we identify include feedback regarding performer gesture, information about the state of the instrument, and use as an aesthetic performance component. }
}

@inproceedings{rgraham2015,
  author = {Richard Graham and Brian Bridges},
  title = {Managing Musical Complexity with Embodied Metaphors},
  pages = {103--106},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179066},
  url = {http://www.nime.org/proceedings/2015/nime2015_303.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/303/0303-file1.mov},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/303/0303-file2.wav},
  abstract = {This paper presents the ideas and mapping strategies behind a performance system that uses a combination of motion tracking and feature extraction tools to manage complex multichannel audio materials for real-time music composition. The use of embodied metaphors within these mappings is seen as a means of managing the complexity of a musical performance across multiple modalities. In particular, we will investigate how these mapping strategies may facilitate the creation of performance systems whose accessibility and richness are enhanced by common integrating bases. A key focus for this work is the investigation of the embodied image schema theories of Lakoff and Johnson alongside similarly embodied metaphorical models within Smalley's influential theory of electroacoustic music (spectromorphology). These metaphors will be investigated for their use as grounding structural components and dynamics for creative practices and musical interaction design. We argue that pairing metaphorical models of forces with environmental forms may have particular significance for the design of complex mappings for digital music performance.}
}

@inproceedings{apon2015,
  author = {Aura Pon and Johnty Wang and Laurie Radford and Sheelagh Carpendale},
  title = {Womba: A Musical Instrument for an Unborn Child},
  pages = {87--90},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179156},
  url = {http://www.nime.org/proceedings/2015/nime2015_304.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/304/0304-file1.mp4},
  abstract = {This paper describes the motivation and process of developing a musical instrument for an unborn child. Well established research shows a fetus in the womb can respond to and benefit from stimuli from the outside world. A musical instrument designed for this unique context can leverage the power of this interaction. Two prototypes were constructed and tested during separate pregnancies and the experiences are presented, and the limitation of the sensor technology identified. We discuss our discoveries about design considerations and challenges for such an instrument, and project thought-provoking questions that arise from its potential applications.}
}

@inproceedings{amarquez-borbon2015,
  author = {Adnan Marquez-Borbon and Paul Stapleton},
  title = {Fourteen Years of NIME: The Value and Meaning of `Community' in Interactive Music Research},
  pages = {307--312},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179128},
  url = {http://www.nime.org/proceedings/2015/nime2015_308.pdf},
  abstract = {This paper examines the notion of community as commonly employed within NIME discourses. Our aim is to clarify and define the term through the community of practice framework. We argue that through its formal use and application, the notion of community becomes a significant space for the examination of emergent musical practices that could otherwise be overlooked. This paper defines community of practice, as originally developed in the social sciences by Lave and Wegener, and applies it within the NIME context through the examination of existing communities of practice such as the laptop performance community, laptop orchestras, as well as the Satellite CCRMA and Patchblocks communities. }
}

@inproceedings{croberts2015,
  author = {Charles Roberts and Matthew Wright and JoAnn Kuchera-Morin},
  title = {Beyond Editing: Extended Interaction with Textual Code Fragments},
  pages = {126--131},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179164},
  url = {http://www.nime.org/proceedings/2015/nime2015_310.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/310/0310-file1.mov},
  abstract = {We describe research extending the interactive affordances of textual code fragments in creative coding environments. In particular we examine the potential of source code both to display the state of running processes and also to alter state using means other than traditional text editing. In contrast to previous research that has focused on the inclusion of additional interactive widgets inside or alongside text editors, our research adds a parsing stage to the runtime evaluation of code fragments and imparts additional interactive capabilities on the source code itself. After implementing various techniques in the creative coding environment Gibber, we evaluate our research through a survey on the various methods of visual feedback provided by our research. In addition to results quantifying preferences for certain techniques over others, we found near unanimous support among survey respondents for including similar techniques in other live coding environments.}
}

@inproceedings{anovello2015,
  author = {Alberto Novello and Antoni Rayzhekov},
  title = {A prototype for pitched gestural sonification of surfaces using two contact microphones},
  pages = {170--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179148},
  url = {http://www.nime.org/proceedings/2015/nime2015_311.pdf},
  abstract = {We present the prototype of a hybrid instrument, which uses two contact microphones to sonify the gestures of a player on a generic surface, while a gesture localization algorithm controls the pitch of the sonified output depending on the position of the gestures. To achieve the gesture localization we use a novel approach combining attack parametrization and template matching across the two microphone channels. With this method we can correctly localize 80 $\pm$ 9 % of the percussive gestures. The user can assign determined pitches to specific positions and change the pitch palette in real time. The tactile feedback characteristic of every surface opens a set of new playing strategies and possibilities specific to any chosen object. The advantages of such a system are the affordable production, flexibility of concert location, object-specific musical instruments, portability, and easy setup.}
}

@inproceedings{oizmirli2015,
  author = {Ozgur Izmirli},
  title = {Framework for Exploration of Performance Space},
  pages = {99--102},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179094},
  url = {http://www.nime.org/proceedings/2015/nime2015_312.pdf},
  abstract = {This paper presents a framework for the analysis and exploration of performance space. It enables the user to visualize performances in relation to other performances of the same piece based on a set of features extracted from audio. A performance space is formed from a set of performances through spectral analysis, alignment, dimensionality reduction and visualization. Operation of the system is demonstrated initially with synthetic MIDI performances and then with a case study of recorded piano performances.}
}

@inproceedings{tbarraclough2015,
  author = {{Timothy J.} Barraclough and {Dale A.} Carnegie and Ajay Kapur},
  title = {Musical Instrument Design Process for Mobile Technology},
  pages = {289--292},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179012},
  url = {http://www.nime.org/proceedings/2015/nime2015_313.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/313/0313-file1.mp4},
  abstract = {This paper presents the iterative design process based upon multiple rounds of user studies that guided the the design of a novel social music application, Pyxis Minor. The application was designed based on the concept of democratising electronic music creation and performance. This required the development to be based upon user studies to inform and drive the development process in order to create a novel musical interface that can be enjoyed by users of any prior musicianship training.}
}

@inproceedings{rduindam2015,
  author = {Rhys Duindam and Diemo Schwarz and Hans Leeuw},
  title = {Tingle: A Digital Music Controller Re-Capturing the Acoustic Instrument Experience},
  pages = {219--222},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179054},
  url = {http://www.nime.org/proceedings/2015/nime2015_319.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/319/0319-file1.mp4},
  abstract = {Tingle is a new digital music controller that attempts to recapture the acoustic touch and feel, and also gives new opportunities for expressive play. Tingle resembles a pin-art toy which has been made interactive through a new sensing technology, with added haptic feedback and motion control. It pushes back, vibrates, and warps the sound through the musicians nuanced input. In this article Tingle will be discussed in combination with CataRT. }
}

@inproceedings{sgelineck2015,
  author = {Steven Gelineck and Dannie Korsgaard and Morten B\''uchert},
  title = {Stage- vs. Channel-strip Metaphor --- Comparing Performance when Adjusting Volume and Panning of a Single Channel in a Stereo Mix},
  pages = {343--346},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179064},
  url = {http://www.nime.org/proceedings/2015/nime2015_320.pdf},
  abstract = {This study compares the stage metaphor and the channel strip metaphor in terms of performance. Traditionally, music mixing consoles employ a channels strip control metaphor for adjusting parameters such as volume and panning of each track. An alternative control metaphor, the so-called stage metaphor lets the user adjust volume and panning by positioning tracks relative to a virtual listening position. In this study test participants are given the task to adjust volume and panning of one channel (in mixes consisting of three channels) in order to replicate a series of simple pre-rendered mixes. They do this using (1) a small physical mixing controller and (2) using an iPad app, which implements a simple stage metaphor interface. We measure how accurately they are able to replicate mixes in terms of volume and panning and how fast they are at doing so. Results reveal that performance is surprisingly similar and thus we are not able to detect any significant difference in performance between the two interfaces. Qualitative data however, suggests that the stage metaphor is largely favoured for its intuitive interaction --- confirming earlier studies. }
}

@inproceedings{jwu2015,
  author = {{J. Cecilia} Wu and {Yoo Hsiu} Yeh and Romain Michon and Nathan Weitzner and Jonathan Abel and Matthew Wright},
  title = {Tibetan Singing Prayer Wheel: A Hybrid Musical- Spiritual Instrument Using Gestural Control},
  pages = {91--94},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179196},
  url = {http://www.nime.org/proceedings/2015/nime2015_322.pdf},
  abstract = {This paper presents the Tibetan Singing Prayer Wheel, a hand-held, wireless, sensor-based musical instrument with a human-computer interface that simultaneously processes vocals and synthesizes sound based on the performer's hand gestures with a one-to-many mapping strategy. A physical model simulates the singing bowl, while a modal reverberator and a delay-and-window effect process the performer's vocals. This system is designed for an electroacoustic vocalist interested in using a solo instrument to achieve performance goals that would normally require multiple instruments and activities.}
}

@inproceedings{ifranco2015,
  author = {Ivan Franco and Marcelo Wanderley},
  title = {Pratical Evaluation of Synthesis Performance on the Beaglebone Black},
  pages = {223--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179062},
  url = {http://www.nime.org/proceedings/2015/nime2015_323.pdf},
  abstract = {The proliferation and easy access to a new breed of ARM-based single-board computers has promoted an increased usage of these platforms in the creation of self-contained Digital Music Instruments. These directly incorporate all of the necessary processing power for tasks such as sensor signal acquisition, control data processing and audio synthesis. They can also run full Linux operating systems, through which domain-specific languages for audio computing facilitate a low entry barrier for the community. In computer music the adoption of these computing platforms will naturally depend on their ability to withstand the demanding computing tasks associated to high-quality audio synthesis. In the context of computer music practice there are few reports about this quantification for practical purposes. This paper aims at presenting the results of performance tests of SuperCollider running on the BeagleBone Black, a popular mid-tier single-board computer, while performing commonly used audio synthesis techniques.}
}

@inproceedings{cbrown2015,
  author = {Courtney Brown and Sharif Razzaque and Garth Paine},
  title = {Rawr! A Study in Sonic Skulls: Embodied Natural History},
  pages = {5--10},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179036},
  url = {http://www.nime.org/proceedings/2015/nime2015_325.pdf},
  abstract = {Lambeosaurine hadrosaurs are duck-billed dinosaurs known for their large head crests, which researchers hypothesize were resonators for vocal calls. This paper describes the motivation and process of iteratively designing a musical instrument and interactive sound installation based on imagining the sounds of this extinct dinosaur. We used scientific research as a starting point to create a means of sound production and resonator, using a 3D model obtained from Computed Topology (CT) scans of a Corythosaurus skull and an endocast of its crest and nasal passages. Users give voice to the dinosaur by blowing into a mouthpiece, exciting a larynx mechanism and resonating the sound through the hadrosaur's full-scale nasal cavities and skull. This action allows an embodied glimpse into an ancient past. Users know the dinosaur through the controlled exhalation of their breath, how the compression of the lungs leads to a whisper or a roar.}
}

@inproceedings{kyerkes2015,
  author = {{Muhammad Hafiz Wan} Rosli and Karl Yerkes and Matthew Wright and Timothy Wood and Hannah Wolfe and Charlie Roberts and Anis Haron and {Fernando Rincon} Estrada},
  title = {Ensemble Feedback Instruments},
  pages = {144--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179170},
  url = {http://www.nime.org/proceedings/2015/nime2015_329.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/329/0329-file1.mp4},
  abstract = {We document results from exploring ensemble feedback in loosely-structured electroacoustic improvisations. A conceptual justification for the explorations is provided, in addition to discussion of tools and methodologies. Physical configurations of intra-ensemble feedback networks are documented, along with qualitative analysis of their effectiveness.}
}

@inproceedings{jgregorio2015,
  author = {Jeff Gregorio and David Rosen and Michael Caro and {Youngmoo E.} Kim},
  title = {Descriptors for Perception of Quality in Jazz Piano Improvisation},
  pages = {327--328},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179072},
  url = {http://www.nime.org/proceedings/2015/nime2015_331.pdf},
  abstract = {Quality assessment of jazz improvisation is a multi-faceted, high-level cognitive task routinely performed by educators in university jazz programs and other discriminating music listeners. In this pilot study, we present a novel dataset of 88 MIDI jazz piano improvisations with ratings of creativity, technical proficiency, and aesthetic appeal provided by four jazz experts, and we detail the design of a feature set that can represent some of the rhythmic, melodic, harmonic, and other expressive attributes humans recognize as salient in assessment of performance quality. Inherent subjectivity in these assessments is inevitable, yet the recognition of performance attributes by which humans perceive quality has wide applicability to related tasks in the music information retrieval (MIR) community and jazz pedagogy. Preliminary results indicate that several musiciologically-informed features of relatively low computational complexity perform reasonably well in predicting performance quality labels via ordinary least squares regression.}
}

@inproceedings{amarquez-borbonb2015,
  author = {Adnan Marquez-Borbon},
  title = {But Does it Float? Reflections on a Sound Art Ecological Intervention},
  pages = {335--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179126},
  url = {http://www.nime.org/proceedings/2015/nime2015_333.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/333/0333-file1.mp4},
  abstract = {This paper discusses the particular aesthetic and contextual considerations emergent from the design process of a site-specific sound art installation, the Wave Duet. The main point of this paper proposes that beyond the initial motivation produced by new technologies and their artistic potential, there are many profound artistic considerations that drive the development and design of a work in unique ways. Thus, in the case of the Wave Duet, the produced buoys were prompted by investigating the relationship between sonic objects and natural phenomena. As a result, the mappings, physical and sound designs directly reflect these issues. Finally, it is also suggested that during the course of development, unintended issues may emerge and further inform how the work is perceived in a broader sense. }
}

@inproceedings{mblessing2015,
  author = {Matthew Blessing and Edgar Berdahl},
  title = {Textural Crossfader},
  pages = {180--181},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179032},
  url = {http://www.nime.org/proceedings/2015/nime2015_337.pdf},
  urlsuppl1 = {http://www.nime.org/proceedings/2015/337/0337-file1.mp4},
  urlsuppl2 = {http://www.nime.org/proceedings/2015/337/0337-file2.mov},
  abstract = {A LapBox derivative, the Textural Crossfader is a keyboard-based embedded acoustic instrument, which sits comfortably across the performer's lap and radiates sound out of integrated stereo speakers. The performer controls the sound by manipulating the keys on a pair of mini-keyboard interfaces. A unique one-to-one mapping enables the performer to precisely crossfade among a set of looped audio wave files, creating a conveniently portable system for navigating through a complex timbre space. The axes of the timbre space can be reconfigured by replacing the wave files stored in the flash memory.}
}

@inproceedings{acabrera2015,
  author = {Andres Cabrera},
  title = {Serverless and Peer-to-peer distributed interfaces for musical control},
  pages = {355--358},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Edgar Berdahl and Jesse Allison},
  year = {2015},
  month = {May},
  publisher = {Louisiana State University},
  address = {Baton Rouge, Louisiana, USA},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179040},
  url = {http://www.nime.org/proceedings/2015/nime2015_351.pdf},
  abstract = {This paper presents the concept and implementation of a decentralized, server-less and peer-to-peer network for the interchange of musical control interfaces and data using the OSC protocol. Graphical control elements that form the control interface can be freely edited and exchanged to and from any device in the network, doing away with the need for a separate server or editing application. All graphical elements representing the same parameter will have their value synchronized through the network mechanisms. Some practical considerations surrounding the implementation of this idea like automatic layout of control, editing interfaces on mobile touch-screen devices and auto-discovery of network nodes are discussed. Finally, GoOSC, a mobile application implementing these ideas is presented.}
}

