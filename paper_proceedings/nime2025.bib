@article{nime2025_1,
  author = {Sabina Hyoju Ahn and Ryan Millett and Seyeon Park},
  title = {Eco-Sonic Interfaces for Embodied AI Sound Exploration},
  pages = {1--5},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {1},
  track = {Paper},
  doi = {10.5281/zenodo.15698772},
  url = {http://nime.org/proceedings/2025/nime2025_1.pdf},
  presentation-video = {https://youtu.be/GX4YHeGIreY},
  abstract = {Neural Tides is a neural network-based granular synthesizer that examines plastiglomerates—hybrid formations of plastic and organic material in marine environments. The system maps sound grains from oceanic field recordings to a navigable latent space using autoencoders and clustering techniques, controlled via hand gestures and touch. This interface physically connects performers with sonic representations of anthropogenic material transformations in coastal environments.},
  numpages = {5}
}

@article{nime2025_2,
  author = {Benedict Gaster and Nathan Renney and Jasmine Butt},
  title = {Looping slowly: Diffraction through the lens of nostalgia},
  pages = {6--16},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {2},
  track = {Paper},
  doi = {10.5281/zenodo.15698774},
  url = {http://nime.org/proceedings/2025/nime2025_2.pdf},
  presentation-video = {https://youtu.be/Znhoz8XMeLc},
  abstract = {This paper concerns magnetic tape and the nostalgia of media, finding new relevance in old technology, remaking and adapting practices to fit within a modern workflow. Pushing against the driving force of economic structures, which emphasises a continuous cycle of replacement, musicians and instrument designers are drawing on a shared history to create new pieces of art and machines. This can be read as reflecting NIME's Code of Practice and, more generally, the unfolding climate crisis.For some, NIME may convey a focus on new musical instruments, but here, we focus on the notion of new through the diffracted lens of the old. Defined in recent NIME conferences by zooming in on the 'O' in NIME through the importance of reusing and repurposing old musical instruments and, in our case, old practices and processes. This paper considers magnetic tape and the machines that process it as the material and instrument.Following a survey, we present a diffracted reading through an intra-related process of how musicians, producers, and others who work with audio integrate tape into their practice. Drawing on post-humanist theories, we explore how slowness, community, and the old can inform NIME as a methodology. It provides insight for NIME to continue moving forward while focusing, through its Code of Practice, on sustainability, connection with our past, our history, years of artistic practice, and workflows that  re not simply optimised for efficiency or the new.},
  numpages = {11}
}

@article{nime2025_3,
  author = {Pasquale Mainolfi},
  title = {Simulated EEG-Driven Audio Information Mapping Using Inner Hair-Cell Model and Spiking Neural Network },
  pages = {17--25},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {3},
  track = {Paper},
  doi = {10.5281/zenodo.15698778},
  url = {http://nime.org/proceedings/2025/nime2025_3.pdf},
  presentation-video = {},
  abstract = {This study presents a framework for mapping audio information into simulated neural signals and dynamic control maps. The system is based on a biologically-inspired architecture that traces the sound pathway from the cochlea to the auditory cortex. The system transforms acoustic features into neural representations by integrating Meddis's Inner Hair-Cell (IHC) model with spiking neural networks (SNN).The mapping process occurs in three phases: the IHC model converts sound waves into neural impulses, simulating hair cell mechano-electrical transduction. These impulses are then encoded into spatio-temporal patterns through an Izhikevich-based neural network, where spike-timing-dependent plasticity (STDP) mechanisms enable the emergence of activation structures reflecting the acoustic information's complexity. Finally, these patterns are mapped into both EEG-like signals and continuous control maps for real-time interactive performance control.This approach bridges neural dynamics and signal processing, offering a new paradigm for sound information representation. The generated control maps provide a natural interface between acoustic and parametric domains, enabling applications from generative sound design to adaptive performance control, where neuromorphological sound translation explores new forms of audio-driven interaction.},
  numpages = {9}
}

@article{nime2025_4,
  author = {Hugh Aynsley and Pete Bennett and Dave Meckin and Sven Hollowell and Thomas J. Mitchell},
  title = {"Instant Design": Five Strategies for the use of Generative AI in NIME Ideation Workshops},
  pages = {26--32},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {4},
  track = {Paper},
  doi = {10.5281/zenodo.15698780},
  url = {http://nime.org/proceedings/2025/nime2025_4.pdf},
  presentation-video = {},
  abstract = {This paper presents five strategies for facilitating workshops that incorporate AI text-to-image (TTI) generators in the conceptual design of new musical instruments. Developed through a series of iterative workshops, this approach examines the integration of generative AI (GenAI) within creative processes, with a particular focus on idea generation and the interplay between AI-driven tools and traditional craft-based activities in workshop contexts.The primary study was conducted at the Artificial Intelligence and Musical Creativity (AIMC) Conference 2023 and the paper shares insights from the workshop, including the combination of physical prototyping and GenAI concept design through image creation. The paper emphasises the practical implications of incorporating AI tools into group design fiction workshops and offers five suggestions for facilitators and practitioners. It considers the tensions and opportunities that arise in the collaboration between AI and human creativity, underscoring the importance of iterative feedback and the benefits of clearly defined design briefs within speculative design practices.},
  numpages = {7}
}

@article{nime2025_5,
  author = {Ryan Ingebritsen and Daniel Evans and Christopher Knowlton},
  title = {A Study to Discover Metrics to Measure Kinesthetic Empathy in Interactive Music Performance},
  pages = {33--39},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {5},
  track = {Paper},
  doi = {10.5281/zenodo.15698782},
  url = {http://nime.org/proceedings/2025/nime2025_5.pdf},
  presentation-video = {},
  abstract = {Kinesthetic empathy is a term used in performance and kinesthetic interaction, defined as the ability of participants to “read, decode and react to each other’s input”.  In prior studies, performers of interactive music self-reported sensing the presence of other musicians. The purpose of this exploratory study was to identify kinesthetic empathy between two individuals in a live electronic performance reported as perceived interactivity. Participants viewed eight videos, both real duets, and spliced solos appearing as real duets, rating each video. The questions guiding this study were: (a) is there a difference in perceived interactivity between the live and spliced duets, (b) is there a relationship between performance rating and perceived interactivity. Results showed a significant difference in the perceived interactivity of the video conditions. Further, the results showed a significant relationship between performance rating and perceived interactivity of the performers. The results suggest that perceived interactivity between performers could be a metric to measure kinesthetic empathy between performers facilitated by an interactive performance system that could be used to objectively measure the effectiveness of design and pedagogical interventions for new interfaces for musical expression.  },
  numpages = {7}
}

@article{nime2025_6,
  author = {Tatsunori Hirai and Jack Topliss and Thammathip Piumsomboon},
  title = {XR Musical Keyboard: An Extended Reality Keyboard with an Arbitrary Number of Keys and Pitches},
  pages = {40--45},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {6},
  track = {Paper},
  doi = {10.5281/zenodo.15698784},
  url = {http://nime.org/proceedings/2025/nime2025_6.pdf},
  presentation-video = {},
  abstract = {We introduce the Extended Reality (XR) Musical Keyboard, a system allowing users to overlay a virtual keyboard onto a tabletop surface, such as a standard PC keyboard. This virtual keyboard is highly customizable: users can freely program the number of keys and their respective pitches. Modern software instruments offer advanced capabilities, including microtonal scales (pitches outside the standard 12-tone equal temperament). However, playing these instruments often remains challenging due to the lack of corresponding physical hardware. Our proposed solution addresses this gap by projecting a programmable virtual keyboard onto a tangible object within the XR space. This approach combines the software's flexibility with the tactile feedback of a physical surface, enhancing playability. Users can simplify the keyboard layout (e.g., fewer keys than a piano) or expand it beyond conventional limits to explore new expressive possibilities, particularly for microtonal music. We conducted a small pilot study (N=4) involving participants mostly inexperienced with keyboards to gather preliminary feedback on the interface's ease of use for performance.},
  numpages = {6}
}

@article{nime2025_7,
  author = {Jasmine Butt and Benedict Gaster and Nathan Renney and Maisie Palmer},
  title = {Entangling with Light and Shadow: layers of interaction with the pattern organ},
  pages = {46--55},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {7},
  track = {Paper},
  doi = {10.5281/zenodo.15698786},
  url = {http://nime.org/proceedings/2025/nime2025_7.pdf},
  presentation-video = {},
  abstract = {This paper explores the design and use of a camera-based digital musical instrument as a thinking tool for considering entangled, post-human perspectives. The design of the pattern organ, inspired by experimental optical sound-on-film practices, employs a method of visual-to-audio synthesis that responds closely to the material behaviours captured by its camera input.Drawing on findings from exploratory workshops and short material experiments, we describe how interactions emerge and are shaped by both the physical configuration of the instrument and the material behaviours captured by its camera. We consider how frugal mappings and the ‘rawness’ of data can give rise to instruments whose inputs remain open to material complexity, extending the sound engine beyond their enclosures.In the case of the pattern organ, this complexity emerges through overlapping and interfering interactions, where structural forms, human influence, light, shadows, lens distortions, and system quirks all contribute to the shifting harmonic content of the wavetable. We reflect on the instrument as a fluid assemblage, composed of human and non-human entanglements, encouraging us to think beyond traditional notions of human-centred control.},
  numpages = {10}
}

@article{nime2025_8,
  author = {Ruoxi Jia and Xuebiao Liu},
  title = {Appropriating Technology for Interactive Media in Theatre: Design Strategies and Aesthetic Insights  },
  pages = {56--62},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {8},
  track = {Paper},
  doi = {10.5281/zenodo.15698788},
  url = {http://nime.org/proceedings/2025/nime2025_8.pdf},
  presentation-video = {},
  abstract = {This paper investigates interactive media design as a narrative agent in theatrical performance through a practice-based design approach. We clarify the role of the Interaction Director in the production team and examine challenges in appropriating technology for theatrical interaction design. A two-layer workflow is proposed, integrating macro-scale conceptual design with micro-scale cue-to-cue interaction mapping. We address the mutual dependency among creative disciplines, highlighting the collaborative processes necessary to resolve conflicts during the design and rehearsal stages. Furthermore, we adopt a scenographic perspective to analyse how interactive media contributes to dramaturgical storytelling by crafting visual and auditory metaphors. We contextualize interaction design with reader- response theory and the horizon of expectations, demonstrating how interactive media fosters collaborative creativity and expands the narrative potential of theatrical storytelling.},
  numpages = {7}
}

@article{nime2025_9,
  author = {Teresa Pelinski and Giulio Moro and Andrew McPherson},
  title = {pybela: a Python library to interface scientific and physical computing},
  pages = {63--72},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {9},
  track = {Paper},
  doi = {10.5281/zenodo.15698790},
  url = {http://nime.org/proceedings/2025/nime2025_9.pdf},
  presentation-video = {},
  abstract = {Workflows to obtain, examine and prototype with sensor data often involve a back and forth between environments, platforms and programming languages. Usually, sensors are connected to physical computing platforms, and solutions to transmit data to the computer often rely on low-bandwidth communicating channels. It is not obvious how to interface physical computing platforms with data science environments, which also operate under distinct constraints and programming styles. We introduce pybela, a Python library that facilitates real-time, high-bandwidth, bidirectional data streaming between the Bela embedded computing platform and Python, bridging the gap between physical computing environments and data-driven workflows. In this paper, we outline its design, implementation and applications, including deep learning examples.},
  numpages = {10}
}

@article{nime2025_10,
  author = {Franco Caspe and Andrew McPherson and Mark Sandler},
  title = {Waveform Autoencoding at the Edge of Perceivable Latency},
  pages = {73--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {10},
  track = {Paper},
  doi = {10.5281/zenodo.15699550},
  url = {http://nime.org/proceedings/2025/nime2025_10.pdf},
  presentation-video = {},
  abstract = {We introduce an audio plugin implementation of BRAVE, a waveform autoencoder presented recently, that affords Neural Audio Synthesis with low latency and jitter. As a redesign of the well-known RAVE model, BRAVE introduces a series of architectural modifications for supporting instrumental interaction with almost imperceptible latency (<10 ms) and jitter (~ 3 ms). By comparing both designs, we highlight key architectural differences between the models that impact their instrumental performance capability, arguing that no model fits all purposes, and calling for their careful selection for each interactive design. Finally, we discuss challenges and opportunities for leveraging low-latency waveform autoencoders to develop interactive systems, such as Digital Musical Instruments, that can foster control intimacy through enhanced responsiveness and space for nuance.},
  numpages = {4}
}

@article{nime2025_11,
  author = {Mat Dalgleish},
  title = {Chimera: Prototyping a New DMI for Congenital One-Handed Musicianship Through an Autoethnographic Lens},
  pages = {77--83},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {11},
  track = {Paper},
  doi = {10.5281/zenodo.15698794},
  url = {http://nime.org/proceedings/2025/nime2025_11.pdf},
  presentation-video = {},
  abstract = {Chimera is a Digital Musical Instrument (DMI) prototype developed through an autoethnographic lens. That is, a lens shaped by congenital one-handedness as well as extensive experience as both a disabled player of standard instruments and a designer of DMIs for other players. Leveraging Eurorack synthesizer modules as a flexible prototyping toolkit enables an iterative prototyping process that explores the distinctive possibilities of one-handed musicianship. Reflection on a three-month period of iteration and refinement highlights a series of design issues, but also the interconnectedness of physical impairments, and the difficulties of designing for a body in flux. Some directions for future work are outlined. Finally, by discussing the various entangled layers of this instrument prototype, and starting to tease out what Koutsomichalis calls its “stories of a sort”, this paper contributes an until now underrepresented perspective to the dialogue around accessible and inclusive musical instrument design, and disability and musicianship more broadly.},
  numpages = {7}
}

@article{nime2025_12,
  author = {Frederick Rodrigues},
  title = {Synthetic Ornithology: Machine learning, simulations and hyper-real soundscapes},
  pages = {84--92},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {12},
  track = {Paper},
  doi = {10.5281/zenodo.15698797},
  url = {http://nime.org/proceedings/2025/nime2025_12.pdf},
  presentation-video = {},
  abstract = {This paper presents Synthetic Ornithology, an interactive sound-based installation that uses machine learning to simulate sonic representations of localised Australian ecological futures, extending work in soundscape composition to engage in a speculative domain. Central to Synthetic Ornithology is a bespoke ML model, Environmental Audio Generation for Localised Ecologies (EAGLE), capable of generating high-quality, birdsong-focused soundscapes, up to 23 seconds in length. This paper outlines the development of the installation and how its design aims to influence audience perception of the sonic content of the work, extending established practices in NIME and sonic arts to a parafictional approach, and hyperreal aesthetics. Additionally, the paper examines the design and capabilities of the EAGLE model, and reflecting on how generative tools are positioned within a creative context, re-imagines the technical processes of training and configuring ML models as sites of artistic authorship in an expanded creative audio practice.},
  numpages = {9}
}

@article{nime2025_13,
  author = {Nolan Hildebrand and Timothy Roth},
  title = {Out-of-Control Feedback Systems and Collaborative Influence with the Instrumentalist Mixer Feedback Transmutation System},
  pages = {93--98},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {13},
  track = {Paper},
  doi = {10.5281/zenodo.15698799},
  url = {http://nime.org/proceedings/2025/nime2025_13.pdf},
  presentation-video = {},
  abstract = {This paper explores the Instrumentalist Mixer Feedback Transmutation (IMFT) system, a modification of the typical no-input mixer paradigm meant for collaborative improvisatory performance (formally called NIMB+)[9]. IMFT occurs when an instrumentalist is patched into a mixing board with feedback loops (a.k.a no-input mixer). The instrumentalist interacts and influences the mixer’s feedback together with another performer operating the mixer. Introducing an instrument into the no-input mixer’s previously closed system creates possibilities for new collaborative interactions between humans and chaotic feedback systems. In this system, a chaotic, out-of-control relationship can be formed where the output of the mixer and the gestures from the mixer performer can be in battle with the input from the instrumentalist and vice-versa.After a brief historical contextualization of mixer feedback, the IMFT system and the complex relationships that form between human and machine are introduced. No-input mixer performance practices are discussed, followed by exploration of a single feedback loop to illustrate some of the mixer’s possible sound worlds and the nature of the instrument. Performance experiences from two recent compositions by the first author, generative open graphic score #1 (2023) and noise ritual (2023), are described in order to explore different performance interactions created by different instrumentalists working with the IMFT system. This practice-based research provides a useful case study examining the entangled relationship between performers and interfaces in feedback-based music systems and how innovative approaches to an established electronic practice can create new perspectives and collaborative opportunities.},
  numpages = {6}
}

@article{nime2025_14,
  author = {Marco Fiorini and Nicolas Brochec and Joakim Borg and Riccardo Pasini},
  title = {Introducing EG-IPT and ipt~: a novel electric guitar dataset and a new Max/MSP object for real-time classification of instrumental playing techniques},
  pages = {99--107},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {14},
  track = {Paper},
  doi = {10.5281/zenodo.15699591},
  url = {http://nime.org/proceedings/2025/nime2025_14.pdf},
  presentation-video = {},
  abstract = {This paper presents two key contributions to the real-time classification of Instrumental Playing Techniques (IPTs) in the context of NIME and human-machine interactive systems: the EG-IPT dataset and the ipt~ Max/MSP object. The EG-IPT dataset, specifically designed for electric guitar, encompasses a broad range of IPTs captured across six distinct audio sources (five microphones and one direct input) and three pickup configurations. This diversity in recording conditions provides a robust foundation for training accurate models. We evaluate the dataset by employing a Convolutional Neural Network-based classifier (CNN), achieving state-of-the-art performance across a wide array of IPT classes, thereby validating the dataset’s efficacy. The ipt~ object is a new Max/MSP external enabling real-time classification of IPTs via pre-trained CNN models. While in this paper it's demonstrated with the EG-IPT dataset, the ipt~ object is adaptable to models trained on various instruments. By integrating EG-IPT and ipt~, we introduce a novel, end-to-end workflow that spans from data collection, model training to real-time classification and human-computer interaction. This workflow exemplifies the entanglement of diverse components (data acquisition, machine learning, real-time processing, and interactive control) within a unified system, advancing the potential for dynamic, real-time music performance and human-computer interaction in the context of NIME.},
  numpages = {9}
}

@article{nime2025_15,
  author = {Lewis Wolstanholme and Jordie Shier and Rodrigo Constanzo and Andrew McPherson},
  title = {Drum Modal Feedback: Concept Design of an Augmented Percussion Instrument},
  pages = {108--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {15},
  track = {Paper},
  doi = {10.5281/zenodo.15698805},
  url = {http://nime.org/proceedings/2025/nime2025_15.pdf},
  presentation-video = {},
  abstract = {We here outline the concept design of an augmented percussion instrument, conceived for and used as part of a variety of distinct performances and compositions. Throughout the curation of this project, each creative act has enabled us to contextualise, examine and reflect upon the design of this augmented instrument. In accordance with Stolterman and Wiberg’s concept driven design methodology, we do not present a singular instrument design, but instead an overarching design concept alongside its developmental and evaluative narrative. This augmentation centres upon the use of a drum trigger and a tactile transducer, which when coupled together can be used to feedback or resonate a drum. The resultant soundworld develops upon the idiomatic sonority of a drum, and allows for the duration and timbre of a drum strike to be continuously manipulated and shaped throughout a performance. In exploring the soundworld which results from this approach, we have experimented with numerous configurations of these pieces of hardware, and have also employed various pieces of software to parametrise the sonic subtleties that this approach engenders. Most prominently, we have developed a bespoke piece of software which analyses the modes of a drum prior to performance, and uses this modal analysis to shape the overall feedback and resonance. Throughout this design process, we have consistently been met with new creative criteria that challenge our approach and ideas, in response to the particularities of the musicians we are working alongside, as well as the performative and aesthetic environments we are working within.},
  numpages = {8}
}

@article{nime2025_16,
  author = {Nicholas Evans and Behzad Haki and Sergi Jordà},
  title = {Repurposing a Rhythm Accompaniment System for Pipe Organ Performance},
  pages = {116--120},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {16},
  track = {Paper},
  doi = {10.5281/zenodo.15698807},
  url = {http://nime.org/proceedings/2025/nime2025_16.pdf},
  presentation-video = {},
  abstract = {This paper presents an overview of a human-machine collaborative musical performance by Raül Refree utilizing multiple MIDI-enabled pipe organs at Palau Güell, as part of the Organic concert series. Our earlier collaboration focused on live performances using drum generation systems, where generative models captured rhythmic transient structures while ignoring harmonic information. For the organ performance, we required a system capable of generating harmonic sequences in real-time, conditioned on Refree's performance. Instead of developing a comprehensive state-of-the-art model, we integrated a more traditional generative method to convert our pitch-agnostic rhythmic patterns into harmonic sequences. This paper details the development process, the creative and technical considerations behind the final performance, and a reflection on the efficacy and adaptability of the chosen methodology.},
  numpages = {5}
}

@article{nime2025_17,
  author = {Tak Cheung Hui and Xiaoqiao  Li and Yu Chia Kuo},
  title = {Turntable-Based Electronic Music and Embodied Audience Interaction},
  pages = {121--125},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {17},
  track = {Paper},
  doi = {10.5281/zenodo.15699598},
  url = {http://nime.org/proceedings/2025/nime2025_17.pdf},
  presentation-video = {},
  abstract = {Rings… Through Rings transforms archival maps of Hong Kong’s military fortifications into playable surfaces for turntable-based electronic music. Laser-etched discs encode cartographic data, producing sonic textures manipulated through turntables and enhanced by audio techniques like cross-synthesis, concatenative synthesis, and spatialization. Grounded in theories of transcoding, productive agency, and participatory culture, the project reimagines the turntable as a cultural interface, bridging analog heritage with computational sound. This hybrid system blends pre-composed musical structures with real-time audience interaction, allowing participants to alter playback, swap discs, and influence spatial audio. By merging cartography, sound, and participatory design, the work offers a collaborative, multisensory approach to intangible heritage. Future developments include expanded spatial configurations, real-time disc fabrication, and AI integration to deepen engagement and cultural reinterpretation.},
  numpages = {5}
}

@article{nime2025_18,
  author = {David Kim-Boyle},
  title = {Collaborative Musical Expression Through Interactive VR Scores},
  pages = {126--132},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {18},
  track = {Paper},
  doi = {10.5281/zenodo.15698811},
  url = {http://nime.org/proceedings/2025/nime2025_18.pdf},
  presentation-video = {},
  abstract = {While the technical affordances of virtual reality (VR) have provided new ways for artists to aestheticize immersion, spectator agency, embodiment and multi-sensory engagement, they have also opened new possibilities for composers interested in exploring how interactive musical scores might become a means through which collaboration itself becomes the locus of aesthetic expression. In this paper, the author will provide an overview of an ongoing project which explores new ways of thinking about musical collaboration in VR through the 3D visualization of interactive, graphic scores adapted from works by composers Earle Brown, Christian Wolff, and Toru Takemitsu. The research demonstrates how VR can transform traditional score interpretation by creating dynamic, interactive environments that enable collaborative musical expression, challenge conventional notation, and offer novel ways of negotiating musical performance through networked, multi-user interactions.},
  numpages = {7}
}

@article{nime2025_19,
  author = {Kunwoo Kim and Andrew Zhu and Eito Murakami and Marise van Zyl and Yikai Li and Max Jardetzky and Ge Wang},
  title = {SVOrk: Stanford Virtual Reality Orchestra},
  pages = {133--141},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {19},
  track = {Paper},
  doi = {10.5281/zenodo.15698813},
  url = {http://nime.org/proceedings/2025/nime2025_19.pdf},
  presentation-video = {},
  abstract = {This paper chronicles the creation of the Stanford Virtual Reality Orchestra (SVOrk), a new computer music ensemble where both performers and audience engage in a shared, fully immersive virtual reality (VR) chamber-esque concert experience. Motivated to explore group-based live performance within VR, SVOrk has designed and crafted virtual musical interfaces, fantastical 3D-modeled environments, and a network infrastructure to support real-time shared participation. Inherent within this initiative is a reimagining of conventional concert experiences, introducing virtual lobbies, customizable avatars, and immersive audience interactions. These experimental features explore new forms of social presence, audience identities, and expressive communication to help address the overarching question, “What does it mean to participate in a VR musical performance?” SVOrk’s premiere concert took place in June 2024 with five performers and approximately 60 audience members (across five sessions), featuring a program of six musical works. This paper describes the motivations behind SVOrk, its research and development process—including designs for networking, avatar, and audience interaction—and takeaways from the premiere concert. We also present audience feedback and reflect on our experiences in curating group VR performances.},
  numpages = {9}
}

@article{nime2025_20,
  author = {Ciaran Frame and Erick Mitsak and Alon Ilsar},
  title = {You’re An Instrument!: Creating active music-making experiences through worldbuilding and storytelling},
  pages = {142--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {20},
  track = {Paper},
  doi = {10.5281/zenodo.15698815},
  url = {http://nime.org/proceedings/2025/nime2025_20.pdf},
  presentation-video = {},
  abstract = {This paper outlines the development and use of NIME supporting active audience participation within You’re an Instrument!, an immersive childrens’ theatre show that turns a planted audience member into a musical instrument. We outline the use of wireless gestural instruments in the show, exploring their novel use as hidden props and theatrical devices that help invite audience members into a fictional world. Through the creation of this fictional world, we illustrate how Digital Musical Instruments can be employed to build a narrative that may help to actively involve audience members. This paper is a call for instrument designers to consider using worldbuilding and storytelling techniques to more actively engage audience members in discovering the workings of new instruments.},
  numpages = {8}
}

@article{nime2025_21,
  author = {Nathan Carter and Jim Murphy and Mo Zareei},
  title = {Sculpting the Sound Atom: Towards Per-Grain Parameterisation in Granular Synthesiser Design},
  pages = {150--154},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {21},
  track = {Paper},
  doi = {10.5281/zenodo.15698817},
  url = {http://nime.org/proceedings/2025/nime2025_21.pdf},
  presentation-video = {},
  abstract = {This paper presents a number of custom-designed granular synthesisers built around interfacing with sound grains on an ‘atomic’ level. Developed in Max/MSP by the first author (Nathan Carter), these synthesisers explore per-grain voice parameterisation that uniquely interfaces with individual grain signal processing properties in larger granular sequences. The paper outlines how these synthesisers provided the sound materials to compose Carter’s original soundscape work ‘Matter and Void’ – conceptually inspired by ancient Epicurean physics and painterly expositions on atomism in Lucretius’ poem ‘The Nature of Things’ (c. 55 BC).},
  numpages = {5}
}

@article{nime2025_22,
  author = {Wing Hei Cheryl Hui and Patrick Hartono},
  title = {Harmonix Series: Accessible Digital Musical Instruments for Mindfulness and Creativity },
  pages = {155--160},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {22},
  track = {Paper},
  doi = {10.5281/zenodo.15698819},
  url = {http://nime.org/proceedings/2025/nime2025_22.pdf},
  presentation-video = {},
  abstract = {This paper introduces the Harmonix series, a collection of Accessible Digital Musical Instruments (ADMIs) designed to enhance mood stability and mindfulness through intuitive and interactive music-making. Recognising the barriers posed by traditional digital musical instruments, including steep learning curves, high costs, and uninspiring outputs, Harmonix prioritises affordability, portability, and user-friendly interfaces to cater to individuals with no prior musical training. The study evaluates two instruments, ZenithChimes and Equilibrio. ZenithChimes employs touch-sensitive keys mapped to meditative tones in the Aeolian mode, promoting creativity and relaxation. Equilibrio, a 3D-printed "stone stack," uses tilt gestures to modulate soundscapes, symbolising balance and harmony. Both instruments integrate calming auditory outputs and minimalist design aesthetics to create an engaging and meditative experience. A workshop-based study with 10 participants, spanning diverse backgrounds, demonstrated the instruments' accessibility and therapeutic potential. Results showed that participants found the instruments easy to use, aesthetically appealing, and suitable for mindfulness practices, with 70 percent identifying their integration into meditation or yoga sessions as beneficial. However, feedback highlighted the need for more customisation options, particularly in Equilibrio's soundscapes. By bridging art, technology, and mindfulness, Harmonix fosters creative exploration and emotional regulation, with implications for therapeutic, educational, and artistic applications. Future work will explore sustainability, inclusivity, and multi-sensory feedback to enhance the instruments’ design and impact. This study underscores the potential of ADMIs to transcend conventional music-making, offering innovative tools for well-being and self-expression.},
  numpages = {6}
}

@article{nime2025_23,
  author = {Pierrick Uro and Florent Berthaut and Thomas Pietrzak and Marcelo Wanderley},
  title = {Decoupling Physical and Virtual Spaces for New Collaboration Strategies in Co-Located Mixed Reality Instruments},
  pages = {161--166},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {23},
  track = {Paper},
  doi = {10.5281/zenodo.15698823},
  url = {http://nime.org/proceedings/2025/nime2025_23.pdf},
  presentation-video = {},
  abstract = {Collaborative co-located Mixed Reality musical instruments combine some of the expressive opportunities of 3D interaction and communication and cooperation of physical multi-user instruments. However in existing instruments, the fixed coupling between the virtual and physical environments constrains the affordances brought by Mixed Reality, such as per-musician free navigation in or multi-scale control of virtual structures.We designed gRAinyCloud, as a way to reintegrate these lost affordances to a co-located instrument.It allows for the expressive exploration of a set of sounds represented by a virtual structure of shapes placed in the physical space and shared between musicians. Above all, gRAinyCloud enables each musician to freely manipulate their own viewpoint, changing its scale, position and rotation, effectively decoupling the physical and virtual spaces, and to switch between self, other's and absolute viewpoint while playing. We describe the implementation of this decoupling of spaces and analyse its uses and implications for collective musical expression, by relying on a first-person approach.},
  numpages = {6}
}

@article{nime2025_24,
  author = {Qiaosheng Lyu and Ryo Ikeshiro},
  title = {Between Garment and Prosthesis:  The Design of an E-Textile Musical Interface},
  pages = {167--170},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {24},
  track = {Paper},
  doi = {10.5281/zenodo.15698825},
  url = {http://nime.org/proceedings/2025/nime2025_24.pdf},
  presentation-video = {},
  abstract = {This paper presents Noisy Flesh, an e-textile musical interface designed to control sound through body movement in performance. Unlike traditional wearable interfaces that function as garments, this work reimagines the textile interface as a prosthetic extension that augments the performer’s body. The paper discusses both the design of the interface and its sonification method, emphasising how the flexibility of e-textiles can transform bodily movement and shape interactive experiences. This work explores the potential of e-textile interfaces to challenge conventional notions of wearability and embodiment in performance.},
  numpages = {4}
}

@article{nime2025_25,
  author = {Austin Oting Har and Kurt Mikolajczyk},
  title = {Creating a White Noise Instrument for Collaborative Improvisation},
  pages = {171--174},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {25},
  track = {Paper},
  doi = {10.5281/zenodo.15698827},
  url = {http://nime.org/proceedings/2025/nime2025_25.pdf},
  presentation-video = {https://vimeo.com/1021218658},
  abstract = {This paper introduces shiki, a virtual instrument developed for performing Renga for White Noise, an interdisciplinary project that transmediates Japanese renga poetry principles into a framework for collaborative improvisation with human and AI agents. We discuss two areas: (1) our transmediation of renga’s structuring principles into shiki’s design (2) the technical aspects behind the AI agent’s performance with shiki. Through its interdisciplinary and intercultural entanglements with renga, transmediation as a method can illuminate new perspectives on the design and performance of NIMEs.},
  numpages = {4}
}

@article{nime2025_26,
  author = {Iurii Kuzmin and Omar Al Kanawati and Raul Masu},
  title = {Collaboration and Recursion: reflections on calligraphy and feedback},
  pages = {175--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {26},
  track = {Paper},
  doi = {10.5281/zenodo.15698829},
  url = {http://nime.org/proceedings/2025/nime2025_26.pdf},
  presentation-video = {https://vimeo.com/1037811492},
  abstract = {This paper offers reflections on the collaboration between a sound artist and an artist specializing in Chinese calligraphy, which resulted in the creation of a sound installation combining contemporary Chinese calligraphy with electroacoustic feedback. Adopting the “reflection-on-action” approach, the authors engaged in an in-depth discussion, revisiting the details of the creative process based on the extensive documentation compiled in the form of a visual diary. The paper highlights three orders of recursivity that are either physically present in the work (electroacoustic feedback), defined the creative process (collaboration) or served as an analytical tool (ecology) to discuss the dynamics of collaboration and cultural influences in NIME practice.},
  numpages = {9}
}

@article{nime2025_27,
  author = {Maham Riaz and Ioannis Theodoridis and Çağrı Erdem and Alexander Refsum Jensenius},
  title = {VentHackz: Exploring the Musicality of Ventilation Systems},
  pages = {184--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {27},
  track = {Paper},
  doi = {10.5281/zenodo.15698831},
  url = {http://nime.org/proceedings/2025/nime2025_27.pdf},
  presentation-video = {},
  abstract = {Ventilation systems can be seen as huge examples of interfaces for musical expression, with the potential of merging sound, space, and human interaction. This paper explores conceptual similarities between ventilation systems and wind instruments and explores approaches to “hacking” ventilation systems with components that produce and modify sound. These systems enable the creation of unique sonic and visual experiences by manipulating airflow and making mechanical adjustments. Users can treat ventilation systems as musical interfaces by altering shape, material, and texture or augmenting vents. We call for heightened attention to the sound-making properties of ventilation systems and call for action (#VentHackz) to playfully improve the soundscapes of our indoor environments.},
  numpages = {8}
}

@article{nime2025_28,
  author = {Margaret Needham},
  title = {Hacking Sound, Hacking History: Patricia Cadavid and the Electronic_Khipu_},
  pages = {192--196},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {28},
  track = {Paper},
  doi = {10.5281/zenodo.15698833},
  url = {http://nime.org/proceedings/2025/nime2025_28.pdf},
  presentation-video = {},
  abstract = {To better understand researcher and artist Patricia Cadavid Hinojosa’s instrument the Electronic_Khipu_, we must define the project as an instance of hacking. Cadavid deconstructs colonial understandings of the Andean device known as the khipu, pulling apart the academic view of khipus as artifacts to be deciphered, the strict delineation between administrative and ritualistic uses of the khipu, and the separation of the oral tradition from the object. Through deliberate design choices and musical expression in performance, Cadavid emphasizes the inextricability of coding, art, and ritual by creating a tactile device that re-tells history and challenges the false oppositional binary between Indigeneity and technology. Understanding this project of digital lutherie as an act of creation through hacking - specifically as the deconstruction and reconfigurement of artistic and historical components, utilizing scholars Astrida Neimani's and Vít Bohal's definitions - allows us to appreciate its power.},
  numpages = {5}
}

@article{nime2025_29,
  author = {Charalampos Saitis and Courtney N. Reed and Ashley Laurent Noel-Hirst and Giacomo Lepri and Andrew McPherson},
  title = {(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design},
  pages = {197--206},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {29},
  track = {Paper},
  doi = {10.5281/zenodo.15698835},
  url = {http://nime.org/proceedings/2025/nime2025_29.pdf},
  presentation-video = {},
  abstract = {Timbre, pitch, and timing are often relevant in digital musical instrument (DMI) design. Compared with the latter two, timbre is neither easy to define nor discretise when negotiating audio representations and gesture-sound mappings. We conduct a corpus assisted discourse analysis of "timbre" in all NIME proceedings to date (2001-2024). Combining this with a detailed review of 18 timbre-focused papers at NIME, we examine how definitions of timbre and timbre interaction methods are constructed through, for instance, Wessel's numerical timbre control space, synthesis tools and programming languages, machine learning and AI approaches, and other trends in digital lutherie practices. While acknowledging the practical utility of technical constructions of timbre in NIME (and other digital music research communities), we contribute discussion on the entanglement of technology and aesthetics in instrument design, which constitutes what "timbre" becomes in NIME research and reflect on the tension between technoscientific and constructivist understandings of timbre: how DMIs and musical practices have been reconstituted around particular timbral values operationalised in NIME. In response, we propose ways that the NIME community can embrace more critical approaches and awareness to how our methods and tools shape and co-create our notions of timbre, as well as other musical concepts, connecting more openly with diverse types of sonic phenomena.},
  numpages = {10}
}

@article{nime2025_30,
  author = {Seth Thorn and Anani Vasquez and Corey Reutlinger and Margarita Pivovarova and Mirka Koro},
  title = {Towards Neurodiverse Sensemaking: Pluralizing Agency in Wearable Music and Participatory Workshopping},
  pages = {207--217},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {30},
  track = {Paper},
  doi = {10.5281/zenodo.15698837},
  url = {http://nime.org/proceedings/2025/nime2025_30.pdf},
  presentation-video = {},
  abstract = {We, a team of teachers and researchers, share examples of collectively playable instruments that challenge normative assumptions about intention and agency in digital musical instruments. These instruments enliven neurodiverse sensemaking in participatory design and STEAM learning. Through a multiyear research-practice partnership (RPP), we collaborated with teaching fellows to co-design a curriculum for neurodiverse middle school students that activates computational thinking (CT). This collaboration led to a web-based, quasi-modular interface connected to wearable music sensors. We situate our work within the growing literature on participatory design of collaborative accessible digital musical instruments (CADMIs). We describe how our co-design methods address the complex demands of ecosystemic thinking, sensitive to the varied entanglements that complicate traditional human-computer interaction (HCI) design and evaluation methods. Our pedagogical and methodological approach diverges from deficit-focused strategies that aim to develop neurotypical communication skills in neurodivergent individuals. Instead, we promote cross-neurotype collaboration without presuming a single mode of "correct" communication. Furthermore, we surface the potential of CADMIs by linking this notion to a pluralization of agency that extends beyond one-to-one body-sensor relationships. We develop accessible instruments within neurodiversity and autism contexts, avoiding reification of mindbody relations and recognizing them as dynamic, field-like, and embedded in facilitative relations for these communities.},
  numpages = {11}
}

@article{nime2025_31,
  author = {Andrew McMillan and Fabio Morreale},
  title = {The Slide-A-Phone: a Tactile Accessible Musical Instrument},
  pages = {218--223},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {31},
  track = {Paper},
  doi = {10.5281/zenodo.15698839},
  url = {http://nime.org/proceedings/2025/nime2025_31.pdf},
  presentation-video = {},
  abstract = {This paper details the design and development of the Slide-A-Phone, an Accessible Musical Instrument (AMI). The first author's spinal cord injury in 2004 hindered their ability to play traditional instruments, which motivated the development of the Slide-A-Phone. The Slide-A-Phone utilises tactile interfaces coupled with analogue and digital sensors to replicate the playability and expressive control of a saxophone, the instrument the first author used to play before the incident. The design process incorporated phenomenological perspectives and a blend of design methodologies, with the specific goal of fostering a robust musician-instrument relationship. We report insights into how personal experiences shape design and functionality, and the importance of accessible instruments in enabling creative practice and performance for individuals with limited functionality. We also describe the design and technical implementation of the Slide-A-Phone evaluate the instrument's effectiveness and reflect on its potential to enhance musical engagement, social connections, cultural participation, and professional development.},
  numpages = {6}
}

@article{nime2025_32,
  author = {Isaac Clarke and Francesco Ardan  Dal Rí and Raul Masu},
  title = {Longevity of Deep Generative Models in NIME: Challenges and Practices for Reactivation},
  pages = {224--230},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {32},
  track = {Paper},
  doi = {10.5281/zenodo.15698841},
  url = {http://nime.org/proceedings/2025/nime2025_32.pdf},
  presentation-video = {},
  abstract = {In this paper, we present an investigation into the longevity, reproducibility, and documentation quality of Deep Generative Models (DGMs) introduced in previous editions of the NIME conference. We begin by assessing whether DGM presented at NIME are still available in terms of code, data, and weights; afterward, we present the recreation process of seven unavailable models, to the end of investigation of the issues related to longevity and documentation. We examine the availability and completeness of resources needed to recreate DGM models, and discuss specific challenges encountered during such recreation. Drawing from  this experience, we highlight key obstacles that hinder the long-term viability and reuse of DGMs in the NIME context, and propose guidelines to improve their documentation and future reuse within the community.},
  numpages = {7}
}

@article{nime2025_33,
  author = {David Dalmazzo and Ken Déguernel and Bob Sturm},
  title = {A Computer Application to Explore 53-Tone Equal Temperament Harmonies Through Modal Interchange},
  pages = {231--237},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {33},
  track = {Paper},
  doi = {10.5281/zenodo.15698843},
  url = {http://nime.org/proceedings/2025/nime2025_33.pdf},
  presentation-video = {},
  abstract = {We present a novel computer application for real-time exploration of microtonal harmonies through intuitive visualization and integrated MIDI controllers, bridging theoretical concepts and practical musical applications. We extend modern harmonic principles from 12-tone equal temperament (12-TET), incorporating interval distinctions from 31-TET (subminor, neutral, supermajor), and further expanding into detailed harmonic possibilities of 53-tone equal temperament (53-TET). Our application leverages modal interchange and parallel chord substitutions, offering intuitive navigation through microtonal harmonic trajectories. The implementation utilizes MIDI Polyphonic Expression (MPE) via our custom MaxForLive application, The Bridge, ensuring precise microtonal control and compatibility with digital audio workstations (Ableton Live 11/12). The system includes real-time visualization, interactive chord manipulation, and a comprehensive Scale Editor for harmonic experimentation. Through practical examples and theoretical analysis, we demonstrate how this approach reveals new harmonic possibilities while maintaining connections to established modal frameworks. This research contributes to the growing microtonal music field by providing theoretical foundations and practical tools that incorporate extended tuning systems into contemporary musical practice.},
  numpages = {7}
}

@article{nime2025_34,
  author = {Xinran Chen and Iurii Kuzmin and Mela Bettega and Raul Masu},
  title = {Embodying Sustainability: Paving Opportunities for NIME Research},
  pages = {238--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {34},
  track = {Paper},
  doi = {10.5281/zenodo.15698845},
  url = {http://nime.org/proceedings/2025/nime2025_34.pdf},
  presentation-video = {},
  abstract = {While sustainability has gained attention in NIME research, primarily focusing on instrument longevity and durability, the role musical interfaces play in promoting environmental awareness remains unexplored. This paper investigates how musical interfaces can foster sustainability through designing embodied experiences. We present a literature review that examines the integration of sustainability and embodiment in sonic interaction, in which we synthesize practical points on how sound, materials, data, and interactions can aesthetically support embodying sustainability. We further explore these concepts through a design case study. Our findings suggest that embodied musical experiences offer unique opportunities to cultivate environmental consciousness, contributing to a deeper understanding of sustainable musical interfaces relying on artistic expressions.},
  numpages = {12}
}

@article{nime2025_35,
  author = {João  Coimbra and Luís Aly and Henrique  Portovedo and Sara Carvalho and Tiago  Bolaños},
  title = {EMMA: Enhancing Real-Time Musical Expression through Electromyographic Control},
  pages = {250--254},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {35},
  track = {Paper},
  doi = {10.5281/zenodo.15698847},
  url = {http://nime.org/proceedings/2025/nime2025_35.pdf},
  presentation-video = {https://youtu.be/Hh3rGWhMHI0},
  abstract = {This paper presents the Electromyographic Music Avatar (EMMA), a digital musical instrument (DMI) designed to enhance real-time sound-based composition through gestural control. Developed as part of a doctoral research project, EMMA combines electromyography (EMG) and motion sensors to capture nuanced finger, hand, and arm movements, treating each finger as an independent instrument. This approach bridges embodied performance with computational sound generation, enabling expressive and intuitive interaction. The system features a glove-based design with EMG sensors for each finger and motion detection for the wrist and arm, allowing seamless control of musical parameters. By addressing key challenges in DMI design, such as action-sound immediacy and performer-instrument dynamics, EMMA contributes to developing expressive and adaptable tools for contemporary music-making.},
  numpages = {5}
}

@article{nime2025_36,
  author = {Victor Shepardson and Halla Steinunn Stefánsdóttir and Thor Magnusson},
  title = {Evolving the Living Looper: Artistic Research, Online Learning, and Tentacle Pendula},
  pages = {255--260},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {36},
  track = {Paper},
  doi = {10.5281/zenodo.15698851},
  url = {http://nime.org/proceedings/2025/nime2025_36.pdf},
  presentation-video = {},
  abstract = {The Living Looper is a neural audio synthesis looper system for live input. It combines online learning with pre-trained neural network models to resynthesize incoming audio into "living loops" that transform over time. This paper describes new features of the Living Looper and musician perspectives on its use. A new graphical interface facilitates use of the instrument by non-programmers and visualizes each loop to aid performers in tracking which loop is making which sound.  We also describe a new living loop algorithm including incremental learning with partial least squares regression. Finally, we report on an artistic project using the Looper and lessons learned, resulting in an increased importance of training data and a developing sense of relationality.},
  numpages = {6}
}

@article{nime2025_37,
  author = {Brittney Allen and Andrew Mcpherson and Alexandria Smith and Jason Freeman},
  title = {Making the Immaterial Material: A Diffractive Approach Toward a Politics of Material Culture Within NIME},
  pages = {261--267},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {37},
  track = {Paper},
  doi = {10.5281/zenodo.15698853},
  url = {http://nime.org/proceedings/2025/nime2025_37.pdf},
  presentation-video = {https://vimeo.com/942352610},
  abstract = {Traditional Human-Computer Interaction has often been critiqued for its ostensibly opaque position on ethical, ontological, and epistemological concerns, particularly in relation to completed design artifacts. More recently, similar criticisms have been directed at the New Interfaces for Musical Expression (NIME) community for its relative silence on contemporary political issues. However, it is possible that an implicit ethics of material culture is already embedded within NIME discourse — one that could be critically examined and potentially mobilized as a foundation for a more explicitly political ethics.Inspired by feminist discourse, namely Karen Barad's theory of agential realism, and contextualized through Bruno Latour's remarks regarding the ethics of design, this paper explores the possibilities of entanglement in DMI design. We begin with a discussion of diffraction and entanglement followed by a brief overview of values-oriented and "world-building" theoretical models and methodologies of design research. We continue with our generative "DMI-as-apparatus" approach to diffractive methodology and conclude with a case study BRAIDS_, a digital music instrument based upon the Black American cultural practice of hair braiding, that examines critical design decisions that are otherwise deemed invisible by traditional methods of scientific inquiry.},
  numpages = {7}
}

@article{nime2025_38,
  author = {Adam Schmidt and Jeffrey Snyder and Gian Torrano Jacobs and Joseph Gascho and Joyce Chen and Andrew McPherson},
  title = {The Sparksichord: Practical Implementation of a Lorentz Force Electromagnetic Actuation and Feedback System},
  pages = {268--279},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {38},
  track = {Paper},
  doi = {10.5281/zenodo.15698857},
  url = {http://nime.org/proceedings/2025/nime2025_38.pdf},
  presentation-video = {},
  abstract = {In line with a sustained community interest in electromagnetic actuation of musical instruments, we describe practical considerations for Lorentz Force actuation in conductive strings, exemplified by the Sparksichord – an augmented harpsichord that uses Lorentz Force actuation, optical feedback, and analog circuitry to sustain vibrations of its brass strings. Electromagnetically-actuated and feedback instruments have grown increasingly popular in NIME, though most systems rely on the use of solenoid-style electromagnetic coils. By running current through the string itself, Lorentz Force actuation offers an alternate arrangement of magnets and wire that can afford new modes of interaction, a broader frequency response, and cheaper implementation.  We aim to empower practitioners with a toolbox for designing and building actuated instruments of this style and describe our specific implementation for this instrument.},
  numpages = {12}
}

@article{nime2025_39,
  author = {Linnea Kirby and Christiana Rose and Jeremy Cooperstock and Marcelo Wanderley},
  title = {Creating a Real-Time Responsive Handbalancing Interface with HAND★CS},
  pages = {280--286},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {39},
  track = {Paper},
  doi = {10.5281/zenodo.15699614},
  url = {http://nime.org/proceedings/2025/nime2025_39.pdf},
  presentation-video = {https://youtu.be/ePlOPTdzB7A},
  abstract = {This paper introduces HAND★CS, a new interface for interdisciplinary expression for music, movement, and light. Our interface augments a pedagogical interface for hand-balancing, Haptics-Assisted iNversions Device (HAND), and transforms it into one for artistic expression. It draws upon Licklider’s concept of man-computer symbiosis, specifically the commensalism form of symbiosis. HAND★CS strives to embody a performance apparatus and system with symbiotic connectivity between performer and interface. This paper discusses the inspiration and background for such a system pulling from the fields of human-computer interaction (HCI), music technology and new interfaces for musical expression (NIME), and circus arts. In addition, it defines the design and implementation, evaluation of the prototype of HAND★CS, and future work.},
  numpages = {7}
}

@article{nime2025_40,
  author = {John Lettang and August Black},
  title = {Aquapella- Gestural Interactions with Liquid Turbulence as Musical Expression},
  pages = {287--295},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {40},
  track = {Paper},
  doi = {10.5281/zenodo.15699626},
  url = {http://nime.org/proceedings/2025/nime2025_40.pdf},
  presentation-video = {https://youtu.be/KEIr1re4Gls},
  abstract = {The Aquapella is a hand-held gestural instrument for exploring the unique relationship of liquid turbulence and musical expression. The device consists of eight conductive water-level sensors in a custom 3D printed container. As a musician moves the device, it generates a chaotic flow of water within the container and translates the motion into real-time midi signals for audio-visual interpretation. In our initial performances and tests with the Aquapella, we have focused on turning the flowing characteristics of the device into ambient and glitch soundscapes that move between noise and harmonics. We present the primary findings in developing the Aquapella including related works, description of the project development, and ideas for future iterations. },
  numpages = {9}
}

@article{nime2025_41,
  author = {Sandy Ma and Charles Martin},
  title = {Touching Wires: tactility and a quilted musical interface for human-AI musical co-creation},
  pages = {296--303},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {41},
  track = {Paper},
  doi = {10.5281/zenodo.15698865},
  url = {http://nime.org/proceedings/2025/nime2025_41.pdf},
  presentation-video = {},
  abstract = {Interactions with computers have traditionally been mediated by rigid materials, but as technology evolves, there is increasing potential to rethink these relationships. This paper explores how a soft, textile-based interface can reshape human-AI interaction, particularly in musical co-creation. We introduce a textile-based human-AI system used both for musical performance and public interaction. This system enables embodied, tactile engagement with an AI agent, offering users a more unique and participatory experience in human-AI musical co-creation. We aim to examine the potential for soft materiality to mediate more dynamic human-AI interactions. Our findings reveal that users’ choices when interacting with novel systems are informed by their expectations and biases, that embodied learning is built iteratively on layered multi-sensory experiences, and that there is a desire for familiarity and understanding when interacting with AI systems. We found that the materiality of our textile human-AI interfaces influenced how users choose to interact, and that users sought clarity in the AI’s role in collaborative creation. This work contributes to our understanding of how entanglement, embodiment, and materiality impact our relationships in human-AI collaborations.},
  numpages = {8}
}

@article{nime2025_42,
  author = {Sally Jane Norman and Paul Stapleton and John Bowers},
  title = {NIME: A Mis-User's Manual},
  pages = {304--311},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {42},
  track = {Paper},
  doi = {10.5281/zenodo.15698867},
  url = {http://nime.org/proceedings/2025/nime2025_42.pdf},
  presentation-video = {},
  abstract = {Ever since the initiating workshop at the 2001 ACM CHI'01 Conference, annual New Interfaces for Musical Expression conferences have seen a proliferation of work featuring different forms of music, research values, philosophical, ethical and political standpoints. The 2025 ‘Entangled’ theme celebrates this diversity of creative, technical, and social ‘intelligencings’ (Thrift [68, p153-154]). It is precisely the non- or pluri-paradigmatic character of NIME that is its strength. Drawing on Maria Lugones [41], we characterise NIME less as an entangled weave—where threads maintain their separate yet assembled and interconnected character—than as a ‘curdling’ where relationships are more complex, varied, mutually interrupting and shaping, indeterminate and unknown without careful dialogue. We do not consider it appropriate to offer unifying frameworks or mappings with often hidden authoritarian implications. Rather, following Rancière [5], we prefer a radically democratic dissensus and, following Lugones, a spirit of ‘festive resistance’ where we poke at the limits of our inherited metaphors to undermine attempts to provide a fixed orderliness, (re)framing topics to kickstart exchange on new fertile grounds for collaboration. Multiple kinds and collisions of agency, and the lively openness of what some might deem ‘failure’ are prioritised over the often inhibiting closure and certainty of ‘success’ [e.g. 10, 11, 12, 40]. Our topics include: multiple ways of making as a means of maximising exposure to possible failure; shifting from interfaces to interfacing to create arenas for action rather than tools for purposes; foregrounding risk, inefficiency and forgetting; formulating improvisation as knowing-when and composing-the-now; performance practice, settings and contingencies; alternative resourcings/reframings for research; a wild spirit of tactical oppositionalism, dynamic uncompromise, and existential pluralism, to embrace the independence of divergent voices. },
  numpages = {8}
}

@article{nime2025_43,
  author = {Darlene Castro},
  title = {The Shadow Harvester: Sonifying the Body Through Light},
  pages = {312--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {43},
  track = {Paper},
  doi = {10.5281/zenodo.15698869},
  url = {http://nime.org/proceedings/2025/nime2025_43.pdf},
  presentation-video = {},
  abstract = {This paper explores the use of a violinist’s shadow as input for a NIME prototype called the Shadow Harvester. For centuries, shadows have captivated humanity’s imagination, and this project follows many artists, philosophers, and researchers equally captivated by the potential of shadows and silhouettes. This interface consists of a semi-translucent screen embedded with light-detecting sensors. These sensors register the movement of the violinist’s shadow and produce data that can be mapped to generate, trigger, or process sound in Max/MSP. The Shadow Harvester turns a human shadow into a real-time, life-size avatar, splitting the attention of the violinist between their shadow self and carnal self. They are ensnared in a web of sensors that require the same attention as the visceral joints in their body because any movement carries sonic repercussions through either their physical body playing the violin or their shadow body “playing” this interface. The Shadow Harvester creates a highly entangled feedback loop between the violinist, centuries of violin performance practice, and composition. As such, it carries the potential to encourage new ways of incorporating movement into the folds composition, notation, and performance.},
  numpages = {7}
}

@article{nime2025_44,
  author = {Derek Holzer and Henrik Frisk and André Holzapfel},
  title = {The Imperfect Copy: Role Playing Reenactments of Historical Electronic Sound Instruments},
  pages = {319--327},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {44},
  track = {Paper},
  doi = {10.5281/zenodo.15698871},
  url = {http://nime.org/proceedings/2025/nime2025_44.pdf},
  presentation-video = {},
  abstract = {Reenactment forms a unique method of exploring the social, political, historical, conceptual, contextual and other aspects of electronic sound instruments from the past, without necessarily reproducing the instrument’s physical, functional or sonic characteristics. Rather, the reenactment presents a novel instrument, realized through contemporary means, reflecting on contemporary concerns and within a contemporary context. We find reenactment complementary to conservation, maintenance, reconstruction and emulation in working with archival and museum objects. Our paper presents an analytic framework developed for use in workshop scenarios. This series of questions helps determine and understand which aspects of an instrument might be reenacted. To illustrate the process in action, we describe an example workshop wherein participants use methods of media archaeology, design fiction and role playing to imagine and reenact new features, affordances, contexts and applications of electronic instruments from a museum exhibition.},
  numpages = {9}
}

@article{nime2025_45,
  author = {Sam Trolland and Alon Ilsar and Jon McCormack},
  title = {Visually-Led Design for Gestural Audiovisual Instruments},
  pages = {328--336},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {45},
  track = {Paper},
  doi = {10.5281/zenodo.15699633},
  url = {http://nime.org/proceedings/2025/nime2025_45.pdf},
  presentation-video = {},
  abstract = {In this paper we present our visually-led design method for creating gestural mappings in a new audiovisual percussion work titled Cymbalism. Unlike most audiovisual works, Cymbalism was inspired by the creation of a series of interactive visual scenes that respond to the performer’s real-time movements. In leading with the visual interaction, we discuss how this approach fostered a union between the physical, audio and visual elements of the work, creating a performance where the visualisation is not simply a feedback mechanism but fundamental in inspiring compositional concepts and new ways of interacting with sound. Through practice-based research, we use the insights gained through creative development and performance outcomes to guide the continued evolution of an established wearable gestural DMI.},
  numpages = {9}
}

@article{nime2025_46,
  author = {Yue Wang and Meiling Wu},
  title = {Exploring Musical Creation Through Brain-Body Digital Musical Instruments},
  pages = {337--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {46},
  track = {Paper},
  doi = {10.5281/zenodo.15698878},
  url = {http://nime.org/proceedings/2025/nime2025_46.pdf},
  presentation-video = {},
  abstract = {Brain-Body Digital Musical Instruments (BBDMI) merge physiological signals with real-time sound processing, enabling performers to use Electromyographic (EMG) data for musical expression. This study explores the creative and technical potential of BBDMI, focusing on signal acquisition, calibration, and mapping for use in composition and performance. Demonstrations with flute, piano showcase its ability to enhance expressivity through gestural control. While key advancements include improved signal stability and refined mapping, challenges such as connectivity issues and notation limitations remain. This research highlights BBDMI’s promise as a transformative tool in contemporary music.},
  numpages = {6}
}

@article{nime2025_47,
  author = {Tace McNamara and Jon McCormack and Maria Teresa Llano},
  title = {Mixer Metaphors: audio interfaces for non-musical applications},
  pages = {343--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {47},
  track = {Paper},
  doi = {10.5281/zenodo.15699645},
  url = {http://nime.org/proceedings/2025/nime2025_47.pdf},
  presentation-video = {},
  abstract = {The NIME conference traditionally focuses on interfaces for music and musical expression. In this paper we reverse this tradition to ask, can interfaces developed for music be successfully appropriated to non-musical applications? To help answer this question we designed and developed a new device, which uses interface metaphors borrowed from analogue synthesisers and audio mixing to physically control the intangible aspects of a Large Language Model. We compared two versions of the device, with and without the audio-inspired augmentations, with a group of artists who used each version over a one week period. Our results show that the use of audio-like controls afforded more immediate, direct and embodied control over the LLM, allowing users to creatively experiment and play with the device over it's non-mixer counterpart. Our project demonstrates how cross-sensory metaphors can support creative thinking and embodied practice when designing new technological interfaces. },
  numpages = {9}
}

@article{nime2025_48,
  author = {Maya Caren},
  title = {ViVo: Piano Learning Through Visualizing Vocalizations on a Lighted Keyboard},
  pages = {352--354},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {48},
  track = {Paper},
  doi = {10.5281/zenodo.15698882},
  url = {http://nime.org/proceedings/2025/nime2025_48.pdf},
  presentation-video = {},
  abstract = {Vocalization and visualization are recognized as two powerful methods for internalizing music that are effective with beginner and skilled musicians alike. Despite the well-researched benefits of each practice, integrated visualization of vocalizations for instrument learning has seen little attention in the music technology community. This paper introduces the design and implementation of ViVo, a piano learning tool that connects the embodied sense of pitch offered by vocalization with the spatial intuition provided by in situ visualization. ViVo offers two modes: a real-time mode that hears live user vocalizations to concurrently illuminate the corresponding piano keys, and a practice mode that visualizes recorded vocalizations for repeated practice. By providing an integrated system to foster and visualize vocalizations, ViVo aims to leverage the noted benefits of both practices to make learning piano more effective, intuitive, and engaging.},
  numpages = {3}
}

@article{nime2025_49,
  author = {Michaella Moon and Dale Carnegie and Jim Murphy},
  title = {Audiation Development System for Gugak's Fluid Musical Parameters Utilizing Audio Feedback Stimuli},
  pages = {355--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {49},
  track = {Paper},
  doi = {10.5281/zenodo.15698886},
  url = {http://nime.org/proceedings/2025/nime2025_49.pdf},
  presentation-video = {},
  abstract = {Gugak, the traditional music of Korea, is defined by its distinctive musical characteristics, including flexible tuning, non-metric rhythms, and intricate ornamentation. These unique features, while artistically rich, pose significant challenges for novice learners, particularly when approached through conventional, textbook-based methods. To bridge this gap, we introduce the Audiation Development System for Gugak, an interactive platform that leverages algorithmic analysis to support both learning and teaching. Central to this system is the development of signal processing functions for rhythmic guidance, pitch detection, and additive harmonic synthesis—algorithms specifically designed to capture the expressive nuances of Gugak. These functions generate real-time auditory and visual feedback, providing a responsive learning environment aligned with the updated Korean music curriculum. The system not only enables student-centred exploration of Gugak's fluid structures but also supports educators through dynamic, visualized feedback tools. Beyond its technical foundation, this research sets the stage for future development of user interfaces and investigates the educational efficacy of computer-driven learning compared to traditional methods. By integrating music technology and pedagogy, this work contributes to both the accessibility and sustainability of Korea’s musical heritage.},
  numpages = {9}
}

@article{nime2025_50,
  author = {Rebecca Abraham},
  title = {Tiny Touch Instruments: Composing for Collaborative Mobile Performance},
  pages = {364--368},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {50},
  track = {Paper},
  doi = {10.5281/zenodo.15698888},
  url = {http://nime.org/proceedings/2025/nime2025_50.pdf},
  presentation-video = {},
  abstract = {This paper explores Tiny Touch Instruments (TTIs), a set of mobile instruments, and how they facilitate collaborative, unrehearsed music-making. Through the composition and performance of two pieces, Skating and Skipping, this work investigates how multimodal notation and instrument design can shape performer experience. Performances were documented through participant observation, interviews, and a survey, revealing key themes such as the role of notation in guiding improvisation, the balance between agency and unpredictability in digital instruments, and the recontextualization of mobile devices as musical tools.},
  numpages = {5}
}

@article{nime2025_51,
  author = {Taisei Goto and Kazuhiro Jo},
  title = {Ongoing Production of a “Growing Instrument” Using Mycelium-Based Materials},
  pages = {369--372},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {51},
  track = {Paper},
  doi = {10.5281/zenodo.15698892},
  url = {http://nime.org/proceedings/2025/nime2025_51.pdf},
  presentation-video = {},
  abstract = {This study explores “growing instruments” made from fungal mycelium, highlighting their natural unpredictability and role as musical instruments. Mycelium’s growth and interactions with the environment create unique features not found in traditional instruments. Positioned within non-human-centric approaches in design and art, the research emphasizes the mutual creation and interconnectedness of diverse actors. By shaping mycelium into tubes and adding recorder heads, playable flute-like instruments were created. However, their condition and playability were highly influenced by environmental factors such as temperature and humidity. The study emphasizes embracing uncertainty and suggests that these imperfections can offer new insights into musical instrument design.},
  numpages = {4}
}

@article{nime2025_52,
  author = {Antonis Christou},
  title = {LIMITER: A Gamified Interface for Harnessing Just Intonation Systems},
  pages = {373--378},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {52},
  track = {Paper},
  doi = {10.5281/zenodo.15698896},
  url = {http://nime.org/proceedings/2025/nime2025_52.pdf},
  presentation-video = {https://vimeo.com/1079127921},
  abstract = { This paper introduces LIMITER, a gamified digital musical instrument for harnessing and performing microtonal and justly intonated sounds. While microtonality in Western music remains a niche and esoteric system that can be difficult both to conceptualize and to perform with, LIMITER presents a novel, easy to pickup interface that utilizes color, geometric transformations, and game-like controls to create a simpler inlet into utilizing these sounds as a means of expression. We report on the background of the development of LIMITER, as well as explain the underlying musical and engineering systems that enable its function. Additionally, we offer a discussion and preliminary evaluation of the creativity-enhancing effects of the interface. },
  numpages = {6}
}

@article{nime2025_53,
  author = {Brian Lindgren},
  title = {The EV: An Iterative Journey in Digital-Acoustic String Instrument Augmentation},
  pages = {379--388},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {53},
  track = {Paper},
  doi = {10.5281/zenodo.15698898},
  url = {http://nime.org/proceedings/2025/nime2025_53.pdf},
  presentation-video = {},
  abstract = {Numerous experiments in bowed string augmentation have been undertaken, each reflecting the values and interests of the builder. The EV takes a unique approach, with the convolution of a synthesized and acoustic string signal at the foundation of its design. Through an iterative hardware and software development process, three versions of the instrument have been created, each building toward the goal of a robust compositional and performative platform for exploring the shared boundary of electronic and acoustic sound. Spatialization and physical modeling algorithms have furthered the instrument’s engagement with the interaction between physical and virtual acoustics. This paper examines the iterative design process behind the instrument and its relationship between digital augmentation and acoustic resonance.},
  numpages = {10}
}

@article{nime2025_54,
  author = {Misagh Azimi and Mo H. Zareei},
  title = {Live Improvisation with Fine-Tuned Generative AI: A Musical Metacreation Approach},
  pages = {389--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {54},
  track = {Paper},
  doi = {10.5281/zenodo.15698902},
  url = {http://nime.org/proceedings/2025/nime2025_54.pdf},
  presentation-video = {},
  abstract = {This paper presents a pipeline to integrate a fine-tuned open-source text-to-audio latent diffusion model into a workflow with Ableton Live for the improvisation of contemporary electronic music. The system generates audio fragments based on text prompts provided in real time by the performer, enabling dynamic interaction. Guided by Musical Metacreation as a framework, this case study reframes generative AI as a co-creative agent rather than a mere style imitator. By fine-tuning Stable Audio Open on a dataset of the first author’s compositions and field recordings, this approach demonstrates the ethical and practical benefits of open-source solutions. Beyond showcasing the model’s creative potential, this study highlights the model’s significant challenges and the need for democratized tools with real-world applications.},
  numpages = {5}
}

@article{nime2025_55,
  author = {Jason Smith and Jason Freeman},
  title = {Adaptation and Perceived Creative Autonomy in Gesture-Controlled Interactive Music},
  pages = {394--403},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {55},
  track = {Paper},
  doi = {10.5281/zenodo.15698904},
  url = {http://nime.org/proceedings/2025/nime2025_55.pdf},
  presentation-video = {},
  abstract = {With the variety and rapid pace of developments in Artificial Intelligence (AI), musicians can face difficulty when working with AI-based interfaces for musical expression as understanding and adaptation to AI behaviors takes time. In this paper, we explore the use of AI in an interactive music system designed to adapt to users as they learn to perform with it. We present GestAlt, an AI-based interactive music system that collaborates with a performer by analyzing their gestures and motion to generate audio changes. It uses computer vision, online machine learning, and reinforcement learning to adapt to a user's hand motion patterns and allow a user to communicate their musical goals to the system. It communicates its decision-making to the user through visualizations and its musical output. We conducted a study in which five musicians performed using this software over multiple sessions. Participants discussed how their preferences for the system’s behavior were influenced by their experiences as musicians, how adaptive reinforcement learning affected their expectations for the system’s autonomy, and how their perceptions of the system as a creatively autonomous, collaborative partner evolved as they learned how to perform with the system.},
  numpages = {10}
}

@article{nime2025_56,
  author = {Joseph Burgess and Toby Gifford},
  title = {Threading the Sound: The Carpet Tufting Gun as an Electroacoustic Performance Interface},
  pages = {404--405},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {56},
  track = {Paper},
  doi = {10.5281/zenodo.15698906},
  url = {http://nime.org/proceedings/2025/nime2025_56.pdf},
  presentation-video = {},
  abstract = {This paper explores the carpet tufting gun as a novel electroacoustic performance interface. Leveraging its distinctive acoustic properties and electromechanical kinetics, the tufting gun presents a range of physical affordances that can be creatively repurposed for musical expression. While prior intersections between textile production processes and musical practices exist, the tufting gun remains largely underexplored as a tool for structured musical composition. This work reimagines the gun’s mechanical gestures and performative affordances, transforming its utilitarian motions into expressive sonic gestures. By positioning the tufting gun as both an acoustic source and an interactive performance interface, this project works at the intersection of fibre craft and experimental sound art, where both historico-cultural context of textile making, and the ergonomics of the gun, present musical affordances.},
  numpages = {2}
}

@article{nime2025_57,
  author = {Travis West and Ninad Puranik and Gary Scavone and Marcelo Wanderley},
  title = {Towards the Continuous Harmonium: Replicating the Continuous Keyboard},
  pages = {406--409},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {57},
  track = {Paper},
  doi = {10.5281/zenodo.15699652},
  url = {http://nime.org/proceedings/2025/nime2025_57.pdf},
  presentation-video = {https://youtu.be/iFCblP3tDxk},
  abstract = {In our effort to develop an augmented harmonium to enable the performance of continuous pitch ornamentation while preserving typical harmonium gestures, we have replicated the continuous keyboard presented by McPherson et al. in prior work. We present 1) our adaptations to the design of the sensing system, 2) our preliminary novel mapping design, and 3) a report on our replication process.},
  numpages = {4}
}

@article{nime2025_58,
  author = {Andrew Brown},
  title = {Maximum Silence to Noise: Sound synthesis for responsive gestural control},
  pages = {410--414},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {58},
  track = {Paper},
  doi = {10.5281/zenodo.15698910},
  url = {http://nime.org/proceedings/2025/nime2025_58.pdf},
  presentation-video = {},
  abstract = {Modulation synthesis has been a foundational technique in the development of electronic musical instruments since their inception. This paper presents a novel approach to ring modulation synthesis, termed Maximum Silence to Noise (MSN), along with an associated method of gestural control facilitated by a pressure-sensitive multi-touch controller. The primary objective of this research is to develop an instrument capable of producing a broad and diverse range of audio spectra that can be expressively articulated through responsive touch-based interaction. Integrating the synthesis process with gestural parameter mapping is crucial for the performative capabilities of New Interfaces for Musical Expression (NIMEs). The technical development of an MSN-based instrument was subject to an iterative design process with mixed method evaluation. The usability and practical application of the MSN instrument was refined through performance experiences, which illustrate the effectiveness of the synthesis-gesture mappings in providing dynamic and expressive control over the diverse generated audio spectra.},
  numpages = {5}
}

@article{nime2025_59,
  author = {Thomas Studley},
  title = {SwimTunes: A gamified music performance system for co-creating with a novice audience},
  pages = {415--421},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {59},
  track = {Paper},
  doi = {10.5281/zenodo.15698912},
  url = {http://nime.org/proceedings/2025/nime2025_59.pdf},
  presentation-video = {},
  abstract = {This paper presents SwimTunes, a prototype game system designed for novice multi-user music-making in live performance settings. The system features a digital game and a public web app that allows audience members to participate using their mobile devices. After connecting via QR code, participants create and pilot virtual fish that generate music as they bump into one another. The performer then enters the game as a shark, using camera-based hand tracking to chase and consume the participants’ fish. The result is a performance dynamic that evolves from playful co-creation to one of gameful contest between the performer and audience. SwimTunes explores how this shifting interaction context can shape the instantiation of a set of musical parameters, and further how performers can harness gameplay metaphors to conduct live audiences in shared acts of musical expression. The paper details the design considerations and conceptual motivations that informed SwimTunes before describing its implementation via Node.js, Open Sound Control, Unreal Engine 5, and MetaSounds. It discusses technical challenges and opportunities unearthed during development and outlines future directions for the project and gamified music performance at large.},
  numpages = {7}
}

@article{nime2025_60,
  author = {Toby Gifford},
  title = {A Synthetic Cicada Soundscape Controlled by Breath},
  pages = {422--423},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {60},
  track = {Paper},
  doi = {10.5281/zenodo.15698914},
  url = {http://nime.org/proceedings/2025/nime2025_60.pdf},
  presentation-video = {},
  abstract = {This paper describes an interactive installation featuring a generative soundscape with breath control, that aims to capture the feeling of being in a forest full of cicadas. Inspired by a period of deep listening to cicada stridulations – in which I found the spatio-temporal pulsation of the sound mass reminiscent of breathing – this installation uses breath control to give a sense of breathing with a forest. The sound mass consists of multiple generative sources, each loosely modelled on an individual cicada stridulating. Each ‘cicada’ comprises a temporal hierarchy of pulse trains modulating a carrier frequency, with a simple sonic spatialization algorithm applied to give the sense of immersion in the sound mass. The algorithm is implemented in the Extempore audiovisual programming language, and utilizes an architecture in which each sonic parameter is inherently stochastic, much as the sound production mechanisms of actual cicadas exhibit natural variation. },
  numpages = {2}
}

@article{nime2025_61,
  author = {Matthew Hamilton and Michele Ducceschi and Roberto Livi and Catalina Vicens and Andrew McPherson},
  title = {Augmentation of a Historical Harpsichord Keyboard Replica for Haptic-Enabled Interaction in Museum Exhibitions},
  pages = {424--431},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {61},
  track = {Paper},
  doi = {10.5281/zenodo.15698916},
  url = {http://nime.org/proceedings/2025/nime2025_61.pdf},
  presentation-video = {},
  abstract = {This paper describes the design and creation of an electronically augmented replica of a historical harpsichord keyboard with a typical 17th-century Italian layout to create a digital musical instrument. The keyboard was commissioned for exhibition in a musical instrument museum to enhance the visitor experience by providing an interface to digitised versions of instruments within the collection. The replica balances the competing demands of historical authenticity, public accessibility, and preservation. It replicates the original instrument’s tactile feedback and mechanical resistance using historically informed construction techniques. Optical sensors integrated within the mechanism capture the jacks’ motion data, enabling MIDI message generation. This work situates itself within broader discussions on the role of technology in museums. A keyboard interface of this type offers an opportunity to enhance visitor interaction with musical heritage while safeguarding delicate artefacts. The paper examines the keyboard’s design principles, technical implementation, and implications, emphasising its contribution to public engagement and the long-term preservation of musical heritage.},
  numpages = {8}
}

@article{nime2025_62,
  author = {Davor Vincze and Roberto Alonso and Peter Nelson},
  title = {Metabow: Gesture Mapping in Immersive Sonic Environments},
  pages = {432--435},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {62},
  track = {Paper},
  doi = {10.5281/zenodo.15698918},
  url = {http://nime.org/proceedings/2025/nime2025_62.pdf},
  presentation-video = {},
  abstract = {This paper presents the MetaBow, an augmented violin bow designed to control digital sound processing through real-time motion tracking. We discuss the challenges of mapping Inertial Measurement Unit (IMU) data to audio parameters in immersive multi-speaker environments and propose hybrid strategies using both direct mapping and machine learning models. We reflect on design choices, trade-offs, and performer experience, drawing from technical development and performance contexts. Three condensed case studies illustrate the system’s versatility in spatial and interactive musical performance.},
  numpages = {4}
}

@article{nime2025_63,
  author = {Aditya Arora and Erica Tandori and James Marshall and Stuart Favilla},
  title = {Designing Sensory NIME for Autism },
  pages = {436--442},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {63},
  track = {Paper},
  doi = {10.5281/zenodo.15698920},
  url = {http://nime.org/proceedings/2025/nime2025_63.pdf},
  presentation-video = {},
  abstract = {This paper explores how sensory NIME design principles may inform the design of musical interfaces tailored for children with Autism Spectrum Disorder (ASD), focusing on their sensory processing challenges. Given the prevalence of sensory over-responsivity (SOR) and under-responsivity (SUR) in ASD, traditional sensory interventions often fail to accommodate the highly individualized and fluctuating sensory needs of autistic individuals. The authors highlight the potential for multisensory NIME to address the diverse range of sensory needs, promoting emotional regulation and sensory balance through new creative musical opportunities and activities. This paper presents research in the form of a narrative review and comparative case study of recent NIME and sensory intervention research, exploring emerging approaches, rhythm-based interventions, generative algorithms, play-centered designs and other possibilities for enhancing sensory engagement and emotional regulation. Drawing on insights from 30 recent NIME papers, this research explores the boundaries of current approaches and seeks to establish an understanding of multisensory NIME for ASD. The research underscores the profound variability in sensory profiles for ASD, necessitating a shift from clinician-directed interventions to creative, inclusive, multisensory solutions. Finally, a set of sensory NIME design principles are offered, emphasizing the importance of sensory perception, sensory equilibrium and the promotion of emotional regulation for ASD.  },
  numpages = {7}
}

@article{nime2025_64,
  author = {Krzysztof Cybulski and Szczepan Busko and Zachariasz Zalewski},
  title = {PanMan - a modular tangible controller for sound spatialization},
  pages = {443--445},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {64},
  track = {Paper},
  doi = {10.5281/zenodo.15699656},
  url = {http://nime.org/proceedings/2025/nime2025_64.pdf},
  presentation-video = {},
  abstract = {PanMan is a performance-oriented modular midi controller, conceived as a tangible interface for panning multiple sound sources in multichannel audio systems. It consists of four independent control units and a docking base - the modular design allows each of the units to be either physically attached to the base (in which case it might be used as a single controller by a single user) or connected to it via extension cords, allowing up to four users to participate in an interactive sound installation experience or a collaborative performance setting.The physical controls on a single module consist of a joystick/trackball hybrid – a dome-shaped control device designed to be operated with a single finger – and a thumbwheel for additional parameter control, positioned at the edge of the module, allowing for one-handed operation of three parameters. The design facilitates operation by both right- and left-handed users, also allowing a single user to operate two or more controllers simultaneously, controlling a number of parameters at once.},
  numpages = {3}
}

@article{nime2025_65,
  author = {Yann Seznec},
  title = {The Memory Cloud: Personal media libraries as affordance and constraint},
  pages = {446--451},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {65},
  track = {Paper},
  doi = {10.5281/zenodo.15698924},
  url = {http://nime.org/proceedings/2025/nime2025_65.pdf},
  presentation-video = {},
  abstract = {The Memory Cloud is a musical instrument that uses a player’s own library of personal recordings as sonic material. This paper presents the design of the instrument, situating it within sustainability HCI studies and constraints-based design, before describing the instrument being used by two musicians in a professional context. Over 2000 sounds from the musician's personal cloud library, dating back over 10 years, were placed in the instrument as the only sonic material available for exploring. I argue that a radically small scale and personal approach could be one strategy for addressing the issues of longevity in NIME, and I suggest that using personal media libraries presents a potential affordance and constraint for musical instrument design.},
  numpages = {6}
}

@article{nime2025_66,
  author = {Jordan Shier and Rodrigo Constanzo and Charalampos Saitis and Andrew Robertson and Andrew McPherson},
  title = {Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces},
  pages = {452--461},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {66},
  track = {Paper},
  doi = {10.5281/zenodo.15698926},
  url = {http://nime.org/proceedings/2025/nime2025_66.pdf},
  presentation-video = {},
  abstract = {Timbre remapping is an approach to audio-to-synthesizer parameter mapping that aims to transfer timbral expressions from a source instrument onto synthesizer controls. This process is complicated by the ill-defined nature of timbre and the complex relationship between synthesizer parameters and their sonic output. In this work, we focus on real-time timbre remapping with percussion instruments, combining technical development with practice-based methods to address these challenges. As a technical contribution, we introduce a genetic algorithm - applicable to black-box synthesizers including VSTs and modular synthesizers - to generate datasets of synthesizer presets that vary according to target timbres. Additionally, we propose a neural network-based approach to predict control features from short onset windows, enabling low-latency performance and feature-based control. Our technical development is grounded in musical practice, demonstrating how iterative and collaborative processes can yield insights into open-ended challenges in DMI design. Experiments on various audio representations uncover meaningful insights into timbre remapping by coupling data-driven design with practice-based reflection. This work is accompanied by an annotated portfolio, presenting a series of musical performances and experiments with reflections.},
  numpages = {10}
}

@article{nime2025_67,
  author = {Tug F. O'Flaherty and Luigi Marino and Charalampos Saitis and Anna Xambó Sedó},
  title = {Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning},
  pages = {462--467},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {67},
  track = {Paper},
  doi = {10.5281/zenodo.15698928},
  url = {http://nime.org/proceedings/2025/nime2025_67.pdf},
  presentation-video = {},
  abstract = {This paper explores crossmodal mappings of colour to sound. The instrument presented analyses the colour of physical objects via a colour light-to-frequency sensor and maps the corresponding red, green, and blue data values to parameters of a synthesiser. Interactive machine learning is used to facilitate the discovery of new relationships between sound and colour. The role of interactive machine learning is to find unexpected relationships between the visual features of the objects and the sound synthesis. The performance is evaluated by its ability to provide the user with a playful interaction between the visual and tactile exploration of coloured objects, and the generation of synthetic sounds. We conclude by outlining the potential of this approach for musical interaction design and music performance.},
  numpages = {6}
}

@article{nime2025_68,
  author = {Kevin Blackistone},
  title = {A Spherical Tape Topology for Non-linear Audio Looping},
  pages = {468--472},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {68},
  track = {Paper},
  doi = {10.5281/zenodo.15698932},
  url = {http://nime.org/proceedings/2025/nime2025_68.pdf},
  presentation-video = {},
  abstract = {There have been many physical design formats used in the field of audio recording. As audio has an inherently a linear, time-based structure, these have generally followed logical layouts such as tape, or grooved records and cylinders. This project explores magnetic recording technology and digital analogues for recording and playback that are instead on spherical topology. This instrument expands the concept of the audio loop through a more tangible and randomized approach than traditional record playback techniques of tape, while maintaining a familiarity with historic techniques of audio looping and scrubbing. Through it, one can not only create linear time-loops but blends between different times of the recording non-sequentially. The size and mass of the spheres enhances the performative elements through the physics of inertia. The movement possibilities allow for non-linear circles, circuits, spirals and other patterns of sound not traditionally possible through linear tape or digital loop, including accelerations and decelerations – akin to a turntable, but with greater freedom of direction, thus offering surreal record/playback possibilities.},
  numpages = {5}
}

@article{nime2025_69,
  author = {Ari Liloia and Roger Dannenberg},
  title = {Exploiting Latency In The Design Of A Networked Music Performance System For Percussive Collective Improvisation},
  pages = {473--480},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {69},
  track = {Paper},
  doi = {10.5281/zenodo.15698934},
  url = {http://nime.org/proceedings/2025/nime2025_69.pdf},
  presentation-video = {},
  abstract = {We present the design, prototype implementation, and informal testing of a distributed web-based networked music performance (NMP) system for collaborative improvisation and experimentation. Influenced by composition and interaction design techniques from a wide range of work on collaborative virtual music environments, rather than treating latency as inherently disruptive to the musical and social engagement that characterizes traditional performance, we incorporate and exploit network delay to facilitate and visualize them, providing a novel approach to creating "jam session"-like experiences without a separate audience. During sessions, users collaboratively perform semi-improvised music in quasi-real time. The production and interpretation of individual musical gestures ("drum hits") are visualized in a continuously devised feedback network. The music produced can be treated as a starting point for compositions developed asynchronously, or as complete pieces of music produced live.},
  numpages = {8}
}

@article{nime2025_70,
  author = {Hanyu Qu and Francesco Dal Rí  and Hao Zou and Hanqing Zhou and Raul Masu},
  title = {O – : An Epistemic DMI for Cross-Cultural Reflection on Time and Music},
  pages = {481--488},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {70},
  track = {Paper},
  doi = {10.5281/zenodo.15698936},
  url = {http://nime.org/proceedings/2025/nime2025_70.pdf},
  presentation-video = {},
  abstract = {This paper introduces a Digital Musical Instrument (DMI) that inscribes a linear and a circular conception of time, inspired by Western and Eastern time philosophies. The DMI employs two 3D-printed boards equipped with ESP32 chips for wireless communication and WS2812 LEDs providing visual representation feedback, and interactive boxes, each fitted with a light sensor and ESP32 Mini boards. Such interface is designed to be coupled with a software counterpart for sound generation. The project originates from a collaboration with two composers from diverse cultural backgrounds - one Chinese and one Italian. Through collaborative design and co-composing practice, the proposed DMI emerged as an epistemic tool, promoting cultural understanding and critically highlighting the socio-cultural role of technology. Through such process, the significance of rediscovering time in contemporary globalization and philosophy was explored, challenging the conception of time as a mere measurement parameter and striving to reveal the importance of understanding the role of time across different cultural contexts. This project wishes to expand the constitutive role of musical time, demonstrating its diversity and prompting a reflective layer of the perception of performative and musical time in NIME.},
  numpages = {8}
}

@article{nime2025_71,
  author = {Casper Preisler and Daniel Overholt},
  title = {Hybrid Hand Drum: Where Tradition Resonates Through Technology},
  pages = {489--494},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {71},
  track = {Paper},
  doi = {10.5281/zenodo.15698938},
  url = {http://nime.org/proceedings/2025/nime2025_71.pdf},
  presentation-video = {},
  abstract = {This paper presents a hybrid frame drum that entangles acoustic and digital elements, merging the expressive depth of traditional percussion with the expanded possibilities of digital sound processing. Designed with 4 key principles - portability, hybridity, simplicity and low latency - the instrument allows for a fluid interplay between physical and real-time digital augmentation. Equipped with piezoelectric sensors, an FSR, and a DSP algorithm the drum extends its sonic landscape while preserving its acoustic presence. The design maintains an organic relationship between physical interaction and digital processing, and the three potentiometers provide intuitive yet flexible control, maintaining a balance between minimalism and expressivity. The bela platform ensures very low latency (7.55 ± 0.13 ms), making it highly responsive for live performance.User evaluation highlights its potential for expressive control and seamless hybrid performance while suggesting ergonomic and functional refinements. Future enhancements, such as feedback control and DSP presets, could deepen the entanglement between performer, instrument, and sound. This research explores the intersection of acoustic and digital sound, contributing to the design of hybrid instruments that blur the boundaries between physical resonance and electronic transformation, expanding possibilities for musical interaction.},
  numpages = {6}
}

@article{nime2025_72,
  author = {Francesco Di Maggio and Bart Hengeveld and Atau Tanaka},
  title = {Embedded Comparo: Small DSP Systems Side-by-Side},
  pages = {495--504},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {72},
  track = {Paper},
  doi = {10.5281/zenodo.15698940},
  url = {http://nime.org/proceedings/2025/nime2025_72.pdf},
  presentation-video = {},
  abstract = {This paper presents a comparative analysis of four embedded platforms designed for real-time audio processing: Bela, Daisy, OWL, and Raspberry Pi. These platforms have become integral tools in the field of digital musical instrument design, offering a variety of workflows, programming environments, and deployment methods. Although each system carries its own distinct strengths and constraints, the current workflow to embed DSP code across multiple devices lacks standardized approaches. To address this challenge, we develop a methodology that focuses on deploying Pure Data patches across all four platforms. Our study is structured around four test patches. Our findings highlight the trade-offs in latency, processing power, and memory constraints across the selected platforms. As a result, we propose a streamlined workflow to deploy Pd patches on each board using Plugdata, the Heavy Compiler, and their respective Web IDEs. As an ongoing contribution to the NIME community, we document our methodologies, workflows, and best practices in an open source repository, which serves as a continuously evolving resource for future research in the hands of musicians, researchers, and developers working with embedded musical systems.},
  numpages = {10}
}

@article{nime2025_73,
  author = {Wenqi WU and Hanyu QU},
  title = {Gesture-Driven DDSP Synthesis for Digitizing the Chinese Erhu},
  pages = {505--510},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {73},
  track = {Paper},
  doi = {10.5281/zenodo.15698942},
  url = {http://nime.org/proceedings/2025/nime2025_73.pdf},
  presentation-video = {},
  abstract = {This paper presents a gesture-controlled digital Erhu system that merges traditional Chinese instrumental techniques with contemporary machine learning and interactive technologies. By leveraging the Erhu’s expressive techniques, we develop a dual-hand spatial interaction framework using real-time gesture tracking. Hand movement data is mapped to sound synthesis parameters to control pitch, timbre, and dynamics, while a differentiable digital signal processing (DDSP) model, trained on a custom Erhu dataset, transforms basic waveforms into authentic timbre which remians sincere to  the instrument’s nuanced articulations. The system bridges traditional musical aesthetics with digital interactivity, emulating Erhu bowing dynamics and expressive techniques through embodied interaction. The study contributes a novel framework for digitizing Erhu performance practices, explores methods to align culturally informed gestures with DDSP-based synthesis, and offers insights into preserving traditional instruments within digital music interfaces.},
  numpages = {6}
}

@article{nime2025_74,
  author = {Fangzheng Liu and Lancelot Blanchard and Don D Haddad and Joseph Paradiso},
  title = {Two Sonification Methods for the MindCube},
  pages = {511--515},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {74},
  track = {Paper},
  doi = {10.5281/zenodo.15698944},
  url = {http://nime.org/proceedings/2025/nime2025_74.pdf},
  presentation-video = {},
  abstract = {In this work, we explore the musical interface potential of the MindCube, an interactive device designed to study emotions. Embedding diverse sensors and input devices, this interface resembles a fidget cube toy commonly used to help users relieve their stress and anxiety. As such, it is a particularly well-suited controller for musical systems that aim to help with emotion regulation. In this regard, we present two different mappings for the MindCube, with and without AI. With our generative AI mapping, we propose a way to infuse meaning within a latent space and techniques to navigate through it with an external controller. We discuss our results and propose directions for future work.},
  numpages = {5}
}

@article{nime2025_75,
  author = {Levin Schnabel and Dan Overholt},
  title = {Acoustic-digital hybrid synthesizer},
  pages = {516--523},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {75},
  track = {Paper},
  doi = {10.5281/zenodo.15698946},
  url = {http://nime.org/proceedings/2025/nime2025_75.pdf},
  presentation-video = {https://vimeo.com/1053084541/550b428fdd},
  abstract = {This paper explores the design and evaluation of an acoustic-digital hybrid instrument that aims to address key criticisms of Digital Musical Instruments (DMIs), particularly the separation of control and sound generation. By integrating an interactable physical string with coupled Finite Difference Schemes (FDS) for physical modeling synthesis, the instrument creates a tactile and responsive playing experience.The instrument was evaluated through a mixed-methods approach, combining qualitative think-aloud protocols with the Musician’s Perception of the Experiential Quality of Musical Instruments Questionnaire (MPX-Q). Results indicate that the instrument fosters curiosity and creativity but highlights challenges in achieving traditional acoustic playability, such as latency and perceptual dissonance. These findings emphasize the potential and limitations of acoustic-digital hybrids in reuniting control and sound, offering valuable insights for future developments in musical interface design.},
  numpages = {8}
}

@article{nime2025_76,
  author = {Danny Perreault and Victor Drouin-Trempe and Vincent Cusson and David Drouin and Sofian Audry},
  title = {From Performance to Installation: How Interactive Reinforcement Learning Reframes the Roles of Performers and Audiences},
  pages = {524--529},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {76},
  track = {Paper},
  doi = {10.5281/zenodo.15698948},
  url = {http://nime.org/proceedings/2025/nime2025_76.pdf},
  presentation-video = {},
  abstract = {This paper explores how interactive reinforcement learning (IRL) reconfigures the roles of performers and audiences in audiovisual performance and immersive installations. We adapt the Co-Explorer (a software tool originally developed for musical co-creation) to audiovisual immersive contexts and examine its creative potential using a reflexive research-creation approach. Our study reveals how IRL splits the role of the performer into three distinct positions: (1) the designer, who defines the parametric space; (2) the guide, who reinforces the agent’s behavior; and (3) the machine performer, whose actions are shaped by interactive training. As IRL introduces agency into the creative process, it transforms traditional notions of authorship and control, enabling unexpected emergent outcomes. By showcasing an interactive installation/performance, we further explore how audiences contribute to collective creation through reinforcement-based interaction. Our findings underscore the challenges of balancing the temporality of IRL with the demands of public-facing works and of adapting RL-based systems to different exhibition contexts. Our work contributes to the discourse on co-creative systems, emphasizing the evolving roles of artists, artificial agents, and audiences in hybrid creative ecosystems.},
  numpages = {6}
}

@article{nime2025_77,
  author = {Lucía Montesínos and Halfdan Hauch Jensen and Anders Løvlie},
  title = {Enabling Embodied Music-Making for Non-Musicians},
  pages = {530--536},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {77},
  track = {Paper},
  doi = {10.5281/zenodo.15699662},
  url = {http://nime.org/proceedings/2025/nime2025_77.pdf},
  presentation-video = {},
  abstract = {We present a Research through Design exploration of the potential for using tangible and embodied interactions to enable active music experiences - musicking - for non-musicians. We present the Tubularium prototype, which aims to facilitate music-making to non-musicians by not requiring any initial skill while still eliciting agency and overall, providing a meaningful experience. We present the design of the prototype and the features implemented and reflect on insights from a public event in which the prototype was trialed.},
  numpages = {7}
}

@article{nime2025_78,
  author = {Hyunkyung Shin and Henrik von Coler},
  title = {AR Matchmaking: The Compatibility of Musical Instruments with an AR Interface},
  pages = {537--544},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {78},
  track = {Paper},
  doi = {10.5281/zenodo.15698952},
  url = {http://nime.org/proceedings/2025/nime2025_78.pdf},
  presentation-video = {},
  abstract = {Augmented Reality (AR) interfaces offer new possibilities for musical expression by extending the capabilities of acoustic, electronic, and electroacoustic instruments. This study investigates the usability of the ARCube, an AR-based spatial audio controller, with twelve distinct musical instruments played by experienced musicians. We identify usability challenges specific to certain instruments, particularly for two-handed playing, as well as issues related to gesture recognition and cube stability. Our analysis shows that interaction patterns, such as cube placement, sound effect usage, and gesture strategies, vary significantly between instruments. These differences are driven by the physical form of the instruments, the required playing techniques, and user expectations for control and responsiveness. Based on these insights, we suggest directions for developing adaptable AR interfaces that better accommodate diverse instruments and support broader integration of AR technologies into musical practice. },
  numpages = {8}
}

@article{nime2025_79,
  author = {Xiaowan Yi and Mathieu Barthet},
  title = {The Drum Machine of Tao},
  pages = {545--548},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {79},
  track = {Paper},
  doi = {10.5281/zenodo.15698954},
  url = {http://nime.org/proceedings/2025/nime2025_79.pdf},
  presentation-video = {},
  abstract = {The Drum Machine of Tao (Tao) is a machine learning–based system that reverse-engineers sequencer parameters and one-shot percussive samples from drum loops, restoring low-level editability to sampled loops that would otherwise be frozen in audio waveforms. The philosophy behind this system is inspired from Taoism: that which returns to its primal state is the great Way of Tao. In this paper, we present the system design of Tao, which includes a state-of-the-art drum source separation model, a sequencer parameter estimation model, and a bespoke one-shot sample extraction algorithm that leverages differentiable audio synthesis. Results from a prototype are available for listening.},
  numpages = {4}
}

@article{nime2025_80,
  author = {André Santos and Amílcar Cardoso and Matthew  E. P. Davies and Roger B. Dannenberg},
  title = {Tapping Into a New Paradigm: A Synthetic Strategy for Automatic Drum TapScription},
  pages = {549--555},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {80},
  track = {Paper},
  doi = {10.5281/zenodo.15698956},
  url = {http://nime.org/proceedings/2025/nime2025_80.pdf},
  presentation-video = {},
  abstract = {We introduce Automatic Drum TapScription (ADTS), a novel paradigm for rhythmic interaction consisting of transcribing arbitrarily-timbred taps into drum representations. Our approach targets taps produced on a variety of surfaces without other controlled timbral characteristics other than playing style. Our long-term goal is to enable more accessible and creative percussive exploration, but presents significant challenges due to the minimal timbre variation between taps intended to represent different drum classes. To address these challenges, we take the first steps toward achieving ADTS by designing an effective dataset synthesis strategy. This strategy enables new opportuneties for musical expression by considering drumming at a more semantic or functional level as opposed to a simple collection of timbres. We present initial results, comparing three different models: one trained on drum data, another trained on a small dataset of quasi-aligned tapped performances, and another trained on our synthetic dataset. Our synthetic approach shows promise, demonstrating progress in this untapped domain.},
  numpages = {7}
}

@article{nime2025_81,
  author = {Alessandro Fiordelmondo and Matteo Spanio and Patricia Cadavid and Xinran Chen and Sergio Canazza and Raul Masu},
  title = {Towards a Repository Template for Music Technology Research},
  pages = {556--562},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {81},
  track = {Paper},
  doi = {10.5281/zenodo.15698958},
  url = {http://nime.org/proceedings/2025/nime2025_81.pdf},
  presentation-video = {},
  abstract = {Documenting and sharing research output is essential to construct the critical discourse on new music technology. Documentation feeds the knowledge and the values with which to evaluate and discuss current achievements and musical creations as well as to plan for the future. Besides publishing our research in conferences and journals, sharing research materials and outcomes like software, hardware, instruments, and datasets is important. This allows others to use the latest technology and improve it. For this purpose, the repository is increasingly commonly used by researchers and artists to store and share their works. However, creating repositories does not follow a clear and organised structure like the one we find, for example, in papers. The heterogeneity of repositories makes it hard to use both practically and for analysis. Although the variety and differences of research products in the field of new musical technologies are obvious, we believe that defining repositories with common guidelines could significantly improve the critical discourse in this area. This issue has been discussed at the NIME conference through workshops and papers. In this article, we want to continue this discussion and propose a flexible repository template to organise and present research materials and outcomes in the field of musical technologies research.The article provides a short and focused review of how repositories are currently used at the NIME conference, with special attention to the platforms used. Based on this study, we introduce a repository template that will be applied to case studies. We hope this proposal will encourage further discussion and advancement on this issue and, at the same time, support and facilitate the creation of new repositories.},
  numpages = {7}
}

@article{nime2025_82,
  author = {Steph OHara and Alon Ilsar},
  title = {The Sound Tree Project: Developing Personal and Collective Expression with Accessible Digital Musical Instruments},
  pages = {563--569},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {82},
  track = {Paper},
  doi = {10.5281/zenodo.15698960},
  url = {http://nime.org/proceedings/2025/nime2025_82.pdf},
  presentation-video = {},
  abstract = {The Sound Tree Project investigates how accessible digital musical instruments (ADMIs) can champion both personal and collective musical expression. Through a sustained six-month ethnographic engagement with five performers and two support artists, we explored how to create personalised instruments for a public performance outcome. The technical framework combined multiple wireless motion sensor devices placed inside different objects and the development of a real-time movement-to-sound processing hub within a live coding environment. The performance was centred on an accessible sound sculpture, the Sound Tree, where digital instruments coexisted with traditional sound making objects. Drawing from our shared process of experimentation, improvisation, and personalised instrument creation, we present some key ‘magic moments’ that were woven into the final performance and discuss how they might serve as evidence of personal expression and validation of the design process. The emergence of these moments demonstrate the value of real-time system adaptation in encouraging individual expression, the importance of sustained engagement in developing personalised instruments and having effective strategies for balancing personal and collective music-making.These insights have implications in developing accessible music technology and broader approaches to designing technologies that support diverse forms of creative collaboration.},
  numpages = {7}
}

@article{nime2025_83,
  author = {Pierre-Valery Tchetgen},
  title = {Designing A Tangible Rhythmic Interface for Digital Drum Talk},
  pages = {570--577},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {83},
  track = {Paper},
  doi = {10.5281/zenodo.15698962},
  url = {http://nime.org/proceedings/2025/nime2025_83.pdf},
  presentation-video = {},
  abstract = {I propose a tangible user interface and communication protocol for computer mediated rhythm-based interaction for educational applications (music, language, mathematics). Thinking beyond the paradigm of keyboard-and-screen-based interfaces, this project is based on previous work on the Drumball. By converting rhythmic input into multimodal output, it creates an entangled ecosystem where the human body, digital musical instruments, and the Internet of Things intersect. Such a digital orality system could offer parents and practitioners a novel method for introducing children to literacy, STEAM skills and multimodal communication in the early years. I present design iterations of 1) a tangible rhythmic interface for digital drum talk inspired by the style of play of the Djembe, 2) a protocol for sending piezo sensor outputs over a custom PCB shield, which can be recognized across multiple platforms and web-based environments without additional customization; and 3) a suite of rhythm-based learning games using the Alphariddims multimodal symbol system based on the Morse code. I argue that such a culturally-grounded approach to music technology design provides a viable avenue for the preservation and revitalization of the vibrant, yet intangible, cultural heritage and traditions of the African talking drum cultural systems. },
  numpages = {8}
}

@article{nime2025_84,
  author = {Lancelot Blanchard and Cameron Holt and Joseph Paradiso},
  title = {AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System},
  pages = {578--581},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {84},
  track = {Paper},
  doi = {10.5281/zenodo.15698966},
  url = {http://nime.org/proceedings/2025/nime2025_84.pdf},
  presentation-video = {},
  abstract = {Vocals harmonizers are powerful tools to help solo vocalists enrich their melodies with harmonically supportive voices. These tools exist in various forms, from commercially available pedals and software to custom-built systems, each employing different methods to generate harmonies. Traditional harmonizers often require users to manually specify a key or tonal center, while others allow pitch selection via an external keyboard–both approaches demanding some degree of musical expertise. The AI Harmonizer introduces a novel approach by autonomously generating musically coherent four-part harmonies without requiring prior harmonic input from the user. By integrating state-of-the-art generative AI techniques for pitch detection and voice modeling with custom-trained symbolic music models, our system arranges any vocal melody into rich choral textures. In this paper, we present our methods, explore potential applications in performance and composition, and discuss future directions for real-time implementations. While our system currently operates offline, we believe it represents a significant step toward AI-assisted vocal performance and expressive musical augmentation. We release our implementation on GitHub.},
  numpages = {4}
}

@article{nime2025_85,
  author = {Ena Fumihira and Andrew Johnston},
  title = {Physical Music Albums in the Digital Era: Exploring Experiential Value Through the Integration of AR},
  pages = {582--589},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {85},
  track = {Paper},
  doi = {10.5281/zenodo.15698968},
  url = {http://nime.org/proceedings/2025/nime2025_85.pdf},
  presentation-video = {},
  abstract = {This study explores physical music albums in the digital age, as well as the creation of new music experiences through the integration of Augmented Reality (AR) into physical albums. An online survey was conducted to examine the differences in user experiences between digital and physical albums, and this informed the development of a physical music album incorporating AR. We provided 8 K-POP fans, who engage with physical albums more frequently than fans of other genres, the opportunity to test existing AR-integrated albums and a new prototype featuring AR packaging animations, multiplayer virtual concerts, and interactive photo features. The results underscored the importance of understanding and respecting fan culture when using AR. The results suggest that, compared to digital albums, physical albums derive significant experiential value from traditional supplementary materials such as booklets and lyric cards. However, AR has the potential as a complementary new material to provides users with novel experiences.  This work leads to a reconsideration of Walter Benjamin’s concept of aura, which critiques the reproducibility of art.},
  numpages = {8}
}

@article{nime2025_86,
  author = {Samuel Dietz and Charles Martin},
  title = {Luna: An AR Musical Instrument on the Meta Quest 2},
  pages = {590--593},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {86},
  track = {Paper},
  doi = {10.5281/zenodo.15698970},
  url = {http://nime.org/proceedings/2025/nime2025_86.pdf},
  presentation-video = {},
  abstract = {Head-mounted augmented reality (AR) computers present the opportunity to develop new musical interfaces that would be impossible to build physically or with conventional computing devices. Unfortunately, typical computer music tools have not been easy to apply within AR development tool chains. Integrating standard computer music tools in AR development would allow more rapid prototyping of new instrument ideas and transfer of knowledge from experienced computer musicians. The goal of this paper is to demonstrate that AR digital musical instruments can be developed using libpd, the library version of the standard computer music environment Pure Data. We present a case study of an AR instrument developed for the Meta Quest 2 integrating libpd in the AR development tool-chain for the interactive audio components. The iterative development process was tracked through autoethnographic reflections and analysed with thematic analysis. We found that Pure Data was an effective way to develop audio interactions on the Quest 2 and that the hand tracking on this platform was capable of complex gestural interactions. This work could enable a broader community of computer musicians to explore AR NIME development, taking advantage of the unique affordances of this medium.},
  numpages = {4}
}

@article{nime2025_87,
  author = {Qiance Zhou and Charles Martin},
  title = {Exploring the Impact of Spatial Awareness on Large-Scale AR DMIs},
  pages = {594--600},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {87},
  track = {Paper},
  doi = {10.5281/zenodo.15698972},
  url = {http://nime.org/proceedings/2025/nime2025_87.pdf},
  presentation-video = {},
  abstract = {Large-scale Digital Musical Instruments (DMIs) offer immersive performance experiences and rich forms of expression, but often pose physical challenges and limit accessibility. The size of traditional large-scale DMIs limits the ability of performers to interact with the instrument, causing discomfort when engaging with distant components, highlighting the need for more flexible and user-friendly large-scale DMI designs. We present an Augmented Reality (AR) DMI that removes physical constraints by allowing performers to customise the size and layout of the instrument according to their performance environment. We aim to show how AR-based configuration supports immersive performance, promotes expressive gestures, and improves spatial awareness without sacrificing large-scale instrument capabilities. Our user study revealed increased physical engagement and spatial immersion, a strong sense of ownership, and a positive user experience. These findings indicate that our AR DMI is creatively empowering, reasonably addressing the constraints of large-scale instruments. Our research emphasises the potential of AR to enable flexible and customisable DMI design where interfaces can be adapted to suit the needs of individual performers.},
  numpages = {7}
}

@article{nime2025_88,
  author = {Vitor Pinheiro},
  title = {A Gesture-Based Approach to Spatialization in Dolby Atmos},
  pages = {601--604},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {88},
  track = {Paper},
  doi = {10.5281/zenodo.15698974},
  url = {http://nime.org/proceedings/2025/nime2025_88.pdf},
  presentation-video = {},
  abstract = {This paper presents a system for the spatialization of sound objects in the Dolby Atmos format, implemented through the integration of an infrared sensor with a chain of three software tools. The setup enables translating hand gestures into spatialization data within the constraints of the Atmos format. The design and parameter mapping are described, along with its usability, strengths, and limitations, as assessed through a preliminary evaluation conducted by the author. Beyond the technical aspects, this article reflects on the author's experience using the system as a mixing engineer and connects these insights to the conceptual framework of related works. This perspective offers a critical reflection on spatialization as a performative practice within studio workflows, highlighting how such devices may be integrated into the multimodal studio environment to introduce new means of interaction in sound spatialization.},
  numpages = {4}
}

@article{nime2025_89,
  author = {Shomit Barua},
  title = {Hyperwilding: Sonic Perplexity as Urban Acupuncture to Promote Environmental Kinship},
  pages = {605--609},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {89},
  track = {Paper},
  doi = {10.5281/zenodo.15698978},
  url = {http://nime.org/proceedings/2025/nime2025_89.pdf},
  presentation-video = {},
  abstract = {This paper invites discussion of how sound art installations, specifically those situated in urban environments, can serve as respite from urban stressors as well as advocate for increased awareness and engagement of acoustic ecology. The author invokes the theoretical framework of Karen Barad to juxtapose the Urban Acupuncture movement with the Solarpunk ethos, arguing that sound installations may be crafted as agential cuts to the entangled relationship of humans and their built environments. This paper surveys sound artists that have specifically engaged the urban space—an environment that one could argue is more “natural” to humans than the remote picturesque landscapes commonly associated with the concept. Finally, the author describes some of his past sonic interventions and expounds on his current project, “Standing Wave,” commissioned by the city government and non-profits to address Extreme Urban Heat. He discusses how this installation, coupled with targeted community engagement through “Environmental Listening” workshops, urges us to rethink the temporality of intervention, recognizing that long-term strategies, while not immediate solutions, are crucial for future cooling and remediating the effects of climate change.},
  numpages = {5}
}

@article{nime2025_90,
  author = {Nicholas Shaheed and Ge Wang},
  title = {ChuMP and the Zen of Package Management},
  pages = {610--617},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {90},
  track = {Paper},
  doi = {10.5281/zenodo.15698984},
  url = {http://nime.org/proceedings/2025/nime2025_90.pdf},
  presentation-video = {},
  abstract = {ChuMP stands for “ChucK Manager of Packages”, designed to automate the process of installing, upgrading, and removing software components for the ChucK programming ecosystem. ChuMP manages libraries, tools, audio and graphics plugins in a centralized, structured, and versioned manner. This project originated out of the recent ChucK development “renaissance” alongside a growing user community, now entering its third decade. The time for package management, as the ChucK slogan goes, is now. What began as a practical project has expanded into broader reflections on tool-building, service, and community. As we labored on what seemed like a “no-brainer” tool that everyone wanted but that no one wanted to build, questions arose: “how did we get here?”, “what is the role of service-based tool-building in our field–and what, if any, is its research value?”—in short, “can we even write a paper about a package manager?” Meanwhile, we couldn’t help but notice that the act of creating a package manager seems to unify not only disparate software fragments, but also something of community. In other words, there may be more than meets the eye. This paper chronicles the making of a package manager and all that goes along with it. This is the story of ChuMP.},
  numpages = {8}
}

@article{nime2025_91,
  author = {Ian Clester},
  title = {Synthesizing Music with Logic Gate Networks},
  pages = {618--622},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {91},
  track = {Paper},
  doi = {10.5281/zenodo.15698986},
  url = {http://nime.org/proceedings/2025/nime2025_91.pdf},
  presentation-video = {},
  abstract = {Small digital circuits consisting of basic logic gates (AND, XOR, etc.) are capable of generating surprisingly complex musical output. In this paper, I present physical and web-based interfaces for exploring the space of audio-generating logic gate networks and 'bending' such networks via touch (or mouse) gestures to interfere with their operation and change their output while they are running. This work follows in the vein of bytebeat practices, in which music is generated by short code snippets at the level of individual audio samples, but takes things further by relying on an even lower-level form of computation. In addition to presenting the system, I offer some preliminary analysis of why these logic gate networks tend to produce musical output.},
  numpages = {5}
}

@article{nime2025_92,
  author = {Alexander Han and Kiran Bhat and Ge Wang},
  title = {SMucK: Symbolic Music in ChucK},
  pages = {623--631},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {92},
  track = {Paper},
  doi = {10.5281/zenodo.15698988},
  url = {http://nime.org/proceedings/2025/nime2025_92.pdf},
  presentation-video = {},
  abstract = {SMucK (Symbolic Music in ChucK) is a library and workflow for creating music with symbolic data in the ChucK programming language. It extends ChucK by providing a framework for symbolic music representation, playback, and manipulation. SMucK introduces classes for scores, parts, measures, and notes; the latter encode musical information such as pitch, rhythm, and dynamics. These data structures allow users to organize musical information sequentially and hierarchically in ways that reflect familiar conventions of Western music notation. SMucK supports data interchange with formats like MusicXML and MIDI, enabling users to import notated scores and performance data into SMucK data structures. SMucK also introduces SMucKish, a compact high-level input syntax, designed to be efficient, human-readable, and live-codeable. The SMucK playback system extends ChucK’s strongly-timed mechanism with dynamic temporal control over real-time audio synthesis and other systems including graphics and interaction. Taken as a whole, SMucK’s design philosophy treats symbolic music data not only as static representations but also as mutable, recombinant building blocks for algorithmic and interactive processing. By integrating symbolic music into a strongly-timed, concurrent programming language, SMucK’s workflow goes beyond data representation and playback, and opens new possibilities for algorithmic composition, instrument design, and musical performance.},
  numpages = {9}
}

@article{nime2025_93,
  author = {Matthew Caren and Joshua Bennett},
  title = {Melia: An Expressive Harmonizer at the Limits of AI},
  pages = {632--634},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {93},
  track = {Paper},
  doi = {10.5281/zenodo.15698990},
  url = {http://nime.org/proceedings/2025/nime2025_93.pdf},
  presentation-video = {},
  abstract = {We present Melia, a digital harmonizer instrument that explores how common failure modes of machine learning and artificial intelligence (ML/AI) systems can be used in expressive and musical ways. The instrument is anchored by an audio-to-audio neural network trained on a hand-curated dataset to perform pitch-shifting and dynamic filtering. Biased training data and poor out-of-distribution generalization are deliberately leveraged as musical devices and sources of instrument-defining idiosyncrasies. Melia features a custom hardware interface with a MIDI keyboard that polyphonically allocates instances of the model to harmonize live audio input, as well as controls that manipulate model parameters and various audio effects in real-time. This paper presents an overview of related work, the instrument itself, and a discussion of how audio-to-audio AI models might fit into the long-standing tradition of musicians, artists, and instrument-makers finding inspiration in a medium's shortcomings.},
  numpages = {3}
}

@article{nime2025_94,
  author = {Nicole Robson and Andrew  McPherson and Nick  Bryan-Kinns},
  title = {Negotiating Entanglements in the Composition and Curation of an Ultrasonic Art Installation},
  pages = {635--642},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {94},
  track = {Paper},
  doi = {10.5281/zenodo.15698992},
  url = {http://nime.org/proceedings/2025/nime2025_94.pdf},
  presentation-video = {},
  abstract = {This paper explores the entangled activities of composing and curating the sound installation 'Sonographies' at a contemporary art gallery. The installation extends our work with an ultrasonic technology that sonifies and magnifies the physical entanglement of a listener with a spatial sound field to produce rich movement-sound interaction without the use of sensors. Taking a research through practice approach, we examine the process of creating 'Sonographies' while deliberately allowing the nonhuman influences of site and technology to inform creative ideation and decision-making. We propose that an attunement to entanglement foregrounds the co-production of aesthetic qualities by the entire musical assemblage and fosters a sensitivity to fragile and changeable qualities of NIMEs, contingent on specific technical, material and social situations.},
  numpages = {8}
}

@article{nime2025_95,
  author = {Kratika Jain and Allwin  Williams and Akhilesh Kumar Bhagat and Sukanth K and Arunav Rajesh and Prashant  Pal and Krishanan Chandran and Rajashekhar  V S  and Anandu Ramesh and Gowdham Prabhakar},
  title = {GraviTone: A Tangible Musical Interface using Gravity Well for Sound and Music Creation},
  pages = {643--649},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {95},
  track = {Paper},
  doi = {10.5281/zenodo.15698994},
  url = {http://nime.org/proceedings/2025/nime2025_95.pdf},
  presentation-video = {},
  abstract = {We propose a new musical interface, “GraviTone” that produces sounds and musical compositions via interaction with spherical objects in a gravity-well benchtop setup. The interface consists of several spherical objects in different colours orbiting around the centrally placed static object on the spacetime fabric. The spherical objects are launched at a certain angle from the setup's periphery, and the objects' motion is tracked by an overhead camera and mapped to different parameters to generate sound. In GraviTone, users can experience sounds generated from objects’ parameters and have control over different configurations. We use the setup to send Open Sound Control (OSC) signals and Musical Instrument Digital Interface (MIDI) signals to map different sounds and musical scales (Indian classical ragas and Western classical scale). We mapped the moving objects’ parameters to control synth parameters, VSTs, and DAWs. Users can also route generated MIDI data into DAWs using preset configurations or customizing their frameworks for sound generation. We also generate real-time visuals corresponding to the object's movements for further immersion and interactivity. This integrated interface combines various domains like musical mathematics, sonification techniques, sound synthesis, and sound design for live music creation and real-time audiovisual composition.},
  numpages = {7}
}

@article{nime2025_96,
  author = {Kana Yamaguchi and Yuga Tsukuda and Yoichi Ochiai},
  title = {Drawing Space with Rain: The Umbrella as a Flow Interface},
  pages = {650--657},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Doga Cavdir and Florent Berthaut},
  year = {2025},
  month = {June},
  address = {Canberra, Australia},
  issn = {2220-4806},
  articleno = {96},
  track = {Paper},
  doi = {10.5281/zenodo.15699671},
  url = {http://nime.org/proceedings/2025/nime2025_96.pdf},
  presentation-video = {},
  abstract = {This study explores new possibilities for transforming perceived space by using an umbrella as a dynamic spatial auditory interface. While spatial audio technologies have been widely applied across various domains, there are few opportunities in daily life to consciously perceive the boundary between personal and external space. Due to its physical structure and everyday usage, the umbrella has a unique ability to render such boundaries perceptible.Focusing on the “flow” of raindrops across the umbrella’s surface, the system detects continuous rain movement in real time rather than merely capturing impact sounds. Spatial auditory feedback encourages users to actively perceive dynamic spatial boundaries, as the rain draws auditory contours through interaction with the umbrella.To this end, the umbrella is conceptualized as an interactive interface that senses raindrop movement and applies spatial audio processing. In addition, users can dynamically alter the virtual size of the umbrella, enabling perceptual shifts in spatial scale.Rather than treating the umbrella as a mere protective object, this system reimagines it as a medium for perceiving environmental change through sound. By integrating natural phenomena with spatial audio, this approach suggests new directions for embodied perception and expression.},
  numpages = {8}
}
