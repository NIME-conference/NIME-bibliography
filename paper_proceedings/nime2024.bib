@article{nime2024_1,
  author = {Esther Gruy and Florent Berthaut},
  title = {MagneTip: Reintroducing a Physical Interaction Loop for 3D Musical Drawing in eXtended Reality},
  pages = {1--4},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {1},
  track = {Papers},
  doi = {10.5281/zenodo.13904764},
  url = {http://nime.org/proceedings/2024/nime2024_1.pdf},
  presentation-video = {},
  abstract = {Extended Reality Interfaces open numerous opportunities for musical expression. One of them is 3D musical drawing, i.e., the ability to draw sonic and visual paths in virtual and physical spaces. However, in existing instruments (controllers, hand tracking) most of the interaction happens in the digital/virtual space which reduces primary audio and haptic feedback and might limit the expressiveness one has with physical drawing and acoustic instruments. In this paper, we propose a novel design approach which moves part of the 3D interaction back to the physical space for more intimate controls and more direct feedback, creating a physical interaction loop connected with the virtual interaction loop. We then present MagneTip, a first implementation of this approach for 3D musical drawing, which enables one or two handed interaction and combines co-localised and spatialised feedback. We believe that our physical interaction loop approach could be applied to other paradigms of 3D musical interaction.},
  numpages = {4}
}

@article{nime2024_2,
  author = {Albert-Ngabo Niyonsenga and Marcelo Wanderley},
  title = {Take Five: Improving Maintainability and Reliability of the T-Stick},
  pages = {5--11},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {2},
  track = {Papers},
  doi = {10.5281/zenodo.13904766},
  url = {http://nime.org/proceedings/2024/nime2024_2.pdf},
  presentation-video = {https://youtu.be/N6hmc0lyDFU?si=AQsppAYYa9pQtx46},
  abstract = {The T-Stick, an interface with a lifetime of nearly 17 years, has undergone multiple changes over time. Over the first decade and a half of its existence, the T-Stick was manufactured in various sizes (four) and copies (> 30 units). Despite the relatively large numbers in the context of in-house academic interfaces, over the past 6 years many T-Sticks were made fairly artisanally, with graduate students manufacturing their own devices.  Despite this strategy's clear pedagogical advantages, the reliability of these devices was not always a priority, leading to the need for repairs and downtime during use. In this paper, we present the design for the 5th generation of T-Sticks, the T-Stick 5GW, designed to improve the reliability and maintainability of the interface to reduce faults during extended periods of use dramatically. The main changes in this new generation are the use of a custom-made ESP32-S3 board, which integrates a fuel gauge and IMU. In addition, the touch sensor is printed on a flexible PCB and interfaces with the touch board using a 32-pin FFC connector, dramatically reducing the need for soldering parts. Requirements relating to the instrument's reliability/availability and manufacturing are described in detail and were evaluated analytically, showing the effectiveness of the new design. Further reliability testing is ongoing through the use of the latest generation in musical performances.},
  numpages = {7}
}

@article{nime2024_3,
  author = {Stefano Fasciani},
  title = {Bibliometric Analysis of NIME References and Citations},
  pages = {12--22},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {3},
  track = {Papers},
  doi = {10.5281/zenodo.13904768},
  url = {http://nime.org/proceedings/2024/nime2024_3.pdf},
  presentation-video = {https://youtu.be/ZResveqmrYg?si=6Xc_AmLqM32ox2Ca},
  abstract = {This paper presents a bibliometric analysis that examines the works cited in, as well as those citing, NIME papers; for brevity, we refer to these as ‘references’ and ‘citations’. Utilizing existing tools, we have computationally extracted data from the NIME proceedings archive and retrieved metadata from an academic database, including details of associated references and citations. From this data, we computed a range of metrics and statistics, which we present in this paper. We offer quantitative insights into NIME as a scholarly publication venue, its connections to other venues, and its relationship with various fields of study and authors. Based on our data interpretations, we provide several recommendations for the community's future. In sharing the software we developed for this study, and the summarized raw data, we enable other NIME researchers to conduct more in-depth investigations and examine specific trends.},
  numpages = {11}
}

@article{nime2024_4,
  author = {Jocelyn Ho and Margaret Schedel and Bryan Jacobs},
  title = {Housework Commons: Rheostat Rotary Rack},
  pages = {23--27},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {4},
  track = {Papers},
  doi = {10.5281/zenodo.13904770},
  url = {http://nime.org/proceedings/2024/nime2024_4.pdf},
  presentation-video = {https://youtu.be/n7Q5ZjM0d30?si=ofzMrPfOuT9lDeQN},
  abstract = {This paper describes the creation of a NIME, Rheostat Rotary Rack (RRR), based on a rotary drying rack. Part of the Women's Labor feminist project that creates Embedded Acoustic Instruments using old domestic tools, RRR is anchored by a base that acts as resonator for the system and includes sensors—potentiometers (rheostats) and a rotary encoder—embedded into its physical architecture. RRR detects the weight of hanging clothes and velocity of rotation by hand or wind. Motivated by the feminist concept of reproductive commons, we invite the public to do and witness communal housework in interactive installation and musical performance, in order to challenge gender inequality in domestic work division. A composition written for RRR, A Body of Resistance, draws upon quotes by rheostat inventor Mary Hallock-Greenewalt to address women's struggles for equal treatment. The sound designs for RRR and A Body of Resistance take inspiration from the drying rack’s original wind-powered utility, using aerophone sounds and vibrational modelling of pipes and bars. By reimagining domestic tools with embedded technologies, we envision a musical housework commons that can act as a catalyst for social change.},
  numpages = {5}
}

@article{nime2024_5,
  author = {Jiin Ko and Youngjun Choi and Jinjoon Lee},
  title = {Harmonic Words, Narrative Chords: Textual Sonification Using Part of Speech},
  pages = {28--32},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {5},
  track = {Papers},
  doi = {10.5281/zenodo.13904772},
  url = {http://nime.org/proceedings/2024/nime2024_5.pdf},
  presentation-video = {},
  abstract = {This research aims to identify the commonalities between words (language) and chords (music), exploring if text itself can be musical and harmonic. Rather than letters or phonemes, we focus on words and their relationship between each other and have realized texts as music by using part of speech as chord progression on an automated algorithm. Other elements such as length and sentiment information inherent in text were also assigned to corresponding musical elements. For this research, a full-text of George Orwell’s Animal Farm was turned into a three-hour harmonic piece.},
  numpages = {5}
}

@article{nime2024_6,
  author = {Lloyd May and Lateef McLeod and Michael Mulshine},
  title = {Bishop BoomBox: A Physically Accessible Drum Machine},
  pages = {33--39},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {6},
  track = {Papers},
  doi = {10.5281/zenodo.13904774},
  url = {http://nime.org/proceedings/2024/nime2024_6.pdf},
  presentation-video = {https://youtu.be/pBXyZxrn2GI?si=rsgDs-Dtg1qy-ChX},
  abstract = {This paper presents the design, aesthetic considerations, and technical details of the Bishop BoomBox, an innovative physically accessible drum machine and sequencer inspired by classic drum machines, golden-era Hip-Hop culture, and chess. The BBB features an 8-step, 4-track sequencer, with steps triggered through physical touch or the placement of custom high-capacitance "chess'' pieces, which trigger capacitive sensors monitored by a Bela microcontroller. It provides volume, swing, tempo, and recording controls housed in a movable module. The BBB contains a rechargeable LiPo battery, detachable magnetic monitoring speakers, three-way toggles for per-track sample selection, and a custom stand designed to attach the device to the player’s power wheelchair securely. The BBB was co-designed through 10 collaborative co-design sessions. Drawing influence from Crip kinship, Disabled joy and the aesthetics and poetics of interaction were emphasized as key design metrics, challenging conventional Disability design norms that have tended to focus on utility and usability.},
  numpages = {7}
}

@article{nime2024_7,
  author = {Lloyd May and Peter Larson and Joel Mansour and Tord Bremnes},
  title = {Riddare of The Round Table: Inclusive Haptic Performance Practices in The Wild},
  pages = {40--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {7},
  track = {Papers},
  doi = {10.5281/zenodo.13904776},
  url = {http://nime.org/proceedings/2024/nime2024_7.pdf},
  presentation-video = {https://youtu.be/5KiUZ0tczH4?si=edhbHXfITsDzaLKB},
  abstract = {This demo paper presents the development of "Riddare of The Round Table'', a structured improvisational piece lasting approximately 20 minutes. Written in close collaboration with the mixed-ability inclusive ensemble Parasonic, the piece features cello, daxophone, found objects, a wooden table with vibrotactile transducers and contact microphones, tablet synthesizers and samplers, and electronics. At the beginning of the piece, audience members were invited to feel the vibrations being transduced into the wooden table before taking their seats. The vibrations served as feedback for the ensemble members, who were largely using tablets, as well as a source of feedback generation as the output of the contact microphone was fed into the transducers. },
  numpages = {3}
}

@article{nime2024_8,
  author = {Balázs Iványi and Truls Tjemsland and Lloyd May and Matt Robidoux and Stefania Serafin},
  title = {Participatory Design of a Collaborative Accessible Digital Musical Interface with Children with Autism Spectrum Disorder},
  pages = {43--51},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {8},
  track = {Papers},
  doi = {10.5281/zenodo.13904778},
  url = {http://nime.org/proceedings/2024/nime2024_8.pdf},
  presentation-video = {https://youtu.be/mw1UIUM_2Eg?si=GO20hEoseZhisJCm},
  abstract = {This research project aims to address the challenges faced by children with autism spectrum condition (ASC) by developing a collaborative and accessible digital musical interface (CADMI) through a participatory design (PD) process. Six PD workshops were conducted in collaboration with Stanbridge Academy (n=6) and Skolen Sputnik (n=6), incorporating fictional inquiry narratives and tailored non-digital activities. The resulting musical tablet app, 'boxsound', prioritizes the user's perspective and enables the practice of various social skills by bridging divergent viewpoints. The CADMI was evaluated through a survey and semi-structured interviews with both children with autism and their educators. },
  numpages = {9}
}

@article{nime2024_9,
  author = {Lloyd May and Rabia Malik and AnnMarie Thomas},
  title = {Co-Designing Haptic Instruments With Deaf and Hard-of-Hearing Children},
  pages = {52--61},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {9},
  track = {Papers},
  doi = {10.5281/zenodo.13904780},
  url = {http://nime.org/proceedings/2024/nime2024_9.pdf},
  presentation-video = {},
  abstract = {This paper explores haptic art and tactile experiences as an independent form of artistic expression, distinct from audio-visual (AV) technologies. Haptic technologies have seen significant advances in gaming, virtual reality training, and as an auxiliary output in select music performance and playback systems. However, there are currently no stand-alone haptic music/art systems that allow for the easy creation and distribution of haptic music/art. Despite the convergence of haptics research and music technology, the perspectives of the Deaf and Hard-of-Hearing (DHH) communities remain underrepresented in the field of creative haptic technology development. Recognizing the potential of more readily available haptic art creation and sharing systems for the Deaf and/or Disabled community, we conducted a co-design workshop with 27 DHH middle school children, focusing on their experiences with haptic vibrations, creating low-fidelity prototypes for haptic art presentation devices, and composing short pieces of haptic music. Through a mixed-methods analysis of survey responses, reflections, and thematic analysis of the prototypes and haptic art compositions, we gained valuable insights into the aesthetic possibilities and considerations for future haptic art instruments. By elevating haptic music/art as a distinct category of work not always subservient to auditory music, we hope to pave the way for more inclusive and accessible technologies in the realm of artistic expression and enrich the experiences of diverse communities in the world of art and creativity.},
  numpages = {10}
}

@article{nime2024_10,
  author = {Yunyu Ong and Robert Sazdov and Andrew Johnston},
  title = {Opening DAWs to Interactive Music - Making an Orchestra out of Soloists},
  pages = {62--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {10},
  track = {Papers},
  doi = {10.5281/zenodo.13904782},
  url = {http://nime.org/proceedings/2024/nime2024_10.pdf},
  presentation-video = {https://youtu.be/xbYOptVQrA8?si=CU4pz-rfw84kKMHp},
  abstract = {This paper explores a novel composition for an electronic drum (Roland’s Taiko-1) that transforms a solo drummer into an orchestral presence, aligning with NIME’s theme of “Tactility in a Hybrid World”. It presents a unique method applied to Logic Pro X, bypassing complex learning curves associated with interactive music systems. The composition employs game audio and instrument design techniques to ensure each performance is unique and responsive. Key focus areas include enhancing the visceral, behavioral, and reflective stimuli levels between performer and audience. The paper presents narrative frameworks that guide interactive compositions as well as a method of manipulating Logic X’s linear DAW behaviours to create real-time compostions with low learning curves and fast auditioning times. This paper is aimed at composers more at home with linear DAWS but want to venture into interactive compositions within the comfort of a familiar interface.},
  numpages = {8}
}

@article{nime2024_11,
  author = {Erin M Demastes},
  title = {The Conductive Kinetic Box},
  pages = {70--72},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {11},
  track = {Papers},
  doi = {10.5281/zenodo.13904786},
  url = {http://nime.org/proceedings/2024/nime2024_11.pdf},
  presentation-video = {https://youtu.be/G5mpKDSifnY?si=owQvLZ9o3ZqttZOR},
  abstract = {The Conductive Kinetic Box is an experiment in creating a unified and portable tactile instrument made from five separate kinetic elements. These kinetic elements are paired with five circuits that connect into five small speakers allowing the instrument to be played both acoustically and electronically. As the kinetic elements move and connect with themselves, an electrical connection is made which turns the speakers on and off at the same rate as the physical connection. These elements are designed to make distinct sounds based on the physics of their motion and materials, but all are made with metal to ensure an electrical connection when powered. The Conductive Kinetic Box also explores repurposed and handmade materials as the wooden enclosure is handmade, and many of the metal elements are repurposed, such as a small wind chime, perches for hamsters and birds, aluminum foil, jar lids, photo holders, and a piece of a coat rack.},
  numpages = {3}
}

@article{nime2024_12,
  author = {Çağrı Erdem and Carsten Griwodz},
  title = {dB: A Web-based Drummer Bot for Finger-Tapping},
  pages = {73--83},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {12},
  track = {Papers},
  doi = {10.5281/zenodo.13904788},
  url = {http://nime.org/proceedings/2024/nime2024_12.pdf},
  presentation-video = {},
  abstract = {dB is a web-based interface that serves as a "drummer bot" for exploring interactive groove-making experiences with an AI percussion system. This system, leveraging Variational Autoencoders (VAEs), transforms simple rhythmic inputs into complex drum patterns with microtiming and dynamics. Designed for accessibility and playfulness, dB is easily operated via a computer keyboard, making it suitable for a wide range of users. This paper outlines dB's foundational concepts, data collection, and a comprehensive overview of system and interface architecture. We then present our preliminary user study that investigated specific aspects of user engagement, including joy and boredom states, as well as perceptions of effort and control. The study's results underscore the musical background, expertise, and generational differences as significant influences on user experiences. Notably, test conditions characterized by greater randomness and rhythmic variation were consistently perceived as more engaging, and emerging trends were observed in user responses diverging over time.},
  numpages = {11}
}

@article{nime2024_13,
  author = {Stella Paschalidou},
  title = {Technology-mediated haptic interactions in Dhrupad vocal music pedagogy: what next?},
  pages = {84--90},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {13},
  track = {Papers},
  doi = {10.5281/zenodo.13904790},
  url = {http://nime.org/proceedings/2024/nime2024_13.pdf},
  presentation-video = {https://youtu.be/NMJpbFoFl8E?si=_17P5I8TxLOP5fuK},
  abstract = {Acknowledging the importance of embodiment has prompted a recent shift in music teaching methods, emphasizing a holistic, multi-sensory approach involving the entire body. However, the education of oral music genres, traditionally reliant on live demonstration and imitation, is undergoing a contrasting transformation by rapidly embracing online means. This study explores challenges in the embodied aspects of synchronous distance Hindustani music pedagogy, with a special focus on tactility. Taking an ethnomusicological perspective, the paper presents an analysis of interviews with Hindustani music practitioners, which is guided by the principles of the 4E Cognition framework. The findings suggest that, while the adaptation to technology aims to broaden access to music content, it does so at the expense of limiting opportunities for multi-modal interaction among participants. The results highlight constraints in conveying non-verbal, embodied, and multi-sensory cues, as well as disruptions in visual and acoustic, but most importantly tactile elements, that contribute to an otherwise shared spatial and physical context. These challenges hinder meaningful interaction and immersive experiences, crucial elements in music education. The study expresses concerns about the appropriateness of conventional videoconferencing platforms and offers valuable insights for developing alternative technologies, better suited to meet the embodied demands of these pedagogical practices.},
  numpages = {7}
}

@article{nime2024_14,
  author = {Yann Seznec and Nadia Campo Woytuk},
  title = {The Period Instrument},
  pages = {91--99},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {14},
  track = {Papers},
  doi = {10.5281/zenodo.13904792},
  url = {http://nime.org/proceedings/2024/nime2024_14.pdf},
  presentation-video = {https://youtu.be/fMH_ccXoV9o?si=x6tUIAg0v7MeZEZB},
  abstract = {This paper presents the Period Instrument, an interface for musical expression that requires the input of menstrual blood in order to be played. Drawing from both experimental musical instrument design and feminist design, the resulting object exists both as a sound-making device as well as a vector for challenging normative principles of musical instrument design. Moreover, the Period Instrument represents a particular representation and physical embodiment of time. We will discuss the design and technological development of the instrument, focusing primarily on how designing with and for time constraints can result in new interfaces for musical expression.},
  numpages = {9}
}

@article{nime2024_15,
  author = {Nate Hergert and Dillon Simeone and Doga Cavdir and Duncan MacConnell and Shawn Trail and Myles de Bastion},
  title = {GestoLumina: Gesture interpreted Light, Sound and Haptics. Towards a Framework for Universal Music Design},
  pages = {100--103},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {15},
  track = {Papers},
  doi = {10.5281/zenodo.13904794},
  url = {http://nime.org/proceedings/2024/nime2024_15.pdf},
  presentation-video = {},
  abstract = {Introducing GestoLumina (GeLu), a novel interface by the Universal Music Design (UMD) team at CymaSpace, whose mission is to enhance arts and culture accessibility for the Deaf and Hard of Hearing (DHH). UMD is a program that applies principals taken from the "Universal Design" concept towards music accessibility and inclusion for the DHH. The primary goal of UMD is to create experiential, educational pathways for the DHH to interact with music in a meaningful way. GeLu is the first UMD interface in the framework and is modular in its feature set allowing the user to choose, based on use case, between visual and/or haptic feedback along with gestural sensing. GeLu combines two bracelets: a ring system with gesture sensing and haptic feedback; and an audio-reactive LED color feedback array. The ring system captures tactile input through sensors and provides haptic feedback on the fingertips. The audio-reactive LED color feedback bracelet visualizes audio data, creating dynamic visualizations synchronized with sound. },
  numpages = {4}
}

@article{nime2024_16,
  author = {Jack E Hardwick and Stefano Fasciani and Çağrı Erdem},
  title = {CordChord: A String Instrument with Optical Sensing},
  pages = {104--109},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {16},
  track = {Papers},
  doi = {10.5281/zenodo.13904796},
  url = {http://nime.org/proceedings/2024/nime2024_16.pdf},
  presentation-video = {},
  abstract = {This paper introduces CordChord, a hybrid musical instrument that sees the performer interact with two physical cords to control the pitch, amplitude, and timbre of a two-voice granular synthesiser. An existing method for tracking bowing parameters of string instrument performance is adapted to measure the displacement of the cords by the performer's fingers. The method is based on an array of optical distance sensors in combination with a regression machine learning model to predict the position at and the amount by which each cord is displaced from a baseline tensioned position. Capacitive strips on the back of the neck of the instrument afford additional timbral control. A preliminary user study suggests that performers find the system responsive and engaging to play, while highlighting the need to further improve the accuracy of the regression model to make the system more intuitive.},
  numpages = {6}
}

@article{nime2024_17,
  author = {Vicente E Espinoza and Javier Jaimovich},
  title = {Resurfacing an Enactive Approach for Instrument Design: The case of the Tangible Granular Device},
  pages = {110--116},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {17},
  track = {Papers},
  doi = {10.5281/zenodo.13904798},
  url = {http://nime.org/proceedings/2024/nime2024_17.pdf},
  presentation-video = {https://youtu.be/EnxS3pOvDH8?si=tWeeakhzbKz5FOsu},
  abstract = {This paper proposes a tangible interface for controlling a granular sound engine through the manipulation and exploration of physical materials with granular properties. The design of this Tangible Granular Device was primarily, though not exclusively, guided by the design principles of musical instruments with an enactive approach proposed by O’Modhrain and Essl in 2006, presented after the introduction of PebbleBox and CrumbleBag in 2004.  Even two decades after these tactile interfaces, it remains crucial to question why a well-defined research trajectory on this subject has not been established. This places the search for new connections between granular synthesis techniques and tangible interface design at the core of this work, which aims to explore novel expressive forms of interaction with granular synthesis. To achieve this, an enactive exploration of physical materials with granular properties was conducted, followed by the implementation of an apparatus capable of capturing and recognizing interactions with these materials. Subsequently, the Tangible Granular Device was designed and implemented to facilitate interaction with these materials according to a set of guidelines of tangible interfaces and enactivism in musical instruments. Finally, the paper discusses the outcomes of the process, reflects on the current state of enactive design, and proposes improvements for future versions of this instrument. },
  numpages = {7}
}

@article{nime2024_18,
  author = {Rachel Freire and Courtney N. Reed},
  title = {Body Lutherie: Co-Designing a Wearable for Vocal Performance with a Changing Body},
  pages = {117--126},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {18},
  track = {Papers},
  doi = {10.5281/zenodo.13904800},
  url = {http://nime.org/proceedings/2024/nime2024_18.pdf},
  presentation-video = {},
  abstract = {Research at NIME has incorporated embodied perspectives from design and HCI communities to explore how instruments and performers shape each other in interaction. Material perspectives also reveal other more-than-human factors' influence on musical interaction. We propose an additional, currently unaddressed perspective in instrument design: the influence of the body not only the locus of experience, but as a physical, entangled aspect in the more-than-human musicking. Proposing a practice of "Body Lutherie," we explore how digital instrument designers can honour and work with living, dynamic bodies. Our design of a breath-based vocal wearable instrument incorporated uncontrollable aspects of a vocalist's body and its physical change over different timescales. We distinguish the body in the design process and acknowledge its agency in vocal instrument design. Reflection on our co-design process between vocal pedagogy and eTextile fashion perspectives demonstrates how Body Lutherie can generate empathy and understanding of the body as a collaborator in future instrument design and artistic practice.},
  numpages = {10}
}

@article{nime2024_19,
  author = {Enrique Tomás and Florian Goeschke and Martin  Kaltenbrunner},
  title = {Exploring Design Patterns for Spatial Instruments: User-Driven Strategies, Spatialized Synthesis and Loudspeaker Topologies},
  pages = {127--132},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {19},
  track = {Papers},
  doi = {10.5281/zenodo.13904804},
  url = {http://nime.org/proceedings/2024/nime2024_19.pdf},
  presentation-video = {https://youtu.be/JaKZlAo_KpY?si=H_8mATaHHaWPIdGR},
  abstract = {This paper investigates the field of spatialization tools and controllers within the context of immersive sound formats. After examining conventional approaches, we redefine spatialization tools as musical instruments challenging prevailing views of digital musical instruments confined to interface and synthesis components. Our exploration encompasses the projection, movement, and control of sound objects in diverse loudspeaker setups, emphasizing the reconnection of synthesis and space-related modulations. Addressing the multidimensional challenges associated with sound spatialization, we highlight some observed mapping strategies for the control of gestural and spatial information. In particular, we examine user-driven strategies, such as 'repurposed' controllers, 'interpreted' instruments, custom interfaces, and spatialized synthesis algorithms. Additionally, we introduce design patterns emphasizing an aesthetic standpoint, positioning music as space, advocating for the integration of loudspeaker topologies in the design process, and recoupling synthesis and spatialization. Illustrating our theoretical framework, we present three case studies of spatial digital musical instruments. These examples showcase the integration of spatialization with synthesis, offering a comprehensive approach to sound design. In conclusion, our exploration advocates for spatialization controller design strategies urging a transition to a more user-centric, adaptable, and holistic paradigm.},
  numpages = {6}
}

@article{nime2024_20,
  author = {Ahmet Emin Memis and Stefano Fasciani and Çağrı Erdem},
  title = {The Hyper-Ney: An Enhanced Traditional Flute},
  pages = {133--137},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {20},
  track = {Papers},
  doi = {10.5281/zenodo.13904806},
  url = {http://nime.org/proceedings/2024/nime2024_20.pdf},
  presentation-video = {},
  abstract = {The Hyper-Ney is a hyperinstrument prototype enhancing the traditional ney flute by integrating electronic sensors to expand creative possibilities beyond its affordances. These allow the control of sound synthesis and processing parameters to create a blend of acoustic and synthesized sounds. The core idea is to remap the existing fingering gestures, exploiting acoustically unresponsive holes of the ney flute for enhanced control. Additional interaction methods include lip positioning and instrument movement. We evaluated the system through a study focused on audience reactions, suggesting engaging sonic and visual elements. We complemented this evaluation by including the first author's self-reflections as Hyper-Ney performer.},
  numpages = {5}
}

@article{nime2024_21,
  author = {Zeynep Özcan and Anıl Çamcı},
  title = {Juggling for Beginners: Embracing and Fabricating Failure as Musical Expression},
  pages = {138--141},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {21},
  track = {Papers},
  doi = {10.5281/zenodo.13904808},
  url = {http://nime.org/proceedings/2024/nime2024_21.pdf},
  presentation-video = {https://youtu.be/mIuVu9nslMs?si=UAmFsTeS3HJHrvcc},
  abstract = {In this paper, we discuss a NIME practice that incorporates juggling as a means to explore failure and playfulness in music performance. This practice develops alongside our juggling skills, creating a reciprocal relationship between designing a musical interface and learning how to use it. We first review some of the existing perspectives on error and failure in the arts. We then discuss the technical implementation of our NIME in terms of hardware, software, and sound design. Finally, we offer insights into the role of failure in our practice; first, as an unintentional artifact of learning how to juggle, then as a performative medium that we leverage for musical effect.},
  numpages = {4}
}

@article{nime2024_22,
  author = {Federico Visi},
  title = {The Sophtar: a networkable feedback string instrument with embedded machine learning},
  pages = {142--148},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {22},
  track = {Papers},
  doi = {10.5281/zenodo.13904810},
  url = {http://nime.org/proceedings/2024/nime2024_22.pdf},
  presentation-video = {},
  abstract = {The Sophtar is a tabletop string instrument with an embedded system for digital signal processing, networking, and machine learning. It features a pressure-sensitive fretted neck, two sound boxes, and controlled feedback capabilities by means of bespoke interface elements. The design of the instrument is informed by my practice with hyperorgan interaction in networked music performance. I discuss the motivations behind the development of the instrument and describe its structure, interface elements, and the hyperorgan and sound synthesis interactions approaches it implements. Finally, I reflect on the affordances of the Sophtar and the differences and similarities with other instruments and outline future developments and uses.},
  numpages = {7}
}

@article{nime2024_23,
  author = {Marco Amerotti and Bob L. T.  Sturm and Steve Benford and Hugo Maruri-Aguilar and Craig Vear},
  title = {Evaluation of an Interactive Music Performance System in the Context of Irish Traditional Dance Music},
  pages = {149--153},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {23},
  track = {Papers},
  doi = {10.5281/zenodo.13904812},
  url = {http://nime.org/proceedings/2024/nime2024_23.pdf},
  presentation-video = {https://youtu.be/t_uSNj_QUaQ?si=KSVGYMjsUvgSkonO},
  abstract = {We present a preliminary evaluation of an interactive, real-time, and co-creative performance system for Irish Traditional Dance music. We focus on how this musical partnership is experienced by a human musician performing with it in four aspects: enjoyability, musicality, humanness and responsiveness. Our preliminary study with seven traditional musicians reveals that they find playing with the system to be enjoyable, and appreciated its musicality; but they scored its humanness and responsiveness less highly. These findings suggest that such real-time performance systems might bring an enjoyable "otherness" to musical performance, even for traditional forms of music. Finally, we discuss experimental considerations for a future study involving more participants.},
  numpages = {5}
}

@article{nime2024_24,
  author = {Robert Ek},
  title = {Playing with resistance},
  pages = {154--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {24},
  track = {Papers},
  doi = {10.5281/zenodo.13904814},
  url = {http://nime.org/proceedings/2024/nime2024_24.pdf},
  presentation-video = {https://youtu.be/DyZ1VWlYW1g?si=te4WgD8qY_fVwY4-},
  abstract = {Instrument design is not just a matter of hardware, it also concerns strategies for software mapping of input to output data. I will in this paper report on how an augmented clarinet that I, together with a team at LTU began developing in 2015 has continued to develop over the past seven years. The focus will be on the development of artistic applications within the system. Performers frequently describe the resistance of their instrument as a manifestation of the challenges they encounter when playing. One can argue that the goal of a skilled performer is to get rid of resistance, but it is in fact a central part of the relationship between performer and instrument. Acquiring technique and skill seems to be a way for the performer not to overcome resistance but to learn how the instrument responds to force. Realizing the importance of resistance in the artistic process we need to ask ourselves; how can we use this knowledge when creating new instruments? Returning to video documentation of performances with two different mappings and through stimulated recall analysis I seek a deeper understanding of how software, mapping and performance practice interacts, forming the basis of the artistic expression.},
  numpages = {6}
}

@article{nime2024_25,
  author = {Lucija Ivsic and Jon McCormack and Vincent Dziekan},
  title = {Transhuman Ansambl - Voice Beyond Language},
  pages = {160--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {25},
  track = {Papers},
  doi = {10.5281/zenodo.13904816},
  url = {http://nime.org/proceedings/2024/nime2024_25.pdf},
  presentation-video = {https://youtu.be/uDO21ZtDK2s?si=yhZuh7upei0x4E8A},
  abstract = {In this paper we present the design and development of the \emph{Transhuman Ansambl}, a novel interactive singing-voice interface which senses its environment and responds to vocal input with vocalisations using human voice. Designed for live performance with a human performer and as a standalone sound installation, the \emph{ansambl} consists of sixteen bespoke virtual singers arranged in a circle. When performing live, the virtual singers listen to the human performer and respond to their singing by reading pitch, intonation and volume cues. In a standalone sound installation mode, singers use ultrasonic distance sensors to sense audience presence. Developed as part of the 1st author's practice-based PhD and artistic practice as a live performer, this work employs the \emph{singing-voice} to explore voice interactions in HCI beyond language, and innovative ways of live performing. How is technology supporting the effect of intimacy produced through voice? Does the act of surrounding the audience with responsive virtual singers challenge the traditional roles of performer-listener? To answer these questions, we draw upon the 1st author's experience with the system, and the interdisciplinary field of voice studies that consider the voice as the sound medium independent of language, capable of enacting a reciprocal connection between bodies.},
  numpages = {8}
}

@article{nime2024_26,
  author = {Margaret Schedel and Gorka Egino and Sandra Muciño and Boris Shershenkov and Sofy Yuditskaya},
  title = {Suggested Practices for Creating an Inclusive Hackerspace for Music/Sound/NIMEs},
  pages = {168--171},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {26},
  track = {Papers},
  doi = {10.5281/zenodo.13904818},
  url = {http://nime.org/proceedings/2024/nime2024_26.pdf},
  presentation-video = {},
  abstract = {This paper provides guidance on establishing inclusive hackerspaces for creating NIMEs, filling a gap in literature on feminist approaches to community hacking in music and sound. It advocates for spaces that promote innovation in music technology, emphasizing inclusivity, diversity, and equal opportunity. The authors draw from feminist literature to critically redefine terms like 'makerspace' and 'best practices' to and discuss the key features of hackerspaces, the role of community self-management in fostering inclusivity, and offers practical tips for building an inclusive musical hackerspace that prioritizes empowerment and community involvement. It concludes by stressing the importance of a nuanced approach in developing audio-centric hackerspaces, highlighting technological skill, community engagement, and ongoing self-reflection to ensure these spaces are welcoming to everyone.},
  numpages = {4}
}

@article{nime2024_27,
  author = {Michaella J Moon and Jim Murphy and Ajay Kapur and Dale Carnegie},
  title = {Overview of NIME Techniques Applied to Traditional Korean Instruments},
  pages = {172--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {27},
  track = {Papers},
  doi = {10.5281/zenodo.13904820},
  url = {http://nime.org/proceedings/2024/nime2024_27.pdf},
  presentation-video = {},
  abstract = {This article offers an overview of the technological advancements in traditional Korean music instruments, also known as Gugak. The projects detailed in this article show how sensors, physical controls, alternative designs and materials, and software development can be used to enhance performance options and adapt to modern performance settings and needs. Although there are fewer Gugak NIME projects and research available compared to Western instruments, this genre has the potential to inspire various areas of ongoing research, such as specialised signal processing, sensor applications, and interactive educational platforms. This paper is also the first review article that summarises the current state of technological advances and applications specifically for Gugak instruments and musical genres.},
  numpages = {6}
}

@article{nime2024_28,
  author = {Luciana Perc},
  title = {Sonic Wings: A Wearable Live Electronics Device for Performing Mixed Music},
  pages = {178--180},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {28},
  track = {Papers},
  doi = {10.5281/zenodo.13904822},
  url = {http://nime.org/proceedings/2024/nime2024_28.pdf},
  presentation-video = {https://youtu.be/QFB3Mzy-Prc?si=4fnY8bVSdRSPZznq},
  abstract = {This paper introduces the Sonic Wings, a wearable device that can capture audio, apply computational processes, and output new audio data in real time. This study covers the device’s construction and the composition and performance of a stochastic solo piece of mixed music for flute and live electronics using the device. The Sonic Wings were developed using found speakers inside cardboard-made acoustic waveguides, a microcomputer accessed remotely, and a portable audio interface connected to a clippable microphone. Mounted to the performer’s body, this system allows the performer to move freely while playing and interacting with the device across a performance space in which audiences stand and move spontaneously. This study’s approach to interactions with body-mounted interfaces in musical performance challenges notions of control introducing notions of companionship. Engaging with the figure of the cyborg as proposed by Haraway, this paper discusses how the development of such device unfolds the embodiment of hybrid ontologies through musical performance enabling interactions with audiences through movement across an acoustically evolving space.},
  numpages = {3}
}

@article{nime2024_29,
  author = {Alon A Ilsar and Razzly Ilsar},
  title = {How Musical Is Dog? - An Interspecies Improvised Musical Collaboration},
  pages = {181--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {29},
  track = {Papers},
  doi = {10.5281/zenodo.13904826},
  url = {http://nime.org/proceedings/2024/nime2024_29.pdf},
  presentation-video = {https://youtu.be/hS0NH2hnhv8?si=Xj_jMytfxIQAdRfz},
  abstract = {This paper outlines an interspecies improvised musical collaboration with Razzly the dog that utilises a pre-existing gestural DMI, the AirSticks, inside a fetch ball. The evolution of the collaboration across three significantly different performances over a year and across two cities is  described, along with an outline of the mappings created. Through the lens of my experience as an improviser, instrument designer and dog guardian, and drawing from research into dog cognition, animal liberation, human-animal interaction, animal-computer interaction, zoömusicology and posthumanism, I explore the phenomena that is dog-human play, and draw comparisons between it and collaborative musical improvisation. Through the act of turning play into a musical performance, I discuss creativity, agency and consent, focusing on the social, collaborative and physical aspects of musicking, as opposed to the sound making itself, in an attempt to understand the way dogs (starting with this particular dog) might use play, ritual and perhaps even music-making, to navigate the world and connect with humans.},
  numpages = {7}
}

@article{nime2024_30,
  author = {Anna Savery and Sam Ferguson and Andrew Johnston},
  title = {Gesture and Narrative: Blending Human Performance with
Visual Storytelling},
  pages = {188--193},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {30},
  track = {Papers},
  doi = {10.5281/zenodo.13904828},
  url = {http://nime.org/proceedings/2024/nime2024_30.pdf},
  presentation-video = {https://youtu.be/9f1XSgL9MzI?si=Aa4IKVrsa8DaO96J},
  abstract = {Kind Regards-for a friend is a narrative-driven audiovisual composition that examines the interplay between a human performer and a visual agent. This project was integrated with the development of a new musical interface for the violin bow, and encompassed various strategies for gesture mapping solutions and narrative development. An online audience response survey examined audience experiences of the piece, garnering insights into the effectiveness of our creative and technical processes. Reflections on the project underscored its narrative and interactive strengths, while also identifying music and visual elements that could be further refined to augment the immersive quality of the experience.},
  numpages = {6}
}

@article{nime2024_31,
  author = {Zétény Nagy and Kristin Carlson and Greg J Corness},
  title = {Designing a new virtual reality interface for interacting with audio spatialization backends},
  pages = {194--197},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {31},
  track = {Papers},
  doi = {10.5281/zenodo.13904830},
  url = {http://nime.org/proceedings/2024/nime2024_31.pdf},
  presentation-video = {},
  abstract = {xrOSC presents an alternative spatialization tool for composers working in 3D sound formats. We developed this tool to have an easier learning curve, and a low barrier of entry. xrOSC is an isotonic mixed reality controller interface with six degrees of freedom and direct absolute input designed for standalone extended reality devices. By utilizing hand tracking and gestural control, we can enable more natural and intuitive positioning of sound sources in 3D audio spatialization of the external composed space, as opposed to using spatialization tools on a traditional 2D display with mouse/keyboard input methods. This software is designed specifically as a control method, as there is no processing done on the device. xrOSC simply sends control messages over the network to a spatialization backend using the Open Sound Control protocol. This paper discusses the design process and methodological considerations in order to develop a tool for composers to more easily create spatial-considered musical compositions.},
  numpages = {4}
}

@article{nime2024_32,
  author = {Sergey K Kasich},
  title = {Possible applications of knots in Computer Music and NIMEs},
  pages = {198--205},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {32},
  track = {Papers},
  doi = {10.5281/zenodo.13904832},
  url = {http://nime.org/proceedings/2024/nime2024_32.pdf},
  presentation-video = {https://youtu.be/3iasVFJfbzs?si=MriIQJdSZzzmmRjo},
  abstract = {In this paper, we endeavor to examine specific phenomena - knots - which are subjects of study in various scientific disciplines, including a particular field within mathematics. Our main goal is to explore the possibilities that knots open for computer music, particularly for NIME researchers. We aim to achieve this goal by analyzing four aspects of knots: topology, geometry, physics, and semantics. Subsequently, we apply these aspects to areas of computer music with examples, some of which are accompanied by proof-of-concept models, while others remain purely conceptual, awaiting further practical research. Although most cases draw inspiration from mathematical Knot Theory, not all strictly adhere to its conditions.},
  numpages = {8}
}

@article{nime2024_33,
  author = {Alberto Boem and Matteo Tomasetti and Luca Turchet},
  title = {Harmonizing the Musical Metaverse: unveiling needs, tools, and challenges from experts’ point of view},
  pages = {206--214},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {33},
  track = {Papers},
  doi = {10.5281/zenodo.13904834},
  url = {http://nime.org/proceedings/2024/nime2024_33.pdf},
  presentation-video = {https://youtu.be/N5sDikp-RLI?si=cB_mPdpJWaXKwBHX},
  abstract = {The Musical Metaverse (MM) represents an innovative frontier for the field of New Interfaces for Musical Expression (NIME). The MM holds the potential to redefine areas such as musical composition and performance via immersive environments based on technologically mediated social interactions. Despite substantial research on single-user immersive systems, the intersection of NIME and the MM remains largely unexplored. In this paper, we systematically explore this domain by examining previous and current approaches, alongside conducting interviews with eleven experts who have created multi-user immersive musical environments and authored publications on this topic. The goal is to map such an uncharted territory by collecting valuable insights and leveraging the perspective of experts to provide an understanding of the potentials and challenges inherent in creating immersive social environments for musical activities. Our results reveal that existing multi-user immersive environments make use of diverse implementation approaches but face challenges due to the absence of standardized technology stacks, particularly in networking and data synchronization.},
  numpages = {9}
}

@article{nime2024_34,
  author = {Austin A Franklin and Henrik Frisk and Rikard Lindell},
  title = {Sonic Serendipity: Embracing Discovery in File Finder-Based Improvisation},
  pages = {215--220},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {34},
  track = {Papers},
  doi = {10.5281/zenodo.13904836},
  url = {http://nime.org/proceedings/2024/nime2024_34.pdf},
  presentation-video = {https://youtu.be/iYpBfM64YEU?si=-25UUFgJ7GqcK6OC},
  abstract = {This paper describes the design and development of The Unfinder, a prototypical interface that allows users to search and improvise music with audio files from a large local repository using music information retrieval based on content and metadata. The research is part of an ongoing project (IRESAP) concerned with tools incorporating current music information retrieval strategies that both support artistic practices and have utility outside of a performance setting. In The Unfinder, we aim to exploit the balance between accurately and reliably retrieving audio material from a file search system, and the potential for failure in the system to do so. Our prior research is used to frame design choices which are measured with a user study used to evaluate the interface. In the study we observed nine users of varying musical backgrounds playing with the instrument while taking notes of their utterances and ideas. The transcriptions of the user’s comments were analyzed using a thematic analysis method and five (5) themes were identified: perception, parameterization, identity, agency, and imaginaries. These themes indicate that the interface design is promising for artistic output, the use of a single feature for searching does not have much perceptual relevance, and the chosen features are useful for discovering audio files within serendipitous musical situations.},
  numpages = {6}
}

@article{nime2024_35,
  author = {Alberto Boem and Matteo Tomasetti and Alessio Gabriele and Agostino Di Scipio and Luca Turchet},
  title = {User needs in the Musical Metaverse: a case study with electroacoustic musicians},
  pages = {221--229},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {35},
  track = {Papers},
  doi = {10.5281/zenodo.13904838},
  url = {http://nime.org/proceedings/2024/nime2024_35.pdf},
  presentation-video = {https://youtu.be/l5tP2uFtpKg?si=MijFnSO4jt9ilPLw},
  abstract = {The Musical Metaverse (MM) is expected to pave the way for a new era of musical activities in immersive, and technology-mediated environments. However, research on the MM is still in its infancy. Until now, the attention has primarily been directed toward engineering and artistic issues. Few studies have explored the uses and desires of stakeholders in immersive environments, particularly in terms of how these environments can best support musicians’ needs for musical composition, performance, and education. This study aims to explore the needs of electroacoustic composers and musicians in a Metaverse dedicated to musical activities. For this, we conducted a half-day workshop with fourteen participants to collect data. Eleven user needs and potential issues were identified and discussed. The reported findings may contribute to enhance the understanding of the user experience in the MM by considering the needs and requirements expressed by specific stakeholders and early adopters, such as electroacoustic practitioners.},
  numpages = {9}
}

@article{nime2024_36,
  author = {Graham Dunning},
  title = {Ironing In The Creases: Developing An Idiosyncratic Electro-mechanical Musical Instrument By Reinforcing Its Faults},
  pages = {230--240},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {36},
  track = {Papers},
  doi = {10.5281/zenodo.13904840},
  url = {http://nime.org/proceedings/2024/nime2024_36.pdf},
  presentation-video = {https://youtu.be/IL9ADTHZPYw?si=qysv5Ki0SuYP6Td_},
  abstract = {This paper proposes an unusual approach to the development of a musical performance setup through an iterative process which seeks to enhance the errors, faults and shortcomings of the system rather than refine, improve or fine tune them. This particular instrument design approach works in parallel with a performance practice centered on live construction of new music, refinement of a groove and the creative process of troubleshooting. In my practice I use an extended turntable system, developing new electro-mechanical interfaces between a record player and various other devices. The paper describes my approach to developing the Mechanical Techno project, using the system in live musicking contexts and making iterations of the setup. The aesthetic aims of the project are defined in order to highlight the types of mechanical and electronic errors and flaws which are important to its success. Several specific examples are given by way of illustration, demonstrating how physical wobbles, imprecise triggering and out-of-sync mechanisms can lead to interesting and idiosyncratic elements in live performances and recorded compositions. The approach is summarised as a process of 'ironing in the creases': recognising the system's problems and – counterintuitively – deliberately emphasising them.},
  numpages = {11}
}

@article{nime2024_37,
  author = {Davide Lionetti and Luca Turchet and Massimiliano Zanoni and Paolo Belluco},
  title = {Muscle-Guided Guitar Pedalboard: Exploring Interaction Strategies Through Surface Electromyography and Deep Learning},
  pages = {241--251},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {37},
  track = {Papers},
  doi = {10.5281/zenodo.13904842},
  url = {http://nime.org/proceedings/2024/nime2024_37.pdf},
  presentation-video = {},
  abstract = {This paper explores a method to innovate the conventional interaction with a guitar pedalboard. By analyzing muscular contractions tracked via surface Electromyography (sEMG) wearable sensors, we aimed to investigate how to dynamically track guitarists’ sonic intentions to automatically control the guitar sound. Two Recurrent Neural Networks based on Bidirectional Long-Short Term Memory were developed to analyze sEMG signals in real-time. The system was designed as a digital musical instrument that calibrates itself to each user during an initial training process. During training musicians provide their gestural vocabulary, associating each gesture to a corresponding pedalboard preset. The selection of the most effective features, in synergy with the best set of muscles, was conducted to optimize the learning rate of the system. The system was assessed with a user study encompassing seven expert guitar players. Results showed that, on average, participants appreciated the concept underlying the system and deemed it to be able to foster their creativity.},
  numpages = {11}
}

@article{nime2024_38,
  author = {Pedro P Lucas and Stefano Fasciani and Alexander Szorkovszky and Kyrre Glette},
  title = {Interactive Sonification of 3D Swarmalators},
  pages = {252--260},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {38},
  track = {Papers},
  doi = {10.5281/zenodo.13904846},
  url = {http://nime.org/proceedings/2024/nime2024_38.pdf},
  presentation-video = {},
  abstract = {This paper explores the sound and music possibilities obtained from the sonification of a swarm of coupled oscillators moving in a virtual space called "Swarmalators". We describe the design and implementation of a Human-Swarm Interactive Music System based on the 3D version of the Swarmalator model, which is used for signal analysis of the overall sound output in terms of scalability; that is, the effect of varying the number of agents in a swarm system. We also study the behaviour of autonomous swarmalators in the presence of one user-controlled agent, which we call the interactive swarmalator. We observed that sound frequencies barely deviate from their initial values when there are few agents, but they diverge significantly in a highly dense swarm. Additionally, with the inclusion of the interactive swarmalator, the group's behaviour tends to adjust towards it. We use these results to explore the potential of swarmalators in music performance under various scenarios. Finally, we discuss opportunities and challenges to use the Swarmalator model for sound and music systems.},
  numpages = {9}
}

@article{nime2024_39,
  author = {Behzad Haki and Nicholas Evans and Sergi Jordà},
  title = {GrooveTransformer: A Generative Drum Sequencer Eurorack Module},
  pages = {261--265},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {39},
  track = {Papers},
  doi = {10.5281/zenodo.13904848},
  url = {http://nime.org/proceedings/2024/nime2024_39.pdf},
  presentation-video = {},
  abstract = {This paper presents the GrooveTransformer, a Eurorack module designed for generative drum sequencing. Central to its design is a Variational Auto-Encoder (VAE), around which we have designed a deployment context enabling performance through accompaniment and/or user interaction. This module allows the user to use the system as an accompaniment generator while interacting with the generative processes in real-time. In this paper, we review the design principles and technical architecture of the module, while also discussing the potentials and short-comings of our work.},
  numpages = {5}
}

@article{nime2024_40,
  author = {Nicolo Merendino and Mela Bettega and Adam Pultz Melbye and John D. Sullivan and Antonio Rodà and Raul Masu},
  title = {Sustainable digital fabrication in NIME: Nine sustainability strategies for DMI production},
  pages = {266--274},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {40},
  track = {Papers},
  doi = {10.5281/zenodo.13904850},
  url = {http://nime.org/proceedings/2024/nime2024_40.pdf},
  presentation-video = {https://youtu.be/xTGQpNpzD3s?si=AWFat36fHE3BYHCw},
  abstract = {Sustainable NIME practices have gained significant attention in the past few years. This article further develops this perspective by proposing a set of strategies for sustainable digital fabrication processes, which is an important aspect of Digital Musical Instruments (DMIs) creation. We grounded our strategies on recent literature presented at NIME combined with state-of-the-art literature and policy on sustainable products. To start understanding how these strategies could be perceived and adopted by DMI makers, we run a workshop at iii (Instruments Inventors Initiative) - an incubator of musicians/makers. In the workshop we discussed some of these strategies in order to understand how they resonated with our participants. The article concludes by positioning our contributions within the broader context of research within the NIME community and related fields, offering a perspective on how these strategies might influence sustainable practices in DMI production.},
  numpages = {9}
}

@article{nime2024_41,
  author = {Emmanouil Dimogerontakis and Dan Overholt and Stefania Serafin},
  title = {MusiCane: an Accessible Digital Instrument inspired by the white cane},
  pages = {275--281},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {41},
  track = {Papers},
  doi = {10.5281/zenodo.13904852},
  url = {http://nime.org/proceedings/2024/nime2024_41.pdf},
  presentation-video = {https://youtu.be/0vuHWlc_o3M?si=NpgY4Pm1iAkvyd51},
  abstract = {We introduce the design and implementation of MusiCane, a musical device with the aim of creating new accessible avenues for music-making to promote mutual engagement across diverse social groups. MusiCane offers the possibility for active participatory music making, thereby broadening electronic music interactions and aesthetics. It seeks to bridge different communities through playful and engaging means, particularly including individuals who face barriers in traditional music-making practices. This Accessible Digital Musical Instrument (ADMI) is conceptualized based on insights derived from discussions and meetings with blind individuals and therapists. Its primary objective is to explore the creative potential of the white cane as an interactive medium. The design process involved incorporating feedback and perspectives from these stakeholders, to ensure the device's relevance and effectiveness in addressing the unique needs and experiences of users with visual impairments. To realize these objectives, a musical interactive installation has been produced, marking the first iteration of a prototype for a multi-user experience. The project not only contributes to the inclusive design of musical instruments, but also strives to create an environment where individuals from various backgrounds can come together, fostering collaboration, creativity, and engagement in the realm of music.},
  numpages = {7}
}

@article{nime2024_42,
  author = {Jack Armitage and Victor Shepardson and Thor Magnusson},
  title = {Tölvera: Composing With Basal Agencies},
  pages = {282--291},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {42},
  track = {Papers},
  doi = {10.5281/zenodo.13904854},
  url = {http://nime.org/proceedings/2024/nime2024_42.pdf},
  presentation-video = {https://youtu.be/z_xZr0TXHsY?si=G7Qy7q5OcnaiKKsx},
  abstract = {Diverse intelligence is an emerging field that views intelligence as a spectrum, drawing insights from e.g. cell-based models and their evolution, and recognising the combined agential properties of biological and engineered materials across disciplines. Within diverse intelligence, basal cognition encapsulates the simpler end of the continuum, focusing on broadly applicable insights from the behaviour of single-celled organisms and simulation. Based on a desire for more diversity of real-time AI in NIME, we developed a library called Tölvera, initially for composable artificial life. In this paper we present Tölvera's design and the practice-based methodology that drove it, reviewing artistic works and emergent themes from design-practice iteration cycles. We reflect on how an early influence of artificial life gave way to an interest in reading Tölvera as a basal art medium, and how unexpected tendencies and capabilities play in perturbative aesthetic tension with compositional decisions. We describe how basal agency research, aesthetics and toolkits are influencing the direction of both design and practice, and review work-in-progress features. Finally, we reflect on how a cell's eye view of agency is shaping our thinking towards AI in NIME.},
  numpages = {10}
}

@article{nime2024_43,
  author = {Alessandro Fiordelmondo and Giada Zuccolo and Sergio Canazza and Raul Masu},
  title = {Longevity in NIME research: a case study using time-based media art preservation models},
  pages = {292--301},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {43},
  track = {Papers},
  doi = {10.5281/zenodo.13904858},
  url = {http://nime.org/proceedings/2024/nime2024_43.pdf},
  presentation-video = {https://youtu.be/rvFlk3ZsVZg?si=Vue2KyLJvwRnusCh},
  abstract = {This paper presents the reactivation of Soundrise, an interactive multimedia application for deaf children, as a case study to demonstrate the application of time-based media art preservation and reactivation strategies in NIMEs. Drawing upon the Multilevel Dynamic Preservation (MDP) model and the Digital Preservation Object (DPO), structured frameworks designed to comprehensively document time-based media art across different levels of information and iterative processes, this article introduces a novel decision-making process for reactivating NIMEs, outlined in five steps: Collection, Assessment, (re)Design, Implementation, and Archiving. Through an exploration of the Soundrise reactivation, the article elaborates on each step of the proposed reactivation process, illustrating the application of the DPO and MDP model to ensure the preservation of the application. From the analysis of the previous iterations of the application, we also provide design reflections on obsolescence and longevity. By aligning our work with recent NIME literature on documentation and reuse, we aim to offer insights on how to preserve, reuse, or maintain and document research.},
  numpages = {10}
}

@article{nime2024_44,
  author = {Adam Tindale and Colin Clark},
  title = {Reshaping Time - Exploring grid interfaces for ansiorhythmic patterns},
  pages = {302--305},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {44},
  track = {Papers},
  doi = {10.5281/zenodo.13904860},
  url = {http://nime.org/proceedings/2024/nime2024_44.pdf},
  presentation-video = {https://youtu.be/VFYrzlixpmU?si=OV_FC0a85BZ8nc0A},
  abstract = {Grid layouts are popular in music controllers. While they are excellent for many tasks, they usually require a predefinition of a single subdivision to be represented. In this paper we explore techniques for visually representing rhythmic phrases that have multiple subdivisions and tuplet groupings within a grid interface. The paper proposes an ansiorhythmic grid notation system for expressing rhythms that may vary their length, subdivision or phase within a grid structure that has historically been limited to one subdivision per sequence. A proof-of-concept Max/MSP demonstrates a tactile interface for a polyrhythmic, polymetric, and polyphasic sequencer. },
  numpages = {4}
}

@article{nime2024_45,
  author = {Sofya Yuditskaya},
  title = {Exploring Diverse Forms of Bareëmins: A Multifaceted Study on Painted Panels and Sculptures as The Site of Performance, Instruments, and Scores},
  pages = {306--309},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {45},
  track = {Papers},
  doi = {10.5281/zenodo.13904862},
  url = {http://nime.org/proceedings/2024/nime2024_45.pdf},
  presentation-video = {https://youtu.be/_zfI3c8UQC0?si=CSzrOdbYYetwRo1L},
  abstract = {This paper delves into the realm of NIMES called Bareëmins, a distinctive adaptation of the theremin implemented on the Arduino platform, featuring artistically crafted capacitive antennae. Drawing inspiration from the original theremin and the extensive instrument collection of Leon Theremin, Bareëmins represent a fusion of artistic expression and technological innovation. The study explores the transition from traditional theremins, commonly recreated in music programming classes, to the visually engaging Bareëmin family. In this evolution, the conventional antennae are replaced by creatively envisioned antennae, becoming a defining feature across varied shapes and sizes, categorized into handheld, environmental, and painted score species. Rooted in the artist's synesthetic perception, where sight and sound converge, Bareëmins embody diegetic shapes and colors. The antennae, made from conductive paint, copper tape, or found materials, showcase a commitment to sustainability and inclusivity. The paper provides instructions for crafting Bareëmins using readily available materials, fostering an environmentally conscious approach. By enhancing the artistic appeal and making the construction accessible, Bareëmins bridge the gap between visual artistry and musicality, offering a unique and inclusive avenue for exploring the intersection of art and technology in the realm of musical instrument design.},
  numpages = {4}
}

@article{nime2024_46,
  author = {Marinos Koutsomichalis},
  title = {Two experimental instruments inspired by radio technologies},
  pages = {310--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {46},
  track = {Papers},
  doi = {10.5281/zenodo.13904864},
  url = {http://nime.org/proceedings/2024/nime2024_46.pdf},
  presentation-video = {},
  abstract = {This short paper accounts for (a) a wearable radio receiver that can be operated in the fashion of a monochord string instrument, and (b) for a rotating induction coil. Technical aspects are discussed and the process is exposed as laboratory experimentation with radio technologies.},
  numpages = {5}
}

@article{nime2024_47,
  author = {Jonathan M Pigrem and Justin Christensen and Andrew McPherson and Renee Timmers and Luc de Witte and Jennifer MacRitchie},
  title = {Agency and Creativity in Musical Interaction for those living with Dementia and Cognitive Decline},
  pages = {315--323},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {47},
  track = {Papers},
  doi = {10.5281/zenodo.13904867},
  url = {http://nime.org/proceedings/2024/nime2024_47.pdf},
  presentation-video = {},
  abstract = {Musical interventions are becoming a more popular tool in dementia care. Although the use of music is developing in a range of contexts such as choirs, song writing groups, and more specific therapies, these often rely on musical knowledge or the expertise of facilitators. Limited tools are available which facilitate unguided musical experiences, fostering agency for their users through musical creativity. We present a workshop-based study exploring the use of a procedural music platform designed for those living with dementia and cognitive decline. The paper takes a mixed-methods approach, exploring a range of procedural processes, and reviewing participant engagement during their use. We demonstrate the use of the platform and highlight its potential for engagement. We evaluate the techniques implemented and demonstrate an inverse relationship between operational complexity and interaction. We conclude it is possible to facilitate engaging musical interactions which foster agency and creativity while maintaining rich and age-appropriate outputs.},
  numpages = {9}
}

@article{nime2024_48,
  author = {Gaël Stéfan Moriceau and Yulin Yan and Marcelo Wanderley and Dominic Thibault},
  title = {The Obstacle Course of DMI Performance: Two Case Studies with T-Stick and Karlax},
  pages = {324--332},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {48},
  track = {Papers},
  doi = {10.5281/zenodo.13904869},
  url = {http://nime.org/proceedings/2024/nime2024_48.pdf},
  presentation-video = {https://youtu.be/ai4-aFr82hk?si=gl_zHEIEauaHqfMe},
  abstract = {This article presents two case studies featuring DMIs, the T-Stick and the Karlax, aiming to showcase various technical and performance obstacles encountered in existing and new compositions for novel instruments. The analysis covers different stages of a performance, including preparatory processes, practice sessions, and the actual performance, highlighting challenges faced and solutions implemented. The support obtained through communicating with composers is emphasized. Additionally, the discussion delves into technical considerations, score interpretations, and playing techniques specific to the two DMIs, contributing to a deeper understanding of performance practice. These case studies can inspire DMI designers when creating their instruments so that barriers to performance can be eliminated. Furthermore, the findings underscore the necessity for composers to create documentation with comprehensive information, facilitating a thorough understanding for performers of both the instrument’s manipulations and their artistic vision.},
  numpages = {9}
}

@article{nime2024_49,
  author = {Nicholas Shaheed and Ge Wang},
  title = {I Am Sitting in a (Latent) Room},
  pages = {333--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {49},
  track = {Papers},
  doi = {10.5281/zenodo.13904872},
  url = {http://nime.org/proceedings/2024/nime2024_49.pdf},
  presentation-video = {https://youtu.be/BfasOUklu7I?si=KisN4odmQE0CcxcB},
  abstract = {In this paper we describe I Am Sitting in a (Latent) Room, a real-time structured group improvisation system inspired by Alvin Lucier's "I Am Sitting in a Room," and the general process of degrading sound by repeatedly passing it through an acoustic medium. But there is a twist. Unlike "I Am Sitting in a Room," which  unfolds as a gradual process with no further interaction once the process has begun, I Am Sitting in a (Latent) Room gives the improvisers the ability to intervene and interact with the process of degradation in real time. An audio clip is repeatedly encoded and decoded through two parallel instances of a bespoke variational autoencoder (VAE) model. On top of this process, the performers manipulates the model's latent embeddings in real-time, exploring the latent space (or "room") of the model over the course of the performance. Two performances with the composer and live-coding duo RGGTRN are presented. This work explores human-in-the-loop AI systems through group improvisation, interactive AI performance, and creating datasets as a part of the compositional process.},
  numpages = {6}
}

@article{nime2024_50,
  author = {Andrew McMillan and Fabio Morreale},
  title = {Occupational Therapy Methods in the Design of Accessible Musical Instruments},
  pages = {339--345},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {50},
  track = {Papers},
  doi = {10.5281/zenodo.13904874},
  url = {http://nime.org/proceedings/2024/nime2024_50.pdf},
  presentation-video = {https://youtu.be/itl1UgexT-w?si=SbonCYpgmVm0vIiB},
  abstract = {This study explores the integration of Occupational Therapy techniques within the evaluation of movement and functionality concerning users of Accessible Musical Instruments (AMI). It examines the current application of these techniques in design methodologies and contemplates their potential adaptation and incorporation into AMI design. The paper presents findings derived from a conventional occupational therapy approach alongside two inquiries that modify and expand upon this approach through the integration of sensor technologies. The outcomes of two tasks, each employing sensor technology to gauge and appraise movement and functionality, are presented. These findings are to illustrate how designers can integrate methodologies pertaining to the analysis of a range of movement and functionality within their design frameworks.},
  numpages = {7}
}

@article{nime2024_51,
  author = {Adinda van 't Klooster and Nick Collins},
  title = {Voice Responsive Virtual Reality},
  pages = {346--350},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {51},
  track = {Papers},
  doi = {10.5281/zenodo.13904876},
  url = {http://nime.org/proceedings/2024/nime2024_51.pdf},
  presentation-video = {https://youtu.be/oLGdSgaLe3Q?si=1HBogOyqhvSZvC5d},
  abstract = {Our audio-reactive Virtual Reality (VR) interface encourages a performer to explore extended vocal techniques within an alternative visual aesthetic experience. We are interested in whether this can support creative vocal expression, and whether the sound-responsive animated hand-drawn aesthetics in the VR scene can help immersion and wellbeing. As an interface for musical expression, there is no extraneous sound output, but rather, an attempt to encourage investigation of a user’s own voice through a visual feedback system. We describe the technicalities of building the system as an Oculus Quest app, and evaluation through various existing theories pertinent to VR musical interaction.},
  numpages = {5}
}

@article{nime2024_52,
  author = {Andrew Zhu and Ge Wang},
  title = {ChuGL: Unified Audiovisual Programming in ChucK},
  pages = {351--358},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {52},
  track = {Papers},
  doi = {10.5281/zenodo.13904878},
  url = {http://nime.org/proceedings/2024/nime2024_52.pdf},
  presentation-video = {https://youtu.be/qj0DL-KgtqQ?si=kEgdzA7WJDeB_SbT},
  abstract = {ChuGL (sounds like "chuckle"; rhymes with "juggle") is a unified audiovisual programming framework built into the ChucK language. It extends ChucK's strongly-timed, concurrent programming model with a 3D rendering engine and a new paradigm for coding real-time graphics and audio. ChuGL introduces the notion of a Graphics Generator (GGen) that can be manipulated sample-synchronously alongside audio unit generators (UGens) to unify graphics and audio within a single strongly-timed language. Under the hood, this is made possible by a multithreaded scenegraph architecture that provides low-latency, high performance audiovisual synchronization. In this paper we present the design ethos of ChuGL, describe its integrated graphics-and-audio workflow, highlight architectural decisions, and present an evaluation of ChuGL as a tool for expressive audiovisual design, used in a computer music programming course at Stanford University. ChuGL transforms ChucK into a standalone audiovisual programming language, and argues for a way of thinking and doing in which audio and graphics are given equal importance.},
  numpages = {8}
}

@article{nime2024_53,
  author = {Ciaran Frame},
  title = {Concerts of the Future: Designing an interactive musical experience in VR},
  pages = {359--366},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {53},
  track = {Papers},
  doi = {10.5281/zenodo.13904880},
  url = {http://nime.org/proceedings/2024/nime2024_53.pdf},
  presentation-video = {https://youtu.be/0Gu9NWEACxQ?si=OLDcbzRt38erFRzV},
  abstract = {This paper examines the creation and development of "Concerts of the Future," a Virtual Reality music experience that bridges the gap between music listening and active participation. Collaboratively developed with a chamber ensemble over eight months, the narrative-driven experience places participants alongside a live ensemble, enabling them to perform with a unique gestural instrument in a VR concert setting regardless of musical experience. Reflecting on previous work in the VR and interactive music field, the paper outlines key design decisions that were made in the development of the work over a period of 8 months, including a pilot presentation with participants. The paper discusses the implications of stylised VR design and the use of theatrical elements outside the digital environment to make music more accessible. It highlights the potential of VR in transforming the traditional roles of composer, performer, and listener, thus expanding the scope of participatory musical experiences.},
  numpages = {8}
}

@article{nime2024_54,
  author = {João Tragtenberg and Filipe Calegario and Marcelo Wanderley and Virgínia Pereira Cavalcanti},
  title = {Designing DMIs with(in) a Music Culture: A Participatory Design Process with the Xambá Quilombola Community},
  pages = {367--376},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {54},
  track = {Papers},
  doi = {10.5281/zenodo.13904882},
  url = {http://nime.org/proceedings/2024/nime2024_54.pdf},
  presentation-video = {},
  abstract = {This paper presents a participatory design process with the Xambá community to create new Digital Musical Instruments (DMIs) emphasizing the integration within their music culture. The project yielded two novel instruments, the Agbaixo, and Botões Falantes, engaging the community in every stage and considering the traditional instruments' entanglements in the materiality, corporeality, and sonorities of their music culture. This process was inspired by Paulo Freire's dialogical methods. It was conducted with community members of several ages and musical experience in a workshop over two months. The designed instruments were preliminary evaluated by community members in an informal context. The implications of this work, apart from creating instruments in conjunction with their final players, include an attempt to inspire a new bottom-up design process centered on existing communities situated in specific music cultures.},
  numpages = {10}
}

@article{nime2024_55,
  author = {Jordie Shier and Charalampos Saitis and Andrew Robertson and Andrew McPherson},
  title = {Real-time Timbre Remapping with Differentiable DSP},
  pages = {377--385},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {55},
  track = {Papers},
  doi = {10.5281/zenodo.13904884},
  url = {http://nime.org/proceedings/2024/nime2024_55.pdf},
  presentation-video = {https://youtu.be/OISsdBFzSh8?si=0axzhHntFfx_XUaV},
  abstract = {Timbre is a primary mode of expression in diverse musical contexts. However, prevalent audio-driven synthesis methods predominantly rely on pitch and loudness envelopes, effectively flattening timbral expression from the input. Our approach draws on the concept of timbre analogies and investigates how timbral expression from an input signal can be mapped onto controls for a synthesizer. Leveraging differentiable digital signal processing, our method facilitates direct optimization of synthesizer parameters through a novel feature difference loss. This loss function, designed to learn relative timbral differences between musical events, prioritizes the subtleties of graded timbre modulations within phrases, allowing for meaningful translations in a timbre space. Using snare drum performances as a case study, where timbral expression is central, we demonstrate real-time timbre remapping from acoustic snare drums to a differentiable synthesizer modeled after the Roland TR-808.},
  numpages = {9}
}

@article{nime2024_56,
  author = {Björn Bernreiter and Katharina Gross-Vogt and Marian Weger},
  title = {Dextoria - An embedded system to control electric guitar effects via sound-producing gestures},
  pages = {386--389},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {56},
  track = {Papers},
  doi = {10.5281/zenodo.13904886},
  url = {http://nime.org/proceedings/2024/nime2024_56.pdf},
  presentation-video = {},
  abstract = {Standard guitar effects pedals are limited in their control possibilities, difficult to access on stage, and challenging to operate while playing. Innovative control systems are often not well integrated into the common guitar playing environment. In this paper we present “Dextoria” -- a control system that allows guitarists to make additional use of their sound-producing hand gestures. With the fretting hand, guitarists can switch between two effect loops, depending on the hand’s fret position that is measured by a distance sensor on the guitar’s headstock. The posture of the picking/strumming hand is captured by an IMU sensor on a hand strap, in order to control guitar effects pedals that have an expression input. The Dextoria system is modular and embeds into guitarists’ existing live setups.},
  numpages = {4}
}

@article{nime2024_57,
  author = {Kate Bosen and Dan Overholt},
  title = {Stitch: a Knitting-powered Musical Interface using Computer Vision},
  pages = {390--394},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {57},
  track = {Papers},
  doi = {10.5281/zenodo.13904888},
  url = {http://nime.org/proceedings/2024/nime2024_57.pdf},
  presentation-video = {https://youtu.be/bs9guoMieEI?si=aPphSzJqzvUHDlip},
  abstract = {This paper describes a new instrument for musical expres- sion that makes music from knitting. This interface uses only knitting needles, yarn, and a computer as hardware. The webcam input on a laptop captures the player knit- ting in real-time, and a bespoke MaxMSP patch processes the incoming data stream. Movements are detected using computer vision principles to identify shapes, lines, and the motions of the performer’s stitches. Gestures the performer uses are then mapped to a synthesizer that produces mu- sic according to how the player moves, while they knit and purl. Each performance varies due to the speed at which the performer knits, the technical knitting style of the per- former, the kinds of stitches cast on the needles, the color and texture of yarn used during performance, and the size of the knitting project.},
  numpages = {5}
}

@article{nime2024_58,
  author = {Zhen Wu and Ze Gao and Hua Xu and Xingxing Yang and Tristan Braud},
  title = {SoundMorphTPU: Exploring Gesture Mapping in Deformable Interfaces for Music Interaction},
  pages = {395--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {58},
  track = {Papers},
  doi = {10.5281/zenodo.13904891},
  url = {http://nime.org/proceedings/2024/nime2024_58.pdf},
  presentation-video = {https://youtu.be/JkGwgeSRY4M?si=ohtTy-kiQ2JtjfvR},
  abstract = {Deformable interface is an emerging field with significant potential for use in computing applications, particularly in the design of Digital Music Instruments (DMIs). While prior works have investigated the design of gestural input for deformable interfaces and developed novel musical interactions, there remains a gap in understanding the tangible gestures as input and their corresponding output from the user's perspectives. This study explores the relationship between gestural input and the output of a deformable interface for multi-gestural music interaction. Following a pilot study to explore materials and their corresponding intuitive gestures with participants, we develop a TPU fabric interface as a probe to investigate this question in the context of musical interaction. Through user engagement with the probe as a sound control, we discovered that the input-output relationship between gestures and the sound can have meaningful implications for users' embodied interaction with the system. Our research deepens the understanding of designing deformable interfaces and their capacity  to enhance embodied experiences in music interaction scenarios.},
  numpages = {12}
}

@article{nime2024_59,
  author = {Shiqing Lyu and Hanxuan Li and Ruhan Wang and Haipeng Mi},
  title = {Malletwand: the Pendulum as a Handheld Interface to Musical Timing},
  pages = {407--412},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {59},
  track = {Papers},
  doi = {10.5281/zenodo.13904893},
  url = {http://nime.org/proceedings/2024/nime2024_59.pdf},
  presentation-video = {https://youtu.be/5kKm6VBpbBg?si=-Q5FnCj4gOJzUkOC},
  abstract = {We devise and implement an interface in the form of a handheld pendulum device for manipulating the timing of musical playback. The physical properties of the pendulum make this interaction scheme steady and intuitive, and particularly suitable as an introductory means of musical engagement for untrained participants. We build a self-playing glockenspiel around this interface to demonstrate how our design encourages musical exploration and social play. We conclude by discussing potential extrapolations and integrations of the design in mobile and hybrid scenarios.},
  numpages = {6}
}

@article{nime2024_60,
  author = {Gyuchul Moon},
  title = {Coupled Oscillator Networks: Perspectives on Synchronization and Nonlinear Musical Frameworks},
  pages = {413--418},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {60},
  track = {Papers},
  doi = {10.5281/zenodo.13904895},
  url = {http://nime.org/proceedings/2024/nime2024_60.pdf},
  presentation-video = {https://youtu.be/5Tf0jhZvILc?si=X-VlMo8ImRtmhG4x},
  abstract = {The central emphasis of the paper is exploring the potential of utilizing the state of a specific system or natural phenomenon as a means of musical expression. This paper delves into applying coupled oscillator networks from sound synthesis to composition. It employs mathematical viewpoints grounded in coupled oscillators, comparing the Kuramoto model and the recently discovered Janus model for simulating musical frameworks by formulating and implementing the simulation of Janus oscillator networks to exploit the perspectives of generative and nonlinear in the composition process. Coupled oscillators implemented with multiple agents can be used at an analytic level with different types of parametric space, and complex behavior depends on each coupling value of the group and each agent’s relationship. It can be exploited at various levels of settings in the composition and sound synthesis stages. I outline the way of sound synthesis through the simulation of Janus network oscillators to find the meaning of compositional techniques underlying the self-organized principles.},
  numpages = {6}
}

@article{nime2024_61,
  author = {Krishnan Chandran and Lars Engeln and Daniel Zeidler and Matthew McGinity},
  title = {Sonic Structures: 4E Visual Sound in Multi-User Mixed Reality},
  pages = {419--423},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {61},
  track = {Papers},
  doi = {10.5281/zenodo.13904897},
  url = {http://nime.org/proceedings/2024/nime2024_61.pdf},
  presentation-video = {},
  abstract = {Sonic Structures is a multi-user mixed reality experiment that explores the relationship between visual form and sound and music creation through the lens of 4E (embodied, enactive, embedded and extended) cognition. By extending a custom platform allowing large scale mixed reality experiences for up to 20 users with real-time audio processing and generative 3D graphics, Sonic Structures provides a sandbox environment for the real-time transformation of sound and music into persistent visual structures. Described here is the conceptual and architectural design of the project and the results of a workshop exploring the musical, educational and artistic potential of the system.},
  numpages = {5}
}

@article{nime2024_62,
  author = {Nicola Privato and Victor Shepardson and Giacomo Lepri and Thor Magnusson},
  title = {Stacco: Exploring the Embodied Perception of Latent Representations in Neural Synthesis},
  pages = {424--431},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {62},
  track = {Papers},
  doi = {10.5281/zenodo.13904899},
  url = {http://nime.org/proceedings/2024/nime2024_62.pdf},
  presentation-video = {https://youtu.be/AJpuVTn_tPM?si=0qIAkXQZjIdl6r_o},
  abstract = {The application of neural audio synthesis methods for sound generation has grown significantly in recent years. Among such systems, streaming autoencoders such as RAVE are particularly suitable for instrument design, as they map audio to and from control signals in an abstract latent space with acceptable latency. Despite the uptake of autoencoders in NIME design, little research has been done to characterize the latent spaces of audio models, and to investigate their affordances in practical musical scenarios. In this paper we present Stacco, an instrument specifically designed for the intuitive control of neural audio synthesis latent parameters through the displacement of magnetic objects on a wooden board with four magnetic attractors. We then examine models trained on the same data with different seeds, we explore strategies for more consistent mappings from audio to latent space, and propose a method for stitching the latent space of one model to another. Finally, in a user study, we investigate whether and how these techniques are perceived through embodied practice with Stacco.},
  numpages = {8}
}

@article{nime2024_63,
  author = {Nicola Privato and Thor Magnusson},
  title = {Querying the Ghost: AI Hauntography in NIME},
  pages = {432--438},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {63},
  track = {Papers},
  doi = {10.5281/zenodo.13904901},
  url = {http://nime.org/proceedings/2024/nime2024_63.pdf},
  presentation-video = {},
  abstract = {The discourse around creative AI is populated by eerie and otherwordly presences, often evoked by artists to reflect on the social and cultural paradoxes that this technology embodies. This tendency of AI art to bring forth the uncanny, emerging also in my design and performative work with NIMEs, echoes the methods of an artistic movement known as sonic hauntology. In this paper I elaborate on Derrida's and Fisher's notion of hauntology, a theoretical framework investigating ontology's liminalities, and an artistic current addressing the paradoxes of postmodern aesthetics through the magnification of the technological uncanny. I then apply this paradigm to creative AI, arguing that the model's algorithmic manipulation of the training data reproduces and exponentially accelerates the processes of temporal and semantic flattening that characterise postmodern aesthetics. The frictions produced by creative AI as it operates with and within the culture bring forth hauntological disjunctures, that artists might harness as an instrument of critique, and scholars as a novel epistemic method. Finally, I introduce AI hauntography, a practice-based methodology combining artistic practice and observation to investigate the phenomenological aspects of creative AI as they intersect with the broader technical and sociopolitical discourse. },
  numpages = {7}
}

@article{nime2024_64,
  author = {Damian Dziwis and Aristotelis Hadjakos},
  title = {Interface Modules for Extended Reality in Music},
  pages = {439--446},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {64},
  track = {Papers},
  doi = {10.5281/zenodo.13904903},
  url = {http://nime.org/proceedings/2024/nime2024_64.pdf},
  presentation-video = {},
  abstract = {The growing possibilities and availability of immersive technologies in the field of Extended Reality (XR) are leading to new potentials in the context of Musical XR, like the emergence of new concepts for virtual reality musical instruments (VRMIs), reception, and composition and performance practices. Recent XR devices for Virtual and Augmented Reality allow not only to present virtual content, but also to blend real and virtual environments to create hybrid Mixed Realities. Therefore, in addition to using standard interfaces such as controllers or hand tracking from XR systems for interacting with VRMIs, developing custom musical interfaces can provide enhanced physical possibilities and experiences for interacting with VRMIs and other virtual sound-generating systems. This paper describes the development and possibilities of an open-source modular interface system for the realization of VRMIs with physical musical interfaces, interactive installations and the augmentation of virtual environments with physical environmental information.The tool was created by integrating the SPINE sensor-based musical interface toolkit with the IVES modular 3D engine for the Max programming environment. Merging these systems, with their no-code/no-soldering approach, results in a customizable tool for rapidly creating VRMIs or augmented virtual environments with corresponding physical interfaces.},
  numpages = {8}
}

@article{nime2024_65,
  author = {Tim-Tarek Grund and Huệ Trinh Lương and Alex Hofmann},
  title = {Challenges and Prospects in Remote Cross-cultural Musical Interface Design},
  pages = {447--451},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {65},
  track = {Papers},
  doi = {10.5281/zenodo.13904905},
  url = {http://nime.org/proceedings/2024/nime2024_65.pdf},
  presentation-video = {https://youtu.be/F2WQMDwhOLE?si=3m10AmnAIDG0TARj},
  abstract = {Roles in the context of live-electronic music performance are often overlapping. We used a cross-cultural collaboration between a Vietnamese live-electronic composer-performer and a German instrument maker to study the development of roles. We followed the approach of building a digital music instrument inspired by existing acoustic instruments and conducted a case study with the task of developing a granular effect sample player, the Grain Bau, inspired by the Vietnamese monochord zither đàn Bầu. We analyzed the milestones, tasks and remote collaboration strategies and assessed the depth of participation. We found that in-person collaborative work tended to dissolve role definitions, while remote working emphasised them. We examine our strategies for remote working and discuss the influence of gender on our role development.},
  numpages = {5}
}

@article{nime2024_66,
  author = {Jonas Braasch},
  title = {Expanding the saxophone with different tone generators and a foot controller for complementary voices},
  pages = {452--455},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {66},
  track = {Papers},
  doi = {10.5281/zenodo.13904907},
  url = {http://nime.org/proceedings/2024/nime2024_66.pdf},
  presentation-video = {https://youtu.be/wBXe_ok-t6c?si=toRnK-_ho3HUnKSr},
  abstract = {This paper focuses on expanding saxophone performance practice through exchangeable tone generators and a foot controller utilizing fine motor skills. The combination of both expansions extends the timbral qualities of the saxophone into new territories. The different tone generators turn the saxophone into a flute, a sarrusophone, a modern variation of the Renaissance cornett, and a free-reed instrument with each instrument class's distinct sonic characters. The foot-controller system consists of a trackball operated by the big toe of one foot with separate pedals to simulate the mouse buttons with the other foot. The system also includes a traditional MIDI bass pedal, an expression pedal, and a wide-spaced ASCII keyboard. In particular, the trackball system enables complex timbre changes and a flexible processing flow needed for freely improvised music. It can be used to control a complete personal computer. The learning curve to develop the feet's fine motor skills is comparable to learning new embouchures for the different tone generators.},
  numpages = {4}
}

@article{nime2024_67,
  author = {Doga Cavdir},
  title = {Creative Practice as an Evaluation Method: A Case Study with a Movement-based Musical Instrument},
  pages = {456--464},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {67},
  track = {Papers},
  doi = {10.5281/zenodo.13904909},
  url = {http://nime.org/proceedings/2024/nime2024_67.pdf},
  presentation-video = {https://youtu.be/8UViTSj2F30?si=SzvITKJvgXFIKeG3},
  abstract = {The creative process with technology requires experimentation, exploring affordances and limitations, and evaluation of one’s process of different learning stages. Movement-based digital musical instruments (MDMIs) offer many opportunities to study performers’ creative processes since performers can artistically explore both the familiar and unfamiliar interactions with the instruments. In this research, we integrate creative process as a qualitative evaluation method into studying performers’ interactions. While these processes are often non-linear and iterative, we observe how creativity, through sonic and movement interaction, impacts participants’ learning processes. We study these processes with participants from music and/or dance dance backgrounds and report on their experiences. },
  numpages = {9}
}

@article{nime2024_68,
  author = {Vincenzo Madaghiele and Arife Dila Demir},
  title = {Pain Creature: interdisciplinary collaboration in the design of an embodied textile instrument for interactive dance},
  pages = {465--473},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {68},
  track = {Papers},
  doi = {10.5281/zenodo.13904917},
  url = {http://nime.org/proceedings/2024/nime2024_68.pdf},
  presentation-video = {https://youtu.be/QgWj58eEPQs?si=pw86H2oY5aXWDc8S},
  abstract = {This paper describes the development of an interactive textile instrument called Pain Creature. The instrument is the result of a collaboration between a sonic interaction designer and a textile designer. Through textile and auditory qualities, Pain Creature explicates different aspects of the second author's experience of chronic pain. The instrument can be used as a tool to reflectively engage with the user's pain experiences and as a performance instrument. This paper introduces the collaborative design process employed to develop the artifact, focusing on how contributions from sound design, textile design and movement practice were combined in the design phase. The paper describes the technical design of the artifact and discusses how knowledge from different disciplines affected the development of the instrument and its use in performance. },
  numpages = {9}
}

@article{nime2024_69,
  author = {Kristian Eicke and Stefano Fasciani and Çağrı Erdem},
  title = {Xyborg: A Wearable Hand-based Instrument for Musical Expression},
  pages = {474--479},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {69},
  track = {Papers},
  doi = {10.5281/zenodo.13904921},
  url = {http://nime.org/proceedings/2024/nime2024_69.pdf},
  presentation-video = {https://youtu.be/TfBJ1B59N0E?si=-g4J9ExiEyd9QyWE},
  abstract = {This paper describes the design, implementation, and evaluation of Xyborg, a wearable instrument designed to leverage hand input gestures and arm movements. Xyborg's aims include developing a low-cost musical instrument, providing the performer with expressive control over sound parameters, and establishing a transparent connection between gestures and the resulting sounds for spectators. The design of this instrument prioritizes controllable sonic features and a fixed comprehensible mapping between gestures and sounds, rather than aiming for a broad range of functions and variable mapping. The participants in the evaluation unanimously agreed on the potential for long-term engagement and recognized Xyborg as an instrument that enables expressive sound control through movement, emphasizing its skill-based nature.},
  numpages = {6}
}

@article{nime2024_70,
  author = {Zhengyang Ma and Iurii Kuzmin and Duan Ruilei and Raul Masu},
  title = {Pharosphones: interactive audience participation using light},
  pages = {480--485},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {70},
  track = {Papers},
  doi = {10.5281/zenodo.13904923},
  url = {http://nime.org/proceedings/2024/nime2024_70.pdf},
  presentation-video = {https://youtu.be/O9gEwMloRLk?si=wO9iCVnpj0DWAnaK},
  abstract = {This paper presents a novel approach to live performance that blends audience participation through the use of mobile phone lights tracking with music conduction. Drawing from the history of audience engagement in the arts and leveraging advancements in digital technology, we foster a dynamic and immersive interactive model that complements traditional musical conduction techniques. The performance was designed for a 270 degree concert hall which allows the audience to see each other. In this setting, by employing mobile phones not only as communication devices but as light emitters, we explore the symbiotic relationship among the various actors in a performance. To achieve that, we developed a computer vision system designed to translate the audience participation into shaping the sonic landscape of the performance. This process is underpinned by considerations of cybernetics and the feedback loop between human participants and technological systems, challenging conventional power structures within performance spaces. Through this work, we aim to expand the boundaries of interactive music creation, offering insights into the potential for technologically-facilitated audience entanglement and further the discussion with the context of music ecology. },
  numpages = {6}
}

@article{nime2024_71,
  author = {Travis J West and Marcelo Wanderley and Stéphane Huot},
  title = {Sygaldry: DMI Components First and Foremost},
  pages = {486--489},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {71},
  track = {Papers},
  doi = {10.5281/zenodo.13904927},
  url = {http://nime.org/proceedings/2024/nime2024_71.pdf},
  presentation-video = {https://youtu.be/D4gxwyczUP4?si=HarA-dNHIni3EaAW},
  abstract = {Motivated by challenges involved in the long-term maintenance of digital musical instruments, the frustrating problem of glue code, and the inherent complexity of evaluating new instruments, we developed Sygaldry, a C++20 library of digital musical instrument components. By emphasising the development of components first and foremost, and through use of C++20 language features, strict management of dependencies, and literate programming, Sygaldry provides immediate benefits to rapid prototyping, maintenance, and replication of DMIs, encourages code portability and code-reuse, and reduces the burden of glue-code in DMI firmware. Recognising that there still remains significant future work, we discuss the advantages of focusing development and research on DMI components rather than individual DMIs, and argue that a modern C++ library is among the most appropriate realisations of these efforts.},
  numpages = {4}
}

@article{nime2024_72,
  author = {Domenico Stefani and Matteo Tomasetti and Filiippo Angeloni and Luca Turchet},
  title = {Esteso: Interactive AI Music Duet Based on Player-Idiosyncratic Extended Double Bass Techniques},
  pages = {490--498},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {72},
  track = {Papers},
  doi = {10.5281/zenodo.13904929},
  url = {http://nime.org/proceedings/2024/nime2024_72.pdf},
  presentation-video = {https://youtu.be/mdb2Tlh4ub8?si=0m-6kqA_a_p-c2-z},
  abstract = {Extended playing techniques are a crucial characteristic of contemporary double bass practice. Players find their voice by developing a personal vocabulary of techniques through practice and experimentation. These player-idiosyncratic techniques are used in composition, performance, and improvisation. Today's AI methods offer the opportunity to recognize such techniques and repurpose them in real-time, leading to new forms of interactions between musicians and machines. This paper is the result of a collaboration between a composer/double-bass player and researchers, born from the musician's desire for an interactive improvisational experience with AI centered around the practice of his extended techniques. With this aim, we developed Esteso: an interactive improvisational system based on extended technique recognition, live electronics, and a timbre-transfer double-bass model. We evaluated our system with the musician with three duet improvisational sessions, each using different mapping strategies between the techniques and the sound of the virtual double bass counterpart. We collected qualitative data from the musician to gather insights about the three configurations and the corresponding improvisational duets, as well as investigate the resulting interactions. We provide a discussion about the outcomes of our analysis and draw more general design considerations.},
  numpages = {9}
}

@article{nime2024_73,
  author = {Maxwell Gentili-Morin and Marcelo Wanderley},
  title = {R-FF: A Single Reed Haptic Library for the TorqueTuner},
  pages = {499--504},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {73},
  track = {Papers},
  doi = {10.5281/zenodo.13904931},
  url = {http://nime.org/proceedings/2024/nime2024_73.pdf},
  presentation-video = {https://youtu.be/Jbir-dPQ5ZM?si=bCVyS5ZsErN13JY_},
  abstract = {R-FF (Reed Force Feedback) is a library of clarinet reed equations adapted for use on the TorqueTuner (TT), a one degree of freedom, rotary force-feedback (FF), haptic device. We created the R-FF library by analyzing the clarinet's physical model and adapting the reed component that defines the forces between the user and the mouthpiece into force-feedback models. The models were designed for implementation on small and portable micro-controllers, like the ESP-32 module, found in the TorqueTuner(TT). Accompanied by a clarinet sound model, results show that the R-FF library could grow into a platform that allows non-wind players to explore the performance space of a single reed instrument, feeling the dynamics of the reed-embouchure system with their fingers instead of with their mouth.},
  numpages = {6}
}

@article{nime2024_74,
  author = {Nathan Renney and Benedict Gaster},
  title = {Insights Into How Digital Luthiers Approach Design},
  pages = {505--515},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {74},
  track = {Papers},
  doi = {10.5281/zenodo.13904935},
  url = {http://nime.org/proceedings/2024/nime2024_74.pdf},
  presentation-video = {},
  abstract = {The design of digital musical instruments represents a complex role that incorporates creative practice, design and engineering. It is, of course, a deep discussion topic and research focus within the NIME community. This paper discusses how digital luthiers approach design through a reflexive thematic analysis that inductively explores their practice. This work builds on a study that includes interviews with 27 digital luthiers from various backgrounds and varied motivations, originally motivated to better understand tool use in digital lutherie. The themes presented provide constructed narratives that affirm assumptions that intuitively appear likely and provide interesting insights and developments for further exploration that are fitting and relevant to the NIME community.  This discussion finds nuances in how digital luthiers approach design, both as a problem and in search of inspiration. It also explores how interaction and control are often the primary focus of digital luthiers and that they emphasise opinionated and directed design choices. These ideas are then considered in relation to existing ideas from the field.},
  numpages = {11}
}

@article{nime2024_75,
  author = {Eleni-Ira Panourgia and Angela Brennecke and Bela Usabaev},
  title = {ClimaSynth: Enhancing Environmental Perception through Climate Change Sonic Interaction},
  pages = {516--520},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {75},
  track = {Papers},
  doi = {10.5281/zenodo.13904937},
  url = {http://nime.org/proceedings/2024/nime2024_75.pdf},
  presentation-video = {},
  abstract = {This paper presents a first prototype of ClimaSynth, a web-based interactive application that allows to creatively transform environmental sounds in response to climate change scenarios. ClimaSynth serves as an instrument for enhancing environmental perception through sonic interaction. The underlying algorithm uses real-time granular synthesis and web technologies to encode climatic effects in sound synthesis configurations in ways that express and speculate about change in future environments. ClimaSynth enables users to navigate and manipulate recorded audio data on a visual surface and produce new versions of environmental sounds. We discuss the design, implementation and resulting relationships between web-based sonic interaction and climate change impacts.},
  numpages = {5}
}

@article{nime2024_76,
  author = {Sophie Skach and Victor Shepardson and Thor Magnusson},
  title = {About TIME: Textile Interfaces for Musical Expression},
  pages = {521--529},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {76},
  track = {Papers},
  doi = {10.5281/zenodo.13904939},
  url = {http://nime.org/proceedings/2024/nime2024_76.pdf},
  presentation-video = {},
  abstract = {Research in new musical interfaces includes exploring sometimes unconventional materials, drawing from a wide range of sources, but typically affiliated with the computing industry. These interfaces are made out of plastic, metal, glass and rubber, built with sensors that seek precision and ergonomic control. However, there are other more unconventional interfaces, such as flexible and soft instruments that allow for different types of interaction. One of the materials that has not been explored extensively in this context, are textiles, in particular e-textiles. Here, we survey the NIME archive and provide an overview of the work on non-rigid interfaces. Further, this paper presents a new textile based musical interface and evaluates its potential as a malleable, expressive instrument through a case study with 6 musicians. The findings of the qualitative analysis conclude a set of guidelines for the development of future e-textile interfaces.},
  numpages = {9}
}

@article{nime2024_77,
  author = {Francesco Manenti and Francesco Dal Rì},
  title = {Accessibility of Graphic Scores: Design and Exploration of Tactile Supports for Blind People},
  pages = {530--535},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {77},
  track = {Papers},
  doi = {10.5281/zenodo.13904941},
  url = {http://nime.org/proceedings/2024/nime2024_77.pdf},
  presentation-video = {https://youtu.be/T3r3_xbzLTo?si=f6d_Xl8VI0OB2tUS},
  abstract = {In this paper, we present the outcome of a musical workshop on accessibility of graphic scores for blind people, held in Brescia, Italy with the support of Unione Italiana dei Ciechi e degli Ipovedenti ETS - APS. Through a series of meetings and side-by-side study sessions, four blind musicians were involved in a collective musical experiment, culminating in a final concert based on three pieces from the historical repertoire. During these meetings, a series of semi-structured interviews with the participants allowed us to collect a series of guidelines and suggestions regarding the design and creation of tactile scores and to discuss performance strategies, in order to facilitate accessibility of this peculiar repertoire to visually impaired people. },
  numpages = {6}
}

@article{nime2024_78,
  author = {Victor Shepardson and Jonathan Reus and Thor Magnusson},
  title = {Tungnaá: a Hyper-realistic Voice Synthesis Instrument for Real-Time Exploration of Extended Vocal Expressions},
  pages = {536--540},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {78},
  track = {Papers},
  doi = {10.5281/zenodo.13904943},
  url = {http://nime.org/proceedings/2024/nime2024_78.pdf},
  presentation-video = {https://youtu.be/e6KFcsyiWa4?si=4Rnf_mtpgx0NpM9R},
  abstract = {This demo showcases Tungnaá, a new voice synthesis system and software instrument for real-time musical exploration of "Deep Voice Synthesis". The design of Tungnaá emphasizes real-time interaction and customization, enabling artists to manipulate various aspects of the synthesis process and to explore aesthetic artefacts unique to autoregressive neural synthesis. The synthesis engine achieves real-time streaming generation of paralinguistic and extended forms of vocal expression, while controlling them using symbolic text notations drawn from the entire unicode character set, allowing for the creation of new notation systems. The interface provides visual display and mouse- or OSC-controllable interventions into the machine vocalisations. The demo showcases Tungnaá on a laptop with headphones and a MIDI controller, allowing participants to explore the instrument via both a textual and physical interface.},
  numpages = {5}
}

@article{nime2024_79,
  author = {Kunwoo Kim and Ge Wang},
  title = {VVRMA: VR Field Trip to a Computer Music Center},
  pages = {541--548},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {79},
  track = {Papers},
  doi = {10.5281/zenodo.13904945},
  url = {http://nime.org/proceedings/2024/nime2024_79.pdf},
  presentation-video = {https://youtu.be/LLwfKZD5008?si=u5slBnJnpN0CF7Zf},
  abstract = {VVRMA is an interactive, audiovisual, fully immersive VR field trip to a virtual reimagining of CCRMA—Stanford University’s Center for Computer Research in Music and Acoustics. Envisioned as a “VR interactive museum for computer music”, VVRMA is the name of the virtual centerpiece building, meticulously modeled after CCRMA’s physical architecture. Within VVRMA, visitors explore various Zones of Interest (ZOIs), which are collections of immersive experiences that thematically mirror various research labs at CCRMA.Asdesign case studies, this paper examines the first two ZOIs: 1) “From Sound to Brain”(a boat ride into the ear canal to learn about the science of hearing), and 2) “The World of Artful Design” (a musical city for exploring interactive audiovisual design and humanistic implications of technology). VVRMA aims to create a playful, expressive, and immersive learning space accessible to a general audience, who are curious about music, technology, and the medium of VR. This paper chronicles the design of VVRMA through the lens of humanistic tool-building, emphasizing programmable audio and VR’s immersive affordances.},
  numpages = {8}
}

@article{nime2024_80,
  author = {Marise van Zyl and Ge Wang},
  title = {What’s up ChucK? Development Update 2024},
  pages = {549--552},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {80},
  track = {Papers},
  doi = {10.5281/zenodo.13904947},
  url = {http://nime.org/proceedings/2024/nime2024_80.pdf},
  presentation-video = {},
  abstract = {Since its inception in early 2000s, the ChucK music programming language has undergone many expansions and changes. In the early years, there was a flurry of contributions that are still in use today. During the 2010s, there was a notable decrease in ChucK development (despite a few dedicated individuals who kept the language on life support). Recently, however, ChucK development has experienced something of a resurrection. This paper highlights the major initiatives since 2018, including new core language features, ChuGL (graphics), ChAI (AI), Chunity (ChucK in Unity), Chunreal (ChucK in Unreal Engine), WebChucK (ChucK in browsers), and further extensions to the language through Chugins (ChucK plugins). Furthermore, we will highlight future directions of ChucK development.},
  numpages = {4}
}

@article{nime2024_81,
  author = {Yikai Li and Ge Wang},
  title = {ChAI => Interactive AI Tools in ChucK},
  pages = {553--559},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {81},
  track = {Papers},
  doi = {10.5281/zenodo.13904949},
  url = {http://nime.org/proceedings/2024/nime2024_81.pdf},
  presentation-video = {https://youtu.be/EIWL380HNkM?si=43eABYON4RDUdh_O},
  abstract = {This paper introduces ChAI, a collection of interactive machine learning tools for the ChucK music programming language, and chronicles its use in Music and AI, a critical-making course at Stanford University. We explore the intersection of AI and music HCI, emphasizing the augmentation of human creativity, rather than its replacement. We introduce ChAI's interactive AI tools and toys and highlight philosophical, ethical, and cultural considerations. Focusing on interactive machine learning, audio analysis, feature extraction, and audio re-synthesis, this paper shows how ChAI could be used to create playful, expressive systems (in an age of generative AI that tends to limit human interaction in favor of prompt-based automation). In the critical-making course, ChAI was used alongside reflections on AI's broad impact on the arts and who makes art, while stressing the importance of understanding AI's societal and cultural implications. This work not only introduces a new tool but also invites broader conversation on the role of HCI, play, and "small-data" approaches, in music and AI practice and education.},
  numpages = {7}
}

@article{nime2024_82,
  author = {Maxwell Addae and Nina Masuelli},
  title = {VocalCords: Exploring Tactile Interaction and Performance with the Singing Voice},
  pages = {560--567},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {82},
  track = {Papers},
  doi = {10.5281/zenodo.13904951},
  url = {http://nime.org/proceedings/2024/nime2024_82.pdf},
  presentation-video = {https://youtu.be/7YkTOnlB81E?si=5ER67OB-zznTo2ub},
  abstract = {The close relationship between touch, gesture, and sound plays a critical role in expressive musical performance. Many acoustic instruments, ranging from strings to brass to percussion, involve some coupling of the “feel” of the instrument in the hands and the corresponding sound produced. The singing voice, however, is one of few musical instruments that typically does not involve touch-mediated interaction. Despite several neurological, psychological, and social connections demonstrated between the hands and voice, the coupling of touch and voice is surprisingly absent from traditional vocal performance technologies. This provides the motivation for VocalCords, which explores the design of a new digital music interface inviting tactile interaction and performance with the singing voice. The interface makes use of physical rubber cords, acting as stretch sensors, which are pulled and manipulated by the hands of the singer as they vocalize to augment and modify their voice in real-time – as if they were able to physically “touch” their own vocal cords. This approach allows for expressive, tactile control over the singing voice, which suggests a striking relationship between physical and musical tension. Through a series of prototyping iterations and a public performance with the interface, I explore the potential of touch-mediated vocal performance, as well as how this added tactile interaction may alter our experience with, and perception of, our singing voices.},
  numpages = {8}
}

@article{nime2024_83,
  author = {Adam G Schmidt and Michael Gurevich},
  title = {The Lorentz Lap Brass: Method for Frugal Integrated
Sonic/Haptic Interaction},
  pages = {568--573},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {83},
  track = {Papers},
  doi = {10.5281/zenodo.13904953},
  url = {http://nime.org/proceedings/2024/nime2024_83.pdf},
  presentation-video = {https://youtu.be/qqTqkYI4f0Y?si=lqLPkIumK2JraI52},
  abstract = {This paper presents an affordable and accessible method for integrated sonic/haptic interaction via a low-cost setup utilizing Lorentz Force Actuation – a form of electromagnetic actuation – and exemplified by the The Lorentz Lap Brass, a new electromagnetically-actuated musical instrument and interface (EMAII). This style of actuation is uncommon in NIMEs, though it presents rich opportunity for cost-effective tactile feedback, infinite sustain, and feedback control. In an effort to encourage open-source knowledge sharing, replication, and adoption of this novel technique, we describe the underlying concepts, designs, and techniques for this method and distribute schematics, CAD, and code used in the setup. Results of a preliminary user study are discussed and offer perspectives and avenues for improving, extending, or iterating on the current system.},
  numpages = {6}
}

@article{nime2024_84,
  author = {Matthew Davison and Andrew McPherson and Craig Webb and Michele Ducceschi},
  title = {A Self-Sensing Haptic Actuator for Tactile Interaction with Physical Modelling Synthesis},
  pages = {574--581},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {84},
  track = {Papers},
  doi = {10.5281/zenodo.13904955},
  url = {http://nime.org/proceedings/2024/nime2024_84.pdf},
  presentation-video = {https://youtu.be/XW-ufFB-4oc?si=Ypp20TEcHz2nk6kN},
  abstract = {The use of transducers to excite physical modelling synthesisers with real-world audio signals is a well established practice within the digital musical instrument design community, yet it is normally presented as a unidirectional process – energy is transferred into the system from human to instrument. In this paper, a novel approach to tactile interaction with physical modelling synthesis is presented, through the use of a self-sensing vibrotactile transducer. This enables simultaneous collocated sensing and haptic actuation with a single moving coil transducer. A current drive amplifier is used for haptic actuation, using signals derived from the physical modelling synthesiser. The varying impedance of the transducer (due to changes in the mechanical damping) enables the sensing of force applied upon the device whilst also acting as a pickup to excite the physical model, all with simultaneous haptic actuation. A digital filter equivalent of the transducer's impedance is used to prevent feedback in the system, allowing simultaneous excitation and haptic actuation without self-oscillation.},
  numpages = {8}
}

@article{nime2024_85,
  author = {Iurii Kuzmin and Zhengyang Ma and Raul Masu},
  title = {Toward Musical Cosmotechnics: the case of zhu nao 竹脑 - a bamboo based instrument },
  pages = {582--590},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {85},
  track = {Papers},
  doi = {10.5281/zenodo.13904957},
  url = {http://nime.org/proceedings/2024/nime2024_85.pdf},
  presentation-video = {https://youtu.be/kCgzQ6iHjiQ?si=OdLOQmu02jv5AraK},
  abstract = {This paper aims at contributing to the discussion of DMI design within specific cultural contexts. It presents a reflection on designing and performing with zhu nao – a new DMI grounded in Chinese culture. In our attempt to avoid stereotypes commonly associated with the orientalist imaginary of Chinese culture we based the design of our instrument on the notion of cosmotechnics proposed by the Hong Kong philosopher of technology Yuk Hui as a way to reconcile nature and technology by acknowledging the influence of locality and local cosmologies on the development of scientific and technical thinking.},
  numpages = {9}
}

@article{nime2024_86,
  author = {Shuoyang Zheng and Bleiz M Del Sette and Charalampos Saitis and Anna Xambó and Nick Bryan-Kinns},
  title = {Building Sketch-to-Sound Mapping with Unsupervised Feature Extraction and Interactive Machine Learning},
  pages = {591--597},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {86},
  track = {Papers},
  doi = {10.5281/zenodo.13904959},
  url = {http://nime.org/proceedings/2024/nime2024_86.pdf},
  presentation-video = {https://youtu.be/phj5gkDijnc?si=4ltb2Vyyncxj_tnw},
  abstract = {In this paper, we explore the interactive construction and exploration of mappings between visual sketches and musical controls. Interactive Machine Learning (IML) allows creators to construct mappings with personalised training examples. However, when it comes to high-dimensional data such as sketches, dimensionality reduction techniques are required to extract features for the IML model. We propose using unsupervised machine learning to encode sketches into lower-dimensional latent representations, which are then used as the source for the IML model to construct sketch-to-sound mappings. We build a proof-of-concept prototype and demonstrate it using two compositions. We reflect on the composing processes to discuss the controllability and explorability in mappings built by this approach and how they contribute to the musical expression.},
  numpages = {7}
}

@article{nime2024_87,
  author = {Oren D Ronen and Marcelo Wanderley},
  title = {Sonic Touch: A Haptic Toolkit for Fast Vibrotactile Prototyping},
  pages = {598--601},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {87},
  track = {Papers},
  doi = {10.5281/zenodo.13904961},
  url = {http://nime.org/proceedings/2024/nime2024_87.pdf},
  presentation-video = {https://youtu.be/UrmL3P7FdoU?si=UZK6e26GzsHQXfS5},
  abstract = {In the realm of Digital Musical Instruments (DMIs), tactile feedback typically inherent in acoustic instruments is notably absent, creating a gap in the sensory experience of musicians. Previous works typically aimed at bridging this gap by designing haptic effects using trial-and-error strategies. This paper introduces Sonic Touch, a toolkit developed in Max/MSP that facilitates the rapid prototyping of audio-driven vibrotactile haptic effects. The toolkit allows users to design a variety of haptic effects by manipulating parameters like wave type, vibration duration, frequency, repetition, and envelope, then store and manage them in buffers for output. The toolkit’s architecture is grounded in a modular, building-block approach, accessible through a simple graphical user interface. Practical use of this toolkit is demonstrated through two examples, first designing a haptic tremolo effect, and second augmenting a Touché controller with vibrotactile feedback.},
  numpages = {4}
}

@article{nime2024_88,
  author = {Jeffrey Snyder and Davis Polito},
  title = {The Electrobass: A New Electronic Instrument Inspired By The Bass Guitar},
  pages = {602--606},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {88},
  track = {Papers},
  doi = {10.5281/zenodo.13904963},
  url = {http://nime.org/proceedings/2024/nime2024_88.pdf},
  presentation-video = {https://youtu.be/XW-ufFB-4oc?si=d63YlFlVsZx88faS},
  abstract = {The Electrobass is an instrument that takes inspiration from the interface of the bass guitar, but is in fact a flexible synthesizer. The Electrobass has roots in instruments like the SynthAxe, the Casio DG series, and other commercial guitar synthesis instruments, but applies this mindset to the bass guitar with a new approach. The technology is closely related to the authors' Electrosteel instrument, and the Electrobass was developed simultaneously as an alternative form for the underlying idea. This paper outlines how the goals of the Electrobass differ from the Electrosteel, and what changes from that paradigm the bass guitar's interface demands.},
  numpages = {5}
}

@article{nime2024_89,
  author = {Jeffrey Snyder and Davis Polito and Forrest Meggers and Genyuan Hu},
  title = {Thermal Music: Exploring Sensation of Temperature as a Performance Parameter},
  pages = {607--611},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {89},
  track = {Papers},
  doi = {10.5281/zenodo.13904969},
  url = {http://nime.org/proceedings/2024/nime2024_89.pdf},
  presentation-video = {https://youtu.be/5KCFWLxL4PA?si=b4KWHl4OR1fYBkiS},
  abstract = {Thermal Music is a project to explore the possibilities of coordinating control of temperature sensation with control of sound and light. Unlike convective heating technologies that change the temperature of the air around us, radiative heating works by directly heating the surface of our skin via infrared. The technology is widely used in portable heaters in outdoor dining areas of restaurants. One interesting advantage of radiative heating is that if the source of infrared radiation is shaded by a thermally reflective surface, the change in perceived temperature by a person near the heater is very rapid. We worked to create a system that could quickly control robotic shades in front of a radiative heater to synchronize changes in perceived temperature with music and light. We also explored the inverse, using thermal camera input as a control method for audio, and presented both of these techniques at a "Thermal Music" concert at Princeton University in October of 2023.},
  numpages = {5}
}

@article{nime2024_90,
  author = {Timothy Tate and Andrew Brown and John Ferguson and Daniel Della-Bosca},
  title = {Hand Turned Synthesis: A One Chip Exploration of CMOS Electronics},
  pages = {612--621},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {90},
  track = {Papers},
  doi = {10.5281/zenodo.13904971},
  url = {http://nime.org/proceedings/2024/nime2024_90.pdf},
  presentation-video = {https://youtu.be/XJ1lfl2fq4I?si=tCnsBBSvPK66jUwy},
  abstract = {We discuss and give a concise overview of the historical practices and approaches to utilising budget-friendly electronic components as a foundation for constructing simple sound making circuits. Building on the work of Lunetta, Collins and Wilson we focus specifically on the use of a 40106 CMOS integrated circuit (IC). Our research goal is to provide a tool kit of circuits for common synthesis elements based around the use of a single 40106 IC that may be of use to those involved in sound installations and frugal instrument design as well as practitioners working within accessibility and workshop/pedagogical scenarios. The authors demonstrate the application of the tool kit in their own projects, such as a monophonic synthesizer as well as several hand cranked sculptures that are powered and ‘played’ by the interaction between the user and the hand crank. Overall, we gather a broad range of relevant information that is difficult to find in one place and provide a practical tool kit that is pitched at an introductory level and describe how this is incorporated into the design of interactive electronic instruments. We also attempt to highlight fundamental electronics and synthesis principles while embracing terminology common to both electronics and synthesizer communities.},
  numpages = {10}
}

@article{nime2024_91,
  author = {Chaeryeong Oh and Dayoung Lee and Alexandria Smith},
  title = {Doongdoong.club: A Web-Based Metaverse Music Sequencer With Korean Onomatopoeic and Mimetic Words},
  pages = {622--626},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {91},
  track = {Papers},
  doi = {10.5281/zenodo.13904973},
  url = {http://nime.org/proceedings/2024/nime2024_91.pdf},
  presentation-video = {https://youtu.be/rgEHgOZqbZ8?si=WQvWUJsWO1ZPP3vL},
  abstract = {DoongDoong.club introduces a unique musical metaverse, where participants collaboratively create real-time music with Korean onomatopoeic and mimetic words as a distinctive sound source. A line rotating clockwise on a vast disc-shaped space scans objects and generates individual sounds from each object. As a New Interface for Musical Expression (NIME), DoongDoong.club enables users of all musical levels to make music in real-time collaboratively, addressing entry barriers heightened by the rapid shift from in-person to online collaboration during the pandemic. This study dissects elements of DoongDoong.club to show how they contribute to the musical metaverse. Y2K graphic style blends optimistic futurism with a touch of 2000s nostalgia. Emoji-based objects play Korean onomatopoeic and mimetic word recordings, enriching cultural and rhythmic nuances in compositions. Additionally, DoongDoong.club’s Cul-de-Sac aligns with 2010’s metafiction trends while showcasing its transition into the metaverse. This exploration emphasizes the platform’s innovative contributions, featuring Korean onomatopoeic and mimetic words as a central, unique sound element.},
  numpages = {5}
}

@article{nime2024_92,
  author = {Eito Murakami and John C Burnett and Ge Wang},
  title = {RayTone: A Node-based Audiovisual Sequencing Environment},
  pages = {627--631},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {92},
  track = {Papers},
  doi = {10.5281/zenodo.13904975},
  url = {http://nime.org/proceedings/2024/nime2024_92.pdf},
  presentation-video = {},
  abstract = {RayTone is a node-based software environment for creating audiovisual compositions. The software emphasizes the aesthetics and joy of patching procedures, aiming to promote a playful workflow for transforming creative ideas into artistic content. RayTone allows for native access to ChucK music programming language and OpenGL Shading Language (GLSL), affording programmability of arbitrary complexity inside each node on canvas. The ability to sequentially connect nodes as well as to live script functionalities therein makes RayTone suitable for users with widely varying skill levels and as an entry point to digital signal processing and shader programming. In this paper, we discuss RayTone’s patching and scripting workflow and how it is designed to facilitate the integration of audio and graphics. RayTone was used as a primary educational tool for a five-day summer workshop, introducing participants to building original live multimedia performances. We present selected student compositions from the workshop as example projects, and evaluate the expressiveness of RayTone as a digital art-making platform. },
  numpages = {5}
}

@article{nime2024_93,
  author = {Sasha Leitman and Dale Carnegie and Jim Murphy},
  title = {Resonant Object Interface: Implementation and Initial Exploration of a Tactile Acoustic Interface},
  pages = {632--634},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {93},
  track = {Papers},
  doi = {10.5281/zenodo.13904977},
  url = {http://nime.org/proceedings/2024/nime2024_93.pdf},
  presentation-video = {},
  abstract = {The Resonant Object Interface (ROI) is an acoustic input system that provides nuanced, embodied, and expressive engagement with control signals in audio software. The ROI’s unique sensing methodology is built from resonant objects, vibration exciters and contact microphones. As users touch the resonant object, their hands dampen the signals from the vibration exciters. The resultant signals are analysed and turned into control data for audio software. This produces a music controller that is deeply tactile, capable of repeated gestures and uniquely tangible.   This paper gives an overview of the design and implementation of the ROI and then describes a series of user engagements with the device.},
  numpages = {3}
}

@article{nime2024_94,
  author = {Rebecca S Abraham and Albert Zhang},
  title = {Choreographing Sound: Co-creating Music and Movement through Touch with the Magical Musical Mat},
  pages = {635--638},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {S M Astrid Bin and Courtney N. Reed},
  year = {2024},
  month = {September},
  address = {Utrecht, Netherlands},
  issn = {2220-4806},
  articleno = {94},
  track = {Papers},
  doi = {10.5281/zenodo.13904980},
  url = {http://nime.org/proceedings/2024/nime2024_94.pdf},
  presentation-video = {},
  abstract = {This paper presents insights and findings from an experimental dance workshop conducted with the Magical Musical Mat (MMM), an interactive instrument that amplifies physical touch with sound. The workshop, held as part of the exploratory phase for a larger performance project, aimed to explore the creative possibilities of the MMM in the context of dance performance. The workshop brought together two dancers versed in contact improvisation to collaboratively experiment with the MMM as both a musical instrument and a choreographic tool. The workshop highlighted the MMM's potential to facilitate expressive interactions between dancers, revealing possibilities for collaborative creativity in interdisciplinary contexts.},
  numpages = {4}
}
