@InProceedings{dAlessandro2011,
  Title                    = {ROOM #81---Agent-Based Instrument for Experiencing Architectural and Vocal Cues},
  Author                   = {d'Alessandro, Nicolas and Calderon, Roberto and M\''{u}ller, Stefanie},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {132--135},
  Keywords                 = {agent,architecture,collaboration,figure 1,installation,instrument,interactive fabric,light,mo-,movements in the installation,space and,tion,voice synthesis},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_132.pdf},
  DOI                      = {10.5281/zenodo.1177933}
}

@InProceedings{Aaron2011,
  Title                    = {A Principled Approach to Developing New Languages for Live Coding},
  Author                   = {Aaron, Samuel and Blackwell, Alan and Hoadley, Richard and Regan, Tim},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {381--386},
  Abstract                 = {This paper introduces Improcess, a novel cross-disciplinarycollaborative project focussed on the design and development of tools to structure the communication between performer and musical process. We describe a 3-tiered architecture centering around the notion of a Common MusicRuntime, a shared platform on top of which inter-operatingclient interfaces may be combined to form new musical instruments. This approach allows hardware devices such asthe monome to act as an extended hardware interface withthe same power to initiate and control musical processesas a bespoke programming language. Finally, we reflect onthe structure of the collaborative project itself, which offers an opportunity to discuss general research strategy forconducting highly sophisticated technical research within aperforming arts environment such as the development of apersonal regime of preparation for performance.},
  Keywords                 = {Improvisation, live coding, controllers, monome, collaboration, concurrency, abstractions },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_381.pdf},
  Presentation-video = {https://vimeo.com/26905683/},
  DOI                      = {10.5281/zenodo.1177935}
}

@InProceedings{Ahola2011,
  Title                    = {Raja -- A Multidisciplinary Artistic Performance},
  Author                   = {Ahola, Tom and Tahiroglu, Koray and Ahmaniemi, Teemu and Belloni, Fabio and Ranki, Ville},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {433--436},
  Abstract                 = {Motion-based interactive systems have long been utilizedin contemporary dance performances. These performancesbring new insight to sound-action experiences in multidisciplinary art forms. This paper discusses the related technology within the framework of the dance piece, Raja. The performance set up of Raja gives a possibility to use two complementary tracking systems and two alternative choices formotion sensors in real-time audio-visual synthesis.},
  Keywords                 = {raja, performance, dance, motion sensor, accelerometer, gyro, positioning, sonification, pure data, visualization, Qt},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_433.pdf},
  DOI                      = {10.5281/zenodo.1177937}
}

@InProceedings{Albin2011,
  Title                    = {Beatscape , a Mixed Virtual-Physical Environment for Musical Ensembles},
  Author                   = {Albin, Aaron and Sent\''{u}rk, Sertan and Van Troyer, Akito and Blosser, Brian and Jan, Oliver and Weinberg, Gil},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {112--115},
  Abstract                 = {A mixed media tool was created that promotes ensemblevirtuosity through tight coordination and interdepence inmusical performance. Two different types of performers interact with a virtual space using Wii remote and tangibleinterfaces using the reacTIVision toolkit [11]. One group ofperformers uses a tangible tabletop interface to place andmove sound objects in a virtual environment. The soundobjects are represented by visual avatars and have audiosamples associated with them. A second set of performersmake use of Wii remotes to create triggering waves thatcan collide with those sound objects. Sound is only produced upon collision of the waves with the sound objects.What results is a performance in which users must negotiate through a physical and virtual space and are positionedto work together to create musical pieces.},
  Keywords                 = {reacTIVision, processing, ensemble, mixed media, virtualization, tangible, sample },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_112.pdf},
  DOI                      = {10.5281/zenodo.1177939}
}

@InProceedings{Ando2011,
  Title                    = {Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure},
  Author                   = {Ando, Daichi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {76--79},
  Abstract                 = {The use of Interactive Evolutionary Computation (IEC) is suitable to the development of art-creation aid system for beginners. This is because of important features of IEC, like the ability of optimizing with ambiguous evaluation measures, and not requiring special knowledge about art-creation. With the popularity of Consumer Generated Media, many beginners in term of art-creation are interested in creating their own original art works. Thus developing of useful IEC system for musical creation is an urgent task. However, user-assist functions for IEC proposed in pastworks decrease the possibility of getting good unexpected results, which is an important feature of art-creation with IEC. In this paper, The author proposes a new IEC evaluation process named "Shopping Basket" procedure IEC. In the procedure, an user-assist function called Similarity-Based Reasoning allows for natural evaluation by the user. The function reduces user's burden without reducing the possibility of unexpected results. The author performs an experiment where subjects use the new interface to validate it. As a result of the experiment, the author concludes that the new interface is better to motivate users to compose with IEC system than the old interface.},
  Keywords                 = {Interactive Evolutionary Computation, User-Interface, Composition Aid },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_076.pdf},
  DOI                      = {10.5281/zenodo.1177941}
}

@InProceedings{Angel2011,
  Title                    = {Creating Interactive Multimedia Works with Bio-data},
  Author                   = {Angel, Claudia R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {421--424},
  Abstract                 = {This paper deals with the usage of bio-data from performers to create interactive multimedia performances or installations. It presents this type of research in some art works produced in the last fifty years (such as Lucier's Music for a Solo Performance, from 1965), including two interactive performances of my ,
,
authorship, which use two different types of bio-interfaces: on the one hand, an EMG (Electromyography) and on the other hand, an EEG (electroencephalography). The paper explores the interaction between the human body and real-time media (audio and visual) by the usage of bio-interfaces. This research is based on biofeedback investigations pursued by the psychologist Neal E. Miller in the 1960s, mainly based on finding new methods to reduce stress. However, this article explains and shows examples in which biofeedback research is used for artistic purposes only. },
  Keywords                 = {Live electronics, Butoh, performance, biofeedback, interactive sound and video. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_421.pdf},
  DOI                      = {10.5281/zenodo.1177943}
}

@InProceedings{Bokesoy2011,
  Title                    = {1city1001vibrations : Development of a Interactive Sound Installation with Robotic Instrument Performance},
  Author                   = {B\''{o}kesoy, Sinan and Adler, Patrick},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {52--55},
  Keywords                 = {Sound installation, robotic music, interactive systems },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_052.pdf},
  DOI                      = {10.5281/zenodo.1177945}
}

@InProceedings{Baath2011,
  Title                    = {Eye Tapping : How to Beat Out an Accurate Rhythm using Eye Movements},
  Author                   = {B\aa\aath, Rasmus and Strandberg, Thomas and Balkenius, Christian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {441--444},
  Abstract                 = {The aim of this study was to investigate how well subjectsbeat out a rhythm using eye movements and to establishthe most accurate method of doing this. Eighteen subjectsparticipated in an experiment were five different methodswere evaluated. A fixation based method was found to bethe most accurate. All subjects were able to synchronizetheir eye movements with a given beat but the accuracywas much lower than usually found in finger tapping studies. Many parts of the body are used to make music but sofar, with a few exceptions, the eyes have been silent. The research presented here provides guidelines for implementingeye controlled musical interfaces. Such interfaces would enable performers and artists to use eye movement for musicalexpression and would open up new, exiting possibilities.},
  Keywords                 = {Rhythm, Eye tracking, Sensorimotor synchronization, Eye tapping },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_441.pdf},
  DOI                      = {10.5281/zenodo.1177947}
}

@InProceedings{Barenca2011,
  Title                    = {The Manipuller : Strings Manipulation and Multi-Dimensional Force Sensing},
  Author                   = {Barenca, Adri\'{a}n and Torre, Giuseppe},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {232--235},
  Abstract                 = {The Manipuller is a novel Gestural Controller based on strings manipulation and multi-dimensional force sensing technology. This paper describes its motivation, design and operational principles along with some of its musical applications. Finally the results of a preliminary usability test are presented and discussed. },
  Keywords                 = {1,and force sensors within,force sensing,gestural,gestural controller,manipulation,strings,strings and force sensing,the integration of strings},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_232.pdf},
  DOI                      = {10.5281/zenodo.1177949}
}

@InProceedings{Beck2011,
  Title                    = {Tangible Performance Management of Grid-based Laptop Orchestras},
  Author                   = {Beck, Stephen D. and Branton, Chris and Maddineni, Sharath},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {207--210},
  Abstract                 = {Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups ofperformers to use ordinary laptop computers as instrumentsand sound sources in the performance of specially createdmusic software. Perhaps the biggest challenge for LOs isthe distribution, management and control of software acrossheterogeneous collections of networked computers. Software must be stored and distributed from a central repository, but launched on individual laptops immediately beforeperformance. The GRENDL project leverages proven gridcomputing frameworks and approaches the Laptop Orchestra as a distributed computing platform for interactive computer music. This allows us to readily distribute softwareto each laptop in the orchestra depending on the laptop'sinternal configuration, its role in the composition, and theplayer assigned to that computer. Using the SAGA framework, GRENDL is able to distribute software and managesystem and application environments for each composition.Our latest version includes tangible control of the GRENDLenvironment for a more natural and familiar user experience.},
  Keywords                 = {laptop orchestra, tangible interaction, grid computing },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_207.pdf},
  Presentation-video = {https://vimeo.com/26860960/},
  DOI                      = {10.5281/zenodo.1177951}
}

@InProceedings{Berdahl2011,
  Title                    = {Autonomous New Media Artefacts ( AutoNMA )},
  Author                   = {Berdahl, Edgar and Chafe, Chris},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {322--323},
  Abstract                 = {The purpose of this brief paper is to revisit the question oflongevity in present experimental practice and coin the termautonomous new media artefacts (AutoNMA), which arecomplete and independent of external computer systems,so they can be operable for a longer period of time andcan be demonstrated at a moment's notice. We argue thatplatforms for prototyping should promote the creation ofAutoNMA to make extant the devices which will be a partof the future history of new media.},
  Keywords                 = {autonomous, standalone, Satellite CCRMA, Arduino },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_322.pdf},
  DOI                      = {10.5281/zenodo.1177953}
}

@InProceedings{Berdahl2011a,
  Title                    = {Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform},
  Author                   = {Berdahl, Edgar and Ju, Wendy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {173--178},
  Abstract                 = {This paper describes a new Beagle Board-based platform forteaching and practicing interaction design for musical applications. The migration from desktop and laptop computerbased sound synthesis to a compact and integrated control, computation and sound generation platform has enormous potential to widen the range of computer music instruments and installations that can be designed, and improvesthe portability, autonomy, extensibility and longevity of designed systems. We describe the technical features of theSatellite CCRMA platform and contrast it with personalcomputer-based systems used in the past as well as emergingsmart phone-based platforms. The advantages and tradeoffs of the new platform are considered, and some projectwork is described.},
  Keywords                 = {arduino,beagle board,instruments omap,linux,microcontrollers,music controllers,nime,pd,pedagogy,texas},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_173.pdf},
  Presentation-video = {https://vimeo.com/26833829/},
  DOI                      = {10.5281/zenodo.1177957}
}

@InProceedings{Bergsland2011,
  Title                    = {Phrases from {P}aul {L}ansky's {S}ix {F}antasies},
  Author                   = {Bergsland, Andreas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {523--526},
  Keywords                 = {LPC, software instrument, analysis, modeling, csound },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_523.pdf},
  DOI                      = {10.5281/zenodo.1177959}
}

@InProceedings{Berthaut2011,
  Title                    = {First Person Shooters as Collaborative Multiprocess Instruments},
  Author                   = {Berthaut, Florent and Katayose, Haruhiro and Wakama, Hironori and Totani, Naoyuki and Sato, Yuichi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {44--47},
  Abstract                 = {First Person Shooters are among the most played computer videogames. They combine navigation, interaction and collaboration in3D virtual environments using simple input devices, i.e. mouseand keyboard. In this paper, we study the possibilities broughtby these games for musical interaction. We present the Couacs, acollaborative multiprocess instrument which relies on interactiontechniques used in FPS together with new techniques adding theexpressiveness required for musical interaction. In particular, theFaders For All game mode allows musicians to perform patternbased electronic compositions.},
  Keywords                 = {the couacs, fps, first person shooters, collaborative, 3D interaction, multiprocess instrument },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_044.pdf},
  DOI                      = {10.5281/zenodo.1177961}
}

@InProceedings{Beyer2011,
  Title                    = {Music Interfaces for Novice Users : Composing Music on a Public Display with Hand Gestures},
  Author                   = {Beyer, Gilbert and Meier, Max},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {507--510},
  Keywords                 = {Interactive music, public displays, user experience, out-of-home media, algorithmic composition, soft constraints },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_507.pdf},
  DOI                      = {10.5281/zenodo.1177963}
}

@InProceedings{Bisig2011,
  Title                    = {Flowspace -- A Hybrid Ecosystem},
  Author                   = {Bisig, Daniel and Schacher, Jan C. and Neukom, Martin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {260--263},
  Abstract                 = {In this paper an audio-visual installation is discussed, which combines interactive, immersive and generative elements. After introducing some of the challenges in the field of Generative Art and placing the work within its research context, conceptual reflections are made about the spatial, behavioural, perceptual and social issues that are raised within the entire installation. A discussion about the artistic content follows, focussing on the scenography and on working with flocking algorithms in general, before addressing three specific pieces realised for the exhibition. Next the technical implementation for both hardand software are detailed before the idea of a hybrid ecosystem gets discussed and further developments outlined.},
  Keywords                 = {Generative Art, Interactive Environment, Immersive Installation, Swarm Simulation, Hybrid Ecosystem },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_260.pdf},
  DOI                      = {10.5281/zenodo.1177965}
}

@InProceedings{Bokowiec2011,
  Title                    = {V'OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8~{C}hannel Spatialization},
  Author                   = {Bokowiec, Mark A.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {40--43},
  Abstract                 = {V'OCT(Ritual) is a work for solo vocalist/performer and Bodycoder System, composed in residency at Dartington College of Arts (UK) Easter 2010. This paper looks at the technical and compositional methodologies used in the realization of the work, in particular, the choices made with regard to the mapping of sensor elements to various spatialization functions. Kinaesonics will be discussed in relation to the coding of real-time one-to-one mapping of sound to gesture and its expression in terms of hardware and software design. Four forms of expressivity arising out of interactive work with the Bodycoder system will be identified. How sonic (electro-acoustic), programmed, gestural (kinaesonic) and in terms of the V'Oct(Ritual) vocal expressivities are constructed as pragmatic and tangible elements within the compositional practice will be discussed and the subsequent importance of collaboration with a performer will be exposed. },
  Keywords                 = {Bodycoder, Kinaesonics, Expressivity, Gestural Control, Interactive Performance Mechanisms, Collaboration. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_040.pdf},
  DOI                      = {10.5281/zenodo.1177967}
}

@InProceedings{Brandtsegg2011,
  Title                    = {A Modulation Matrix for Complex Parameter Sets},
  Author                   = {Brandtsegg, \''{O}yvind and Saue, Sigurd and Johansen, Thom},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {316--319},
  Abstract                 = {The article describes a flexible mapping technique realized as a many-to-many dynamic mapping matrix. Digital sound generation is typically controlled by a large number of parameters and efficient and flexible mapping is necessary to provide expressive control over the instrument. The proposed modulation matrix technique may be seen as a generic and selfmodifying mapping mechanism integrated in a dynamic interpolation scheme. It is implemented efficiently by taking advantage of its inherent sparse matrix structure. The modulation matrix is used within the Hadron Particle Synthesizer, a complex granular module with 200 synthesis parameters and a simplified performance control structure with 4 expression parameters. },
  Keywords                 = {Mapping, granular synthesis, modulation, live performance },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_316.pdf},
  DOI                      = {10.5281/zenodo.1177969}
}

@InProceedings{Bryan2011,
  Title                    = {Two Turntables and a Mobile Phone},
  Author                   = {Bryan, Nicholas J. and Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {179--184},
  Keywords                 = {Digital scratching, mobile music, digital DJ, smartphone, turntable, turntablism, record player, accelerometer, gyroscope, vinyl emulation software },
  Abstract = {A novel method of digital scratching is presented as an alternative to currently available digital hardware interfaces and time-coded vinyl (TCV). Similar to TCV, the proposed method leverages existing analog turntables as a physical interface to manipulate the playback of digital audio. To doso, however, an accelerometer/gyroscope–equipped smartphone is firmly attached to a modified record, placed on a turntable, and used to sense a performers movement, resulting in a wireless sensing-based scratching method. The accelerometer and gyroscope data is wirelessly transmitted to a computer to manipulate the digital audio playback in real-time. The method provides the benefit of digital audio and storage, requires minimal additional hardware, accommodates familiar proprioceptive feedback, and allows a single interface to control both digital and analog audio. In addition, the proposed method provides numerous additional benefits including real-time graphical display,multi-touch interaction, and untethered performance (e.g“air-scratching”). Such a method turns a vinyl record into an interactive surface and enhances traditional scratching performance by affording new and creative musical interactions. Informal testing show this approach to be viable,responsive, and robust.},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_179.pdf},
  Presentation-video = {https://vimeo.com/26835277/},
  DOI                      = {10.5281/zenodo.1177971}
}

@InProceedings{Bullock2011,
  Title                    = {Integra Live : a New Graphical User Interface for Live Electronic Music},
  Author                   = {Bullock, Jamie and Beattie, Daniel and Turner, Jerome},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {387--392},
  Keywords                 = {live electronics,software,usability,user experience},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_387.pdf},
  Presentation-video = {https://vimeo.com/26906574/},
  DOI                      = {10.5281/zenodo.1177973}
}

@InProceedings{Cappelen2011,
  Title                    = {Expanding the Role of the Instrument},
  Author                   = {Cappelen, Birgitta and Anderson, Anders-Petter},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {511--514},
  Abstract                 = {The traditional role of the musical instrument is to be the working tool of the professional musician. On the instrument the musician performs music for the audience to listen to. In this paper we present an interactive installation, where we expand the role of the instrument to motivate musicking and cocreation between diverse users. We have made an open installation, where users can perform a variety of actions in several situations. By using the abilities of the computer, we have made an installation, which can be interpreted to have many roles. It can both be an instrument, a co-musician, a communication partner, a toy, a meeting place and an ambient musical landscape. The users can dynamically shift between roles, based on their abilities, knowledge and motivation. },
  Keywords                 = {design,genre,interaction,interactive installation,music instrument,musicking,narrative,open,role,sound art},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_511.pdf},
  DOI                      = {10.5281/zenodo.1177975}
}

@InProceedings{Caramiaux2011,
  Title                    = {Sound Selection by Gestures},
  Author                   = {Caramiaux, Baptiste and Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {329--330},
  Abstract                 = {This paper presents a prototypical tool for sound selection driven by users' gestures. Sound selection by gesturesis a particular case of "query by content" in multimedia databases. Gesture-to-Sound matching is based on computing the similarity between both gesture and sound parameters' temporal evolution. The tool presents three algorithms for matching gesture query to sound target. Thesystem leads to several applications in sound design, virtualinstrument design and interactive installation.},
  Keywords                 = {Query by Gesture, Time Series Analysis, Sonic Interaction },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_329.pdf},
  DOI                      = {10.5281/zenodo.1177977}
}

@InProceedings{Caramiaux2011a,
  Title                    = {Gestural Embodiment of Environmental Sounds: an Experimental Study},
  Author                   = {Caramiaux, Baptiste and Susini, Patrick and Bianco, Tommaso and Bevilacqua, Fr\'{e}d\'{e}ric and Houix, Olivier and Schnell, Norbert and Misdariis, Nicolas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {144--148},
  Abstract                 = {In this paper we present an experimental study concerninggestural embodiment of environmental sounds in a listeningcontext. The presented work is part of a project aiming atmodeling movement-sound relationships, with the end goalof proposing novel approaches for designing musical instruments and sounding objects. The experiment is based onsound stimuli corresponding to "causal" and "non-causal" sounds. It is divided into a performance phase and an interview. The experiment is designed to investigate possiblecorrelation between the perception of the "causality" of environmental sounds and different gesture strategies for thesound embodiment. In analogy with the perception of thesounds' causality, we propose to distinguish gestures that "mimic" a sound's cause and gestures that "trace" a sound'smorphology following temporal sound characteristics. Results from the interviews show that, first, our causal soundsdatabase lead to consistent descriptions of the action at theorigin of the sound and participants mimic this action. Second, non-causal sounds lead to inconsistent metaphoric descriptions of the sound and participants make gestures following sound "contours". Quantitatively, the results showthat gesture variability is higher for causal sounds that noncausal sounds.},
  Keywords                 = {Embodiment, Environmental Sound Perception, Listening, Gesture Sound Interaction },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_144.pdf},
  Presentation-video = {https://vimeo.com/26805553/},
  DOI                      = {10.5281/zenodo.1177979}
}

@InProceedings{Carlson2011,
  Title                    = {The Sound Flinger : A Haptic Spatializer},
  Author                   = {Carlson, Chris and Marschner, Eli and Mccurry, Hunter},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {138--139},
  Keywords                 = {arduino,beagleboard,ccrma,force feedback,haptics,jack,linux audio,multi-channel audio,nime,pd,pure data,satellite ccrma,sound spatialization},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_138.pdf},
  DOI                      = {10.5281/zenodo.1177981}
}

@InProceedings{Carrascal2011,
  Title                    = {Multitouch Interface for Audio Mixing},
  Author                   = {Carrascal, Juan P. and Jord\`{a}, Sergi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {100--103},
  Abstract                 = {Audio mixing is the adjustment of relative volumes, panning and other parameters corresponding to different soundsources, in order to create a technically and aesthetically adequate sound sum. To do this, audio engineers employ "panpots" and faders, the standard controls in audio mixers. The design of such devices has remained practically unchanged for decades since their introduction. At the time,no usability studies seem to have been conducted on suchdevices, so one could question if they are really optimizedfor the task they are meant for.This paper proposes a new set of controls that might beused to simplify and/or improve the performance of audiomixing tasks, taking into account the spatial characteristicsof modern mixing technologies such as surround and 3Daudio and making use of multitouch interface technologies.A preliminary usability test has shown promising results.},
  Keywords                 = {audio mixing,control surface,multitouch,touchscreen},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_100.pdf},
  DOI                      = {10.5281/zenodo.1177983}
}

@InProceedings{Choe2011,
  Title                    = {{SW}AF: Towards a Web Application Framework for Composition and Documentation of Soundscape},
  Author                   = {Choe, Souhwan and Lee, Kyogu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {533--534},
  Abstract                 = {In this paper, we suggest a conceptual model of a Web application framework for the composition and documentation of soundscape and introduce corresponding prototype projects, SeoulSoundMap and SoundScape Composer. We also survey the current Web-based sound projects in terms of soundscape documentation. },
  Keywords                 = {soundscape, web application framework, sound archive, sound map, soundscape composition, soundscape documentation. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_533.pdf},
  DOI                      = {10.5281/zenodo.1177985}
}

@InProceedings{Comajuncosas2011,
  Title                    = {Nuvolet: {3D} Gesture-driven Collaborative Audio Mosaicing},
  Author                   = {Comajuncosas, Josep M. and Barrachina, Alex and O'Connell, John and Guaus, Enric},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {252--255},
  Abstract                 = {This research presents a 3D gestural interface for collaborative concatenative sound synthesis and audio mosaicing.Our goal is to improve the communication between the audience and performers by means of an enhanced correlationbetween gestures and musical outcome. Nuvolet consists ofa 3D motion controller coupled to a concatenative synthesis engine. The interface detects and tracks the performers hands in four dimensions (x,y,z,t) and allows them toconcurrently explore two or three-dimensional sound cloudrepresentations of the units from the sound corpus, as wellas to perform collaborative target-based audio mosaicing.Nuvolet is included in the Esmuc Laptop Orchestra catalogfor forthcoming performances.},
  Keywords                 = {concatenative synthesis, audio mosaicing, open-air interface, gestural controller, musical instrument, 3D },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_252.pdf},
  DOI                      = {10.5281/zenodo.1177987}
}

@InProceedings{Crevoisier2011,
  Title                    = {Mapping Objects with the Surface Editor},
  Author                   = {Crevoisier, Alain and Picard-Limpens, C\'{e}cile},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {236--239},
  Abstract                 = {The Surface Editor is a software tool for creating control interfaces and mapping input actions to OSC or MIDI actions very easily and intuitively. Originally conceived to be used with a tactile interface, the Surface Editor has been extended to support the creation of graspable interfaces as well. This paper presents a new framework for the generic mapping of user actions with graspable objects on a surface. We also present a system for detecting touch on thin objects, allowing for extended interactive possibilities. The Surface Editor is not limited to a particular tracking system though, and the generic mapping approach for objects can have a broader use with various input interfaces supporting touch and/or objects. },
  Keywords                 = {NIME, mapping, interaction, user-defined interfaces, tangibles, graspable interfaces. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_236.pdf},
  DOI                      = {10.5281/zenodo.1177989}
}

@InProceedings{Dahl2011,
  Title                    = {TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data},
  Author                   = {Dahl, Luke and Herrera, Jorge and Wilkerson, Carr},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {272--275},
  Abstract                 = {TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.},
  Keywords                 = {Twitter, audience participation, sonification, data visualization, text processing, interaction, multi-user instrument. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_272.pdf},
  DOI                      = {10.5281/zenodo.1177991}
}

@InProceedings{Derbinsky2011,
  Title                    = {Cognitive Architecture in Mobile Music Interactions},
  Author                   = {Derbinsky, Nate and Essl, Georg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {104--107},
  Abstract                 = {This paper explores how a general cognitive architecture canpragmatically facilitate the development and exploration ofinteractive music interfaces on a mobile platform. To thisend we integrated the Soar cognitive architecture into themobile music meta-environment urMus. We develop anddemonstrate four artificial agents which use diverse learningmechanisms within two mobile music interfaces. We alsoinclude details of the computational performance of theseagents, evincing that the architecture can support real-timeinteractivity on modern commodity hardware.},
  Keywords                 = {cognitive architecture,machine learning,mobile music},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_104.pdf},
  DOI                      = {10.5281/zenodo.1177993}
}

@InProceedings{Diakopoulos2011,
  Title                    = {HIDUINO : A firmware for building driverless {USB}-MIDI devices using the Arduino microcontroller},
  Author                   = {Diakopoulos, Dimitri and Kapur, Ajay},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {405--408},
  Abstract                 = {This paper presents a series of open-source firmwares for the latest iteration of the popular Arduino microcontroller platform. A portmanteau of Human Interface Device and Arduino, the HIDUINO project tackles a major problem in designing NIMEs: easily and reliably communicating with a host computer using standard MIDI over USB. HIDUINO was developed in conjunction with a class at the California Institute of the Arts intended to teach introductory-level human-computer and human-robot interaction within the context of musical controllers. We describe our frustration with existing microcontroller platforms and our experiences using the new firmware to facilitate the development and prototyping of new music controllers. },
  Keywords                 = {Arduino, USB, HID, MIDI, HCI, controllers, microcontrollers },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_405.pdf},
  Presentation-video = {https://vimeo.com/26908264/},
  DOI                      = {10.5281/zenodo.1177995}
}

@InProceedings{Dimitrov2011,
  Title                    = {Audio Arduino -- an ALSA (Advanced Linux Sound Architecture) Audio Driver for FTDI-based Arduinos},
  Author                   = {Dimitrov, Smilen and Serafin, Stefania},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {211--216},
  Abstract                 = {A contemporary PC user, typically expects a sound cardto be a piece of hardware, that: can be manipulated by'audio' software (most typically exemplified by 'media players'); and allows interfacing of the PC to audio reproduction and/or recording equipment. As such, a 'sound card'can be considered to be a system, that encompasses designdecisions on both hardware and software levels -- that also demand a certain understanding of the architecture of thetarget PC operating system.This project outlines how an Arduino Duemillanoveboard (containing a USB interface chip, manufactured byFuture Technology Devices International Ltd [FTDI]company) can be demonstrated to behave as a full-duplex,mono, 8-bit 44.1 kHz soundcard, through an implementation of: a PC audio driver for ALSA (Advanced LinuxSound Architecture); a matching program for theArduino'sATmega microcontroller -- and nothing more than headphones (and a couple of capacitors). The main contributionof this paper is to bring a holistic aspect to the discussionon the topic of implementation of soundcards -- also by referring to open-source driver, microcontroller code and testmethods; and outline a complete implementation of an open -- yet functional -- soundcard system.},
  Keywords                 = {alsa,arduino,audio,driver,linux,sound card},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_211.pdf},
  DOI                      = {10.5281/zenodo.1177997}
}

@InProceedings{Donald2011,
  Title                    = {Designing the EP Trio: Instrument Identities, Control and Performance Practice in an Electronic Chamber Music Ensemble},
  Author                   = {Donald, Erika and Duinker, Ben and Britton, Eliot},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {491--494},
  Abstract                 = {This paper outlines the formation of the Expanded Performance (EP) trio, a chamber ensemble comprised of electriccello with sensor bow, augmented digital percussion, anddigital turntable with mixer. Decisions relating to physical set-ups and control capabilities, sonic identities, andmappings of each instrument, as well as their roles withinthe ensemble, are explored. The contributions of these factors to the design of a coherent, expressive ensemble andits emerging performance practice are considered. The trioproposes solutions to creation, rehearsal and performanceissues in ensemble live electronics.},
  Keywords                 = {Live electronics, digital performance, mapping, chamber music, ensemble, instrument identity },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_491.pdf},
  DOI                      = {10.5281/zenodo.1177999}
}

@InProceedings{Engum2011,
  Title                    = {Real-time Control and Creative Convolution},
  Author                   = {Engum, Trond},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {519--522},
  Abstract                 = {This paper covers and also describes an ongoing research project focusing on new artistic possibilities by exchanging music technological methods and techniques between two distinct musical genres. Through my background as a guitarist and composer in an experimental metal band I have experienced a vast development in music technology during the last 20 years. This development has made a great impact in changing the procedures for composing and producing music within my genre without necessarily changing the strategies of how the technology is used. The transition from analogue to digital sound technology not only opened up new ways of manipulating and manoeuvring sound, it also opened up challenges in how to integrate and control the digital sound technology as a seamless part of my musical genre. By using techniques and methods known from electro-acoustic/computer music, and adapting them for use within my tradition, this research aims to find new strategies for composing and producing music within my genre. },
  Keywords                 = {Artistic research, strategies for composition and production, convolution, environmental sounds, real time control },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_519.pdf},
  DOI                      = {10.5281/zenodo.1178001}
}

@InProceedings{Erkut2011,
  Title                    = {A Structured Design and Evaluation Model with Application to Rhythmic Interaction Displays},
  Author                   = {Erkut, Cumhur and Jylh\''{a}, Antti and Discioglu, Reha},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {477--480},
  Abstract                 = {We present a generic, structured model for design and evaluation of musical interfaces. This model is developmentoriented, and it is based on the fundamental function of themusical interfaces, i.e., to coordinate the human action andperception for musical expression, subject to human capabilities and skills. To illustrate the particulars of this modeland present it in operation, we consider the previous designand evaluation phase of iPalmas, our testbed for exploringrhythmic interaction. Our findings inform the current design phase of iPalmas visual and auditory displays, wherewe build on what has resonated with the test users, and explore further possibilities based on the evaluation results.},
  Keywords                 = {multimodal displays,rhythmic interaction,sonification,uml},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_477.pdf},
  DOI                      = {10.5281/zenodo.1178003}
}

@InProceedings{Fabiani2011,
  Title                    = {MoodifierLive : Interactive and Collaborative Expressive Music Performance on Mobile Devices},
  Author                   = {Fabiani, Marco and Dubus, Ga\''{e}l and Bresin, Roberto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {116--119},
  Abstract                 = {This paper presents MoodifierLive, a mobile phone application for interactive control of rule-based automatic musicperformance. Five different interaction modes are available,of which one allows for collaborative performances with upto four participants, and two let the user control the expressive performance using expressive hand gestures. Evaluations indicate that the application is interesting, fun touse, and that the gesture modes, especially the one based ondata from free expressive gestures, allow for performanceswhose emotional content matches that of the gesture thatproduced them.},
  Keywords                 = {Expressive performance, gesture, collaborative performance, mobile phone },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_116.pdf},
  DOI                      = {10.5281/zenodo.1178005}
}

@InProceedings{VonFalkenstein2011,
  Title                    = {Gliss : An Intuitive Sequencer for the iPhone and iPad},
  Author                   = {von Falkenstein, Jan T.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {527--528},
  Abstract                 = {Gliss is an application for iOS that lets the user sequence five separate instruments and play them back in various ways. Sequences can be created by drawing onto the screen while the sequencer is running. The playhead of the sequencer can be set to randomly deviate from the drawings or can be controlled via the accelerometer of the device. This makes Gliss a hybrid of a sequencer, an instrument and a generative music system. },
  Keywords                 = {Gliss, iOS, iPhone, iPad, interface, UPIC, music, sequencer, accelerometer, drawing },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_527.pdf},
  DOI                      = {10.5281/zenodo.1178007}
}

@InProceedings{Flety2011,
  Title                    = {Latency Improvement in Sensor Wireless Transmission Using {IEEE} 802.15.4},
  Author                   = {Fl\'{e}ty, Emmanuel and Maestracci, C\^{o}me},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {409--412},
  Abstract                 = {We present a strategy for the improvement of wireless sensor data transmission latency, implemented in two current projects involving gesture/control sound interaction. Our platform was designed to be capable of accepting accessories using a digital bus. The receiver features a IEEE 802.15.4 microcontroller associated to a TCP/IP stack integrated circuit that transmits the received wireless data to a host computer using the Open Sound Control protocol. This paper details how we improved the latency and sample rate of the said technology while keeping the device small and scalable. },
  Keywords                 = {Embedded sensors, gesture recognition, wireless, sound and music computing, interaction, 802.15.4, Zigbee. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_409.pdf},
  Presentation-video = {https://vimeo.com/26908266/},
  DOI                      = {10.5281/zenodo.1178009}
}

@InProceedings{Forsyth2011,
  Title                    = {Random Access Remixing on the iPad},
  Author                   = {Forsyth, Jon and Glennon, Aron and Bello, Juan P.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {487--490},
  Abstract                 = {Remixing audio samples is a common technique for the creation of electronic music, and there are a wide variety oftools available to edit, process, and recombine pre-recordedaudio into new compositions. However, all of these toolsconceive of the timeline of the pre-recorded audio and theplayback timeline as identical. In this paper, we introducea dual time axis representation in which these two timelines are described explicitly. We also discuss the randomaccess remix application for the iPad, an audio sample editor based on this representation. We describe an initialuser study with 15 high school students that indicates thatthe random access remix application has the potential todevelop into a useful and interesting tool for composers andperformers of electronic music.},
  Keywords                 = {interactive systems, sample editor, remix, iPad, multi-touch },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_487.pdf},
  DOI                      = {10.5281/zenodo.1178011}
}

@InProceedings{Franinovic2011,
  Title                    = {The Flo)(ps : Negotiating Between Habitual and Explorative Gestures},
  Author                   = {Franinovic, Karmen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {448--452},
  Keywords                 = {exploration,gesture,habit,sonic interaction design},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_448.pdf},
  DOI                      = {10.5281/zenodo.1178013}
}

@InProceedings{Freed2011,
  Title                    = {Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP},
  Author                   = {Freed, Adrian and MacCallum, John and Schmeder, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {308--311},
  Abstract                 = {An effective programming style for gesture signal processing is described using a new library that brings efficient run-time polymorphism, functional and instance-based object-oriented programming to Max/MSP. By introducing better support for generic programming and composability Max/MSP becomes a more productive environment for managing the growing scale and complexity of gesture sensing systems for musical instruments and interactive installations. },
  Keywords                 = {composability,delegation,functional programming,gesture signal,max,msp,object,object-,open sound control,oriented programming,processing},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_308.pdf},
  DOI                      = {10.5281/zenodo.1178015}
}

@InProceedings{Friberg2011,
  Title                    = {Experiences from Video-Controlled Sound Installations},
  Author                   = {Friberg, Anders and K\''{a}llblad, Anna},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {128--131},
  Abstract                 = {This is an overview of the three installations Hoppsa Universum, CLOSE and Flying Carpet. They were all designed as choreographed sound and music installations controlled by the visitors movements. The perspective is from an artistic goal/vision intention in combination with the technical challenges and possibilities. All three installations were realized with video cameras in the ceiling registering the users' position or movement. The video analysis was then controlling different types of interactive software audio players. Different aspects like narrativity, user control, and technical limitations are discussed. },
  Keywords                 = {Gestures, dance, choreography, music installation, interactive music. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_128.pdf},
  DOI                      = {10.5281/zenodo.1178017}
}

@InProceedings{Fyans2011,
  Title                    = {Perceptions of Skill in Performances with Acoustic and Electronic Instruments},
  Author                   = {Fyans, A. Cavan and Gurevich, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {495--498},
  Abstract                 = {We present observations from two separate studies of spectators' perceptions of musical performances, one involvingtwo acoustic instruments, the other two electronic instruments. Both studies followed the same qualitative method,using structured interviews to ascertain and compare spectators' experiences. In this paper, we focus on outcomespertaining to perceptions of the performers' skill, relatingto concepts of embodiment and communities of practice.},
  Keywords                 = {skill, embodiment, perception, effort, control, spectator },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_495.pdf},
  DOI                      = {10.5281/zenodo.1178019}
}

@InProceedings{Fyfe2011,
  Title                    = {JunctionBox : A Toolkit for Creating Multi-touch Sound Control Interfaces},
  Author                   = {Fyfe, Lawrence and Tindale, Adam and Carpendale, Sheelagh},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {276--279},
  Abstract                 = {JunctionBox is a new software toolkit for creating multitouch interfaces for controlling sound and music. Morespecifically, the toolkit has special features which make iteasy to create TUIO-based touch interfaces for controllingsound engines via Open Sound Control. Programmers using the toolkit have a great deal of freedom to create highlycustomized interfaces that work on a variety of hardware.},
  Keywords                 = {Multi-touch, Open Sound Control, Toolkit, TUIO },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_276.pdf},
  DOI                      = {10.5281/zenodo.1178021}
}

@InProceedings{Gallin2011,
  Title                    = {Eobody3: a Ready-to-use Pre-mapped \& Multi-protocol Sensor Interface},
  Author                   = {Gallin, Emmanuelle and Sirguy, Marc},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {437--440},
  Keywords                 = {Controller, Sensor, MIDI, USB, Computer Music, USB, OSC, CV, MIDI, DMX, A/D Converter, Interface. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_437.pdf},
  DOI                      = {10.5281/zenodo.1178023}
}

@InProceedings{Garcia2011,
  Title                    = {Acquisition and Study of Blowing Pressure Profiles in Recorder Playing},
  Author                   = {Garc\'{\i}a, Francisco and Vinceslas, Leny and Tubau, Josep and Maestre, Esteban},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {124--127},
  Abstract                 = {This paper presents a study of blowing pressure profilesacquired from recorder playing. Blowing pressure signalsare captured from real performance by means of a a lowintrusiveness acquisition system constructed around commercial pressure sensors based on piezoelectric transducers.An alto recorder was mechanically modified by a luthierto allow the measurement and connection of sensors whilerespecting playability and intrusiveness. A multi-modaldatabase including aligned blowing pressure and sound signals is constructed from real practice, covering the performance space by considering different fundamental frequencies, dynamics, articulations and note durations. Once signals were pre-processed and segmented, a set of temporalenvelope features were defined as a basis for studying andconstructing a simplified model of blowing pressure profilesin different performance contexts.},
  Keywords                 = {blowing,instrumental gesture,multi-modal data,pressure,recorder,wind instrument},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_124.pdf},
  DOI                      = {10.5281/zenodo.1178025}
}

@InProceedings{Garcia2011a,
  Title                    = {InkSplorer : Exploring Musical Ideas on Paper and Computer},
  Author                   = {Garcia, J\'{e}r\'{e}mie and Tsandilas, Theophanis and Agon, Carlos and Mackay, Wendy E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {361--366},
  Abstract                 = {We conducted three studies with contemporary music composers at IRCAM. We found that even highly computer-literate composers use an iterative process that begins with expressing musical ideas on paper, followed by active parallel exploration on paper and in software, prior to final execution of their ideas as an original score. We conducted a participatory design study that focused on the creative exploration phase, to design tools that help composers better integrate their paper-based and electronic activities. We then developed InkSplorer as a technology probe that connects users' hand-written gestures on paper to Max/MSP and OpenMusic. Composers appropriated InkSplorer according to their preferred composition styles, emphasizing its ability to help them quickly explore musical ideas on paper as they interact with the computer. We conclude with recommendations for designing interactive paper tools that support the creative process, letting users explore musical ideas both on paper and electronically. },
  Keywords                 = {Composer, Creativity, Design Exploration, InkSplorer, Interactive Paper, OpenMusic, Technology Probes. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_361.pdf},
  Presentation-video = {https://vimeo.com/26881368/},
  DOI                      = {10.5281/zenodo.1178027}
}

@InProceedings{Gillian2011,
  Title                    = {Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping},
  Author                   = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {337--342},
  Abstract                 = {This paper presents a novel algorithm that has been specifically designed for the recognition of multivariate temporal musical gestures. The algorithm is based on DynamicTime Warping and has been extended to classify any N dimensional signal, automatically compute a classificationthreshold to reject any data that is not a valid gesture andbe quickly trained with a low number of training examples.The algorithm is evaluated using a database of 10 temporalgestures performed by 10 participants achieving an averagecross-validation result of 99%.},
  Keywords                 = {Dynamic Time Warping, Gesture Recognition, Musician-Computer Interaction, Multivariate Temporal Gestures },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_337.pdf},
  Presentation-video = {https://vimeo.com/26874428/},
  DOI                      = {10.5281/zenodo.1178029}
}

@InProceedings{Gillian2011a,
  Title                    = {A Machine Learning Toolbox For Musician Computer Interaction},
  Author                   = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {343--348},
  Abstract                 = {This paper presents the SARC EyesWeb Catalog, (SEC),a machine learning toolbox that has been specifically developed for musician-computer interaction. The SEC features a large number of machine learning algorithms that can be used in real-time to recognise static postures, perform regression and classify multivariate temporal gestures. The algorithms within the toolbox have been designed to work with any N -dimensional signal and can be quickly trained with a small number of training examples. We also provide the motivation for the algorithms used for the recognition of musical gestures to achieve a low intra-personal generalisation error, as opposed to the inter-personal generalisation error that is more common in other areas of human-computer interaction.},
  Keywords                 = {Machine learning, gesture recognition, musician-computer interaction, SEC },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_343.pdf},
  Presentation-video = {https://vimeo.com/26872843/},
  DOI                      = {10.5281/zenodo.1178031}
}

@InProceedings{Gold2011,
  Title                    = {A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems},
  Author                   = {Gold, Nicolas E. and Dannenberg, Roger B.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {36--39},
  Abstract                 = {Popular music (characterized by improvised instrumental parts, beat and measure-level organization, and steady tempo) poses challenges for human-computer music performance (HCMP). Pieces of music are typically rearrangeable on-the-fly and involve a high degree of variation from ensemble to ensemble, and even between rehearsal and performance. Computer systems aiming to participate in such ensembles must therefore cope with a dynamic high-level structure in addition to the more traditional problems of beat-tracking, score-following, and machine improvisation. There are many approaches to integrating the components required to implement dynamic human-computer music performance systems. This paper presents a reference architecture designed to allow the typical sub-components (e.g. beat-tracking, tempo prediction, improvisation) to be integrated in a consistent way, allowing them to be combined and/or compared systematically. In addition, the paper presents a dynamic score representation particularly suited to the demands of popular music performance by computer. },
  Keywords                 = {live performance,popular music,software design},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_036.pdf},
  DOI                      = {10.5281/zenodo.1178033}
}

@InProceedings{Goncalves2011,
  Title                    = {Towards a Voltage-Controlled Computer Control and Interaction Beyond an Embedded System},
  Author                   = {Goncalves, Andr{\'{e}}},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {92--95},
  Abstract                 = {The importance of embedded devices as new devices to thefield of Voltage-Controlled Synthesizers is realized. Emphasis is directed towards understanding the importance of suchdevices in Voltage-Controlled Synthesizers. Introducing theVoltage-Controlled Computer as a new paradigm. Specifications for hardware interfacing and programming techniquesare described based on real prototypes. Implementationsand successful results are reported.},
  Keywords                 = {Voltage-controlled synthesizer, embedded systems, voltage-controlled computer, computer driven control voltage generation },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_092.pdf},
  DOI                      = {10.5281/zenodo.1178035}
}

@InProceedings{Hahnel2011,
  Title                    = {Studying Interdependencies in Music Performance : An Interactive Tool},
  Author                   = {H\''{a}hnel, Tilo and Berndt, Axel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {48--51},
  Keywords                 = {articula-,duration,dynamics,egales,loudness,notes in,synthetic performance,timing,tion},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_048.pdf},
  DOI                      = {10.5281/zenodo.1178037}
}

@InProceedings{Hansen2011,
  Title                    = {Play Fluency in Music Improvisation Games for Novices},
  Author                   = {Hansen, Anne-Marie S. and Anderson, Hans J. and Raudaskoski, Pirkko},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {220--223},
  Abstract                 = {In this paper a collaborative music game for two pen tablets is studied in order to see how two people with no professional music background negotiated musical improvisation. In an initial study of what it is that constitutes play fluency in improvisation, a music game has been designed and evaluated through video analysis: A qualitative view of mutual action describes the social context of music improvisation: how two people with speech, laughter, gestures, postures and pauses negotiate individual and joint action. The objective behind the design of the game application was to support players in some aspects of their mutual play. Results show that even though players activated additional sound feedback as a result of their mutual play, players also engaged in forms of mutual play that the game engine did not account for. These ways of mutual play are descibed further along with some suggestions for how to direct future designs of collaborative music improvisation games towards ways of mutual play. },
  Keywords                 = {Collaborative interfaces, improvisation, interactive music games, social interaction, play, novice. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_220.pdf},
  DOI                      = {10.5281/zenodo.1178039}
}

@InProceedings{Harriman2011,
  Title                    = {Quadrofeelia -- A New Instrument for Sliding into Notes},
  Author                   = {Harriman, Jiffer and Casey, Locky and Melvin, Linden},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {529--530},
  Abstract                 = {This paper describes a new musical instrument inspired by the pedal-steel guitar, along with its motivations and other considerations. Creating a multi-dimensional, expressive instrument was the primary driving force. For these criteria the pedal steel guitar proved an apt model as it allows control over several instrument parameters simultaneously and continuously. The parameters we wanted control over were volume, timbre, release time and pitch.The Quadrofeelia is played with two hands on a horizontal surface. Single notes and melodies are easily played as well as chordal accompaniment with a variety of timbres and release times enabling a range of legato and staccato notes in an intuitive manner with a new yet familiar interface.},
  Keywords                 = {NIME, pedal-steel, electronic, slide, demonstration, membrane, continuous, ribbon, instrument, polyphony, lead },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_529.pdf},
  DOI                      = {10.5281/zenodo.1178041}
}

@InProceedings{Hayes2011,
  Title                    = {Vibrotactile Feedback-Assisted Performance},
  Author                   = {Hayes, Lauren},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {72--75},
  Keywords                 = {Vibrotactile feedback, human-computer interfaces, digital composition, real-time performance, augmented instruments. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_072.pdf},
  DOI                      = {10.5281/zenodo.1178043}
}

@InProceedings{Hochenbaum2011,
  Title                    = {Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces},
  Author                   = {Hochenbaum, Jordan and Kapur, Ajay},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {240--243},
  Abstract                 = {This paper presents the SmartFiducial, a wireless tangible object that facilitates additional modes of expressivity for vision-based tabletop surfaces. Using infrared proximity sensing and resistive based force-sensors, the SmartFiducial affords users unique, and highly gestural inputs. Furthermore, the SmartFiducial incorporates additional customizable pushbutton switches. Using XBee radio frequency (RF) wireless transmission, the SmartFiducial establishes bipolar communication with a host computer. This paper describes the design and implementation of the SmartFiducial, as well as an exploratory use in a musical context. },
  Keywords                 = {Fiducial, Tangible Interface, Multi-touch, Sensors, Gesture, Haptics, Bricktable, Proximity Sensing },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_240.pdf},
  DOI                      = {10.5281/zenodo.1178045}
}

@InProceedings{Hsu2011,
  Title                    = {On Movement , Structure and Abstraction in Generative Audiovisual Improvisation},
  Author                   = {Hsu, William},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {417--420},
  Keywords                 = {animation,audio-visual,generative,improvisation,interactive},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_417.pdf},
  DOI                      = {10.5281/zenodo.1178047}
}

@InProceedings{Janssen2011,
  Title                    = {A Reverberation Instrument Based on Perceptual Mapping},
  Author                   = {Janssen, Berit},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {68--71},
  Abstract                 = {The present article describes a reverberation instrumentwhich is based on cognitive categorization of reverberating spaces. Different techniques for artificial reverberationwill be covered. A multidimensional scaling experimentwas conducted on impulse responses in order to determinehow humans acoustically perceive spatiality. This researchseems to indicate that the perceptual dimensions are related to early energy decay and timbral qualities. Theseresults are applied to a reverberation instrument based ondelay lines. It can be contended that such an instrumentcan be controlled more intuitively than other delay line reverberation tools which often provide a confusing range ofparameters which have a physical rather than perceptualmeaning.},
  Keywords                 = {Reverberation, perception, multidimensional scaling, mapping },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_068.pdf},
  DOI                      = {10.5281/zenodo.1178049}
}

@InProceedings{Jessop2011,
  Title                    = {Music and Technology in Death and the Powers},
  Author                   = {Jessop, Elena and Torpey, Peter A. and Bloomberg, Benjamin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {349--354},
  Abstract                 = {In composer Tod Machover's new opera Death and the Powers, the main character uploads his consciousness into anelaborate computer system to preserve his essence and agencyafter his corporeal death. Consequently, for much of theopera, the stage and the environment itself come alive asthe main character. This creative need brings with it a hostof technical challenges and opportunities. In order to satisfythe needs of this storyline, Machover's Opera of the Futuregroup at the MIT Media Lab has developed a suite of newperformance technologies, including robot characters, interactive performance capture systems, mapping systems for,
,
authoring interactive multimedia performances, new musical instruments, unique spatialized sound controls, anda unified control system for all these technological components. While developed for a particular theatrical production, many of the concepts and design procedures remain relevant to broader contexts including performance,robotics, and interaction design.},
  Keywords                 = {opera, Death and the Powers, Tod Machover, gestural interfaces, Disembodied Performance, ambisonics },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_349.pdf},
  Presentation-video = {https://vimeo.com/26878423/},
  DOI                      = {10.5281/zenodo.1178051}
}

@InProceedings{Johnston2011,
  Title                    = {Beyond Evaluation : Linking Practice and Theory in New Musical Interface Design},
  Author                   = {Johnston, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {280--283},
  Abstract                 = {This paper presents an approach to practice-based researchin new musical instrument design. At a high level, the process involves drawing on relevant theories and aesthetic approaches to design new instruments, attempting to identify relevant applied design criteria, and then examiningthe experiences of performers who use the instruments withparticular reference to these criteria. Outcomes of this process include new instruments, theories relating to musicianinstrument interaction and a set of design criteria informedby practice and research.},
  Keywords                 = {practice-based research, evaluation, Human-Computer Interaction, research methods, user studies },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_280.pdf},
  DOI                      = {10.5281/zenodo.1178053}
}

@InProceedings{DeJong2011,
  Title                    = {Making Grains Tangible: Microtouch for Microsound},
  Author                   = {de Jong, Staas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {326--328},
  Abstract                 = {This paper proposes a new research direction for the large family of instrumental musical interfaces where sound is generated using digital granular synthesis, and where interaction and control involve the (fine) operation of stiff, flat contact surfaces. First, within a historical context, a general absence of, and clear need for, tangible output that is dynamically instantiated by the grain-generating process itself is identified. Second, to fill this gap, a concrete general approach is proposed based on the careful construction of non-vibratory and vibratory force pulses, in a one-to-one relationship with sonic grains.An informal pilot psychophysics experiment initiating the approach was conducted, which took into account the two main cases for applying forces to the human skin: perpendicular, and lateral. Initial results indicate that the force pulse approach can enable perceivably multidimensional, tangible display of the ongoing grain-generating process. Moreover, it was found that this can be made to meaningfully happen (in real time) in the same timescale of basic sonic grain generation. This is not a trivial property, and provides an important and positive fundament for further developing this type of enhanced display. It also leads to the exciting prospect of making arbitrary sonic grains actual physical manipulanda. },
  Keywords                 = {and others,and today granular,barry truax,curtis roads,granular sound synthesis,instrumental control,tangible display,tangible manipulation},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_326.pdf},
  DOI                     = {10.5281/zenodo.1178055}
}

@InProceedings{Julia2011,
  Title                    = {MTCF : A Framework for Designing and Coding Musical Tabletop Applications Directly in Pure Data},
  Author                   = {Juli\`{a}, Carles F. and Gallardo, Daniel and Jord\`{a}, Sergi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {457--460},
  Abstract                 = {In the past decade we have seen a growing presence of tabletop systems applied to music, lately with even some products becoming commercially available and being used byprofessional musicians in concerts. The development of thistype of applications requires several demanding technicalexpertises such as input processing, graphical design, realtime sound generation or interaction design, and because ofthis complexity they are usually developed by a multidisciplinary group.In this paper we present the Musical Tabletop CodingFramework (MTCF) a framework for designing and codingmusical tabletop applications by using the graphical programming language for digital sound processing Pure Data(Pd). With this framework we try to simplify the creationprocess of such type of interfaces, by removing the need ofany programming skills other than those of Pd.},
  Keywords                 = {Pure Data, tabletop, tangible, framework },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_457.pdf},
  DOI                      = {10.5281/zenodo.1178057}
}

@InProceedings{Kapur2011,
  Title                    = {The KarmetiK NotomotoN : A New Breed of Musical Robot for Teaching and Performance},
  Author                   = {Kapur, Ajay and Darling, Michael and Murphy, Jim and Hochenbaum, Jordan and Diakopoulos, Dimitri and Trimpin, Trimpin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {228--231},
  Abstract                 = {This paper describes the KarmetiK NotomotoN, a new musical robotic system for performance and education. A long time goal of the ,
,
authors has been to provide users with plug-andplay, highly expressive musical robot system with a high degree of portability. This paper describes the technical details of the NotomotoN, and discusses its use in performance and educational scenarios. Detailed tests performed to optimize technical aspects of the NotomotoN are described to highlight usability and performance specifications for electronic musicians and educators. },
  Keywords                 = {music technology,musical robotics,robotic performance},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_228.pdf},
  DOI                      = {10.5281/zenodo.1178059}
}

@InProceedings{Keefe2011,
  Title                    = {The Visual in Mobile Music Performance},
  Author                   = {Keefe, Patrick O. and Essl, Georg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {191--196},
  Abstract                 = {Visual information integration in mobile music performanceis an area that has not been thoroughly explored and currentapplications are often individually designed. From camerainput to flexible output rendering, we discuss visual performance support in the context of urMus, a meta-environmentfor mobile interaction and performance development. Theuse of cameras, a set of image primitives, interactive visualcontent, projectors, and camera flashes can lead to visuallyintriguing performance possibilities.},
  Keywords                 = {Mobile performance, visual interaction, camera phone, mobile collaboration },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_191.pdf},
  Presentation-video = {https://vimeo.com/26836592/},
  DOI                      = {10.5281/zenodo.1178061}
}

@InProceedings{Kerllenevich2011,
  Title                    = {An Open Source Interface based on Biological Neural Networks for Interactive Music Performance},
  Author                   = {Kerlle\~{n}evich, Hern\'{a}n and Egu\'{\i}a, Manuel C. and Riera, Pablo E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {331--336},
  Abstract                 = {We propose and discuss an open source real-time interface that focuses in the vast potential for interactive soundart creation emerging from biological neural networks, asparadigmatic complex systems for musical exploration. Inparticular, we focus on networks that are responsible for thegeneration of rhythmic patterns.The interface relies uponthe idea of relating metaphorically neural behaviors to electronic and acoustic instruments notes, by means of flexiblemapping strategies. The user can intuitively design network configurations by dynamically creating neurons andconfiguring their inter-connectivity. The core of the systemis based in events emerging from his network design, whichfunctions in a similar way to what happens in real smallneural networks. Having multiple signal and data inputsand outputs, as well as standard communications protocolssuch as MIDI, OSC and TCP/IP, it becomes and uniquetool for composers and performers, suitable for different performance scenarios, like live electronics, sound installationsand telematic concerts.},
  Keywords                 = {rhythm generation, biological neural networks, complex patterns, musical interface, network performance },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_331.pdf},
  Presentation-video = {https://vimeo.com/26874396/},
  DOI                      = {10.5281/zenodo.1178063}
}

@InProceedings{Kim2011,
  Title                    = {Clothesline as a Metaphor for a Musical Interface},
  Author                   = {Kim, Seunghun and Kim, Luke K. and Jeong, Songhee and Yeo, Woon Seung},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {60--63},
  Abstract                 = {In this paper, we discuss the use of the clothesline as ametaphor for designing a musical interface called Airer Choir. This interactive installation is based on the function ofan ordinary object that is not a traditional instrument, andhanging articles of clothing is literally the gesture to use theinterface. Based on this metaphor, a musical interface withhigh transparency was designed. Using the metaphor, weexplored the possibilities for recognizing of input gesturesand creating sonic events by mapping data to sound. Thus,four different types of Airer Choir were developed. By classifying the interfaces, we concluded that various musicalexpressions are possible by using the same metaphor.},
  Keywords                 = {musical interface, metaphor, clothesline installation },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_060.pdf},
  DOI                      = {10.5281/zenodo.1178065}
}

@InProceedings{Kim2011a,
  Title                    = {Musical Control of a Pipe Based on Acoustic Resonance},
  Author                   = {Kim, Seunghun and Yeo, Woon Seung},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {217--219},
  Abstract                 = {In this paper, we introduce a pipe interface that recognizestouch on tone holes by the resonances in the pipe instead ofa touch sensor. This work was based on the acoustic principles of woodwind instruments without complex sensors andelectronic circuits to develop a simple and durable interface.The measured signals were analyzed to show that differentfingerings generate various sounds. The audible resonancesignal in the pipe interface can be used as a sonic event formusical expression by itself and also as an input parameterfor mapping different sounds.},
  Keywords                 = {resonance, mapping, pipe },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_217.pdf},
  DOI                      = {10.5281/zenodo.1178067}
}

@InProceedings{Kim2011b,
  Title                    = {Polyhymnia : An Automatic Piano Performance System with Statistical Modeling of Polyphonic Expression and Musical Symbol Interpretation},
  Author                   = {Kim, Tae Hun and Fukayama, Satoru and Nishimoto, Takuya and Sagayama, Shigeki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {96--99},
  Abstract                 = {We developed an automatic piano performance system calledPolyhymnia that is able to generate expressive polyphonicpiano performances with music scores so that it can be usedas a computer-based tool for an expressive performance.The system automatically renders expressive piano musicby means of automatic musical symbol interpretation andstatistical models of structure-expression relations regarding polyphonic features of piano performance. Experimental results indicate that the generated performances of various piano pieces with diverse trained models had polyphonicexpression and sounded expressively. In addition, the models trained with different performance styles reflected thestyles observed in the training performances, and they werewell distinguishable by human listeners. Polyhymnia wonthe first prize in the autonomous section of the PerformanceRendering Contest for Computer Systems (Rencon) 2010.},
  Keywords                 = {performance rendering, polyphonic expression, statistical modeling, conditional random fields },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_096.pdf},
  DOI                      = {10.5281/zenodo.1178069}
}

@InProceedings{Klugel2011,
  Title                    = {An Approach to Collaborative Music Composition},
  Author                   = {Kl\''{u}gel, Niklas and Frie\ss, Marc R. and Groh, Georg and Echtler, Florian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {32--35},
  Abstract                 = {This paper provides a discussion of how the electronic, solely ITbased composition and performance of electronic music can besupported in realtime with a collaborative application on a tabletopinterface, mediating between single-user style music compositiontools and co-located collaborative music improvisation. After having elaborated on the theoretical backgrounds of prerequisites ofco-located collaborative tabletop applications as well as the common paradigms in music composition/notation, we will review related work on novel IT approaches to music composition and improvisation. Subsequently, we will present our prototypical implementation and the results.},
  Keywords                 = {Tabletop Interface, Collaborative Music Composition, Creativity Support },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_032.pdf},
  DOI                      = {10.5281/zenodo.1178071}
}

@InProceedings{Knapp2011,
  Title                    = {MobileMuse: Integral Music Control Goes Mobile},
  Author                   = {Knapp, Benjamin and Bortz, Brennon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {203--206},
  Keywords                 = {affective computing,bile music performance,mo-,physiological signal measurement},
  Abstract = {This paper describes a new interface for mobile music creation, the MobileMuse, that introduces the capability of using  physiological  indicators of emotion as a new mode of interaction. Combining both kinematic and physiological measurement in a mobile environment creates the possibility of integral music control—the use of both gesture and emotion to control sound creation—where it has never been possible before. This paper will review the concept of integral music control and describe the motivation for creating the MobileMuse, its design and future possibilities.},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_203.pdf},
  Presentation-video = {https://vimeo.com/26858339/},
  DOI                      = {10.5281/zenodo.1178073}
}

@InProceedings{Kondapalli2011,
  Title                    = {Daft Datum -- An Interface for Producing Music Through Foot-based Interaction},
  Author                   = {Kondapalli, Ravi and Sung, Ben-Zhen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {140--141},
  Abstract                 = {Daft Datum is an autonomous new media artefact that takes input from movement of the feet (i.e. tapping/stomping/stamping) on a wooden surface, underneath which is a sensor sheet. The sensors in the sheet are mapped to various sound samples and synthesized sounds. Attributes of the synthesized sound, such as pitch and octave, can be controlled using the Nintendo Wii Remote. It also facilitates switching between modes of sound and recording/playing back a segment of audio. The result is music generated by dancing on the device that is further modulated by a hand-held controller. },
  Keywords                 = {Daft Datum, Wii, Dance Pad, Feet, Controller, Bluetooth, Musical Interface, Dance, Sensor Sheet },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_140.pdf},
  DOI                      = {10.5281/zenodo.1178075}
}

@InProceedings{Kruge2011,
  Title                    = {MadPad: A Crowdsourcing System for Audiovisual Sampling},
  Author                   = {Kruge, Nick and Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {185--190},
  Abstract                 = {MadPad is a networked audiovisual sample station for mobile devices. Twelve short video clips are loaded onto thescreen in a grid and playback is triggered by tapping anywhere on the clip. This is similar to tapping the pads of anaudio sample station, but extends that interaction to addvisual sampling. Clips can be shot on-the-fly with a cameraenabled mobile device and loaded into the player instantly,giving the performer an ability to quickly transform his orher surroundings into a sample-based, audiovisual instrument. Samples can also be sourced from an online community in which users can post or download content. The recent ubiquity of multitouch mobile devices and advances inpervasive computing have made this system possible, providing for a vast amount of content only limited by theimagination of the performer and the community. This paper presents the core features of MadPad and the designexplorations that inspired them.},
  Keywords                 = {mobile music, networked music, social music, audiovisual, sampling, user-generated content, crowdsourcing, sample station, iPad, iPhone },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_185.pdf},
  Presentation-video = {https://vimeo.com/26855684/},
  DOI                      = {10.5281/zenodo.1178077}
}

@InProceedings{Kuhara2011,
  Title                    = {Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices},
  Author                   = {Kuhara, Yasuo and Kobayashi, Daiki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {136--137},
  Abstract                 = {We developed a kinetic particles synthesizer for mobile devices having a multi-touch screen such as a tablet PC and a smart phone. This synthesizer generates music based on the kinetics of particles under a two-dimensional physics engine. The particles move in the screen to synthesize sounds according to their own physical properties, which are shape, size, mass, linear and angular velocity, friction, restitution, etc. If a particle collides with others, a percussive sound is generated. A player can play music by the simple operation of touching or dragging on the screen of the device. Using a three-axis acceleration sensor, a player can perform music by shuffling or tilting the device. Each particle sounds just a simple tone. However, a large amount of various particles play attractive music by aggregating their sounds. This concept has been inspired by natural sounds made from an assembly of simple components, for example, rustling leaves or falling rain. For a novice who has no experience of playing a musical instrument, it is easy to learn how to play instantly and enjoy performing music with intuitive operation. Our system is used for musical instruments for interactive music entertainment. },
  Keywords                 = {Particle, Tablet PC, iPhone, iPod touch, iPad, Smart phone, Kinetics, Touch screen, Physics engine. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_136.pdf},
  DOI                      = {10.5281/zenodo.1178079}
}

@InProceedings{Lamb2011,
  Title                    = {Seaboard : a New Piano Keyboard-related Interface Combining Discrete and Continuous Control},
  Author                   = {Lamb, Roland and Robertson, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {503--506},
  Abstract                 = {This paper introduces the Seaboard, a new tangible musicalinstrument which aims to provide musicians with significantcapability to manipulate sound in real-time in a musicallyintuitive way. It introduces the core design features whichmake the Seaboard unique, and describes the motivationand rationale behind the design. The fundamental approachto dealing with problems associated with discrete and continuous inputs is summarized.},
  Keywords                 = {Piano keyboard-related interface, continuous and discrete control, haptic feedback, Human-Computer Interaction (HCI) },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_503.pdf},
  DOI                      = {10.5281/zenodo.1178081}
}

@InProceedings{Lee2011,
  Title                    = {Sonicstrument : A Musical Interface with Stereotypical Acoustic Transducers},
  Author                   = {Lee, Jeong-seob and Yeo, Woon Seung},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {24--27},
  Keywords                 = {Stereotypical transducers, audible sound, Doppler effect, handfree interface, musical instrument, interactive performance },
  Abstract = {This paper introduces Sonicstrument, a sound-based interface that traces the user's hand motions. Sonicstrument utilizes stereotypical acoustic transducers (i.e., a pair of earphones and a microphone) for transmission and reception of acoustic signals whose frequencies are within the highest area of human hearing range that can rarely be perceived by most people. Being simpler in structure and easier to implement than typical ultrasonic motion detectors with special transducers, this system is robust and offers precise results without introducing any undesired sonic disturbance to users. We describe the design and implementation of Sonicstrument, evaluate its performance, and present two practical applications of the system in music and interactive performance.},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_024.pdf},
  Presentation-video = {https://vimeo.com/26804455/},
  DOI                      = {10.5281/zenodo.1180259}
}

@InProceedings{Leslie2011,
  Title                    = {MoodMixer : {EEG}-based Collaborative Sonification},
  Author                   = {Leslie, Grace and Mullen, Tim},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {296--299},
  Abstract                 = {MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.},
  Keywords                 = {EEG, BCMI, collaboration, sonification, visualization },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_296.pdf},
  DOI                      = {10.5281/zenodo.1178089}
}

@InProceedings{Liang2011,
  Title                    = {A Framework for Coordination and Synchronization of Media},
  Author                   = {Liang, Dawen and Xia, Guangyu and Dannenberg, Roger B.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {167--172},
  Abstract                 = {Computer music systems that coordinate or interact with human musicians exist in many forms. Often, coordination is at the level of gestures and phrases without synchronization at the beat level (or perhaps the notion of "beat" does not even exist). In music with beats, fine-grain synchronization can be achieved by having humans adapt to the computer (e.g. following a click track), or by computer accompaniment in which the computer follows a predetermined score. We consider an alternative scenario in which improvisation prevents traditional score following, but where synchronization is achieved at the level of beats, measures, and cues. To explore this new type of human-computer interaction, we have created new software abstractions for synchronization and coordination of music and interfaces in different modalities. We describe these new software structures, present examples, and introduce the idea of music notation as an interactive musical interface rather than a static document. },
  Keywords                 = {automatic accompaniment,interactive,music display,popular music,real-time,synchronization},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_167.pdf},
  Presentation-video = {https://vimeo.com/26832515/},
  DOI                      = {10.5281/zenodo.1178091}
}

@InProceedings{Lopez2011,
  Title                    = {Battle of the DJs: an HCI Perspective of Traditional, Virtual, Hybrid and Multitouch DJing},
  Author                   = {Lopez, Pedro and Ferreira, Alfredo and Pereira, J. A. Madeiras},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {367--372},
  Abstract                 = {The DJ culture uses a gesture lexicon strongly rooted in thetraditional setup of turntables and a mixer. As novel toolsare introduced in the DJ community, this lexicon is adaptedto the features they provide. In particular, multitouch technologies can offer a new syntax while still supporting the oldlexicon, which is desired by DJs.We present a classification of DJ tools, from an interaction point of view, that divides the previous work into Traditional, Virtual and Hybrid setups. Moreover, we presenta multitouch tabletop application, developed with a groupof DJ consultants to ensure an adequate implementation ofthe traditional gesture lexicon.To conclude, we conduct an expert evaluation, with tenDJ users in which we compare the three DJ setups with ourprototype. The study revealed that our proposal suits expectations of Club/Radio-DJs, but fails against the mentalmodel of Scratch-DJs, due to the lack of haptic feedback torepresent the record's physical rotation. Furthermore, testsshow that our multitouch DJ setup, reduces task durationwhen compared with Virtual setups.},
  Keywords                 = {DJing, Multitouch Interaction, Expert User evaluation, HCI },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_367.pdf},
  Presentation-video = {https://vimeo.com/26881380/},
  DOI                      = {10.5281/zenodo.1178093}
}

@InProceedings{Luhtala2011,
  Title                    = {Designing a Music Performance Space for Persons with Intellectual Learning Disabilities},
  Author                   = {Luhtala, Matti and Kym\''{a}l\''{a}inen, Tiina and Plomp, Johan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {429--432},
  Keywords                 = {Music interfaces, music therapy, modifiable interfaces, design tools, Human-Technology Interaction (HTI), User-Centred Design (UCD), design for all (DfA), prototyping, performance. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_429.pdf},
  DOI                      = {10.5281/zenodo.1178095}
}

@InProceedings{Marchini2011,
  Title                    = {A Hair Ribbon Deflection Model for Low-intrusiveness Measurement of Bow Force in Violin Performance},
  Author                   = {Marchini, Marco and Papiotis, Panos and P\'{e}rez, Alfonso and Maestre, Esteban},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {481--486},
  Abstract                 = {This paper introduces and evaluates a novel methodologyfor the estimation of bow pressing force in violin performance, aiming at a reduced intrusiveness while maintaininghigh accuracy. The technique is based on using a simplifiedphysical model of the hair ribbon deflection, and feeding thismodel solely with position and orientation measurements ofthe bow and violin spatial coordinates. The physical modelis both calibrated and evaluated using real force data acquired by means of a load cell.},
  Keywords                 = {bow pressing force, bow force, pressing force, force, violin playing, bow simplified physical model, 6DOF, hair ribbon ends, string ends },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_481.pdf},
  DOI                      = {10.5281/zenodo.1178097}
}

@InProceedings{Marquez-Borbon2011,
  Title                    = {Designing Digital Musical Interactions in Experimental Contexts},
  Author                   = {Marquez-Borbon, Adnan and Gurevich, Michael and Fyans, A. Cavan and Stapleton, Paul},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {373--376},
  Abstract                 = {As NIME's focus has expanded beyond the design reportswhich were pervasive in the early days to include studies andexperiments involving music control devices, we report on aparticular area of activity that has been overlooked: designsof music devices in experimental contexts. We demonstratethis is distinct from designing for artistic performances, witha unique set of novel challenges. A survey of methodologicalapproaches to experiments in NIME reveals a tendency torely on existing instruments or evaluations of new devicesdesigned for broader creative application. We present twoexamples from our own studies that reveal the merits ofdesigning purpose-built devices for experimental contexts.},
  Keywords                 = {Experiment, Methodology, Instrument Design, DMIs },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_373.pdf},
  Presentation-video = {https://vimeo.com/26882375/},
  DOI                      = {10.5281/zenodo.1178099}
}

@InProceedings{Marshall2011,
  Title                    = {Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument},
  Author                   = {Marshall, Mark T. and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {399--404},
  Abstract                 = {This paper deals with the effects of integrated vibrotactile feedback on the "feel" of a digital musical instrument(DMI). Building on previous work developing a DMI withintegrated vibrotactile feedback actuators, we discuss howto produce instrument-like vibrations, compare these simulated vibrations with those produced by an acoustic instrument and examine how the integration of this feedbackeffects performer ratings of the instrument. We found thatintegrated vibrotactile feedback resulted in an increase inperformer engagement with the instrument, but resulted ina reduction in the perceived control of the instrument. Wediscuss these results and their implications for the design ofnew digital musical instruments.},
  Keywords                 = {Vibrotactile Feedback, Digital Musical Instruments, Feel, Loudspeakers },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_399.pdf},
  DOI                      = {10.5281/zenodo.1178101}
}

@InProceedings{Martin2011,
  Title                    = {Strike on Stage: a Percussion and Media Performance},
  Author                   = {Martin, Charles and Lai, Chi-Hsia},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {142--143},
  Abstraction              = {This paper describes Strike on Stage, an interface and corresponding  audio-visual  performance  work  developed  and performed in 2010 by percussionists and media artists Chi-Hsia  Lai  and  Charles  Martin.   The  concept  of Strike on Stage is  to  integrate  computer  visuals  and  sound  into  animprovised percussion performance.  A large projection surface  is  positioned  directly  behind  the  performers,  while  acomputer vision system tracks their movements.  The setup allows computer visualisation and sonification to be directly responsive and unified with the performers' gestures.},
  Keywords                 = {computer vision, media performance, percussion},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_142.pdf},
  DOI                      = {10.5281/zenodo.1178103}
}

@InProceedings{Mcgee2011,
  Title                    = {BioRhythm : a Biologically-inspired Audio-Visual Installation},
  Author                   = {Mcgee, Ryan and Fan, Yuan-Yi and Ali, Reza},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {80--83},
  Abstract                 = {BioRhythm is an interactive bio-feedback installation controlled by the cardiovascular system. Data from a photoplethysmograph (PPG) sensor controls sonification and visualization parameters in real-time. Biological signals areobtained using the techniques of Resonance Theory in Hemodynamics and mapped to audiovisual cues via the Five Element Philosophy. The result is a new media interface utilizing sound synthesis and spatialization with advanced graphics rendering. BioRhythm serves as an artistic explorationof the harmonic spectra of pulse waves.},
  Keywords                 = {bio-feedback,bio-sensing,fm synthesis,open sound control,parallel computing,sonification,spa-,spatial audio,tialization,tion,visualiza-},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_080.pdf},
  DOI                      = {10.5281/zenodo.1178105}
}

@InProceedings{Mealla2011,
  Title                    = {Listening to Your Brain: Implicit Interaction in Collaborative Music Performances},
  Author                   = {Mealla, Sebasti\'{a}n and V\''{a}aljam\''{a}ae, Aleksander and Bosi, Mathieu and Jord\`{a}, Sergi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {149--154},
  Abstract                 = {The use of physiological signals in Human Computer Interaction (HCI) is becoming popular and widespread, mostly due to sensors miniaturization and advances in real-time processing. However, most of the studies that use physiology based interaction focus on single-user paradigms, and its usage in collaborative scenarios is still in its beginning. In this paper we explore how interactive sonification of brain and heart signals, and its representation through physical objects (physiopucks) in a tabletop interface may enhance motivational and controlling aspects of music collaboration. A multimodal system is presented, based on an electrophysiology sensor system and the Reactable, a musical tabletop interface. Performance and motivation variables were assessed in an experiment involving a test "Physio" group(N=22) and a control "Placebo" group (N=10). Pairs of participants used two methods for sound creation: implicit interaction through physiological signals, and explicit interaction by means of gestural manipulation. The results showed that pairs in the Physio Group declared less difficulty, higher confidence and more symmetric control than the Placebo Group, where no real-time sonification was provided as subjects were using pre-recorded physiological signal being unaware of it. These results support the feasibility of introducing physiology-based interaction in multimodal interfaces for collaborative music generation.},
  Keywords                 = {bci, collaboration, cscw, hci, multimodal interfaces, music, physiological computing, physiopucks, tabletops, universitat pompeu fabra},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_149.pdf},
  Presentation-video = {https://vimeo.com/26806576/},
  DOI                      = {10.5281/zenodo.1178107}
}

@InProceedings{Milne2011,
  Title                    = {Hex Player --- A Virtual Musical Controller},
  Author                   = {Milne, Andrew J. and Xamb\'{o}, Anna and Laney, Robin and Sharp, David B. and Prechtl, Anthony and Holland, Simon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {244--247},
  Keywords                 = {generalized keyboard, isomorphic layout, multi-touch surface, tablet, musical interface design, iPad, microtonality },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_244.pdf},
  DOI                      = {10.5281/zenodo.1178109}
}

@InProceedings{Mitchell2011,
  Title                    = {SoundGrasp : A Gestural Interface for the Performance of Live Music},
  Author                   = {Mitchell, Thomas and Heap, Imogen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {465--468},
  Abstract                 = {This paper documents the first developmental phase of aninterface that enables the performance of live music usinggestures and body movements. The work included focuseson the first step of this project: the composition and performance of live music using hand gestures captured using asingle data glove. The paper provides a background to thefield, the aim of the project and a technical description ofthe work completed so far. This includes the developmentof a robust posture vocabulary, an artificial neural networkbased posture identification process and a state-based system to map identified postures onto a set of performanceprocesses. The paper is closed with qualitative usage observations and a projection of future plans.},
  Keywords                 = {Music Controller, Gestural Music, Data Glove, Neural Network, Live Music Composition, Looping, Imogen Heap },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_465.pdf},
  DOI                      = {10.5281/zenodo.1178111}
}

@InProceedings{Molina2011,
  Title                    = {BeatJockey : A New Tool for Enhancing DJ Skills},
  Author                   = {Molina, Pablo and Haro, Mart\'{\i}n and Jord\`{a}, Sergi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {288--291},
  Abstract                 = {We present BeatJockey, a prototype interface which makesuse of Audio Mosaicing (AM), beat-tracking and machinelearning techniques, for supporting Diskjockeys (DJs) byproposing them new ways of interaction with the songs onthe DJ's playlist. This prototype introduces a new paradigmto DJing in which the user has the capability to mix songsinteracting with beat-units that accompany the DJ's mix.For this type of interaction, the system suggests song slicestaken from songs selected from a playlist, which could gowell with the beats of whatever master song is being played.In addition the system allows the synchronization of multiple songs, thus permitting flexible, coherent and rapid progressions in the DJ's mix. BeatJockey uses the Reactable,a musical tangible user interface (TUI), and it has beendesigned to be used by all DJs regardless of their level ofexpertise, as the system helps the novice while bringing newcreative opportunities to the expert.},
  Keywords                 = {DJ, music information retrieval, audio mosaicing, percussion, turntable, beat-mash, interactive music interfaces, realtime, tabletop interaction, reactable. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_288.pdf},
  DOI                      = {10.5281/zenodo.1178113}
}

@InProceedings{Montag2011,
  Title                    = {A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications},
  Author                   = {Montag, Matthew and Sullivan, Stefan and Dickey, Scott and Leider, Colby},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {8--13},
  Keywords                 = {multi-touch, haptics, frustrated total internal reflection, music performance, music composition, latency, DIY },
  Abstract = {During the past decade, multi-touch surfaces have emerged as valuable tools for collaboration, display, interaction, and musical expression. Unfortunately, they tend to be costly and often suffer from two drawbacks for music performance:(1) relatively high latency owing to their sensing mechanism, and (2) lack of haptic feedback. We analyze the latency present in several current multi-touch platforms, and we describe a new custom system that reduces latency to an average of 30 ms while providing programmable haptic feed-back to the user. The paper concludes with a description of ongoing and future work.},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_008.pdf},
  Presentation-video = {https://vimeo.com/26799018/},
  DOI                      = {10.5281/zenodo.1178115}
}

@InProceedings{Mullen2011,
  Title                    = {Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface},
  Author                   = {Mullen, Tim and Warp, Richard and Jansch, Adam},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {469--472},
  Abstract                 = {The use of non-invasive electroencephalography (EEG) in the experimental arts is not a novel concept. Since 1965, EEG has been used in a large number of, sometimes highly sophisticated, systems for musical and artistic expression. However, since the advent of the synthesizer, most such systems have utilized digital and/or synthesized media in sonifying the EEG signals. There have been relatively few attempts to create interfaces for musical expression that allow one to mechanically manipulate acoustic instruments by modulating one's mental state. Secondly, few such systems afford a distributed performance medium, with data transfer and audience participation occurring over the Internet. The use of acoustic instruments and Internet-enabled communication expands the realm of possibilities for musical expression in Brain-Computer Music Interfaces (BCMI), while also introducing additional challenges. In this paper we report and examine a first demonstration (Music for Online Performer) of a novel system for Internet-enabled manipulation of robotic acoustic instruments, with feedback, using a non-invasive EEG-based BCI and low-cost, commercially available robotics hardware. },
  Keywords                 = {EEG, Brain-Computer Music Interface, Internet, Arduino. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_469.pdf},
  DOI                      = {10.5281/zenodo.1178117}
}

@InProceedings{Murray-Browne2011,
  Title                    = {The Medium is the Message: Composing Instruments and Performing Mappings},
  Author                   = {Murray-Browne, Tim and Mainstone, Di and Bryan-Kinns, Nick and Plumbley, Mark D.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {56--59},
  Abstract                 = {Many performers of novel musical instruments find it difficult to engage audiences beyond those in the field. Previousresearch points to a failure to balance complexity with usability, and a loss of transparency due to the detachmentof the controller and sound generator. The issue is oftenexacerbated by an audience's lack of prior exposure to theinstrument and its workings.However, we argue that there is a conflict underlyingmany novel musical instruments in that they are intendedto be both a tool for creative expression and a creative workof art in themselves, resulting in incompatible requirements.By considering the instrument, the composition and theperformance together as a whole with careful considerationof the rate of learning demanded of the audience, we propose that a lack of transparency can become an asset ratherthan a hindrance. Our approach calls for not only controllerand sound generator to be designed in sympathy with eachother, but composition, performance and physical form too.Identifying three design principles, we illustrate this approach with the Serendiptichord, a wearable instrument fordancers created by the ,
,
authors.},
  Keywords                 = {Performance, composed instrument, transparency, constraint. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_056.pdf},
  DOI                      = {10.5281/zenodo.1178119}
}

@InProceedings{Newton2011,
  Title                    = {Examining How Musicians Create Augmented Musical Instruments},
  Author                   = {Newton, Dan and Marshall, Mark T.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {155--160},
  Abstract                 = {This paper examines the creation of augmented musicalinstruments by a number of musicians. Equipped with asystem called the Augmentalist, 10 musicians created newaugmented instruments based on their traditional acousticor electric instruments. This paper discusses the ways inwhich the musicians augmented their instruments, examines the similarities and differences between the resultinginstruments and presents a number of interesting findingsresulting from this process.},
  Keywords                 = {Augmented Instruments, Instrument Design, Digital Musical Instruments, Performance },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_155.pdf},
  Presentation-video = {https://vimeo.com/26807158/},
  DOI                      = {10.5281/zenodo.1178121}
}

@InProceedings{Nishino2011,
  Title                    = {Cognitive Issues in Computer Music Programming},
  Author                   = {Nishino, Hiroki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {499--502},
  Keywords                 = {Computer music, programming language, the psychology of programming, usability },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_499.pdf},
  DOI                      = {10.5281/zenodo.1178123}
}

@InProceedings{Nymoen2011,
  Title                    = {SoundSaber -- A Motion Capture Instrument},
  Author                   = {Nymoen, Kristian and Skogstad, Ståle A. and Jensenius, Alexander Refsum},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {312--315},
  Abstract                 = {The paper presents the SoundSaber-a musical instrument based on motion capture technology. We present technical details of the instrument and discuss the design development process. The SoundSaber may be used as an example of how high-fidelity motion capture equipment can be used for prototyping musical instruments, and we illustrate this with an example of a low-cost implementation of our motion capture instrument.},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_312.pdf},
  DOI                      = {10.5281/zenodo.1178125}
}

@InProceedings{Overholt2011,
  Title                    = {The Overtone Fiddle: an Actuated Acoustic Instrument},
  Author                   = {Overholt, Dan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {30--33},
  Abstract                 = {The Overtone Fiddle is a new violin-family instrument that incorporates electronic sensors, integrated DSP, and physical actuation of the acoustic body. An embedded tactile sound transducer creates extra vibrations in the body of the Overtone Fiddle, allowing performer control and sensation via both traditional violin techniques, as well as extended playing techniques that incorporate shared man/machine control of the resulting sound. A magnetic pickup system is mounted to the end of the fiddle's fingerboard in order to detect the signals from the vibrating strings, deliberately not capturing vibrations from the full body of the instrument. This focused sensing approach allows less restrained use of DSP-generated feedback signals, as there is very little direct leakage from the actuator embedded in the body of the instrument back to the pickup. },
  Keywords                 = {Actuated Musical Instruments, Hybrid Instruments, Active Acoustics, Electronic Violin },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_004.pdf},
  Presentation-video = {https://vimeo.com/26795157/},
  DOI                      = {10.5281/zenodo.1178127}
}

@InProceedings{Papetti2011,
  Title                    = {Rhythm'n'Shoes: a Wearable Foot Tapping Interface with Audio-Tactile Feedback},
  Author                   = {Papetti, Stefano and Civolani, Marco and Fontana, Federico},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {473--476},
  Abstract                 = {A shoe-based interface is presented, which enables users toplay percussive virtual instruments by tapping their feet.The wearable interface consists of a pair of sandals equippedwith four force sensors and four actuators affording audiotactile feedback. The sensors provide data via wireless transmission to a host computer, where they are processed andmapped to a physics-based sound synthesis engine. Sincethe system provides OSC and MIDI compatibility, alternative electronic instruments can be used as well. The audiosignals are then sent back wirelessly to audio-tactile excitersembedded in the sandals' sole, and optionally to headphonesand external loudspeakers. The round-trip wireless communication only introduces very small latency, thus guaranteeing coherence and unity in the multimodal percept andallowing tight timing while playing.},
  Keywords                 = {interface, audio, tactile, foot tapping, embodiment, footwear, wireless, wearable, mobile },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_473.pdf},
  DOI                      = {10.5281/zenodo.1178129}
}

@InProceedings{Pardue2011,
  Title                    = {Gamelan Elektrika: An Electronic Balinese Gamelan},
  Author                   = {Pardue, Laurel S. and Boch, Andrew and Boch, Matt and Southworth, Christine and Rigopulos, Alex},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {18--23},
  Abstract                 = {This paper describes the motivation and construction of Gamelan Elektrika, a new electronic gamelan modeled after a Balinese Gong Kebyar. The first of its kind, Elektrika consists of seven instruments acting as MIDI controllers accompanied by traditional percussion and played by 11 or more performers following Balinese performance practice. Three main percussive instrument designs were executed using a combination of force sensitive resistors, piezos, and capacitive sensing. While the instrument interfaces are designedto play interchangeably with the original, the sound andt ravel possiblilities they enable are tremendous. MIDI enables a massive new sound palette with new scales beyond the quirky traditional tuning and non-traditional sounds. It also allows simplified transcription for an aurally taught tradition. Significantly, it reduces the transportation challenges of a previously large and heavy ensemble, creating opportunities for wider audiences to experience Gong Kebyar's enchanting sound. True to the spirit of oneness in Balinese music, as one of the first large all-MIDI ensembles, ElekTrika challenges performers to trust silent instruments and develop an understanding of highly intricate and interlocking music not through the sound of the individual, but through the sound of the whole.},
  Keywords                 = {bali, gamelan, musical instrument design, MIDI ensemble },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_018.pdf},
  Presenation-video = {https://vimeo.com/26803278/},
  DOI                      = {10.5281/zenodo.1178131}
}

@InProceedings{Pigott2011,
  Title                    = {Vibration , Volts and Sonic Art: A Practice and Theory of Electromechanical Sound},
  Author                   = {Pigott, Jon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {84--87},
  Keywords                 = {Electromechanical sonic art, kinetic sound art, prepared speakers, Infinite Spring. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_084.pdf},
  DOI                      = {10.5281/zenodo.1178133}
}

@InProceedings{Pirro2011,
  Title                    = {Physical Modelling Enabling Enaction: an Example},
  Author                   = {Pirr\`{o}, David and Eckel, Gerhard},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {461--464},
  Keywords                 = {embod-,enactive interfaces,has been ap-,iment,interaction,motion tracking,of sound and music,physical modelling,to movement and gesture},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_461.pdf},
  DOI                      = {10.5281/zenodo.1178135}
}

@InProceedings{Polotti2011,
  Title                    = {EGGS in Action},
  Author                   = {Polotti, Pietro and Goina, Maurizio},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {64--67},
  Abstract                 = {In this paper, we discuss the results obtained by means of the EGGS (Elementary Gestalts for Gesture Sonification) system in terms of artistic realizations. EGGS was introduced in a previous edition of this conference. The works presented include interactive installations in the form of public art and interactive onstage performances. In all of the works, the EGGS principles of simplicity based on the correspondence between elementary sonic and movement units, and of organicity between sound and gesture are applied. Indeed, we study both sound as a means for gesture representation and gesture as embodiment of sound. These principles constitute our guidelines for the investigation of the bidirectional relationship between sound and body expression with various strategies involving both educated and non-educated executors. },
  Keywords                 = {Gesture sonification, Interactive performance, Public art. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_064.pdf},
  DOI                      = {10.5281/zenodo.1178137}
}

@InProceedings{Popp2011,
  Title                    = {Intuitive Real-Time Control of Spectral Model Synthesis},
  Author                   = {Popp, Phillip and Wright, Matthew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {284--287},
  Keywords                 = {Spectral Model Synthesis, Gesture Recognition, Synthesis Control, Wacom Tablet, Machine Learning },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_284.pdf},
  DOI                      = {10.5281/zenodo.1178139}
}

@InProceedings{Ramkissoon2011,
  Title                    = {The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance},
  Author                   = {Ramkissoon, Izzi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {224--227},
  Abstract                 = {The Bass Sleeve uses an Arduino board with a combination of buttons, switches, flex sensors, force sensing resistors, and an accelerometer to map the ancillary movements of a performer to sampling, real-time audio and video processing including pitch shifting, delay, low pass filtering, and onscreen video movement. The device was created to augment the existing functions of the electric bass and explore the use of ancillary gestures to control the laptop in a live performance. In this research it was found that incorporating ancillary gestures into a live performance could be useful when controlling the parameters of audio processing, sound synthesis and video manipulation. These ancillary motions can be a practical solution to gestural multitasking allowing independent control of computer music parameters while performing with the electric bass. The process of performing with the Bass Sleeve resulted in a greater amount of laptop control, an increase in the amount of expressiveness using the electric bass in combination with the laptop, and an improvement in the interactivity on both the electric bass and laptop during a live performance. The design uses various gesture-to-sound mapping strategies to accomplish a compositional task during an electro acoustic multimedia musical performance piece. },
  Keywords                 = {Interactive Music, Interactive Performance Systems, Gesture Controllers, Augmented Instruments, Electric Bass, Video Tracking },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_224.pdf},
  DOI                      = {10.5281/zenodo.1178141}
}

@InProceedings{Reus2011,
  Title                    = {Crackle: A Dynamic Mobile Multitouch Topology for Exploratory Sound Interaction},
  Author                   = {Reus, Jonathan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {377--380},
  Abstract                 = {This paper describes the design of Crackle, a interactivesound and touch experience inspired by the CrackleBox.We begin by describing a ruleset for Crackle's interactionderived from the salient interactive qualities of the CrackleBox. An implementation strategy is then described forrealizing the ruleset as an application for the iPhone. Thepaper goes on to consider the potential of using Crackleas an encapsulated interaction paradigm for exploring arbitrary sound spaces, and concludes with lessons learned ondesigning for multitouch surfaces as expressive input sensors.},
  Keywords                 = {touchscreen, interface topology, mobile music, interaction paradigm, dynamic mapping, CrackleBox, iPhone },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_377.pdf},
  Presentation-video = {https://vimeo.com/26882621/},
  DOI                      = {10.5281/zenodo.1178143}
}

@InProceedings{Roh2011,
  Title                    = {Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers},
  Author                   = {Roh, Jung-Sim and Mann, Yotam and Freed, Adrian and Wessel, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {393--398},
  Abstract                 = {The design space of fabric multitouch surface interaction is explored with emphasis on novel materials and construction techniques aimed towards reliable, repairable pressure sensing surfaces for musical applications. },
  Keywords                 = {Multitouch, surface interaction, piezoresistive, fabric sensor, e-textiles, tangible computing, drum controller },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_393.pdf},
  Presentation-video = {https://vimeo.com/26906580/},
  DOI                      = {10.5281/zenodo.1178145}
}

@InProceedings{Rosenbaum2011,
  Title                    = {MelodyMorph: A Reconfigurable Musical Instrument},
  Author                   = {Rosenbaum, Eric},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {445--447},
  Abstract                 = {I present MelodyMorph, a reconfigurable musical instrument designed with a focus on melodic improvisation. It is designed for a touch-screen interface, and allows the user to create "bells" which can be tapped to play a note, and dragged around on a pannable and zoomable canvas. Colors, textures and shapes of the bells represent pitch and timbre properties. "Recorder bells" can store and play back performances. Users can construct instruments that are modifiable as they play, and build up complex melodies hierarchically from simple parts. },
  Keywords                 = {Melody, improvisation, representation, multi-touch, iPad },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_445.pdf},
  DOI                      = {10.5281/zenodo.1178147}
}

@InProceedings{Schacher2011,
  Title                    = {Traces -- Body, Motion and Sound},
  Author                   = {Schacher, Jan C. and Stoecklin, Angela},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {292--295},
  Abstract                 = {In this paper the relationship between body, motion and sound is addressed. The comparison with traditional instruments and dance is shown with regards to basic types of motion. The difference between gesture and movement is outlined and some of the models used in dance for structuring motion sequences are described. In order to identify expressive aspects of motion sequences a test scenario is devised. After the description of the methods and tools used in a series of measurements, two types of data-display are shown and the applied in the interpretation. One salient feature is recognized and put into perspective with regards to movement and gestalt perception. Finally the merits of the technical means that were applied are compared and a model-based approach to motion-sound mapping is proposed. },
  Keywords                 = {Interactive Dance, Motion and Gesture, Sonification, Motion Perception, Mapping },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_292.pdf},
  DOI                      = {10.5281/zenodo.1178149}
}

@InProceedings{Schedel2011,
  Title                    = {Wekinating 000000{S}wan : Using Machine Learning to Create and Control Complex Artistic Systems},
  Author                   = {Schedel, Margaret and Perry, Phoenix and Fiebrink, Rebecca},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {453--456},
  Abstract                 = {In this paper we discuss how the band 000000Swan uses machine learning to parse complex sensor data and create intricate artistic systems for live performance. Using the Wekinator software for interactive machine learning, we have created discrete and continuous models for controlling audio and visual environments using human gestures sensed by a commercially-available sensor bow and the Microsoft Kinect. In particular, we have employed machine learning to quickly and easily prototype complex relationships between performer gesture and performative outcome. },
  Keywords                 = {Wekinator, K-Bow, Machine Learning, Interactive, Multimedia, Kinect, Motion-Tracking, Bow Articulation, Animation },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_453.pdf},
  DOI                      = {10.5281/zenodo.1178151}
}

@InProceedings{Schnell2011,
  Title                    = {Playing the "MO" -- Gestural Control and Re-Embodiment of Recorded Sound and Music},
  Author                   = {Schnell, Norbert and Bevilacqua, Fr\'{e}d\'{e}ric and Rasamimanana, Nicolas and Blois, Julien and Gu\'{e}dy, Fabrice and Fl\'{e}ty, Emmanuel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {535--536},
  Abstract                 = {We are presenting a set of applications that have been realized with the MO modular wireless motion capture deviceand a set of software components integrated into Max/MSP.These applications, created in the context of artistic projects,music pedagogy, and research, allow for the gestural reembodiment of recorded sound and music. They demonstrate a large variety of different "playing techniques" inmusical performance using wireless motion sensor modulesin conjunction with gesture analysis and real-time audioprocessing components.},
  Keywords                 = {Music, Gesture, Interface, Wireless Sensors, Gesture Recognition, Audio Processing, Design, Interaction },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_535.pdf},
  DOI                      = {10.5281/zenodo.1178153}
}

@InProceedings{Schoonderwaldt2011,
  Title                    = {Effective and Expressive Movements in a French-Canadian fiddler's Performance},
  Author                   = {Schoonderwaldt, Erwin and Jensenius, Alexander Refsum},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {256--259},
  Abstract                 = {We report on a performance study of a French-Canadian fiddler. The fiddling tradition forms an interesting contrast toclassical violin performance in several ways. Distinguishingfeatures include special elements in the bowing techniqueand the presence of an accompanying foot clogging pattern.These two characteristics are described, visualized and analyzed using video and motion capture recordings as sourcematerial.},
  Keywords                 = {fiddler, violin, French-Canadian, bowing, feet, clogging, motion capture, video, motiongram, kinematics, sonification },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_256.pdf},
  DOI                      = {10.5281/zenodo.1178155}
}

@InProceedings{Schroeder2011,
  Title                    = {A Physically Based Sound Space for Procedural Agents},
  Author                   = {Schroeder, Benjamin and Ainger, Marc and Parent, Richard},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {120--123},
  Keywords                 = {a human performer,agents,agents smoothly changing the,behavioral animation,figure 1,length of,physically based sound,pro-,strings being played by},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_120.pdf},
  DOI                      = {10.5281/zenodo.1178157}
}

@InProceedings{Seldess2011,
  Title                    = {Tahakum: A Multi-Purpose Audio Control Framework},
  Author                   = {Seldess, Zachary and Yamada, Toshiro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {161--166},
  Abstract                 = {We present {Tahakum}, an open source, extensible collection of software tools designed to enhance workflow on multichannel audio systems within complex multi-functional research and development environments. Tahakum aims to provide critical functionality required across a broad spectrum of audio systems usage scenarios, while at the same time remaining sufficiently open as to easily support modifications and extensions via 3rd party hardware and software. Features provided in the framework include software for custom mixing/routing and audio system preset automation, software for network message routing/redirection and protocol conversion, and software for dynamic audio asset management and control. },
  Keywords                 = {Audio Control Systems, Audio for VR, Max/MSP, Spatial Audio },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_161.pdf},
  Presentation-video = {https://vimeo.com/26809966/},
  DOI                      = {10.5281/zenodo.1178159}
}

@InProceedings{Shear2011,
  Title                    = {The Electromagnetically Sustained Rhodes Piano},
  Author                   = {Shear, Greg and Wright, Matthew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {14--17},
  Abstract                 = {The Electromagnetically Sustained Rhodes Piano is an augmentation of the original instrument with additional control over the amplitude envelope of individual notes. Thisincludes slow attacks and infinite sustain while preservingthe familiar spectral qualities of this classic electromechanical piano. These additional parameters are controlled withaftertouch on the existing keyboard, extending standardpiano technique. Two sustain methods were investigated,driving the actuator first with a pure sine wave, and secondwith the output signal of the sensor. A special isolationmethod effectively decouples the sensors from the actuatorsand tames unruly feedback in the high-gain signal path.},
  Keywords                 = {Rhodes, keyboard, electromagnetic, sustain, augmented instrument, feedback, aftertouch },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_014.pdf},
  Presentation-video = {https://vimeo.com/26802504/},
  DOI                      = {10.5281/zenodo.1178161}
}

@InProceedings{Sioros2011,
  Title                    = {Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator},
  Author                   = {Sioros, George and Guedes, Carlos},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {88--91},
  Abstract                 = {We introduce a novel algorithm for automatically generating rhythms in real time in a certain meter. The generated rhythms are "generic" in the sense that they are characteristic of each time signature without belonging to a specific musical style. The algorithm is based on a stochastic model in which various aspects and qualities of the generated rhythm can be controlled intuitively and in real time. Such qualities are the density of the generated events per bar, the amount of variation in generation, the amount of syncopation, the metrical strength, and of course the meter itself. The kin.rhythmicator software application was developed to implement this algorithm. During a performance with the kin.rhythmicator the user can control all aspects of the performance through descriptive and intuitive graphic controls. },
  Keywords                 = {automatic music generation, generative, stochastic, metric indispensability, syncopation, Max/MSP, Max4Live },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_088.pdf},
  DOI                      = {10.5281/zenodo.1178163}
}

@InProceedings{Skogstad2011,
  Title                    = {OSC Implementation and Evaluation of the Xsens MVN Suit},
  Author                   = {Skogstad, Ståle A. and de Quay, Yago and Jensenius, Alexander Refsum},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {300--303},
  Abstract                 = {The paper presents research about implementing a full body inertial motion capture system, the Xsens MVN suit, for musical interaction. Three different approaches for stream-ing real time and prerecorded motion capture data with Open Sound Control have been implemented. Furthermore, we present technical performance details and our experience with the motion capture system in realistic practice.},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_300.pdf},
  DOI                      = {10.5281/zenodo.1178165}
}

@InProceedings{Smallwood2011,
  Title                    = {Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies},
  Author                   = {Smallwood, Scott},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {28--31},
  Abstract                 = {This paper describes recent developments in the creation of sound-making instruments and devices powered by photovoltaic (PV) technologies. With the rise of more efficient PV products in diverse packages, the possibilities for creating solar-powered musical instruments, sound installations, and loudspeakers are becoming increasingly realizable. This paper surveys past and recent developments in this area, including several projects by the ,
,
author, and demonstrates how the use of PV technologies can influence the creative process in unique ways. In addition, this paper discusses how solar sound arts can enhance the aesthetic direction taken by recent work in soundscape studies and acoustic ecology. Finally, this paper will point towards future directions and possibilities as PV technologies continue to evolve and improve in terms of performance, and become more affordable. },
  Keywords                 = {Solar Sound Arts, Circuit Bending, Hardware Hacking, Human-Computer Interface Design, Acoustic Ecology, Sound Art, Electroacoustics, Laptop Orchestra, PV Technology },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_028.pdf},
  DOI                      = {10.5281/zenodo.1178167}
}

@InProceedings{Smith2011,
  Title                    = {The Self-Supervising Machine},
  Author                   = {Smith, Benjamin D. and Garnett, Guy E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {108--111},
  Abstract                 = {Supervised machine learning enables complex many-to-manymappings and control schemes needed in interactive performance systems. One of the persistent problems in theseapplications is generating, identifying and choosing inputoutput pairings for training. This poses problems of scope(limiting the realm of potential control inputs), effort (requiring significant pre-performance training time), and cognitive load (forcing the performer to learn and remember thecontrol areas). We discuss the creation and implementationof an automatic "supervisor", using unsupervised machinelearning algorithms to train a supervised neural networkon the fly. This hierarchical arrangement enables networktraining in real time based on the musical or gestural control inputs employed in a performance, aiming at freeing theperformer to operate in a creative, intuitive realm, makingthe machine control transparent and automatic. Three implementations of this self supervised model driven by iPod,iPad, and acoustic violin are described.},
  Keywords                 = {NIME, machine learning, interactive computer music, machine listening, improvisation, adaptive resonance theory },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_108.pdf},
  DOI                      = {10.5281/zenodo.1178169}
}

@InProceedings{Snyder2011,
  Title                    = {Snyderphonics Manta Controller, a Novel {USB} Touch-Controller},
  Author                   = {Snyder, Jeff},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {413--416},
  Abstract                 = {The Snyderphonics Manta controller is a USB touch controller for music and video. It features 48 capacitive touch sensors, arranged in a hexagonal grid, with bi-color LEDs that are programmable from the computer. The sensors send continuous data proportional to surface area touched, and a velocitydetection algorithm has been implemented to estimate attack velocity based on this touch data. In addition to these hexagonal sensors, the Manta has two high-dimension touch sliders (giving 12-bit values), and four assignable function buttons. In this paper, I outline the features of the controller, the available methods for communicating between the device and a computer, and some current uses for the controller. },
  Keywords                 = {Snyderphonics, Manta, controller, USB, capacitive, touch, sensor, decoupled LED, hexagon, grid, touch slider, HID, portable, wood, live music, live video },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_413.pdf},
  Presentation-video = {https://vimeo.com/26908273/},
  DOI                      = {10.5281/zenodo.1178171}
}

@InProceedings{Sosnick2011,
  Title                    = {Implementing a Finite Difference-Based Real-time Sound Synthesizer using {GPU}s},
  Author                   = {Sosnick, Marc and Hsu, William},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {264--267},
  Abstract                 = {In this paper, we describe an implementation of a real-time sound synthesizer using Finite Difference-based simulation of a two-dimensional membrane. Finite Difference (FD) methods can be the basis for physics-based music instrument models that generate realistic audio output. However, such methods are compute-intensive; large simulations cannot run in real time on current CPUs. Many current systems now include powerful Graphics Processing Units (GPUs), which are a good fit for FD methods. We demonstrate that it is possible to use this method to create a usable real-time audio synthesizer. },
  Keywords                 = {Finite Difference, GPU, CUDA, Synthesis },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_264.pdf},
  DOI                      = {10.5281/zenodo.1178173}
}

@InProceedings{Tidemann2011,
  Title                    = {An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation},
  Author                   = {Tidemann, Axel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {268--271},
  Abstract                 = {Interacting with musical avatars have been increasingly popular over the years, with the introduction of games likeGuitar Hero and Rock Band. These games provide MIDIequipped controllers that look like their real-world counterparts (e.g. MIDI guitar, MIDI drumkit) that the users playto control their designated avatar in the game. The performance of the user is measured against a score that needs tobe followed. However, the avatar does not move in responseto how the user plays, it follows some predefined movementpattern. If the user plays badly, the game ends with theavatar ending the performance (i.e. throwing the guitar onthe floor). The gaming experience would increase if theavatar would move in accordance with user input. This paper presents an architecture that couples musical input withbody movement. Using imitation learning, a simulated human robot learns to play the drums like human drummersdo, both visually and auditory. Learning data is recordedusing MIDI and motion tracking. The system uses an artificial intelligence approach to implement imitation learning,employing artificial neural networks.},
  Keywords                 = {artificial intelli-,drumming,modeling human behaviour},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_268.pdf},
  DOI                      = {10.5281/zenodo.1178175}
}

@InProceedings{Todoroff2011,
  Title                    = {Wireless Digital/Analog Sensors for Music and Dance Performances},
  Author                   = {Todoroff, Todor},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {515--518},
  Abstract                 = {We developed very small and light sensors, each equippedwith 3-axes accelerometers, magnetometers and gyroscopes.Those MARG (Magnetic, Angular Rate, and Gravity) sensors allow for a drift-free attitude computation which in turnleads to the possibility of recovering the skeleton of bodyparts that are of interest for the performance, improvingthe results of gesture recognition and allowing to get relative position between the extremities of the limbs and thetorso of the performer. This opens new possibilities in termsof mapping. We kept our previous approach developed atARTeM [2]: wireless from the body to the host computer,but wired through a 4-wire digital bus on the body. Byrelieving the need for a transmitter on each sensing node,we could built very light and flat sensor nodes that can bemade invisible under the clothes. Smaller sensors, coupledwith flexible wires on the body, give more freedom of movement to dancers despite the need for cables on the body.And as the weight of each sensor node, box included, isonly 5 grams (Figure 1), they can also be put on the upper and lower arm and hand of a violin or viola player, toretrieve the skeleton from the torso to the hand, withoutadding any weight that would disturb the performer. Weused those sensors in several performances with a dancingviola player and in one where she was simultaneously controlling gas flames interactively. We are currently applyingthem to other types of musical performances.},
  Keywords                 = {wireless MARG sensors },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_515.pdf},
  DOI                      = {10.5281/zenodo.1178177}
}

@InProceedings{Tseng2011,
  Title                    = {Sound Low Fun},
  Author                   = {Tseng, Yu-Chung and Liu, Che-Wei and Chi, Tzu-Heng and Wang, Hui-Yu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {320--321},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_320.pdf},
  DOI                      = {10.5281/zenodo.1178179}
}

@InProceedings{Ustarroz2011,
  Title                    = {TresnaNet Musical Generation based on Network Protocols},
  Author                   = {Ustarroz, Paula},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {425--428},
  Abstract                 = {TresnaNet explores the potential of Telematics as a generator ofmusical expressions. I pretend to sound the silent flow ofinformation from the network.This is realized through the fabrication of a prototypefollowing the intention of giving substance to the intangibleparameters of our communication. The result may haveeducational, commercial and artistic applications because it is aphysical and perceptible representation of the transfer ofinformation over the network. This paper describes the design,implementation and conclusions about TresnaNet.},
  Keywords                 = {Interface, musical generation, telematics, network, musical instrument, network sniffer. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_425.pdf},
  DOI                      = {10.5281/zenodo.1178181}
}

@InProceedings{Verplank2011,
  Title                    = {Can Haptics Make New Music ? -- Fader and Plank Demos},
  Author                   = {Verplank, Bill and Georg, Francesco},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {539--540},
  Abstract                 = {Haptic interfaces using active force-feedback have mostly been used for emulating existing instruments and making conventional music. With the right speed, force, precision and software they can also be used to make new sounds and perhaps new music. The requirements are local microprocessors (for low-latency and high update rates), strategic sensors (for force as well as position), and non-linear dynamics (that make for rich overtones and chaotic music).},
  Keywords                 = {NIME, Haptics, Music Controllers, Microprocessors. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_539.pdf},
  DOI                      = {10.5281/zenodo.1178183}
}

@InProceedings{Waadeland2011,
  Title                    = {Rhythm Performance from a Spectral Point of View},
  Author                   = {Waadeland, Carl H.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {248--251},
  Keywords                 = {gesture,movement,rhythm performance,spectral analysis},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_248.pdf},
  DOI                      = {10.5281/zenodo.1178185}
}

@InProceedings{Wang2011,
  Title                    = {Designing for the iPad: Magic Fiddle},
  Author                   = {Wang, Ge and Oh, Jieun and Lieber, Tom},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {197--202},
  Abstract                 = {This paper describes the origin, design, and implementation of Smule's Magic Fiddle, an expressive musical instrument for the iPad. Magic Fiddle takes advantage of the physical aspects of the device to integrate game-like and pedagogical elements. We describe the origin of Magic Fiddle, chronicle its design process, discuss its integrated music education system, and evaluate the overall experience. },
  Keywords                 = {Magic Fiddle, iPad, physical interaction design, experiential design, music education. },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_197.pdf},
  Presentation-video = {https://vimeo.com/26857032/},
  DOI                      = {10.5281/zenodo.1178187}
}

@InProceedings{Wang2011a,
  Title                    = {SQUEEZY : Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis},
  Author                   = {Wang, Johnty and d'Alessandro, Nicolas and Fels, Sidney S. and Pritchard, Bob},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {531--532},
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_531.pdf},
  DOI                      = {10.5281/zenodo.1178189}
}

@InProceedings{Wyse2011,
  Title                    = {The Effect of Visualizing Audio Targets in a Musical Listening and Performance Task},
  Author                   = {Wyse, Lonce and Mitani, Norikazu and Nanayakkara, Suranga},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {304--307},
  Abstract                 = {The goal of our research is to find ways of supporting and encouraging musical behavior by non-musicians in shared public performance environments. Previous studies indicated simultaneous music listening and performance is difficult for non-musicians, and that visual support for the task might be helpful. This paper presents results from a preliminary user study conducted to evaluate the effect of visual feedback on a musical tracking task. Participants generated a musical signal by manipulating a hand-held device with two dimensions of control over two parameters, pitch and density of note events, and were given the task of following a target pattern as closely as possible. The target pattern was a machine-generated musical signal comprising of variation over the same two parameters. Visual feedback provided participants with information about the control parameters of the musical signal generated by the machine. We measured the task performance under different visual feedback strategies. Results show that single parameter visualizations tend to improve the tracking performance with respect to the visualized parameter, but not the non-visualized parameter. Visualizing two independent parameters simultaneously decreases performance in both dimensions. },
  Keywords                 = {Mobile phone, Interactive music performance, Listening, Group music play, Visual support },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_304.pdf},
  DOI                      = {10.5281/zenodo.1178191}
}

@InProceedings{Yoo2011,
  Title                    = {Creating Musical Expression using Kinect},
  Author                   = {Yoo, Min-Joon and Beak, Jin-Wook and Lee, In-Kwon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {324--325},
  Abstract                 = {Recently, Microsoft introduced a game interface called Kinect for the Xbox 360 video game platform. This interface enables users to control and interact with the game console without the need to touch a controller. It largely increases the users' degree of freedom to express their emotion. In this paper, we first describe the system we developed to use this interface for sound generation and controlling musical expression. The skeleton data are extracted from users' motions and the data are translated to pre-defined MIDI data. We then use the MIDI data to control several applications. To allow the translation between the data, we implemented a simple Kinect-to-MIDI data convertor, which is introduced in this paper. We describe two applications to make music with Kinect: we first generate sound with Max/MSP, and then control the adlib with our own adlib generating system by the body movements of the users. },
  Keywords                 = {Kinect, gaming interface, sound generation, adlib generation },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_324.pdf},
  DOI                      = {10.5281/zenodo.1178193}
}

@InProceedings{Zamborlin2011,
  Title                    = {({LAN}D)MOVES},
  Author                   = {Zamborlin, Bruno and Partesana, Giorgio and Liuni, Marco},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {537--538},
  Abstract                 = {(land)moves is an interactive installation: the user's gestures control the multimedia processing with a total synergybetween audio and video synthesis and treatment.},
  Keywords                 = {mapping gesture-audio-video, gesture recognition, landscape, soundscape },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_537.pdf},
  DOI                      = {10.5281/zenodo.1178195}
}

@InProceedings{Zappi2011,
  Title                    = {Design and Evaluation of a Hybrid Reality Performance},
  Author                   = {Zappi, Victor and Mazzanti, Dario and Brogni, Andrea and Caldwell, Darwin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2011},
  Address                  = {Oslo, Norway},
  Pages                    = {355--360},
  Abstract                 = {In this paper we introduce a multimodal platform for Hybrid Reality live performances: by means of non-invasiveVirtual Reality technology, we developed a system to presentartists and interactive virtual objects in audio/visual choreographies on the same real stage. These choreographiescould include spectators too, providing them with the possibility to directly modify the scene and its audio/visual features. We also introduce the first interactive performancestaged with this technology, in which an electronic musician played live five tracks manipulating the 3D projectedvisuals. As questionnaires have been distributed after theshow, in the last part of this work we discuss the analysisof collected data, underlining positive and negative aspectsof the proposed experience.This paper belongs together with a performance proposalcalled Dissonance, in which two performers exploit the platform to create a progressive soundtrack along with the exploration of an interactive virtual environment.},
  Keywords                 = {Interactive Performance, Hybrid Choreographies, Virtual Reality, Music Control },
  Url                      = {http://www.nime.org/proceedings/2011/nime2011_355.pdf},
  Presentation-video = {https://vimeo.com/26880256/},
  DOI                      = {10.5281/zenodo.1178197}
}
