@inproceedings{Collicutt2009,
  author = {Collicutt, Mike and Casciato, Carmine and Wanderley, Marcelo M.},
  title = {From Real to Virtual : A Comparison of Input Devices for Percussion Tasks},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177491},
  url = {http://www.nime.org/proceedings/2009/nime2009_001.pdf},
  keywords = {Evaluation of Input Devices, Motion Capture, Buchla Lightning II, Radio Baton. },
  abstract = {This paper presents an evaluation and comparison of four input devices for percussion tasks: a standard tom drum, Roland V-Drum, and two established examples of gestural controllers: the Buchla Lightning II, and the Radio Baton. The primary goal of this study was to determine how players' actions changed when moving from an acoustic instrument like the tom drum, to a gestural controller like the Buchla Lightning, which bears little resemblance to an acoustic percussion instrument. Motion capture data was analyzed by comparing a subject's hand height variability and timing accuracy across the four instruments as they performed simple musical tasks. Results suggest that certain gestures such as hand height amplitude can be adapted to these gestural controllers with little change and that in general subjects' timing variability is significantly affected when playing on the Lightning and Radio Baton when compared to the more familiar tom drum and VDrum. Possible explanations and other observations are also presented. }
}

@inproceedings{Hadjakos2009,
  author = {Hadjakos, Aristotelis and Aitenbichler, Erwin and M\''{u}hlh\''{a}user, Max},
  title = {Probabilistic Model of Pianists' Arm Touch Movements},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177567},
  url = {http://www.nime.org/proceedings/2009/nime2009_007.pdf},
  keywords = {Piano, arm movement, gesture, classification, augmented instrument, inertial sensing. },
  abstract = {Measurement of pianists' arm movement provides a signal,which is composed of controlled movements and noise. Thenoise is composed of uncontrolled movement generated bythe interaction of the arm with the piano action and measurement error. We propose a probabilistic model for armtouch movements, which allows to estimate the amount ofnoise in a joint. This estimation helps to interpret the movement signal, which is of interest for augmented piano andpiano pedagogy applications.}
}

@inproceedings{Gelineck2009,
  author = {Gelineck, Steven and Serafin, Stefania},
  title = {A Quantitative Evaluation of the Differences between Knobs and Sliders},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177549},
  url = {http://www.nime.org/proceedings/2009/nime2009_013.pdf},
  keywords = {Evaluation, Interfaces, Sliders, Knobs, Physi- cal Modeling, Electronic Musicians, Exploration, Creativ- ity, Affordances. },
  abstract = {This paper presents a HCI inspired evaluation of simple physical interfaces used to control physical models. Specifically knobs and sliders are compared in a creative and exploratory framework, which simulates the natural environment in which an electronic musician would normally explore a new instrument. No significant difference was measured between using knobs and sliders for controlling parameters of a physical modeling electronic instrument. Thereported difference between the tested instruments were mostlydue to the sound synthesis models.}
}

@inproceedings{Pedrosa2009,
  author = {Pedrosa, Ricardo and Maclean, Karon E.},
  title = {Evaluation of {3D} Haptic Target Rendering to Support Timing in Music Tasks},
  pages = {19--24},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177657},
  url = {http://www.nime.org/proceedings/2009/nime2009_019.pdf},
  keywords = {music interfaces, force feedback, tempo, comfort, target acquisition. },
  abstract = {Haptic feedback is an important element that needs to be carefully designed in computer music interfaces. This paper presents an evaluation of several force renderings for target acquisition in space when used to support a music related task. The study presented here addresses only one musical aspect: the need to repeat elements accurately in time and in content. Several force scenarios will be rendered over a simple 3D target acquisition task and users' performance will be quantitatively and qualitatively evaluated. The results show how the users' subjective preference for a particular kind of force support does not always correlate to a quantitative measurement of performance enhancement. We describe a way in which a control mapping for a musical interface could be achieved without contradicting the users' preferences as obtained from the study. }
}

@inproceedings{Hsu2009,
  author = {Hsu, William and Sosnick, Marc},
  title = {Evaluating Interactive Music Systems : An HCI Approach},
  pages = {25--28},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177579},
  url = {http://www.nime.org/proceedings/2009/nime2009_025.pdf},
  keywords = {Interactive music systems, human computer interaction, evaluation tests. },
  abstract = {In this paper, we discuss a number of issues related to the design of evaluation tests for comparing interactive music systems for improvisation. Our testing procedure covers rehearsal and performance environments, and captures the experiences of a musician/participant as well as an audience member/observer. We attempt to isolate salient components of system behavior, and test whether the musician or audience are able to discern between systems with significantly different behavioral components. We report on our experiences with our testing methodology, in comparative studies of our London and ARHS improvisation systems [1]. }
}

@inproceedings{Spowage2009,
  author = {Spowage, Neal},
  title = {The Ghetto Bastard : A Portable Noise Instrument},
  pages = {29--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177683},
  url = {http://www.nime.org/proceedings/2009/nime2009_029.pdf},
  keywords = {nime09},
  abstract = {Due to the accelerating development of ‘rapidly to become redundant’ technologies, there is a growing mountain of perfectly serviceable discarded electronic devices hiding quietly at the bottom of almost every domestic rubbish pile or at the back of nearly every second hand shop. If you add in to this scenario the accelerating nature of our society where people don’t have time or the motivation in their lives to sell or auction their redundant electronics, one can discover a plethora of discarded materials available for salvage. Using this as a starting point, I have produced a portable noise instrument from recycled materials, that is primarily an artistic led venture, built specifically for live performance.}
}

@inproceedings{Humphrey2009,
  author = {Humphrey, Eric and Leider, Colby},
  title = {The Navi Activity Monitor : Toward Using Kinematic Data to Humanize Computer Music},
  pages = {31--32},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177581},
  url = {http://www.nime.org/proceedings/2009/nime2009_031.pdf},
  keywords = {Musical kinematics, expressive tempo, machine music. },
  abstract = {Motivated by previous work aimed at developing mathematical models to describe expressive timing in music, and specifically the final ritardandi, using measured kinematic data, we further investigate the linkage of locomotion and timing in music. The natural running behavior of four subjects is measured with a wearable sensor prototype and analyzed to create normalized tempo curves. The resulting curves are then used to modulate the final ritard of MIDI scores, which are also performed by an expert musician. A Turing-inspired listening test is conducted to observe a human listener's ability to determine the nature of the performer. }
}

@inproceedings{Muller2009,
  author = {M\''{u}ller, Alexander and Essl, Georg},
  title = {Utilizing Tactile Feedback to Guide Movements Between Sounds},
  pages = {33--34},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177623},
  url = {http://www.nime.org/proceedings/2009/nime2009_033.pdf},
  keywords = {tactile feedback, intuitive interaction, gestural interaction, MIDI controller },
  abstract = {Vibetone is a musical input device which was build to explore tactile feedback in gesture based interaction. It is a prototype aimed to allow the performer to play both continuously and discrete pitched sounds in the same space. Our primary focus is on tactile feedback to guide the artist's movements during his performance. Thus, also untrained users are enabled to musical expression through bodily actions and precisely arm movements, guided through tactile feedback signals. }
}

@inproceedings{Ferguson2009,
  author = {Ferguson, Sam and Beilharz, Kirsty},
  title = {An Interface for Live Interactive Sonification},
  pages = {35--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177511},
  url = {http://www.nime.org/proceedings/2009/nime2009_035.pdf},
  keywords = {Sonification, Interactive Sonification, Auditory Display. },
  abstract = {Sonification is generally considered in a statistical data analysis context. This research discusses the development of an interface for live control of sonification – for controlling and altering sonifications over the course of their playback. This is designed primarily with real-time sources in mind, rather than with static datasets, and is intended as a performative, live data-art creative activity. The interface enables the performer to use the interface as an instrument for iterative interpretations and variations of sonifications of multiple datastreams. Using the interface, the performer can alter the scale, granularity, timbre, hierarchy of elements, spatialisation, spectral filtering, key/modality, rhythmic distribution and register ‘on-the-fly’ to both perform data-generated music, and investigate data in a live exploratory, interactive manner.}
}

@inproceedings{Reben2009,
  author = {Reben, Alexander and Laibowitz, Mat and Paradiso, Joseph A.},
  title = {Responsive Music Interfaces for Performance},
  pages = {37--38},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177663},
  url = {http://www.nime.org/proceedings/2009/nime2009_037.pdf},
  keywords = {haptics, robotics, dynamic interfaces },
  abstract = {In this project we have developed reactive instruments for performance. Reactive instruments provide feedback for the performer thereby providing a more dynamic experience. This is achieved through the use of haptics and robotics. Haptics provide a feedback system to the control surface. Robotics provides a way to actuate the instruments and their control surfaces. This allows a highly coordinated "dance" between performer and the instrument. An application for this idea is presented as a linear slide interface. Reactive interfaces represent a dynamic way for music to be portrayed in performance. }
}

@inproceedings{Lai2009,
  author = {Lai, Chi-Hsia},
  title = {Hands On Stage : A Sound and Image Performance Interface},
  pages = {39--40},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177609},
  url = {http://www.nime.org/proceedings/2009/nime2009_039.pdf},
  keywords = {audiovisual, interface design, performance. },
  abstract = {Hands On Stage, designed from a percussionist's perspective, is a new performance interface designed for audiovisual improvisation. It comprises a custom-built table interface and a performance system programmed in two environments, SuperCollider 3 and Isadora. This paper traces the interface's evolution over matters of relevant technology, concept, construction, system design, and its creative outcomes. }
}

@inproceedings{McDonald2009,
  author = {McDonald, Kyle and Kouttron, Dane and Bahn, Curtis and Braasch, Jonas and Oliveros, Pauline},
  title = {The Vibrobyte : A Haptic Interface for Co-Located Performance},
  pages = {41--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177627},
  url = {http://www.nime.org/proceedings/2009/nime2009_041.pdf},
  keywords = {haptics,interface,nime09,performance,telematic},
  abstract = {The Vibrobyte is a wireless haptic interface specialized forco-located musical performance. The hardware is designedaround the open source Arduino platform, with haptic control data encapsulated in OSC messages, and OSC/hardwarecommunications handled by Processing. The Vibrobyte wasfeatured at the International Computer Music Conference2008 (ICMC) in a telematic performance between ensembles in Belfast, Palo Alto (California, USA), and Troy (NewYork, USA).}
}

@inproceedings{Wiley2009,
  author = {Wiley, Meason and Kapur, Ajay},
  title = {Multi-Laser Gestural Interface --- Solutions for Cost-Effective and Open Source Controllers},
  pages = {43--44},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177709},
  url = {http://www.nime.org/proceedings/2009/nime2009_043.pdf},
  keywords = {Lasers, photocell sensor, UltraSound, Open Source controller design, digital gamelan, digital tanpura },
  abstract = {This paper describes a cost-effective, modular, open source framework for a laser interface design that is open to community development, interaction and user modification. The following paper highlights ways in which we are implementing the multi-laser gestural interface in musical, visual, and robotic contexts. }
}

@inproceedings{Kanda2009,
  author = {Kanda, Ryo and Hashida, Mitsuyo and Katayose, Haruhiro},
  title = {Mims : Interactive Multimedia Live Performance System},
  pages = {45--47},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177595},
  url = {http://www.nime.org/proceedings/2009/nime2009_045.pdf},
  keywords = {Interaction, audience, performer, visualize, sensor, physical, gesture. },
  abstract = {We introduce Mims, which is an interactive-multimedia live-performance system, where pieces rendered by a performer’s voice are translated into floating objects called voice objects. The voice objects are generated from the performer’s current position on the screen, and absorbed by another flying object called Mims. Voice sounds are modulated by the behavior of Mims. Performers can control these objects and sound effects by using their own gestures. Mims provides performers and their audiences with expressive visual feedback in terms of sound manipulations and results.}
}

@inproceedings{Goto2009a,
  author = {Goto, Suguru and Powell, Rob},
  title = {netBody --- "Augmented Body and Virtual Body II" with the System, BodySuit, Powered Suit and Second Life --- Its Introduction of an Application of the System},
  pages = {48--49},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177559},
  url = {http://www.nime.org/proceedings/2009/nime2009_048.pdf},
  keywords = {artificial intelligence,gesture controller,humanoid robot,interaction,internet,nime09,robot},
  abstract = {This is intended to introduce the system, which combines BodySuit, especially Powered Suit, and Second Life, as well as its possibilities and its uses in a musical performance application. The system which we propose contains both a gesture controller and robots at the same time. In this system, the Data Suit, BodySuit controls the avatar in Second Life and Second Life controls the exoskeleton, Powered Suit in real time. These are related with each other in conjunction with Second Life in Internet. BodySuit doesn't contain a hand-held controller. A performer, for example a dancer, wears a suit. Gestures are transformed into electronic signals by sensors. Powered Suit is another suit that a dancer wears, but gestures are generated by motors. This is a sort of wearable robot. Second Life is software that is developed by Linden Lab. It allows creating a virtual world and a virtual human (avatar) in Internet. Working together with BodySuit, Powered Suit, and Second Life the idea behind the system is that a human body is augmented by electronic signals and is reflected in a virtual world in order to be able to perform interactively. }
}

@inproceedings{Ogawa2009,
  author = {Ogawa, Keisuke and Kuhara, Yasuo},
  title = {Life Game Orchestra as an Interactive Music Composition System Translating Cellular Patterns of Automata into Musical Scales},
  pages = {50--51},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177647},
  url = {http://www.nime.org/proceedings/2009/nime2009_050.pdf},
  keywords = {Conway's Game of Life, Cellular automata, Cell pattern, scale, Interactive composition, performance. },
  abstract = {We developed a system called Life Game Orchestra that generates music by translating cellular patterns of Conway's Game of Life into musical scales. A performer can compose music by controlling varying cell patterns and sounds with visual and auditory fun. A performer assigns the elements of tone to two-dimensional cell patterns in the matrix of the Game of Life. Our system searches defined cell patterns in the varying matrix dynamically. If the patterns are matched, corresponding tones are generated. A performer can make cells in the matrix by moving in front of a camera and interactively influencing the generation of music. The progress of the Game of Life is controlled with a clock defined by the performer to configure the groove of the music. By running multiple matrices with different pattern mapping, clock timing, and instruments, we can perform an ensemble. The Life Game Orchestra is a fusion system of the design of a performer and the emergence of cellular automata as a complex system. }
}

@inproceedings{Toenjes2009,
  author = {Toenjes, John},
  title = {Natural Materials on Stage : Custom Controllers for Aesthetic Effect},
  pages = {52--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177693},
  url = {http://www.nime.org/proceedings/2009/nime2009_052.pdf},
  keywords = {control surface, interface, tactile, natural, organic, interactive dance. },
  abstract = {This article describes the implications of design and materials of computer controllers used in the context of interactive dance performance. Size, shape, and layout all influence audience perception of the performer, and materials imply context for further interpretation of the interactive performance work. It describes the construction of the "Control/Recorder" and the "VideoLyre", two custom computer control surfaces made for Leonardo's Chimes, a work by Toenjes, Marchant and Smith, and how these controllers contribute to theatrical aesthetic intent. }
}

@inproceedings{Keith2009,
  author = {Keith, Sarah},
  title = {Controlling Live Generative Electronic Music with Deviate},
  pages = {54--55},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177599},
  url = {http://www.nime.org/proceedings/2009/nime2009_054.pdf},
  keywords = {generative, performance, laptop, popular music },
  abstract = {Deviate generates multiple streams of melodic and rhythmic output in real-time, according to user-specified control parameters. This performance system has been implemented using Max 5 [1] within the genre of popular contemporary electronic music, incorporating techno, IDM, and related forms. The aim of this project is not musical style synthesis, but to construct an environment in which a range of creative and musical goals may be achieved. A key aspect is control over generative processes, as well as consistent yet varied output. An approach is described which frees the user from determining note-level output while allowing control to be maintained over larger structural details, focusing specifically on the melodic aspect of this system. Audio examples are located online at http://www.cetenbaath.com/cb/about-deviate/. }
}

@inproceedings{Dolphin2009,
  author = {Dolphin, Andy},
  title = {SpiralSet : A Sound Toy Utilizing Game Engine Technologies},
  pages = {56--57},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177497},
  url = {http://www.nime.org/proceedings/2009/nime2009_056.pdf},
  keywords = {Sound Toys, Game Engines, Animated Interfaces, Spectral Synthesis, Open Work, Max/MSP. },
  abstract = {SpiralSet is a sound toy incorporating game enginesoftware used in conjunction with a spectral synthesissound engine constructed in Max/MSP/Jitter. SpiralSetwas presented as an interactive installation piece at theSonic Arts Expo 2008, in Brighton, UK. A custom madesensor-based interface is used for control of the system.The user interactions are designed to be quickly accessiblein an installation context, yet allowing the potential forsonic depth and variation.}
}

@inproceedings{Gao2009,
  author = {Gao, Mingfei and Hanson, Craig},
  title = {LUMI : Live Performance Paradigms Utilizing Software Integrated Touch Screen and Pressure Sensitive Button Matrix},
  pages = {58--59},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177547},
  url = {http://www.nime.org/proceedings/2009/nime2009_058.pdf},
  keywords = {live performance interface,lumi,nime09,pressure},
  abstract = {This paper explores a rapidly developed, new musical interface involving a touch-screen, 32 pressure sensitive button pads, infrared sensor, 8 knobs and cross-fader. We provide a versatile platform for computer-based music performance and production using a human computer interface that has strong visual and tactile feedback as well as robust software that exploits the strengths of each individual system component. }
}

@inproceedings{Gillian2009,
  author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
  title = {The {SAR}C EyesWeb Catalog : A Pattern Recognition Toolbox for Musician-Computer Interaction},
  pages = {60--61},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177551},
  url = {http://www.nime.org/proceedings/2009/nime2009_060.pdf},
  keywords = {SARC EyesWeb Catalog, gesture recognition },
  abstract = {This paper presents the SARC EyesWeb Catalog (SEC), agroup of blocks designed for real-time gesture recognitionthat have been developed for the open source program EyesWeb. We describe how the recognition of real-time bodymovements can be used for musician-computer-interaction.}
}

@inproceedings{Nishino2009,
  author = {Nishino, Hiroki},
  title = {A {2D} Fiducial Tracking Method based on Topological Region Adjacency and Angle Information},
  pages = {62--63},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177643},
  url = {http://www.nime.org/proceedings/2009/nime2009_062.pdf},
  keywords = {fiducial tracking, computer vision, tangible user interface, interaction techniques. },
  abstract = {We describe a new method for 2D fiducial tracking. We use region adjacency information together with angles between regions to encode IDs inside fiducials, whereas previous research by Kaltenbrunner and Bencina utilize region adjacency tree. Our method supports a wide ID range and is fast enough to accommodate real-time video. It is also very robust against false positive detection. }
}

@inproceedings{Solis2009,
  author = {Solis, Jorge and Ninomiya, Takeshi and Petersen, Klaus and Takeuchi, Masaki and Takanishi, Atsuo},
  title = {Anthropomorphic Musical Performance Robots at Waseda University : Increasing Understanding of the Nature of Human Musical Interaction Abstract},
  pages = {64--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177681},
  url = {http://www.nime.org/proceedings/2009/nime2009_064.pdf},
  keywords = {nime09},
  abstract = {During several decades, the research at Waseda University has been focused on developing anthropomorphic robots capable performing musical instruments. As a result of our research efforts, the Waseda Flutist Robot WF-4RIV and the Waseda Saxophonist Robot WAS-1 have been designed to reproduce the human player performance. As a long-term goal, we are proposing to enable the interaction between musical performance robots as well as with human players. In general the communication of humans within a band is a special case of conventional human social behavior. Rhythm, harmony and timbre of the music played represent the emotional states of the musicians. So the development of an artificial entity that participates in such an interaction may contribute to the better understanding of some of the mechanisms that enable the communication of humans in musical terms. Therefore, we are not considering a musical performance robot (MPR) just as a mere sophisticated MIDI instrument. Instead, its human-like design and the integration of perceptual capabilities may enable to act on its own autonomous initiative based on models which consider its own physical constrains. In this paper, we present an overview of our research approaches towards enabling the interaction between musical performance robots as well as with musicians. }
}

@inproceedings{Weinberg2009a,
  author = {Weinberg, Gil and Blosser, Brian and Mallikarjuna, Trishul and Raman, Aparna},
  title = {The Creation of a Multi-Human, Multi-Robot Interactive Jam Session},
  pages = {70--73},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177705},
  url = {http://www.nime.org/proceedings/2009/nime2009_070.pdf},
  keywords = {Robotic musicianship, Shimon, Haile. },
  abstract = {This paper presents an interactive and improvisational jam session, including human players and two robotic musicians. The project was developed in an effort to create novel and inspiring music through human-robot collaboration. The jam session incorporates Shimon, a newly-developed socially-interactive robotic marimba player, and Haile, a perceptual robotic percussionist developed in previous work. The paper gives an overview of the musical perception modules, adaptive improvisation modes and human-robot musical interaction models that were developed for the session. The paper also addresses the musical output that can be created from increased interconnections in an expanded multiple-robot multiplehuman ensemble, and suggests directions for future work. }
}

@inproceedings{Gong2009,
  author = {Gong, Nan-Wei and Laibowitz, Mat and Paradiso, Joseph A.},
  title = {MusicGrip : A Writing Instrument for Music Control},
  pages = {74--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177555},
  url = {http://www.nime.org/proceedings/2009/nime2009_074.pdf},
  keywords = {Interactive music control, writing instrument, pen controller, MIDI, group performing activity. },
  abstract = {In this project, we have developed a real-time writing instrument for music control. The controller, MusicGrip, can capture the subtle dynamics of the user's grip while writing or drawing and map this to musical control signals and sonic outputs. This paper discusses this conversion of the common motor motion of handwriting into an innovative form of music expression. The presented example instrument can be used to integrate the composing aspect of music with painting and writing, creating a new art form from the resultant aural and visual representation of the collaborative performing process. }
}

@inproceedings{Partridge2009,
  author = {Partridge, Grant and Irani, Pourang and Fitzell, Gordon},
  title = {Let Loose with WallBalls, a Collaborative Tabletop Instrument for Tomorrow},
  pages = {78--81},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177655},
  url = {http://www.nime.org/proceedings/2009/nime2009_078.pdf},
  keywords = {Tabletop computers, collaborative instruments, collaborative composition, group improvisation, spatial audio interfaces, customizable instruments. },
  abstract = {Tabletops—and by extension, tabletop computers— naturally facilitate group work. In particular, they provide a fascinating platform for exploring the possibilities of collaborative audio improvisation. Existing tabletop instruments (and digital instruments in general) tend to impose either a steep learning curve on novice players or a frustrating ceiling of expressivity upon experts. We introduce WallBalls, an intuitive tabletop instrument designed to support both novice and expert performance. At first glance, WallBalls resembles a toy, game or whimsical sketchpad, but it quickly reveals itself as a deeply expressive and highly adaptable sample-based instrument capable of facilitating a startling variety of collaborative sound art.}
}

@inproceedings{Min2009,
  author = {Min, Hye Ki},
  title = {SORISU : Sound with Numbers},
  pages = {82--85},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177631},
  url = {http://www.nime.org/proceedings/2009/nime2009_082.pdf},
  keywords = {Numbers, Game Interfaces, Mathematics and Sound, Mathematics in Music, Puzzles, Tangible User Interfaces. },
  abstract = {It is surely not difficult for anyone with experience in thesubject known as Music Theory to realize that there is avery definite and precise relationship between music andmathematics. This paper describes the SoriSu, a newelectronic musical instrument based on Sudoku puzzles,which probe the expressive possibilities of mathematicalconcepts in music. The concept proposes a new way ofmapping numbers to sound. This interface was designed toprovide easy and pleasing access to music for users whoare unfamiliar or uncomfortable with current musicaldevices. The motivation behind the project is presented, aswell as hardware and software design.}
}

@inproceedings{Mann2009,
  author = {Mann, Yotam and Lubow, Jeff and Freed, Adrian},
  title = {The Tactus : a Tangible , Rhythmic Grid Interface Using Found-Objects},
  pages = {86--89},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177625},
  url = {http://www.nime.org/proceedings/2009/nime2009_086.pdf},
  keywords = {nime09},
  abstract = {This paper describes the inspiration and implementation of a tactile, tabletop synthesizer/step sequencer. The Tactus is an expandable and inexpensive musical interface for the creation of loop-based music inspired by the Bubblegum Sequencer [2]. An optical camera, coupled with a computer running Max/MSP/Jitter can turn almost any matrix-like object into a step sequencer. The empty cells in the gridded object are filled with a fitting, colored object; the placement of which is analogous to adding an instrument or switching on a box in a step sequencer grid. The color and column position of every element in the matrix are used as parameters for a synthesizer while the row position of that element corresponds to the moment within the loop that entry is sounded. The two dimensional array can be positioned anywhere within the camera's visibility. Both the translation and rotation of the physical matrix are assigned to global parameters that affect the music while preserving the color and order of the cells. A rotation of 180 degrees, for example, will not reverse the sequence, but instead change an assigned global parameter.}
}

@inproceedings{Hockman2009,
  author = {Hockman, Jason A. and Wanderley, Marcelo M. and Fujinaga, Ichiro},
  title = {Real-Time Phase Vocoder Manipulation by Runner's Pace},
  pages = {90--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177575},
  url = {http://www.nime.org/proceedings/2009/nime2009_090.pdf},
  keywords = {NIME, synchronization, exercise, time-scaling. },
  abstract = {This paper presents a method for using a runner's pacefor real-time control of the time-scaling facility of a phasevocoder, resulting in the automated synchronization of anaudio track tempo to the generated control signal. The increase in usage of portable music players during exercisehas given rise to the development of new personal exerciseaids, most notably the Nike+iPod system, which relies onembedded sensor technologies to provide kinematic workout statistics. There are also systems that select songs basedon the measured step frequency of a runner. The proposedsystem also uses the pace of a runner, but this information isused to change the tempo of the music.}
}

@inproceedings{Nymoen2009,
  author = {Nymoen, Kristian and Jensenius, Alexander R.},
  title = {A Discussion of Multidimensional Mapping in Nymophone2},
  pages = {94--97},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177645},
  url = {http://www.nime.org/proceedings/2009/nime2009_094.pdf},
  keywords = {nime09},
  abstract = {The paper presents Nymophone2, an acoustic instrument with a complex relationship between performance actions and emergent sound. A method for describing the multidimensional control actions needed to play the instrument is presented and discussed.}
}

@inproceedings{Schlessinger2009,
  author = {Schlessinger, Daniel and Smith, Julius O.},
  title = {The Kalichord : A Physically Modeled Electro-Acoustic Plucked String Instrument},
  pages = {98--101},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177671},
  url = {http://www.nime.org/proceedings/2009/nime2009_098.pdf},
  keywords = {Kalichord, physical model, tine, piezo, plucked string, electro-acoustic instruments, kalimba, accordion },
  abstract = { We present the Kalichord: a small, handheld electro/acoustic instrument in which the player's right hand plucks virtual strings while his left hand uses buttons to play independent bass lines. The Kalichord uses the analog signal from plucked acoustic tines to excite a physical string model, allowing a nuanced and intuitive plucking experience. First, we catalog instruments related to the Kalichord. Then we examine the use of analog signals to excite a physical string model and discuss the expressiveness and form factors that this technique affords. We then describe the overall construction of the Kalichord and possible playing styles, and finally we consider ways we hope to improve upon the current prototype. }
}

@inproceedings{Lahdeoja2009,
  author = {L\''{a}hdeoja, Otso},
  title = {Augmenting Chordophones with Hybrid Percussive Sound Possibilities},
  pages = {102--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177607},
  url = {http://www.nime.org/proceedings/2009/nime2009_102.pdf},
  keywords = {augmented instrument,chordophone,contact microphone systems,electric,electronic percussion,even with,guitar,leaving the instrument body,nime09,there is always a,trade-off,virtually mute},
  abstract = {In this paper we describe an approach for introducing newelectronic percussive sound possibilities for stringinstruments by "listening" to the sounds of the instrument'sbody and extracting audio and data from the wood'sacoustic vibrations. A method for capturing, localizing andanalyzing the percussive hits on the instrument's body ispresented, in connection with an audio-driven electronicpercussive sound module. The system introduces a newgesture-sound relationship in the electric string instrumentplaying environment, namely the use of percussivetechniques on the instrument's body which are null inregular circumstances due to selective and exclusivemicrophone use for the strings. Instrument bodypercussions are widely used in the acoustic instrumentalpraxis. They yield a strong potential for providing anextended soundscape via instrument augmentation, directlycontrolled by the musician through haptic manipulation ofthe instrument itself. The research work was carried out onthe electric guitar, but the method used can apply to anystring instrument with a resonating body.}
}

@inproceedings{Kahrs2009,
  author = {Kahrs, Mark and Skulina, David and Bilbao, Stefan and Campbell, Murray},
  title = {An Electroacoustically Controlled Vibrating Plate},
  pages = {106--109},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177593},
  url = {http://www.nime.org/proceedings/2009/nime2009_106.pdf},
  keywords = {Electroacoustics, flat panel },
  abstract = {Large vibrating plates are used as thunder sheets in orchestras. We have extended the use of flat plates by cementing aflat panel electroacoustic transducer on a large brass sheet.Because of the thickness of the panel, the output is subject tononlinear distortion. When combined with a real-time inputand signal processing algorithm, the active brass plate canbecome an effective musical instrument for performance ofnew music.}
}

@inproceedings{Smallwood2009a,
  author = {Smallwood, Scott and Cook, Perry R. and Trueman, Dan and McIntyre, Lawrence},
  title = {Don't Forget the Loudspeaker --- A History of Hemispherical Speakers at Princeton , Plus a DIY Guide},
  pages = {110--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177679},
  url = {http://www.nime.org/proceedings/2009/nime2009_110.pdf},
  keywords = {loudspeakers, hemispherical speakers, sonic display systems, laptop orchestras. },
  abstract = {This paper gives a historical overview of the development of alternative sonic display systems at Princeton University; in particular, the design, construction, and use in live performance of a series of spherical and hemispherical speaker systems. We also provide a DIY guide to constructing the latest series of loudspeakers that we are currently using in our research and music making. }
}

@inproceedings{Freed2009a,
  author = {Freed, Adrian and Schmeder, Andrew},
  title = {Features and Future of Open Sound Control version 1.1 for NIME},
  pages = {116--120},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177517},
  url = {http://www.nime.org/proceedings/2009/nime2009_116.pdf},
  keywords = {Open Sound Control, Time Tag, OSC, Reservation Protocols. },
  abstract = {The history and future of Open Sound Control (OSC) is discussed and the next iteration of the OSC specification is introduced with discussion of new features to support NIME community activities. The roadmap to a major revision of OSC is developed. }
}

@inproceedings{Schmeder2009,
  author = {Schmeder, Andrew and Freed, Adrian},
  title = {A Low-level Embedded Service Architecture for Rapid DIY Design of Real-time Musical Instruments},
  pages = {121--124},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177673},
  url = {http://www.nime.org/proceedings/2009/nime2009_121.pdf},
  keywords = {real-time musical interface, DIY design, em- bedded web services, rapid prototyping, reconfigurable firmware },
  abstract = {An on-the-fly reconfigurable low-level embedded servicearchitecture is presented as a means to improve scalability, improve conceptual comprehensibility, reduce humanerror and reduce development time when designing newsensor-based electronic musical instruments with real-timeresponsiveness. The implementation of the concept ina project called micro-OSC is described. Other sensorinterfacing products are evaluated in the context of DIYprototyping of musical instruments. The capabilities ofthe micro-OSC platform are demonstrated through a set ofexamples including resistive sensing, mixed digital-analogsystems, many-channel sensor interfaces and time-basedmeasurement methods.}
}

@inproceedings{Steiner2009,
  author = {Steiner, Hans-Christoph},
  title = {Firmata : Towards Making Microcontrollers Act Like Extensions of the Computer},
  pages = {125--130},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177689},
  url = {http://www.nime.org/proceedings/2009/nime2009_125.pdf},
  keywords = {arduino,microcontroller,nime09,processing,pure data},
  abstract = {Firmata is a generic protocol for communicating with microcontrollers from software on a host computer. The central goal is to make the microcontroller an extension of theprogramming environment on the host computer in a manner that feels natural in that programming environment. Itwas designed to be open and flexible so that any programming environment can support it, and simple to implementboth on the microcontroller and the host computer to ensurea wide range of implementations. The current reference implementation is a library for Arduino/Wiring and is includedwith Arduino software package since version 0012. Thereare matching software modules for a number of languages,like Pd, OpenFrameworks, Max/MSP, and Processing.}
}

@inproceedings{Baalman2009a,
  author = {Baalman, Marije A. and Smoak, Harry C. and Salter, Christopher L. and Malloch, Joseph and Wanderley, Marcelo M.},
  title = {Sharing Data in Collaborative, Interactive Performances : the SenseWorld DataNetwork},
  pages = {131--134},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177471},
  url = {http://www.nime.org/proceedings/2009/nime2009_131.pdf},
  keywords = {Data exchange, collaborative performance, interactive performance, interactive art works, sensor data, OpenSoundControl, SuperCollider, Max/MSP},
  abstract = {The SenseWorld DataNetwork framework addresses the is- sue of sharing and manipulating multiple data streams among different media systems in a heterogenous interactive per- formance environment. It is intended to facilitate the cre- ation, rehearsal process and performance practice of collab- orative interactive media art works, by making the sharing of data (from sensors or internal processes) between collab- orators easier, faster and more flexible.}
}

@inproceedings{Bouillot2009,
  author = {Bouillot, Nicolas and Cooperstock, Jeremy R.},
  title = {Challenges and Performance of High-Fidelity Audio Streaming for Interactive Performances},
  pages = {135--140},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177485},
  url = {http://www.nime.org/proceedings/2009/nime2009_135.pdf},
  keywords = {Networked Musical Performance, high-fidelity audio streaming, glitch detection, latency measurement },
  abstract = {Low-latency streaming of high-quality audio has the potential to dramatically transform the world of interactive musical applications. We provide methods for accurately measuring the end-to-end latency and audio quality of a delivered audio stream and apply these methods to an empirical evaluation of several streaming engines. In anticipationof future demands for emerging applications involving audio interaction, we also review key features of streamingengines and discuss potential challenges that remain to beovercome.}
}

@inproceedings{Todoroff2009,
  author = {Todoroff, Todor and Bettens, Fr\'{e}d\'{e}ric and Reboursi\`{e}re, Lo\"{i}c and Chu, Wen-Yang},
  title = {''Extension du Corps Sonore'' --- Dancing Viola},
  pages = {141--146},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177691},
  url = {http://www.nime.org/proceedings/2009/nime2009_141.pdf},
  keywords = {Sensor data pre-processing, gesture recognition, mapping, interpolation, extension du corps sonore },
  abstract = {''Extension du corps sonore'' is long-term project initiatedby Musiques Nouvelles [4], a contemporary music ensemble in Mons. It aims at giving instrumental music performers an extended control over the sound of their instrument byextending the understanding of the sound body from the instrument only to the combination of the instrument and thewhole body of the performer. The development started atARTeM and got the benefit of a three month numediartresearch project [1] that focused on three axes of research:pre-processing of sensor data, gesture recognition and mapping through interpolation. The objectives were the development of computing methods and flexible Max/MSP externals to be later integrated in the ARTeM software framework for the concerts with viola player Dominica Eyckmans. They could be used in a variety of other artistic worksand will be made available on the numediart website [1],where more detailed information can be found in the Quarterly Progress Scientific Report \#4.}
}

@inproceedings{Leider2009a,
  author = {Leider, Colby and Mann, Doug and Plazas, Daniel and Battaglia, Michael and Draper, Reid},
  title = {The elBo and footPad : Toward Personalized Hardware for Audio Manipulation},
  pages = {147--148},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177617},
  url = {http://www.nime.org/proceedings/2009/nime2009_147.pdf},
  keywords = {user modeling, user customization },
  abstract = {We describe initial prototypes and a design strategy for new, user-customized audio-manipulation and editing tools. These tools are designed to enable intuitive control of audio-processing tasks while anthropomorphically matching the target user. }
}

@inproceedings{Crawford2009,
  author = {Crawford, Langdon and Fastenow, William D.},
  title = {The Midi-AirGuitar , A serious Musical Controller with a Funny Name Music Technology Program},
  pages = {149--150},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177495},
  url = {http://www.nime.org/proceedings/2009/nime2009_149.pdf},
  keywords = {nime09},
  abstract = {The MIDI-Airguitar is a hand held musical controller based on Force Sensing Resister (FSR) and Accelerometer technology. The hardware and software implementation of the MIDI-Airguitars are described below. Current practices of the authors in performance are discussed.}
}

@inproceedings{Bottcher2009,
  author = {B\''{o}ttcher, Niels and Dimitrov, Smilen},
  title = {An Early Prototype of the Augmented PsychoPhone},
  pages = {151--152},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177467},
  url = {http://www.nime.org/proceedings/2009/nime2009_151.pdf},
  keywords = {Augmented saxophone, Physical computing, hyper instruments, mapping. },
  abstract = {In this poster we present the early prototype of the augmented Psychophone --- a saxophone with various applied sensors, allowing the saxophone player to attach effects like pitch shifting, wah-wah and ring modulation to the saxophone, simply by moving the saxophone as one would do when really being enthusiastic and involved in the performance. The possibility of scratching on the previously recorded sound is also possible directly on the saxophone. }
}

@inproceedings{Siwiak2009,
  author = {Siwiak, Diana and Berger, Jonathan and Yang, Yao},
  title = {Catch Your Breath --- Musical Biofeedback for Breathing Regulation},
  pages = {153--154},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177675},
  url = {http://www.nime.org/proceedings/2009/nime2009_153.pdf},
  keywords = {sensor, music, auditory display. },
  abstract = {Catch Your Breath is an interactive audiovisual bio-feedbacksystem adapted from a project designed to reduce respiratory irregularity in patients undergoing 4D CT scans for oncological diagnosis. The system is currently implementedand assessed as a potential means to reduce motion-induceddistortion in CT images.A museum installation based on the same principle wascreated in which an inexpensive wall-mounted web camera tracks an IR sensor embedded into a pendant worn bythe user. The motion of the subjects breathing is trackedand interpreted as a real-time variable tempo adjustment toa stored musical file. The subject can then adjust his/herbreathing to synchronize with a separate accompanimentline. When the breathing is regular and is at the desiredtempo, the audible result sounds synchronous and harmonious. The accompaniment's tempo progresses and gradually decrease which causes the breathing to synchronize andslow down, thus increasing relaxation.}
}

@inproceedings{Peng2009,
  author = {Peng, Lijuan and Gerhard, David},
  title = {A Wii-Based Gestural Interface for Computer Conducting Systems},
  pages = {155--156},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177659},
  url = {http://www.nime.org/proceedings/2009/nime2009_155.pdf},
  keywords = {Conducting, Gesture, Infrared, Learning, Wii. },
  abstract = {With the increase of sales of Wii game consoles, it is becoming commonplace for the Wii remote to be used as analternative input device for other computer systems. In thispaper, we present a system which makes use of the infraredcamera within the Wii remote to capture the gestures of aconductor using a baton with an infrared LED and battery.Our system then performs data analysis with gesture classification and following, and finally displays the gestures using visual baton trajectories and audio feedback. Gesturetrajectories are displayed in real time and can be comparedto the corresponding diagram shown in a textbook. In addition, since a conductor normally does not look at a screenwhile conducting, tones are played to represent a certainbeat in a conducting gesture. Further, the system can be controlled entirely with the baton, removing the need to switchfrom baton to mouse. The interface is intended to be usedfor pedagogy purposes.}
}

@inproceedings{Parson2009,
  author = {Parson, Dale E.},
  title = {Chess-Based Composition and Improvisation for Non-Musicians},
  pages = {157--158},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177653},
  url = {http://www.nime.org/proceedings/2009/nime2009_157.pdf},
  keywords = {algorithmic composition, chess, ChucK, improvisation, Max/MSP, SuperCollider. },
  abstract = {''Music for 32 Chess Pieces'' is a software system that supports composing, performing and improvising music by playing a chess game. A game server stores a representation of the state of a game, validates proposed moves by players, updates game state, and extracts a graph of piece-to-piece relationships. It also loads a plugin code module that acts as a composition. A plugin maps pieces and relationships on the board, such as support or attack relationships, to a timed sequence of notes and accents. The server transmits notes in a sequence to an audio renderer process via network datagrams. Two players can perform a composition by playing chess, and a player can improvise by adjusting a plugin's music mapping parameters via a graphical user interface. A composer can create a new composition by writing a new plugin that uses a distinct algorithm for mapping game rules and states to music. A composer can also write a new note-to-sound mapping program in the audio renderer language. This software is available at http://faculty.kutztown.edu/parson/music/ParsonMusic.html. }
}

@inproceedings{Dolphin2009a,
  author = {Dolphin, Andy},
  title = {MagNular : Symbolic Control of an External Sound Engine Using an Animated Interface},
  pages = {159--160},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177499},
  url = {http://www.nime.org/proceedings/2009/nime2009_159.pdf},
  keywords = {Sound Toys, Open Work, Game Engines, Animated Interfaces, Max/MSP. },
  abstract = {This paper reports on work in progress on the creativeproject MagNular, part of a wider practical study of thepotential collaborative compositional applications of gameengine technologies. MagNular is a sound toy utilizingcomputer game and physics engine technologies to createan animated interface used in conjunction with an externalsound engine developed within Max/MSP. The playercontrols virtual magnets that attract or repel numerousparticle objects, moving them freely around the virtualspace. Particle object collision data is mapped to controlsound onsets and synthesis/DSP (Digital SignalProcessing) parameters. The user "composes" bycontrolling and influencing the simulated physicalbehaviors of the particle objects within the animatedinterface.}
}

@inproceedings{Feehan2009,
  author = {Feehan, Noah},
  title = {Audio Orienteering -- Navigating an Invisible Terrain},
  pages = {161--162},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177505},
  url = {http://www.nime.org/proceedings/2009/nime2009_161.pdf},
  keywords = {wii, 3-d positioning, audio terrain, collaborative performance. },
  abstract = {AUDIO ORIENTEERING is a collaborative performance environment in which physical tokens are used to navigate an invisible sonic landscape. In this paper, I describe the hardware and software used to implement a prototype audio terrain with multiple interaction modes and sonic behaviors mapped onto three-dimensional space. }
}

@inproceedings{DeJong2009,
  author = {de Jong, Staas},
  title = {Developing the Cyclotactor},
  pages = {163--164},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177591},
  url = {http://www.nime.org/proceedings/2009/nime2009_163.pdf},
  keywords = {Musical controller, tactile interface. },
  abstract = {This paper presents developments in the technology underlying the cyclotactor, a finger-based tactile I/O device for musical interaction. These include significant improvements both in the basic characteristics of tactile interaction and in the related (vibro)tactile sample rates, latencies, and timing precision. After presenting the new prototype's tactile output force landscape, some of the new possibilities for interaction are discussed, especially those for musical interaction with zero audio/tactile latency.}
}

@inproceedings{Schiesser2009,
  author = {Schiesser, S\'{e}bastien},
  title = {midOSC : a Gumstix-Based {MIDI-to-OSC} Converter},
  pages = {165--168},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177669},
  url = {http://www.nime.org/proceedings/2009/nime2009_165.pdf},
  keywords = {MIDI, Open Sound Control, converter, gumstix },
  abstract = {A MIDI-to-OSC converter is implemented on a commercially available embedded linux system, tighly integratedwith a microcontroller. A layered method is developed whichpermits the conversion of serial data such as MIDI to OSCformatted network packets with an overall system latencybelow 5 milliseconds for common MIDI messages.The Gumstix embedded computer provide an interesting and modular platform for the development of such anembedded applications. The project shows great potentialto evolve into a generic sensors-to-OSC ethernet converterwhich should be very useful for artistic purposes and couldbe used as a fast prototyping interface for gesture acquisitiondevices.}
}

@inproceedings{Nagashima2009,
  author = {Nagashima, Yoichi},
  title = {Parallel Processing System Design with "Propeller" Processor},
  pages = {169--170},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177635},
  url = {http://www.nime.org/proceedings/2009/nime2009_169.pdf},
  keywords = {Propeller, parallel processing, MIDI, sensor, interfaces. },
  abstract = {This is a technical and experimental report of parallel processing, using the "Propeller" chip. Its eight 32 bits processors (cogs) can operate simultaneously, either independently or cooperatively, sharing common resources through a central hub. I introduce this unique processor and discuss about the possibility to develop interactive systems and smart interfaces in media arts, because we need many kinds of tasks at a same time with NIMErelated systems and installations. I will report about (1) Propeller chip and its powerful IDE, (2) external interfaces for analog/digital inputs/outputs, (3) VGA/NTSC/PAL video generation, (4) audio signal processing, and (5) originally-developed MIDI input/output method. I also introduce three experimental prototype systems.}
}

@inproceedings{Fyans2009,
  author = {Fyans, A. Cavan and Gurevich, Michael and Stapleton, Paul},
  title = {Where Did It All Go Wrong ? A Model of Error From the Spectator's Perspective},
  pages = {171--172},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177519},
  url = {http://www.nime.org/proceedings/2009/nime2009_171.pdf},
  keywords = {performance, skill, transparency, design, HCI },
  abstract = {The development of new interfaces for musical expressionhas created a need to study how spectators comprehend newperformance technologies and practices. As part of a largerproject examining how interactions with technology can becommunicated with the spectator, we relate our model ofspectator understanding of error to the NIME discourse surrounding transparency, mapping, skill and success.}
}

@inproceedings{dAlessandro2009,
  author = {d'Alessandro, Nicolas and Dutoit, Thierry},
  title = {Advanced Techniques for Vertical Tablet Playing A Overview of Two Years of Practicing the HandSketch 1.x},
  pages = {173--174},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177465},
  url = {http://www.nime.org/proceedings/2009/nime2009_173.pdf},
  keywords = {graphic tablet, playing position, techniques },
  abstract = {In this paper we present new issues and challenges relatedto the vertical tablet playing. The approach is based on apreviously presented instrument, the HANDSKETCH. Thisinstrument has now been played regularly for more than twoyears by several performers. Therefore this is an opportunityto propose a better understanding of the performing strategy.We present the behavior of the whole body as an underlyingaspect in the manipulation of the instrument.}
}

@inproceedings{Hoofer2009,
  author = {H\''{o}ofer, Andreas and Hadjakos, Aristotelis and M\''{u}hlh\''{a}user, Max},
  title = {Gyroscope-Based Conducting Gesture Recognition},
  pages = {175--176},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177565},
  url = {http://www.nime.org/proceedings/2009/nime2009_175.pdf},
  keywords = {nime09},
  abstract = {This paper describes a method for classification of different beat gestures within traditional beat patterns based on gyroscope data and machine learning techniques and provides a quantitative evaluation.}
}

@inproceedings{Berdahl2009b,
  author = {Berdahl, Edgar and Niemeyer, G\''{u}nter and Smith, Julius O.},
  title = {Using Haptics to Assist Performers in Making Gestures to a Musical Instrument},
  pages = {177--182},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177481},
  url = {http://www.nime.org/proceedings/2009/nime2009_177.pdf},
  keywords = {Haptic, detent, pitch selection, human motor system, feedback control, response time, gravity well },
  abstract = {Haptic technology, providing force cues and creating a programmable physical instrument interface, can assist musicians in making gestures. The finite reaction time of thehuman motor control system implies that the execution of abrief musical gesture does not rely on immediate feedbackfrom the senses, rather it is preprogrammed to some degree.Consequently, we suggest designing relatively simple anddeterministic interfaces for providing haptic assistance.In this paper, we consider the specific problem of assisting a musician in selecting pitches from a continuous range.We build on a prior study by O'Modhrain of the accuracyof pitches selected by musicians on a Theremin-like hapticinterface. To improve the assistance, we augment the interface with programmed detents so that the musician can feelthe locations of equal tempered pitches. Nevertheless, themusician can still perform arbitrary pitch inflections such asglissandi, falls, and scoops. We investigate various formsof haptic detents, including fixed detent levels and forcesensitive detent levels. Preliminary results from a subjecttest confirm improved accuracy in pitch selection broughtabout by detents.}
}

@inproceedings{Berdahl2009a,
  author = {Berdahl, Edgar and Niemeyer, G\''{u}nter and Smith, Julius O.},
  title = {Using Haptic Devices to Interface Directly with Digital Waveguide-Based Musical Instruments},
  pages = {183--186},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177479},
  url = {http://www.nime.org/proceedings/2009/nime2009_183.pdf},
  keywords = {haptic musical instrument, digital waveguide, control junction, explicit, implicit, teleoperation },
  abstract = {A haptic musical instrument is an electronic musical instrument that provides the musician not only with audio feedback but also with force feedback. By programming feedback controllers to emulate the laws of physics, many haptic musical instruments have been previously designed thatmimic real acoustic musical instruments. The controllerprograms have been implemented using finite difference and(approximate) hybrid digital waveguide models. We presenta novel method for constructing haptic musical instrumentsin which a haptic device is directly interfaced with a conventional digital waveguide model by way of a junction element, improving the quality of the musician's interactionwith the virtual instrument. We introduce both the explicitdigital waveguide control junction and the implicit digitalwaveguide control junction.}
}

@inproceedings{Havryliv2009,
  author = {Havryliv, Mark and Naghdy, Fazel and Schiemer, Greg and Hurd, Timothy},
  title = {Haptic Carillon -- Analysis \& Design of the Carillon Mechanism},
  pages = {187--192},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177569},
  url = {http://www.nime.org/proceedings/2009/nime2009_187.pdf},
  keywords = {Haptics, force-feedback, mechanical analysis. },
  abstract = {The carillon is one of the few instruments that elicit sophisticated haptic interaction from amateur and professional players alike. Like the piano keyboard, the velocity of a player's impact on each carillon key, or baton, affects the quality of the resultant tone; unlike the piano, each carillon baton returns a different forcefeedback. Force-feedback varies widely from one baton to the next across the entire range of the instrument and with further idiosyncratic variation from one instrument to another. This makes the carillon an ideal candidate for haptic simulation. The application of synthesized forcefeedback based on an analysis of forces operating in a typical carillon mechanism offers a blueprint for the design of an electronic practice clavier and with it the solution to a problem that has vexed carillonists for centuries, namely the inability to rehearse repertoire in private. This paper will focus on design and implementation of a haptic carillon clavier derived from an analysis of the Australian National Carillon in Canberra. }
}

@inproceedings{Leeuw2009,
  author = {Leeuw, Hans},
  title = {The Electrumpet , a Hybrid Electro-Acoustic Instrument},
  pages = {193--198},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177613},
  url = {http://www.nime.org/proceedings/2009/nime2009_193.pdf},
  keywords = {Trumpet, multiple Arduinos, Bluetooth, LCD, low latency, OSC, MAX/MSP. },
  abstract = {The Electrumpet is an enhancement of a normal trumpet with a variety of electronic sensors and buttons. It is a new hybrid instrument that facilitates simultaneous acoustic and electronic playing. The normal playing skills of a trumpet player apply to the new instrument. The placing of the buttons and sensors is not a hindrance to acoustic use of the instrument and they are conveniently located. The device can be easily attached to and detached from a normal Bb-trumpet. The device has a wireless connection with the computer through Bluetooth-serial (Arduino). Audio and data processing in the computer is effected by three separate instances of MAX/MSP connected through OSC (controller data) and Soundflower (sound data). The current prototype consists of 7 analogue sensors (4 valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" controller) and 9 digital switches. An LCD screen that is controlled by a separate Arduino (mini) is attached to the trumpet and displays the current controller settings that are sent through a serial connection. }
}

@inproceedings{Gallin2009,
  author = {Gallin, Emmanuelle and Sirguy, Marc},
  title = {Sensor Technology and the Remaking of Instruments from the Past},
  pages = {199--202},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177521},
  url = {http://www.nime.org/proceedings/2009/nime2009_199.pdf},
  keywords = {Controller, Sensor, MIDI, USB, Computer Music, ribbon controllers, ribbon cello. },
  abstract = {Starting from a parallelism between the effervescence of the 1920s in the exploration of new ways of controlling music and the actual revolution in the design of new control possibilities, this paper aims to explore the possibilities of rethinking instruments from the past towards instruments of the future. Through three examples (the experience of the Persephone, the design of the Persephone2 and the 4 strings ribbon cello project), I will explore the contemporary notion of “instruments of the future” vs. controls that people expect from such instruments nowadays.}
}

@inproceedings{Nicolls2009,
  author = {Nicolls, Sarah},
  title = {Twenty-First Century Piano},
  pages = {203--206},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177641},
  url = {http://www.nime.org/proceedings/2009/nime2009_203.pdf},
  keywords = {sensor, gestural, technology, performance, piano, motors, interactive },
  abstract = {“The reinvigoration of the role of the human body” - as John Richards recently described trends in using homemade electronics to move away from laptop performance [1] - is mirrored in an ambition of instrumentalists to interact more closely with the electronic sounds they are helping to create. For these players, there has often been a one-way street of the ‘instrument feeds MAX patch’ paradigm and arguments are made here for more complete performance feedback systems. Instrumentalists come to the question of interactivity with a whole array of gestures, sounds and associations already in place, so must choose carefully the means by which the instrumental performance is augmented. Frances-Marie Uitti [2] is a pioneer in the field, creating techniques to amplify the cellist’s innate performative gestures and in parallel developing the instrument. This paper intends to give an overview of the author’s work in developing interactivity in piano performance, mechanical augmentation of the piano and possible structural developments of the instrument to bring it into the twenty-first century.}
}

@inproceedings{Johnston2009,
  author = {Johnston, Andrew and Candy, Linda and Edmonds, Ernest},
  title = {Designing for Conversational Interaction},
  pages = {207--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177585},
  url = {http://www.nime.org/proceedings/2009/nime2009_207.pdf},
  keywords = {Music, instruments, interaction. },
  abstract = {In this paper we describe an interaction framework which classifies musicians' interactions with virtual musical instruments into three modes: instrumental, ornamental and conversational. We argue that conversational interactions are the most difficult to design for, but also the most interesting. To illustrate our approach to designing for conversational interactions we describe the performance work Partial Reflections 3 for two clarinets and interactive software. This software uses simulated physical models to create a virtual sound sculpture which both responds to and produces sounds and visuals.}
}

@inproceedings{Gurevich2009,
  author = {Gurevich, Michael and Stapleton, Paul and Bennett, Peter},
  title = {Designing for Style in New Musical Interactions},
  pages = {213--217},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177563},
  url = {http://www.nime.org/proceedings/2009/nime2009_213.pdf},
  keywords = {expression, style, structure, skill, virtuosity },
  abstract = {In this paper we discuss the concept of style, focusing in particular on methods of designing new instruments that facilitate the cultivation and recognition of style. We distinguishbetween style and structure of an interaction and discuss thesignificance of this formulation within the context of NIME.Two workshops that were conducted to explore style in interaction design are described, from which we identify elements of style that can inform and influence the design process. From these, we suggest steps toward designing forstyle in new musical interactions.}
}

@inproceedings{Cook2009,
  author = {Cook, Perry R.},
  title = {Re-Designing Principles for Computer Music Controllers : a Case Study of SqueezeVox Maggie},
  pages = {218--221},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177493},
  url = {http://www.nime.org/proceedings/2009/nime2009_218.pdf},
  keywords = {HCI, Composed Instruments, Voice Synthesis, Wireless, Batteries, Laptop Orchestras, SenSAs.},
  abstract = {This paper revisits/extends “Principles for Designing Computer Music Controllers” (NIME 2001), subsequently updated in a NIME 2007 keynote address. A redesign of SqueezeVox Maggie (a reoccurring NIME character) is used as an example of which principles have held fast over the years, and which have changed due to advances in technology. A few new principles are also added to the list.}
}

@inproceedings{Kapuscinski2009,
  author = {Kapuscinski, Jaroslaw and Sanchez, Javier},
  title = {Interfacing Graphic and Musical Elements in Counterlines},
  pages = {222--225},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177597},
  url = {http://www.nime.org/proceedings/2009/nime2009_222.pdf},
  keywords = {intermedia, Disklavier, piano, Wacom Cintiq, mapping, visual music },
  abstract = {This paper reports on initial stages of research leading to the development of an intermedia performance Counterlines --- a duet for Disklavier and Wacom Cintiq, in which both performers generate audiovisual gestures that relate to each other contrapuntally. The pianist generates graphic elements while playing music and the graphic performer generates piano notes by drawing lines. The paper focuses on interfacing sounds and images performed by the pianist. It provides rationale for the choice of materials of great simplicity and describes our approach to mapping. }
}

@inproceedings{Polfreman2009,
  author = {Polfreman, Richard},
  title = {FrameWorks {3D} : Composition in the Third Dimension},
  pages = {226--229},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177661},
  url = {http://www.nime.org/proceedings/2009/nime2009_226.pdf},
  keywords = {Digital Audio Workstation, graphical user-interfaces, 3D graphics, Max/MSP, Java. },
  abstract = {Music composition on computer is a challenging task, involving a range of data types to be managed within a single software tool. A composition typically comprises a complex arrangement of material, with many internal relationships between data in different locations repetition, inversion, retrograde, reversal and more sophisticated transformations. The creation of such complex artefacts is labour intensive, and current systems typically place a significant cognitive burden on the composer in terms of maintaining a work as a coherent whole. FrameWorks 3D is an attempt to improve support for composition tasks within a Digital Audio Workstation (DAW) style environment via a novel three-dimensional (3D) user-interface. In addition to the standard paradigm of tracks, regions and tape recording analogy, FrameWorks displays hierarchical and transformational information in a single, fully navigable workspace. The implementation combines Java with Max/MSP to create a cross-platform, user-extensible package and will be used to assess the viability of such a tool and to develop the ideas further. }
}

@inproceedings{Freed2009,
  author = {Freed, Adrian},
  title = {Novel and Forgotten Current-steering Techniques for Resistive Multitouch, Duotouch, and Polytouch Position Sensing with Pressure},
  pages = {230--235},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177515},
  url = {http://www.nime.org/proceedings/2009/nime2009_230.pdf},
  keywords = {Piezoresistive Touch Sensor Pressure Sensing Current Steering Multitouch. },
  abstract = {A compendium of foundational circuits for interfacing resistive pressure and position sensors is presented with example applications for music controllers and tangible interfaces. }
}

@inproceedings{Jones2009a,
  author = {Jones, Randy and Driessen, Peter and Schloss, Andrew and Tzanetakis, George},
  title = {A Force-Sensitive Surface for Intimate Control},
  pages = {236--241},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177589},
  url = {http://www.nime.org/proceedings/2009/nime2009_236.pdf},
  keywords = {Multitouch, sensors, tactile, capacitive, percussion controllers. },
  abstract = {This paper presents a new force-sensitive surface designedfor playing music. A prototype system has been implemented using a passive capacitive sensor, a commodity multichannel audio interface, and decoding software running ona laptop computer. This setup has been a successful, lowcost route to a number of experiments in intimate musicalcontrol.}
}

@inproceedings{Kellum2009,
  author = {Kellum, Greg and Crevoisier, Alain},
  title = {A Flexible Mapping Editor for Multi-touch Musical Instruments},
  pages = {242--245},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177601},
  url = {http://www.nime.org/proceedings/2009/nime2009_242.pdf},
  keywords = {NIME, multi-touch, multi-modal interface, sonic interaction design. },
  abstract = {This paper introduces a flexible mapping editor, which transforms multi-touch devices into musical instruments. The editor enables users to create interfaces by dragging and dropping components onto the interface and attaching actions to them, which will be executed when certain userdefined conditions obtain. The editor receives touch information via the non-proprietary communication protocol, TUIO [9], and can, therefore, be used together with a variety of different multi-touch input devices. }
}

@inproceedings{Kiefer2009,
  author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
  title = {Phalanger : Controlling Music Software With Hand Movement Using A Computer Vision and Machine Learning Approach},
  pages = {246--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177603},
  url = {http://www.nime.org/proceedings/2009/nime2009_246.pdf},
  keywords = {nime09},
  abstract = {Phalanger is a system which facilitates the control of music software with hand and finger motion, with the aim of creating a fluid style of interaction that promotes musicality. The system is purely video based, requires no wearables or accessories and uses affordable and accessible technology. It employs a neural network for background segmentation, a combination of imaging techniques for frame analysis, and a support vector machine (SVM) for recognition of hand positions. System evaluation showed the SVM to reliably differentiate between eight different classes. An initial formative user evaluation with ten musicians was carried out to help build a picture of how users responded to the system; this highlighted areas that need improvement and lent some insight into useful features for the next version.}
}

@inproceedings{Nakra2009,
  author = {Nakra, Teresa M. and Ivanov, Yuri and Smaragdis, Paris and Ault, Chris},
  title = {The UBS Virtual Maestro : an Interactive Conducting System},
  pages = {250--255},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177637},
  url = {http://www.nime.org/proceedings/2009/nime2009_250.pdf},
  keywords = {conducting, gesture, interactive installations, Wii Remote },
  abstract = {The UBS Virtual Maestro is an interactive conducting system designed by Immersion Music to simulate the experience of orchestral conducting for the general public attending a classical music concert. The system utilizes the Wii Remote, which users hold and move like a conducting baton to affect the tempo and dynamics of an orchestral video/audio recording. The accelerometer data from the Wii Remote is used to control playback speed and volume in real-time. The system is housed in a UBSbranded kiosk that has toured classical performing arts venues throughout the United States and Europe in 2007 and 2008. In this paper we share our experiences in designing this standalone system for thousands of users, and lessons that we learned from the project. }
}

@inproceedings{Jessop2009,
  author = {Jessop, Elena},
  title = {The Vocal Augmentation and Manipulation Prosthesis (VAMP): A Conducting-Based Gestural Controller for Vocal Performance},
  pages = {256--259},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177583},
  url = {http://www.nime.org/proceedings/2009/nime2009_256.pdf},
  keywords = {musical expressivity, vocal performance, gestural control, conducting. },
  abstract = {This paper describes The Vocal Augmentation and Manipulation Prosthesis (VAMP) a gesture-based wearable controller for live-time vocal performance. This controller allows a singer to capture and manipulate single notes that he or she sings, using a gestural vocabulary developed from that of choral conducting. By drawing from a familiar gestural vocabulary, this controller and the associated mappings can be more intuitive and expressive for both performer and audience. }
}

@inproceedings{Henriques2009,
  author = {Henriques, Tom\'{a}s},
  title = {Double Slide Controller},
  pages = {260--261},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177571},
  url = {http://www.nime.org/proceedings/2009/nime2009_260.pdf},
  keywords = {Musical Instrument, Sensor technologies, Computer Music, Hardware and Software Design.},
  abstract = {The Double Slide Controller is a new electronic music instrument that departs from the slide trombone as a model for its design. Going much beyond a mere simulation of its acoustic counterpart it introduces truly innovative features: two powerful and versatile sets of gesture driven interfaces actuated by the hands of the performer, as well as featuring two independent slides, one for each hand/arm of the musician. The combination of these features make this instrument a great tool to explore new venues in musical expression, given the many degrees of technical and musical complexity that can be achieved during its performance.}
}

@inproceedings{Berdahl2009,
  author = {Berdahl, Edgar and Niemeyer, G\''{u}nter and Smith, Julius O.},
  title = {HSP : A Simple and Effective Open-Source Platform for Implementing Haptic Musical Instruments},
  pages = {262--263},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177477},
  url = {http://www.nime.org/proceedings/2009/nime2009_262.pdf},
  keywords = { haptic musical instrument, HSP, haptics, computer music, physical modeling, Pure Data (Pd), NovInt},
  abstract = {When we asked a colleague of ours why people do not make more haptic musical instruments, he replied that he thought they were “too hard to program and too expensive.” We decided to solve these perceived problems by introducing HSP, a simple platform for implementing haptic musical instruments. HSP obviates the need for employing low-level embedded control software because the haptic device is controlled directly from within the Pure Data (Pd) software running on a general purpose computer. Positions can be read from the haptic device, and forces can be written to the device using messages in Pd. Various additional objects have been created to facilitate rapid prototyping of useful haptic musical instruments in Pd. HSP operates under Linux, OS X, and Windows and supports the mass-produced Falcon haptic device from NovInt, which can currently be obtained for as little as US\$150. All of the above make HSP an especially excellent choice for pedagogical environments where multiple workstations are required and example programs should be complete yet simple.}
}

@inproceedings{Barri2009,
  author = {Barri, Tarik},
  title = {Versum : Audiovisual Composing in 3d},
  pages = {264--265},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177473},
  url = {http://www.nime.org/proceedings/2009/nime2009_264.pdf},
  keywords = {audiovisual, sequencing, collaboration. },
  abstract = {This paper introduces the new audiovisual sequencing system "Versum" that allows users to compose in three dimensions. In the present paper the conceptual soil from which this system has sprung is discussed first. Secondly, the basic concepts with which Versum operates are explained, providing a general idea of what is meant by sequencing in three dimensions and explaining what compositions made in Versum can look and sound like. Thirdly, the practical ways in which a composer can use Versum to make his own audiovisual compositions are presented by means of a more detailed description of the different graphical user interface elements. Fourthly, a short description is given of the modular structure of the software underlying Versum. Finally, several foresights regarding the directions in which Versum will continue to develop in the near future are presented. }
}

@inproceedings{Bullock2009,
  author = {Bullock, Jamie and Coccioli, Lamberto},
  title = {Towards a Humane Graphical User Interface for Live Electronic Music},
  pages = {266--267},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177489},
  url = {http://www.nime.org/proceedings/2009/nime2009_266.pdf},
  keywords = {Integra, User Interface, Usability, Design, Live Electronics, Music Technology },
  abstract = {In this paper we describe findings related to user interfacerequirements for live electronic music arising from researchconducted as part of the first three-year phase of the EUfunded Integra project. A number of graphical user interface(GUI) prototypes developed during the Integra project initial phase are described and conclusions drawn about theirdesign and implementation.}
}

@inproceedings{Laurenzo2009,
  author = {Laurenzo, Tomas and Rodr\'{\i}guez, Ernesto and Castro, Juan Fabrizio},
  title = {YARMI : an Augmented Reality Musical Instrument},
  pages = {268--269},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177611},
  url = {http://www.nime.org/proceedings/2009/nime2009_268.pdf},
  keywords = {Interactive music instruments, visual interfaces, visual feedback, tangible interfaces, augmented reality, collaborative music, networked musical instruments, real-time musical systems, musical sequencer. },
  abstract = {In this paper, we present YARMI, a collaborative, networked, tangible, musical instrument. YARMI operates on augmented-reality space (shared between the performers and the public), presenting a multiple tabletop interface where several musical sequencers and real–time effects machines can be operated.}
}

@inproceedings{Essl2009,
  author = {Essl, Georg},
  title = {SpeedDial : Rapid and On-The-Fly Mapping of Mobile Phone Instruments},
  pages = {270--273},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177503},
  url = {http://www.nime.org/proceedings/2009/nime2009_270.pdf},
  keywords = {mobile phone instruments,nime,nime09,on-the-fly},
  abstract = {When creating new musical instruments on a mobile phone platform one has to map sensory input to synthesis algorithms. We propose that the very task of this mapping belongs in the creative process and to this end we develop a way to rapidly and on-the-fly edit the mapping of mobile phone instruments. The result is that the meaning of the instruments can continuously be changed during a live performance.}
}

@inproceedings{Fels2009,
  author = {Fels, Sidney S. and Pritchard, Bob and Lenters, Allison},
  title = {ForTouch : A Wearable Digital Ventriloquized Actor},
  pages = {274--275},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177509},
  url = {http://www.nime.org/proceedings/2009/nime2009_274.pdf},
  keywords = {nime09},
  abstract = {We have constructed an easy-to-use portable, wearable gesture-to-speech system based on the Glove-TalkII [1] and GRASSP [2] Digital Ventriloquized Actors (DIVAs). Our new portable system, called a ForTouch, is a specific model of a DIVA and refines the use of a formant speech synthesizer. Using ForTouch, a user can speak using hand gestures mapped to synthetic sound using a mapping function that preserves gesture trajectories. By making ForTouch portable and self-contained, speakers can communicate with others in the community and perform in new music/theatre stage productions. Figure 1 shows one performer using the ForTouch. ForTouch performers also allow us to study the relation between gestures and speech/song production.}
}

@inproceedings{Mclean2009,
  author = {Mclean, Alex and Wiggins, Geraint},
  title = {Words , Movement and Timbre},
  pages = {276--279},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177629},
  url = {http://www.nime.org/proceedings/2009/nime2009_276.pdf},
  keywords = {nime09,timbre,vocable synthesis},
  abstract = {Phonetic symbols describe movements of the vocal tract,tongue and lips, and are combined into complex movementsforming the words of language. In music, vocables are wordsthat describe musical sounds, by relating vocal movementsto articulations of a musical instrument. We posit that vocable words allow the composers and listeners to engageclosely with dimensions of timbre, and that vocables couldsee greater use in electronic music interfaces. A preliminarysystem for controlling percussive physical modelling synthesis with textual words is introduced, with particular application in expressive specification of timbre during computer music performances.}
}

@inproceedings{Fiebrink2009,
  author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R.},
  title = {A Meta-Instrument for Interactive, On-the-Fly Machine Learning},
  pages = {280--285},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177513},
  url = {http://www.nime.org/proceedings/2009/nime2009_280.pdf},
  keywords = {Machine learning, mapping, tools. },
  abstract = {Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance. }
}

@inproceedings{Schacher2009,
  author = {Schacher, Jan C.},
  title = {Action and Perception in Interactive Sound Installations : An Ecological Approach},
  pages = {286--289},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177667},
  url = {http://www.nime.org/proceedings/2009/nime2009_286.pdf},
  keywords = {Interaction, adaptive mapping, machine learning, audience engagement },
  abstract = {In this paper mappings and adaptation in the context of interactive sound installations are discussed. Starting from an ecological perspective on non-expert audience interaction a brief overview and discussion of mapping strategies with a special focus on adaptive systems using machine learning algorithms is given. An audio-visual interactive installation is analyzed and its implementation used to illustrate the issues of audience engagement and to discuss the efficiency of adaptive mappings. }
}

@inproceedings{Kirk2009,
  author = {Kirk, Jonathon and Weisert, Lee},
  title = {The Argus Project : Underwater Soundscape Composition with Laser- Controlled Modulation},
  pages = {290--292},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177605},
  url = {http://www.nime.org/proceedings/2009/nime2009_290.pdf},
  keywords = {nime09},
  abstract = {In this paper we describe and analyze The Argus Project, a sound installation involving the real-time processing and spatialized projection of sound sources from beneath a pond’s surface. The primary aim of The Argus Project is to project the natural sound sources from below the pond’s surface while tracking the changes in the environmental factors above the surface so as to map this data onto the real-time audio processing. The project takes as its conceptual model that of a feedback network, or, a process in which the factors that produce a result are themselves modified and reinforced by that result. Examples are given of the compositional process, the execution, and processing techniques.}
}

@inproceedings{StClair2009,
  author = {St. Clair, Michael and Leitman, Sasha},
  title = {PlaySoundGround : An Interactive Musical Playground},
  pages = {293--296},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177685},
  url = {http://www.nime.org/proceedings/2009/nime2009_293.pdf},
  keywords = {Real-time, Music, Playground, Interactive, Installation, Radical Collaboration, Play.},
  abstract = {We describe a novel transformation of a playground - merry-go-round, teeter-totter (also referred to as a see-saw), swings, and climbing structure – from its traditional purpose to a collaborative and interactive musical performance system by equipping key structures with sensors that communicate with a computer. A set of Max/ MSP patches translate the physical gestures of playground play into a variety of performer-selected musical mappings. In addition to the electro-acoustic interactivity, the climbing structure incorporates acoustic musical instruments.}
}

@inproceedings{Jones2009,
  author = {Jones, Daniel and Hodgson, Tim and Grant, Jane and Matthias, John and Outram, Nicholas and Ryan, Nick},
  title = {The Fragmented Orchestra},
  pages = {297--302},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177587},
  url = {http://www.nime.org/proceedings/2009/nime2009_297.pdf},
  keywords = {distributed,emergent,environmental,installation,neural network,nime09,sound,streaming audio},
  abstract = {The Fragmented Orchestra is a distributed musical instrument which combines live audio streams from geographically disparate sites, and granulates each according to thespike timings of an artificial spiking neural network. Thispaper introduces the work, outlining its historical context,technical architecture, neuronal model and network infrastructure, making specific reference to modes of interactionwith the public.}
}

@inproceedings{Wang2009,
  author = {Wang, Ge},
  title = {Designing Smule's Ocarina : The iPhone's Magic Flute},
  pages = {303--307},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177697},
  url = {http://www.nime.org/proceedings/2009/nime2009_303.pdf},
  keywords = {chuck,design,in,in real-time,interface,iphone,mobile music,multitouch,nime09,ocarina,pulsing waves,social,sonically and onscreen and,sound synthesis takes place,the breath is visualized},
  abstract = {The Smule Ocarina is a wind instrument designed for the iPhone, fully leveraging its wide array of technologies: microphone input (for breath input), multitouch (for fingering), accelerometer, real-time sound synthesis, highperformance graphics, GPS/location, and persistent data connection. In this mobile musical artifact, the interactions of the ancient flute-like instrument are both preserved and transformed via breath-control and multitouch finger-holes, while the onboard global positioning and persistent data connection provide the opportunity to create a new social experience, allowing the users of Ocarina to listen to one another. In this way, Ocarina is also a type of social instrument that enables a different, perhaps even magical, sense of global connectivity. }
}

@inproceedings{Gillian2009a,
  author = {Gillian, Nicholas and O'Modhrain, Sile and Essl, Georg},
  title = {Scratch-Off : A Gesture Based Mobile Music Game with Tactile Feedback},
  pages = {308--311},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177553},
  url = {http://www.nime.org/proceedings/2009/nime2009_308.pdf},
  keywords = {Mobile devices, gesture, audio games. },
  abstract = {This paper presents {Scratch-Off}, a new musical multiplayer DJ game that has been designed for a mobile phone. We describe how the game is used as a test platform for experimenting with various types of multimodal feedback. The game uses movement gestures made by the players to scratch a record and control crossfades between tracks, with the objective of the game to make the correct scratch at the correct time in relation to the music. Gestures are detected using the devices built-in tri-axis accelerometer and multi-touch screen display. The players receive visual, audio and various types of vibrotactile feedback to help them make the correct scratch on the beat of the music track. We also discuss the results of a pilot study using this interface. }
}

@inproceedings{Weinberg2009,
  author = {Weinberg, Gil and Beck, Andrew and Godfrey, Mark},
  title = {ZooZBeat : a Gesture-based Mobile Music Studio},
  pages = {312--315},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177703},
  url = {http://www.nime.org/proceedings/2009/nime2009_312.pdf},
  keywords = {mobile music, gestural control },
  abstract = {ZooZBeat is a gesture-based mobile music studio. It is designed to provide users with expressive and creative access to music making on the go. ZooZBeat users shake the phone or tap the screen to enter notes. The result is quantized, mapped onto a musical scale, and looped. Users can then use tilt and shake movements to manipulate and share their creation in a group. Emphasis is placed on finding intuitive metaphors for mobile music creation and maintaining a balance between control and ease-of-use that allows non-musicians to begin creating music with the application immediately. }
}

@inproceedings{Bianchi2009,
  author = {Bianchi, Andrea and Yeo, Woon Seung},
  title = {The Drummer : a Collaborative Musical Interface with Mobility},
  pages = {316--319},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177483},
  url = {http://www.nime.org/proceedings/2009/nime2009_316.pdf},
  keywords = {collaborative interface, multiplayer, musical expression, musical control, game control, Nintendo DS.},
  abstract = {It has been shown that collaborative musical interfaces encourage novice users to explore the sound space and promote their participation as music performers. Nevertheless, such interfaces are generally physically situated and can limit the possibility of movements on the stage, a critical factor in live music performance. In this paper we introduce the Drummer, a networked digital musical interface that allows multiple performers to design and play drum kits simultaneously while, at the same time, keeping their ability to freely move on the stage. The system consists of multiple Nintendo DS clients with an intuitive, user-configurable interface and a server computer which plays drum sounds. The Drummer Machine, a small piece of hardware to augment the performance of the Drummer, is also introduced. }
}

@inproceedings{Wechsler2009,
  author = {Wechsler, Robert},
  title = {The Oklo Phenomenon},
  pages = {320--320},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177701},
  url = {http://www.nime.org/proceedings/2009/nime2009_320.pdf},
  keywords = {nime09}
}

@inproceedings{Lieberman2009,
  author = {Lieberman, David},
  title = {Anigraphical Etude 9},
  pages = {321--321},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177619},
  url = {http://www.nime.org/proceedings/2009/nime2009_321.pdf},
  keywords = {nime09}
}

@inproceedings{Hong2009,
  author = {Hong, Min Eui},
  title = {Cosmic Strings II},
  pages = {322--322},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177577},
  url = {http://www.nime.org/proceedings/2009/nime2009_322.pdf},
  keywords = {nime09}
}

@inproceedings{Rogers2009,
  author = {Rogers, Troy and Kemper, Steven and Barton, Scott},
  title = {Study no. 1 for {PAM} and MADI},
  pages = {323--323},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177665},
  url = {http://www.nime.org/proceedings/2009/nime2009_323.pdf},
  keywords = {nime09}
}

@inproceedings{Paine2009,
  author = {Paine, Garth and Atherton, Michael},
  title = {Fue Sho -- Electrofusion},
  pages = {324--324},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177651},
  url = {http://www.nime.org/proceedings/2009/nime2009_324.pdf},
  keywords = {nime09}
}

@inproceedings{Barri2009a,
  author = {Barri, Tarik},
  title = {Versum -- Fluor},
  pages = {325--325},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177475},
  url = {http://www.nime.org/proceedings/2009/nime2009_325.pdf},
  keywords = {nime09}
}

@inproceedings{Miyama2009,
  author = {Miyama, Chikashi},
  title = {Angry Sparrow},
  pages = {326--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177633},
  url = {http://www.nime.org/proceedings/2009/nime2009_326.pdf},
  keywords = {nime09}
}

@inproceedings{Lyon2009,
  author = {Lyon, Eric and Knapp, Benjamin and Ouzounian, Gascia},
  title = {Biomuse Trio},
  pages = {327--327},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177621},
  url = {http://www.nime.org/proceedings/2009/nime2009_327.pdf},
  keywords = {nime09}
}

@inproceedings{Goto2009,
  author = {Goto, Suguru},
  title = {BodyJack},
  pages = {328--328},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177557},
  url = {http://www.nime.org/proceedings/2009/nime2009_328.pdf},
  keywords = {nime09}
}

@inproceedings{Baalman2009,
  author = {Baalman, Marije A.},
  title = {Code LiveCode Live, or livecode Embodied},
  pages = {329--329},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177469},
  url = {http://www.nime.org/proceedings/2009/nime2009_329.pdf},
  keywords = {nime09}
}

@inproceedings{Torre2009,
  author = {Torre, Giuseppe and Sazdov, Robert and Konczewska, Dorota},
  title = {MOLITVA --- Composition for Voice, Live Electronics, Pointing-At Glove Device and {3-D} Setup of Speakers},
  pages = {330--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177695},
  url = {http://www.nime.org/proceedings/2009/nime2009_330.pdf},
  keywords = {nime09}
}

@inproceedings{Neill2009,
  author = {Neill, Ben and Singer, Eric},
  title = {Ben Neill and LEMUR},
  pages = {331--331},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177639},
  url = {http://www.nime.org/proceedings/2009/nime2009_331.pdf},
  keywords = {nime09}
}

@inproceedings{Hindman2009,
  author = {Hindman, David and Drummond, Evan},
  title = {Performance: Modal Kombat Plays {PON}G},
  pages = {332--332},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177573},
  url = {http://www.nime.org/proceedings/2009/nime2009_332.pdf},
  keywords = {nime09}
}

@inproceedings{Leider2009,
  author = {Leider, Colby},
  title = {Afflux/Reflux},
  pages = {333--333},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177615},
  url = {http://www.nime.org/proceedings/2009/nime2009_333.pdf},
  keywords = {nime09}
}

@inproceedings{Wang2009a,
  author = {Wang, Ge and Fiebrink, Rebecca},
  title = {PLOrk Beat Science 2.0},
  pages = {334--334},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177699},
  url = {http://www.nime.org/proceedings/2009/nime2009_334.pdf},
  keywords = {nime09}
}

@inproceedings{Wessel2009,
  author = {Wessel, David},
  title = {Hands On --- A New Work from SLABS Controller and Generative Algorithms},
  pages = {335--335},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177707},
  url = {http://www.nime.org/proceedings/2009/nime2009_335.pdf},
  keywords = {nime09}
}

@inproceedings{Dubois2009,
  author = {Dubois, R. Luke and Flanigan, Lesley},
  title = {Bioluminescence},
  pages = {336--336},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177501},
  url = {http://www.nime.org/proceedings/2009/nime2009_336.pdf},
  keywords = {nime09}
}

@inproceedings{Bukvic2009,
  author = {Bukvic, Ivika and Standley, Eric},
  title = {Elemental \& Cyrene Reefs},
  pages = {337--337},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177487},
  url = {http://www.nime.org/proceedings/2009/nime2009_337.pdf},
  keywords = {nime09}
}

@inproceedings{GreshamLancaster2009,
  author = {Gresham-Lancaster, Scot and Bull, Steve},
  title = {Cellphonia: 4'33},
  pages = {338--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177561},
  url = {http://www.nime.org/proceedings/2009/nime2009_338.pdf},
  keywords = {nime09}
}

@inproceedings{Overholt2009,
  author = {Overholt, Dan and Lahey, Byron and Skriver Hansen, Anne-Marie and Burleson, Winslow and Norrgaard Jensen, Camilla},
  title = {Pendaphonics},
  pages = {339--339},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177649},
  url = {http://www.nime.org/proceedings/2009/nime2009_339.pdf},
  keywords = {nime09}
}

@inproceedings{Smallwood2009,
  author = {Smallwood, Scott},
  title = {Sound Lanterns},
  pages = {340--340},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177677},
  url = {http://www.nime.org/proceedings/2009/nime2009_340.pdf},
  keywords = {nime09}
}

@inproceedings{Stearns2009,
  author = {Stearns, Phillip},
  title = {AANN: Artificial Analog Neural Network},
  pages = {341--341},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2009},
  address = {Pittsburgh, PA, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177687},
  url = {http://www.nime.org/proceedings/2009/nime2009_341.pdf},
  keywords = {nime09}
}

