@inproceedings{Gaye2006,
  author = {Gaye, Lalya and Holmquist, Lars E. and Behrendt, Frauke and Tanaka, Atau},
  title = {Mobile Music Technology: Report on an Emerging Community},
  pages = {22--25},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176909},
  url = {http://www.nime.org/proceedings/2006/nime2006_022.pdf}
}

@inproceedings{Tanaka2006,
  author = {Tanaka, Atau and Gemeinboeck, Petra},
  title = {A Framework for Spatial Interaction in Locative Media},
  pages = {26--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177013},
  url = {http://www.nime.org/proceedings/2006/nime2006_026.pdf},
  keywords = {Mobile music, urban fiction, locative media. },
  abstract = {This paper presents the concepts and techniques used in afamily of location based multimedia works. The paper hasthree main sections: 1.) to describe the architecture of anaudio-visual hardware/software framework we havedeveloped for the realization of a series of locative mediaartworks, 2.) to discuss the theoretical and conceptualunderpinnings motivating the design of the technicalframework, and 3.) to elicit from this, fundamental issuesand questions that can be generalized and applicable to thegrowing practice of locative media.}
}

@inproceedings{Rohs2006,
  author = {Rohs, Michael and Essl, Georg and Roth, Martin},
  title = {CaMus: Live Music Performance using Camera Phones and Visual Grid Tracking},
  pages = {31--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176997},
  url = {http://www.nime.org/proceedings/2006/nime2006_031.pdf}
}

@inproceedings{Schiemer2006,
  author = {Schiemer, Greg and Havryliv, Mark},
  title = {Pocket Gamelan: Tuneable Trajectories for Flying Sources in Mandala 3 and Mandala 4},
  pages = {37--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176999},
  url = {http://www.nime.org/proceedings/2006/nime2006_037.pdf},
  keywords = {Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media Performance; Just Intonation. },
  abstract = {This paper describes two new live performance scenarios for performing music using bluetooth-enabled mobile phones. Interaction between mobile phones via wireless link is a key feature of the performance interface for each scenario. Both scenarios are discussed in the context of two publicly performed works for an ensemble of players in which mobile phone handsets are used both as sound sources and as hand-held controllers. In both works mobile phones are mounted in a specially devised pouch attached to a cord and physically swung to produce audio chorusing. During performance some players swing phones while others operate phones as hand-held controllers. Wireless connectivity enables interaction between flying and hand-held phones. Each work features different bluetooth implementations. In one a dedicated mobile phone acts as a server that interconnects multiple clients, while in the other point to point communication takes place between clients on an ad hoc basis. The paper summarises bluetooth tools designed for live performance realisation and concludes with a comparative evaluation of both scenarios for future implementation of performance by large ensembles of nonexpert players performing microtonal music using ubiquitous technology. }
}

@inproceedings{Birchfield2006,
  author = {Birchfield, David and Phillips, Kelly and Kidan\'{e}, Assegid and Lorig, David},
  title = {Interactive Public Sound Art: a case study},
  pages = {43--48},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176873},
  url = {http://www.nime.org/proceedings/2006/nime2006_043.pdf},
  keywords = {Music, Sound, Interactivity, Arts, Public Art, Network Systems, Sculpture, Installation Art, Embedded Electronics. },
  abstract = {Physically situated public art poses significant challenges for the design and realization of interactive, electronic sound works. Consideration of diverse audiences, environmental sensitivity, exhibition conditions, and logistics must guide the artwork. We describe our work in this area, using a recently installed public piece, Transition Soundings, as a case study that reveals a specialized interface and open-ended approach to interactive music making. This case study serves as a vehicle for examination of the real world challenges posed by public art and its outcomes. }
}

@inproceedings{Wang2006,
  author = {Wang, Ge and Misra, Ananya and Cook, Perry R.},
  title = {Building Collaborative Graphical interFaces in the Audicle},
  pages = {49--52},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177017},
  url = {http://www.nime.org/proceedings/2006/nime2006_049.pdf},
  keywords = {Graphical interfaces, collaborative performance, networking, computer music ensemble, emergence, visualization, education. }
}

@inproceedings{Rebelo2006,
  author = {Rebelo, Pedro and Renaud, Alain B.},
  title = {The Frequencyliator -- Distributing Structures for Networked Laptop Improvisation},
  pages = {53--56},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176993},
  url = {http://www.nime.org/proceedings/2006/nime2006_053.pdf},
  keywords = {Networked audio technologies, laptop ensemble, centralized audio server, improvisation },
  abstract = {The culture of laptop improvisation has grown tremendously in recent years. The development of personalized software instruments presents interesting issues in the context of improvised group performances. This paper examines an approach that is aimed at increasing the modes of interactivity between laptop performers and at the same time suggests ways in which audiences can better discern and identify the sonic characteristics of each laptop performer. We refer to software implementation that was developed for the BLISS networked laptop ensemble with view to designing a shared format for the exchange of messages within local and internet based networks. }
}

@inproceedings{Naef2006,
  author = {Naef, Martin and Collicott, Daniel},
  title = {A VR Interface for Collaborative {3D} Audio Performance},
  pages = {57--60},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176975},
  url = {http://www.nime.org/proceedings/2006/nime2006_057.pdf}
}

@inproceedings{Geiger2006,
  author = {Geiger, G\''{u}nter},
  title = {Using the Touch Screen as a Controller for Portable Computer Music Instruments},
  pages = {61--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176911},
  url = {http://www.nime.org/proceedings/2006/nime2006_061.pdf},
  keywords = {touch screen, PDA, Pure Data, controller, mobile musical instrument, human computer interaction }
}

@inproceedings{Holm2006,
  author = {Holm, Jukka and Arrasvuori, Juha and Havukainen, Kai},
  title = {Using {MIDI} to Modify Video Game Content},
  pages = {65--70},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176925},
  url = {http://www.nime.org/proceedings/2006/nime2006_065.pdf},
  keywords = {Games, MIDI, music, rhythm games, background music reactive games, musically controlled games, MIDI-controlled games, Virtual Sequencer. },
  abstract = {This paper discusses the concept of using background music to control video game parameters and thus actions on the screen. Each song selected by the player makes the game look different and behave variedly. The concept is explored by modifying an existing video game and playtesting it with different kinds of MIDI music. Several examples of mapping MIDI parameters to game events are presented. As mobile phones' MIDI players do not usually have a dedicated callback API, a real-time MIDI analysis software for Symbian OS was implemented. Future developments including real-time group performance as a way to control game content are also considered. }
}

@inproceedings{Lippit2006,
  author = {Lippit, Takuro M.},
  title = {Turntable Music in the Digital Era: Designing Alternative Tools for New Turntable Expression},
  pages = {71--74},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176965},
  url = {http://www.nime.org/proceedings/2006/nime2006_071.pdf},
  keywords = {Turntable music, DJ, turntablist, improvisation, Max/MSP, PIC Microcontroller, Physical Computing },
  abstract = {Turntable musicians have yet to explore new expressions with digital technology. New higher-level development tools open possibilities for these artists to build their own instruments that can achieve artistic goals commercial products cannot. This paper will present a rough overview on the practice and recent development of turntable music, followed by descriptions of two projects by the ,
,
author. }
}

@inproceedings{Kiser2006,
  author = {Kiser, Spencer},
  title = {spinCycle: a Color-Tracking Turntable Sequencer},
  pages = {75--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176941},
  url = {http://www.nime.org/proceedings/2006/nime2006_075.pdf},
  keywords = {Color-tracking, turntable, visualization, interactivity, synesthesia },
  abstract = {This report presents an interface for musical performance called the spinCycle. spinCycle enables performers to make visual patterns with brightly colored objects on a spinning turntable platter that get translated into musical arrangements in realtime. I will briefly describe the hardware implementation and the sound generation logic used, as well as provide a historical background for the project.}
}

@inproceedings{Lee2006a,
  author = {Lee, Jason},
  title = {The Chopping Board: Real-time Sample Editor},
  pages = {77--78},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176959},
  url = {http://www.nime.org/proceedings/2006/nime2006_077.pdf}
}

@inproceedings{DeJong2006,
  author = {de Jong, Staas},
  title = {A Tactile Closed-Loop Device for Musical Interaction},
  pages = {79--80},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176935},
  url = {http://www.nime.org/proceedings/2006/nime2006_079.pdf}
}

@inproceedings{Bennett2006,
  author = {Bennett, Peter},
  title = {{PET}ECUBE: a Multimodal Feedback Interface},
  pages = {81--84},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176869},
  url = {http://www.nime.org/proceedings/2006/nime2006_081.pdf},
  keywords = {Multi-modal Feedback. Haptics. Musical Instrument. },
  abstract = {The PETECUBE project consists of a series of musical interfaces designed to explore multi-modal feedback. This paper will briefly describe the definition of multimodal feedback, the aim of the project, the development of the first PETECUBE and proposed further work. }
}

@inproceedings{Lebel2006,
  author = {Lebel, Denis and Malloch, Joseph},
  title = {The G-Spring Controller},
  pages = {85--88},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176955},
  url = {http://www.nime.org/proceedings/2006/nime2006_085.pdf},
  keywords = {Digital musical instrument, kinesthetic feedback }
}

@inproceedings{Lock2006,
  author = {Lock, Damien and Schiemer, Greg},
  title = {Orbophone: a New Interface for Radiating Sound and Image},
  pages = {89--92},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176967},
  url = {http://www.nime.org/proceedings/2006/nime2006_089.pdf},
  keywords = {Immersive Sound; Multi-channel Sound; Loud-speaker Array; Multimedia; Streaming Media; Real-Time Media Performance; Sound Installation. },
  abstract = {The Orbophone is a new interface that radiates rather thanprojects sound and image. It provides a cohesive platformfor audio and visual presentation in situations where bothmedia are transmitted from the same location andlocalization in both media is perceptually correlated. Thispaper discusses the advantages of radiation overconventional sound and image projection for certain kindsof interactive public multimedia exhibits and describes theartistic motivation for its development against a historicalbackdrop of sound systems used in public spaces. Oneexhibit using the Orbophone is described in detail togetherwith description and critique of the prototype, discussingaspects of its design and construction. The paper concludeswith an outline of the Orbophone version 2.}
}

@inproceedings{Kartadinata2006,
  author = {Kartadinata, Sukandar},
  title = {The Gluion Advantages of an {FPGA}-based Sensor Interface},
  pages = {93--96},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176937},
  url = {http://www.nime.org/proceedings/2006/nime2006_093.pdf},
  keywords = {actuators,digital sensors,fpga,osc,sensor interfaces},
  abstract = {The gluion is a sensor interface that was designed to overcomesome of the limitations of more traditional designs based onmicrocontrollers, which only provide a small, fixed number ofdigital modules such as counters and serial interfaces. These areoften required to handle sensors where the physical parametercannot easily be converted into a voltage. Other sensors arepacked into modules that include converters and communicatevia SPI or I2C. Finallly, many designs require outputcapabilities beyond simple on/off.The gluion approaches these challenges thru its FPGA-baseddesign which allows for a large number of digital I/O modules.It also provides superior flexibility regarding theirconfiguration, resolution, and functionality. In addition, theFPGA enables a software implementation of the host link --- inthe case of the gluion the OSC protocol as well as theunderlying Ethernet layers.}
}

@inproceedings{Freed2006,
  author = {Freed, Adrian and Avizienis, Rimas and Wright, Matthew},
  title = {Beyond 0-5{V}: Expanding Sensor Integration Architectures},
  pages = {97--100},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176903},
  url = {http://www.nime.org/proceedings/2006/nime2006_097.pdf},
  keywords = {Gesture, sensor, MEMS, FPGA, network, OSC, configurability },
  abstract = {A new sensor integration system and its first incarnation i sdescribed. As well as supporting existing analog sensorarrays a new architecture allows for easy integration of thenew generation of low-cost digital sensors used in computermusic performance instruments and installation art.}
}

@inproceedings{Johnson2006,
  author = {Johnson, Colin G. and Gounaropoulos, Alex},
  title = {Timbre Interfaces using Adjectives and Adverbs},
  pages = {101--102},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176933},
  url = {http://www.nime.org/proceedings/2006/nime2006_101.pdf},
  keywords = {timbre; natural language; neural networks },
  abstract = {How can we provide interfaces to synthesis algorithms thatwill allow us to manipulate timbre directly, using the sametimbre-words that are used by human musicians to communicate about timbre? This paper describes ongoingwork that uses machine learning methods (principally genetic algorithms and neural networks) to learn (1) to recognise timbral characteristics of sound and (2) to adjust timbral characteristics of existing synthesized sounds.}
}

@inproceedings{Stewart2006,
  author = {Stewart, D. Andrew},
  title = {SonicJumper Composer},
  pages = {103--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177011},
  url = {http://www.nime.org/proceedings/2006/nime2006_103.pdf},
  keywords = {composition, process, materials, gesture, controller, cross- modal interaction }
}

@inproceedings{Steiner2006,
  author = {Steiner, Hans-Christoph},
  title = {Towards a Catalog and Software Library of Mapping Methods},
  pages = {106--109},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177009},
  url = {http://www.nime.org/proceedings/2006/nime2006_106.pdf}
}

@inproceedings{Kobori2006,
  author = {Kobori, Daisuke and Kagawa, Kojiro and Iida, Makoto and Arakawa, Chuichi},
  title = {LINE: Interactive Sound and Light Installation},
  pages = {110--113},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176947},
  url = {http://www.nime.org/proceedings/2006/nime2006_110.pdf}
}

@inproceedings{BryanKinns2006,
  author = {Bryan-Kinns, Nick and Healey, Patrick G.},
  title = {Decay in Collaborative Music Making},
  pages = {114--117},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176885},
  url = {http://www.nime.org/proceedings/2006/nime2006_114.pdf},
  keywords = {creativity,design,group interaction,music improvisation},
  abstract = {This paper reports on ongoing studies of the design and use ofsupport for remote group music making. In this paper weoutline the initial findings of a recent study focusing on thefunction of decay of contributions in collaborative musicmaking. Findings indicate that persistent contributions lendthemselves to individual musical composition and learningnovel interfaces, whilst contributions that quickly decayengender a more focused musical interaction in experiencedparticipants.}
}

@inproceedings{Gurevich2006,
  author = {Gurevich, Michael},
  title = {JamSpace: Designing A Collaborative Networked Music Space for Novices},
  pages = {118--123},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176915},
  url = {http://www.nime.org/proceedings/2006/nime2006_118.pdf},
  keywords = {Collaborative interface, remote jamming, network music, interaction design, novice, media space INTRODUCTION Most would agree that music is an inherently social ac- tivity [30], but since the }
}

@inproceedings{Knapp2006,
  author = {Knapp, Benjamin and Cook, Perry R.},
  title = {Creating a Network of Integral Music Controllers},
  pages = {124--128},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176943},
  url = {http://www.nime.org/proceedings/2006/nime2006_124.pdf},
  keywords = {Community-Institutional Relations,Health Services Accessibility,Medically Uninsured,Organizational Case Studies,Primary Health Care,Public-Private Sector Partnerships,San Francisco},
  abstract = {In this paper, we describe the networking of multiple Integral Music Controllers (IMCs) to enable an entirely new method for creating music by tapping into the composite gestures and emotions of not just one, but many performers. The concept and operation of an IMC is reviewed as well as its use in a network of IMC controllers. We then introduce a new technique of Integral Music Control by assessing the composite gesture(s) and emotion(s) of a group of performers through the use of a wireless mesh network. The Telemuse, an IMC designed precisely for this kind of performance, is described and its use in a new musical performance project under development by the ,
,
authors is discussed. }
}

@inproceedings{Burtner2006,
  author = {Burtner, Matthew},
  title = {Perturbation Techniques for Multi-Performer or Multi- Agent Interactive Musical Interfaces},
  pages = {129--133},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176887},
  url = {http://www.nime.org/proceedings/2006/nime2006_129.pdf},
  keywords = {interface,mapping,movement,multi-agent,multi-performer,music composition,perturbation},
  abstract = {This paper explores the use of perturbation in designing multiperformer or multi-agent interactive musical interfaces. A problem with the multi-performer approach is how to cohesively organize the independent data inputs into useable control information for synthesis engines. Perturbation has proven useful for navigating multi-agent NIMEs. The ,
,
author's Windtree is discussed as an example multi-performer instrument in which perturbation is used for multichannel ecological modeling. The Windtree uses a physical system turbulence model controlled in real time by four performers. }
}

@inproceedings{Aylward2006,
  author = {Aylward, Ryan and Paradiso, Joseph A.},
  title = {Sensemble: A Wireless, Compact, Multi-User Sensor System for Interactive Dance},
  pages = {134--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176865},
  url = {http://www.nime.org/proceedings/2006/nime2006_134.pdf},
  keywords = {Interactive dance, wearable sensor networks, inertial gesture tracking, collective motion analysis, multi-user interface },
  abstract = {We describe the design of a system of compact, wireless sensor modules meant to capture expressive motion whenworn at the wrists and ankles of a dancer. The sensors form ahigh-speed RF network geared toward real-time dataacquisition from multiple devices simultaneously, enabling asmall dance ensemble to become a collective interface formusic control. Each sensor node includes a 6-axis inertialmeasurement unit (IMU) comprised of three orthogonalgyroscopes and accelerometers in order to capture localdynamics, as well as a capacitive sensor to measure closerange node-to-node proximity. The nodes may also beaugmented with other digital or analog sensors. This paperdescribes application goals, presents the prototype hardwaredesign, introduces concepts for feature extraction andinterpretation, and discusses early test results.}
}

@inproceedings{Ramakrishnan2006,
  author = {Ramakrishnan, Chandrasekhar and Go\ss man, Joachim and Br\''{u}mmer, Ludger},
  title = {The ZKM Klangdom},
  pages = {140--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176991},
  url = {http://www.nime.org/proceedings/2006/nime2006_140.pdf},
  keywords = {Sound Spatialization, Ambisonics, Vector Based Additive Panning (VBAP), Wave Field Synthesis, Acousmatic Music }
}

@inproceedings{Wozniewski2006,
  author = {Wozniewski, Mike and Settel, Zack and Cooperstock, Jeremy R.},
  title = {A Framework for Immersive Spatial Audio Performance},
  pages = {144--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177021},
  url = {http://www.nime.org/proceedings/2006/nime2006_144.pdf},
  keywords = {Control paradigms, 3D audio, spatialization, immersive audio environments, auditory display, acoustic modeling, spatial inter- faces, virtual instrument design },
  abstract = {Traditional uses of virtual audio environments tend to focus onperceptually accurate acoustic representations. Though spatialization of sound sources is important, it is necessary to leveragecontrol of the sonic representation when considering musical applications. The proposed framework allows for the creation ofperceptually immersive scenes that function as musical instruments. Loudspeakers and microphones are modeled within thescene along with the listener/performer, creating a navigable 3Dsonic space where sound sources and sinks process audio according to user-defined spatial mappings.}
}

@inproceedings{Francois2006,
  author = {Francois, Alexander R. and Chew, Elaine},
  title = {An Architectural Framework for Interactive Music Systems},
  pages = {150--155},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176901},
  url = {http://www.nime.org/proceedings/2006/nime2006_150.pdf},
  keywords = {Software Architecture, Interactive Systems, Music soft- ware }
}

@inproceedings{Jacquemin2006,
  author = {Jacquemin, Christian and de Laubier, Serge},
  title = {Transmodal Feedback as a New Perspective for Audio-visual Effects},
  pages = {156--161},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176929},
  url = {http://www.nime.org/proceedings/2006/nime2006_156.pdf},
  keywords = {audio-visual composition,feedback,transmodality}
}

@inproceedings{Magnusson2006,
  author = {Magnusson, Thor},
  title = {Screen-Based Musical Interfaces as Semiotic Machines},
  pages = {162--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176969},
  url = {http://www.nime.org/proceedings/2006/nime2006_162.pdf},
  keywords = {Interfaces, interaction design, HCI, semiotics, actors, OSC, mapping, interaction models, creative tools. },
  abstract = {The ixi software project started in 2000 with the intention to explore new interactive patterns and virtual interfaces in computer music software. The aim of this paper is not to describe these programs, as they have been described elsewhere [14][15], but rather explicate the theoretical background that underlies the design of these screen-based instruments. After an analysis of the similarities and differences in the design of acoustic and screen-based instruments, the paper describes how the creation of an interface is essentially the creation of a semiotic system that affects and influences the musician and the composer. Finally the terminology of this semiotics is explained as an interaction model. }
}

@inproceedings{Zadel2006,
  author = {Zadel, Mark and Scavone, Gary},
  title = {Different Strokes: a Prototype Software System for Laptop Performance and Improvisation},
  pages = {168--171},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177025},
  url = {http://www.nime.org/proceedings/2006/nime2006_168.pdf},
  keywords = {Software control of computer music, laptop performance, graphical interfaces, freehand input, dynamic simulation }
}

@inproceedings{Nishibori2006,
  author = {Nishibori, Yu and Iwai, Toshio},
  title = {TENORI-ON},
  pages = {172--175},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176979},
  url = {http://www.nime.org/proceedings/2006/nime2006_172.pdf},
  abstract = {Development of a musical interface which allows people to play music intuitively and create music visibly. }
}

@inproceedings{Jensenius2006a,
  author = {Jensenius, Alexander Refsum and Kvifte, Tellef and Godøy, Rolf Inge},
  title = {Towards a Gesture Description Interchange Format},
  pages = {176--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176931},
  url = {http://www.nime.org/proceedings/2006/nime2006_176.pdf},
  keywords = {Gesture description, gesture analysis, standards },
  abstract = {This paper presents our need for a Gesture Description Interchange Format (GDIF) for storing, retrieving and sharing information about music-related gestures. Ideally, it should be possible to store all sorts of data from various commercial and custom made controllers, motion capture and computer vision systems, as well as results from different types of gesture analysis, in a coherent and consistent way. This would make it possible to use the information with different software, platforms and devices, and also allow for sharing data between research institutions. We present some of the data types that should be included, and discuss issues which need to be resolved.}
}

@inproceedings{Wanderley2006,
  author = {Wanderley, Marcelo M. and Birnbaum, David and Malloch, Joseph and Sinyor, Elliot and Boissinot, Julien},
  title = {SensorWiki.org: A Collaborative Resource for Researchers and Interface Designers},
  pages = {180--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177015},
  url = {http://www.nime.org/proceedings/2006/nime2006_180.pdf},
  keywords = {sensors, Wiki, collaborative website, open content }
}

@inproceedings{Dimitrov2006,
  author = {Dimitrov, Smilen and Serafin, Stefania},
  title = {A Simple Practical Approach to a Wireless Data Acquisition Board},
  pages = {184--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176891},
  url = {http://www.nime.org/proceedings/2006/nime2006_184.pdf}
}

@inproceedings{Hansen2006,
  author = {Hansen, Kjetil F. and Bresin, Roberto},
  title = {Mapping Strategies in DJ Scratching},
  pages = {188--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176921},
  url = {http://www.nime.org/proceedings/2006/nime2006_188.pdf},
  keywords = {controllers,dj,instrument mapping,scratching,virtual}
}

@inproceedings{Kessous2006,
  author = {Kessous, Lo\"{i}c and Castet, Julien and Arfib, Daniel},
  title = {'GXtar', an Interface Using Guitar Techniques},
  pages = {192--195},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176857},
  url = {http://www.nime.org/proceedings/2006/nime2006_192.pdf},
  keywords = {Guitar, alternate controller, sensors, synthesizer, multidimensional control. },
  abstract = {In this paper we describe a new guitar-like musical controller. The 'GXtar' is an instrument which takes as a starting point a guitar but his role is to bring different and new musical possibilities while preserving the spirit and techniques of guitar. Therefore, it was conceived and carried out starting from the body of an electric guitar. The fingerboard of this guitar was equipped with two lines of sensors: linear position sensors, and tactile pressure sensors. These two lines of sensors are used as two virtual strings. Their two ends are the bridge and the nut of the guitar. The design of the instrument is made in a way that the position of a finger, on one of these virtual strings, corresponds to the note, which would have been played on a real and vibrating string. On the soundboard of the guitar, a controller, with 3 degrees of freedom, allows to drive other synthesis parameters. We then describe how this interface is integrated in a musical audio system and serves as a musical instrument. }
}

@inproceedings{Burns2006,
  author = {Burns, Anne-Marie and Wanderley, Marcelo M.},
  title = {Visual Methods for the Retrieval of Guitarist Fingering},
  pages = {196--199},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176850},
  url = {http://www.nime.org/proceedings/2006/nime2006_196.pdf},
  keywords = {finger-tracking,gesture,guitar fingering,hough transform}
}

@inproceedings{Schoonderwaldt2006,
  author = {Schoonderwaldt, Erwin and Rasamimanana, Nicolas and Bevilacqua, Fr\'{e}d\'{e}ric},
  title = {Combining Accelerometer and Video Camera: Reconstruction of Bow Velocity Profiles},
  pages = {200--203},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177003},
  url = {http://www.nime.org/proceedings/2006/nime2006_200.pdf},
  keywords = {Bowing gestures, bowed string, violin, bow velocity, accelerometer, video tracking. },
  abstract = {A cost-effective method was developed for the estimation of the bow velocity in violin playing, using an accelerometer on the bow in combination with point tracking using a standard video camera. The video data are used to detect the moments of bow direction changes. This information is used for piece-wise integration of the accelerometer signal, resulting in a drift-free reconstructed velocity signal with a high temporal resolution. The method was evaluated using a 3D motion capturing system, providing a reliable reference of the actual bow velocity. The method showed good results when the accelerometer and video stream are synchronized. Additional latency and jitter of the camera stream can importantly decrease the performance of the method, depending on the bow stroke type. }
}

@inproceedings{Leroy2006,
  author = {Leroy, Nicolas and Fl\'{e}ty, Emmanuel and Bevilacqua, Fr\'{e}d\'{e}ric},
  title = {Reflective Optical Pickup For Violin},
  pages = {204--207},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176859},
  url = {http://www.nime.org/proceedings/2006/nime2006_204.pdf}
}

@inproceedings{Jorda2006,
  author = {Jord\`{a}, Sergi and Alonso, Marcos},
  title = {Mary Had a Little scoreTable* or the reacTable* Goes Melodic},
  pages = {208--211},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176855},
  url = {http://www.nime.org/proceedings/2006/nime2006_208.pdf},
  keywords = {Musical instrument, Collaborative Music, Computer Supported Collaborative Work, Tangible User Interface, Music Theory. },
  abstract = {This paper introduces the scoreTable*, a tangible interactive music score editor which started as a simple application for demoing "traditional" approaches to music creation, using the reacTable* technology, and which has evolved into an independent research project on its own. After a brief discussion on the role of pitch in music, we present a brief overview of related tangible music editors, and discuss several paradigms in computer music creation, contrasting synchronous with asynchronous approaches. The final part of the paper describes the current state of the scoreTable* as well as its future lines of research.}
}

@inproceedings{Crevoisier2006,
  author = {Crevoisier, Alain and Bornand, C\'{e}dric and Guichard, Arnaud and Matsumura, Seiichiro and Arakawa, Chuichi},
  title = {Sound Rose: Creating Music and Images with a Touch Table},
  pages = {212--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176853},
  url = {http://www.nime.org/proceedings/2006/nime2006_212.pdf}
}

@inproceedings{Davidson2006,
  author = {Davidson, Philip L. and Han, Jefferson Y.},
  title = {Synthesis and Control on Large Scale Multi-Touch Sensing Displays},
  pages = {216--219},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176889},
  url = {http://www.nime.org/proceedings/2006/nime2006_216.pdf},
  keywords = {multi-touch, touch, tactile, bi-manual, multi-user, synthesis, dynamic patching },
  abstract = {In this paper, we describe our experience in musical interface design for a large scale, high-resolution, multi-touch display surface. We provide an overview of historical and presentday context in multi-touch audio interaction, and describe our approach to analysis of tracked multi-finger, multi-hand data for controlling live audio synthesis.}
}

@inproceedings{Kvifte2006,
  author = {Kvifte, Tellef and Jensenius, Alexander Refsum},
  title = {Towards a Coherent Terminology and Model of Instrument Description and Design},
  pages = {220--225},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176951},
  url = {http://www.nime.org/proceedings/2006/nime2006_220.pdf},
  keywords = {Musical instrument design, mapping, gestures, organology. },
  abstract = {This paper discusses the need for a framework for describing musical instruments and their design, and discusses some possible elements in such a framework. The framework is meant as an aid in the development of a coherent terminology for describing, comparing and discussing different musical instruments and musical instrument designs. Three different perspectives are presented; that of the listener, the performer, and the constructor, and various levels of descriptions are introduced.}
}

@inproceedings{Marshall2006,
  author = {Marshall, Mark T. and Wanderley, Marcelo M.},
  title = {Vibrotactile Feedback in Digital Musical Instruments},
  pages = {226--229},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176973},
  url = {http://www.nime.org/proceedings/2006/nime2006_226.pdf},
  keywords = {digital musical instruments,tactile feedback,vibro-tactile}
}

@inproceedings{Koehly2006,
  author = {Koehly, Rodolphe and Curtil, Denis and Wanderley, Marcelo M.},
  title = {Paper FSRs and Latex/Fabric Traction Sensors: Methods for the Development of Home-Made Touch Sensors},
  pages = {230--233},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176949},
  url = {http://www.nime.org/proceedings/2006/nime2006_230.pdf},
  keywords = {Touch sensors, piezoresistive technology, conductive pigments, sensitive materials, interface design },
  abstract = {This paper presents the development of novel "home-made" touch sensors using conductive pigments and various substrate materials. We show that it is possible to build one's own position, pressure and bend sensors with various electrical characteristics, sizes and shapes, and this for a very competitive price. We give examples and provide results from experimental tests of such developments. }
}

@inproceedings{Bowers2006,
  author = {Bowers, John and Villar, Nicolas},
  title = {Creating Ad Hoc Instruments with Pin\&Play\&Perform},
  pages = {234--239},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176881},
  url = {http://www.nime.org/proceedings/2006/nime2006_234.pdf},
  keywords = {Ad hoc instruments, Pin&Play, physical interfaces, music performance, new interfaces for musical expression. }
}

@inproceedings{Serafin2006,
  author = {Serafin, Stefania and de G\''{o}tzen, Amalia and B\''{o}ttcher, Niels and Gelineck, Steven},
  title = {Synthesis and Control of Everyday Sounds Reconstructing Russolo's Intonarumori},
  pages = {240--245},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177005},
  url = {http://www.nime.org/proceedings/2006/nime2006_240.pdf},
  keywords = {Noise machines, everyday sounds, physical models. },
  abstract = {In this paper we introduce the Croaker, a novel input deviceinspired by Russolo's Intonarumori. We describe the components of the controller and the sound synthesis engine whichallows to reproduce several everyday sounds.}
}

@inproceedings{Weinberg2006,
  author = {Weinberg, Gil and Thatcher, Travis},
  title = {Interactive Sonification of Neural Activity},
  pages = {246--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177019},
  url = {http://www.nime.org/proceedings/2006/nime2006_246.pdf},
  keywords = {1,background and motivations,biological research,interactive auditory display,neural patterns,scholars are,sonification,with new developments in}
}

@inproceedings{Remus2006,
  author = {R\'{e}mus, Jacques},
  title = {Non Haptic Control of Music by Video Analysis of Hand Movements: 14 Years of Experience with the `Cam\'{e}ra Musicale'},
  pages = {250--253},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176989},
  url = {http://www.nime.org/proceedings/2006/nime2006_250.pdf},
  keywords = {camera musicale,interface,jacques r\'{e}mus,machines,musical camera,musical hand,non haptic instrument,s mappings,sculptures and mechanical musical,sound}
}

@inproceedings{Borchers2006,
  author = {Borchers, Jan and Hadjakos, Aristotelis and M\''{u}hlh\''{a}user, Max},
  title = {MICON A Music Stand for Interactive Conducting},
  pages = {254--259},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176877},
  url = {http://www.nime.org/proceedings/2006/nime2006_254.pdf},
  keywords = {Music stand, score display, exhibit, conducting. },
  abstract = {The MICON is an electronic music stand extending Maestro!, the latest in a series of interactive conducting exhibits that use real orchestral audio and video recordings. The MICON uses OpenGL-based rendering to display and animate score pages with a high degree of realism. It offers three different score display formats to match the user's level of expertise. A realtime animated visual cueing system helps users with their conducting. The MICON has been evaluated with music students. }
}

@inproceedings{Lee2006,
  author = {Lee, Eric and Gr\''{u}ll, Ingo and Keil, Henning and Borchers, Jan},
  title = {conga: A Framework for Adaptive Conducting Gesture Analysis},
  pages = {260--265},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176957},
  url = {http://www.nime.org/proceedings/2006/nime2006_260.pdf},
  keywords = {gesture recognition, conducting, software gesture frameworks },
  abstract = {Designing a conducting gesture analysis system for public spacesposes unique challenges. We present conga, a software framework that enables automatic recognition and interpretation ofconducting gestures. conga is able to recognize multiple types ofgestures with varying levels of difficulty for the user to perform,from a standard four-beat pattern, to simplified up-down conducting movements, to no pattern at all. conga provides an extendablelibrary of feature detectors linked together into a directed acyclicgraph; these graphs represent the various conducting patterns asgesture profiles. At run-time, conga searches for the best profileto match a user's gestures in real-time, and uses a beat prediction algorithm to provide results at the sub-beat level, in additionto output values such as tempo, gesture size, and the gesture'sgeometric center. Unlike some previous approaches, conga doesnot need to be trained with sample data before use. Our preliminary user tests show that conga has a beat recognition rate ofover 90%. conga is deployed as the gesture recognition systemfor Maestro!, an interactive conducting exhibit that opened in theBetty Brinn Children's Museum in Milwaukee, USA in March2006.}
}

@inproceedings{dAlessandro2006,
  author = {d'Alessandro, Nicolas and d'Alessandro, Christophe and Le Beux, Sylvain and Doval, Boris},
  title = {Real-time CALM Synthesizer: New Approaches in Hands-Controlled Voice Synthesis},
  pages = {266--271},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176863},
  url = {http://www.nime.org/proceedings/2006/nime2006_266.pdf},
  keywords = {Singing synthesis, voice source, voice quality, spectral model, formant synthesis, instrument, gestural control. }
}

@inproceedings{Pritchard2006,
  author = {Pritchard, Bob and Fels, Sidney S.},
  title = {GRASSP: Gesturally-Realized Audio, Speech and Song Performance},
  pages = {272--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176987},
  url = {http://www.nime.org/proceedings/2006/nime2006_272.pdf},
  keywords = {Speech synthesis, parallel formant speech synthesizer, gesture control, Max/MSP, Jitter, Cyberglove, Polhemus, sound diffusion, UBC Toolbox, Glove-Talk, },
  abstract = {We describe the implementation of an environment for Gesturally-Realized Audio, Speech and Song Performance (GRASSP), which includes a glove-based interface, a mapping/training interface, and a collection of Max/MSP/Jitter bpatchers that allow the user to improvise speech, song, sound synthesis, sound processing, sound localization, and video processing. The mapping/training interface provides a framework for performers to specify by example the mapping between gesture and sound or video controls. We demonstrate the effectiveness of the GRASSP environment for gestural control of musical expression by creating a gesture-to-voice system that is currently being used by performers. }
}

@inproceedings{Dobrian2006,
  author = {Dobrian, Christopher and Koppelman, Daniel},
  title = {The E in NIME: Musical Expression with New Computer Interfaces},
  pages = {277--282},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176893},
  url = {http://www.nime.org/proceedings/2006/nime2006_277.pdf},
  keywords = {Expression, instrument design, performance, virtuosity. },
  abstract = {Is there a distinction between New Interfaces for MusicalExpression and New Interfaces for Controlling Sound? Thisarticle begins with a brief overview of expression in musicalperformance, and examines some of the characteristics ofeffective "expressive" computer music instruments. Itbecomes apparent that sophisticated musical expressionrequires not only a good control interface but also virtuosicmastery of the instrument it controls. By studying effectiveacoustic instruments, choosing intuitive but complexgesture-sound mappings that take advantage of establishedinstrumental skills, designing intelligent characterizationsof performance gestures, and promoting long-term dedicatedpractice on a new interface, computer music instrumentdesigners can enhance the expressive quality of computermusic performance.}
}

@inproceedings{Richards2006,
  author = {Richards, John},
  title = {32kg: Performance Systems for a Post-Digital Age},
  pages = {283--287},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176995},
  url = {http://www.nime.org/proceedings/2006/nime2006_283.pdf},
  keywords = {bastardisation,dirty electronics,diy,ebay,live,modular,performance,portability,post-digital,punktronics},
  abstract = {Why is a seemingly mundane issue such as airline baggageallowance of great significance in regards to the performancepractice of electronic music? This paper discusses how aperformance practice has evolved that seeks to question thebinary and corporate digital world. New 'instruments' andapproaches have emerged that explore 'dirty electronics' and'punktronics': DIY electronic instruments made from junk.These instruments are not instruments in the traditionalsense, defined by physical dimensions or by a set number ofparameters, but modular systems, constantly evolving, nevercomplete, infinitely variable and designed to be portable. Acombination of lo- and hi-fi, analogue and digital,synchronous and asynchronous devices offer new modes ofexpression. The development of these new interfaces formusical expression run side-by-side with an emerging postdigital aesthetic.}
}

@inproceedings{DeLaubier2006,
  author = {de Laubier, Serge and Goudard, Vincent},
  title = {Meta-Instrument 3: a Look over 17 Years of Practice},
  pages = {288--291},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176953},
  url = {http://www.nime.org/proceedings/2006/nime2006_288.pdf},
  keywords = {1,audio-graphic portable instrument,ethernet,from 1983 to 1988,genesis of the project,on,puce muse studios,r\'{e}pertoire,we worked at the,wifi}
}

@inproceedings{Goto2006,
  author = {Goto, Suguru},
  title = {The Case Study of An Application of The System, `BodySuit' and `RoboticMusic': Its Introduction and Aesthetics},
  pages = {292--295},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176913},
  url = {http://www.nime.org/proceedings/2006/nime2006_292.pdf},
  keywords = {Robot, Gesture Controller, Humanoid Robot, Artificial Intelligence, Interaction },
  abstract = {This paper is intended to introduce the system, which combines "BodySuit" and "RoboticMusic", as well as its possibilities and its uses in an artistic application. "BodySuit" refers to a gesture controller in a Data Suit type. "RoboticMusic" refers to percussion robots, which are appliedto a humanoid robot type. In this paper, I will discuss their aesthetics and the concept, as well as the idea of the "Extended Body".}
}

@inproceedings{Hindman2006,
  author = {Hindman, David},
  title = {Modal Kombat: Competition and Choreography in Synesthetic Musical Performance},
  pages = {296--299},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176923},
  url = {http://www.nime.org/proceedings/2006/nime2006_296.pdf}
}

@inproceedings{Lehrman2006,
  author = {Lehrman, Paul D. and Singer, Eric},
  title = {A "Ballet M\'{e}canique" for the 21{s}t Century: Performing George Antheil's Dadaist Masterpiece with Robots},
  pages = {300--303},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176961},
  url = {http://www.nime.org/proceedings/2006/nime2006_300.pdf},
  keywords = {Robotics, computer control, MIDI, player pianos, mechanical music, percussion, sound effects, Dadaism. }
}

@inproceedings{Lemouton2006,
  author = {Lemouton, Serge and Stroppa, Marco and Sluchin, Benny},
  title = {Using the Augmented Trombone in "I will not kiss your f.ing flag"},
  pages = {304--307},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176963},
  url = {http://www.nime.org/proceedings/2006/nime2006_304.pdf},
  keywords = {augmented instrument,chamber electronics,computer,interaction,musical motivation,performer,trombone},
  abstract = {This paper deals with the first musical usage of anexperimental system dedicated to the optical detection ofthe position of a trombone's slide.}
}

@inproceedings{Schiesser2006,
  author = {Schiesser, S\'{e}bastien and Traube, Caroline},
  title = {On Making and Playing an Electronically-augmented Saxophone},
  pages = {308--313},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177001},
  url = {http://www.nime.org/proceedings/2006/nime2006_308.pdf},
  keywords = {saxophone, augmented instrument, live electronics, perfor- mance, gestural control }
}

@inproceedings{Smyth2006,
  author = {Smyth, Tamara},
  title = {Handheld Acoustic Filter Bank for Musical Control},
  pages = {314--317},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177007},
  url = {http://www.nime.org/proceedings/2006/nime2006_314.pdf},
  keywords = {khaen, sound synthesis control, mapping, musical acoustics }
}

@inproceedings{Nixdorf2006,
  author = {Nixdorf, Joshua J. and Gerhard, David},
  title = {Real-time Sound Source Spatialization as Used in Challenging Bodies: Implementation and Performance},
  pages = {318--321},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176981},
  url = {http://www.nime.org/proceedings/2006/nime2006_318.pdf},
  keywords = {gem,live systems,pd,performance sys-,real-time systems,sound architecture,sound localization,sound spatialization,surround sound,tems},
  abstract = {In this paper we will report on the use of real-time soundspatialization in Challenging Bodies, a trans-disciplinaryperformance project at the University of Regina. Usingwell-understood spatialization techniques mapped to a custom interface, a computer system was built that allowedlive spatial control of ten sound signals from on-stage performers. This spatial control added a unique dynamic element to an already ultramodern performance. The systemis described in detail, including the main advantages overexisting spatialization systems: simplicity, usability, customization and scalability}
}

@inproceedings{Bottoni2006,
  author = {Bottoni, Paolo and Faralli, Stefano and Labella, Anna and Pierro, Mario},
  title = {Mapping with Planning Agents in the Max/MSP Environment: the GO/Max Language},
  pages = {322--325},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176879},
  url = {http://www.nime.org/proceedings/2006/nime2006_322.pdf},
  keywords = {mapping, planning, agent, Max/MSP }
}

@inproceedings{Bonardi2006,
  author = {Bonardi, Alain and Truck, Isis and Akdag, Herman},
  title = {Towards a Virtual Assistant for Performers and Stage Directors},
  pages = {326--329},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176875},
  url = {http://www.nime.org/proceedings/2006/nime2006_326.pdf},
  keywords = {Virtual Assistant, Intents, Emotion detector, Fuzzy Classes, Stage Director, Performance. },
  abstract = {In this article, we present the first step of our research work todesign a Virtual Assistant for Performers and Stage Directors,able to give a feedback from performances. We use amethodology to automatically construct fuzzy rules in a FuzzyRule-Based System that detects contextual emotions from anactor's performance during a show. We collect video data from a lot of performances of the sameshow from which it should be possible to visualize all the emotions and intents or more precisely "intent graphs". To perform this, the collected data defining low-level descriptors are aggregated and converted into high-level characterizations. Then, depending on the retrieved data and on their distributionon the axis, we partition the universes into classes. The last stepis the building of the fuzzy rules that are obtained from the classes and that permit to give conclusions to label the detected emotions.}
}

@inproceedings{Nagashima2006,
  author = {Nagashima, Yoichi},
  title = {Students' Projects of Interactive Media-installations in SUAC},
  pages = {330--333},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176977},
  url = {http://www.nime.org/proceedings/2006/nime2006_330.pdf},
  keywords = {Interactive Installation, Sensors, Media Arts, Studio Reports },
  abstract = {This is a studio report of researches and projects in SUAC(Shizuoka University of Art and Culture). SUAC was foundedin April 2000, and organized NIME04 as you know. SUAC has "Faculty of Design" and "Department of Art and Science" and all students study interactive systems and media arts.SUAC has organized Media Art Festival (MAF) from 2001 to2005. Domestic/overseas artists participated in SUAC MAF,and SUAC students' projects also joined and exhibited theirworks in MAF. I will introduce the production cases withinteractive media-installations by SUAC students' projectsfrom the aspect experiences with novel interfaces ineducation and entertainment and reports on students projectsin the framework of NIME related courses.}
}

@inproceedings{Breinbjerg2006,
  author = {Breinbjerg, Morten and Caprani, Ole and Lunding, Rasmus and Kramhoft, Line},
  title = {An Acousmatic Composition Environment},
  pages = {334--337},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176883},
  url = {http://www.nime.org/proceedings/2006/nime2006_334.pdf},
  keywords = {Acousmatic listening, aesthetics, tangible interfaces. },
  abstract = {In this paper we describe the intentions, the design and functionality of an Acousmatic Composition Environment that allows children or musical novices to educate their auditory curiosity by recording, manipulating and mixing sounds of everyday life. The environment consists of three stands: A stand for sound recording with a soundproof box that ensure good recording facilities in a noisy environment; a stand for sound manipulation with five simple, tangible interfaces; a stand for sound mixing with a graphical computer interface presented on two touch screens. }
}

@inproceedings{Hamilton2006,
  author = {Hamilton, Robert},
  title = {Bioinformatic Feedback: Performer Bio-data as a Driver for Real-time Composition},
  pages = {338--341},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176919},
  url = {http://www.nime.org/proceedings/2006/nime2006_338.pdf},
  keywords = {Bioinformatics, composition, real-time score generation. }
}

@inproceedings{Pak2006,
  author = {Pak, Jonathan},
  title = {The Light Matrix: An Interface for Musical Expression and Performance},
  pages = {342--345},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176983},
  url = {http://www.nime.org/proceedings/2006/nime2006_342.pdf}
}

@inproceedings{Kobayashi2006,
  author = {Kobayashi, Shigeru and Endo, Takanori and Harada, Katsuhiko and Oishi, Shosei},
  title = {GAINER: A Reconfigurable {I/O} Module and Software Libraries for Education},
  pages = {346--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176945},
  url = {http://www.nime.org/proceedings/2006/nime2006_346.pdf},
  keywords = {learning,rapid prototyping,reconfigurable,sensor interface}
}

@inproceedings{Beilharz2006,
  author = {Beilharz, Kirsty and Jakovich, Joanne and Ferguson, Sam},
  title = {Hyper-shaku (Border-crossing): Towards the Multi-modal Gesture-controlled Hyper-Instrument},
  pages = {352--357},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176867},
  url = {http://www.nime.org/proceedings/2006/nime2006_352.pdf},
  keywords = {Gesture-controllers, sonification, hyper-instrument },
  abstract = {Hyper-shaku (Border-Crossing) is an interactive sensor environment that uses motion sensors to trigger immediate responses and generative processes augmenting the Japanese bamboo shakuhachi in both the auditory and visual domain. The latter differentiates this process from many hyper-instruments by building a performance of visual design as well as electronic music on top of the acoustic performance. It utilizes a combination of computer vision and wireless sensing technologies conflated from preceding works. This paper outlines the use of gesture in these preparatory sound and audio-visual performative, installation and sonification works, leading to a description of the Hyper-shaku environment integrating sonification and generative elements. }
}

@inproceedings{Farwell2006,
  author = {Farwell, Neal},
  title = {Adapting the Trombone: a Suite of Electro-acoustic Interventions for the Piece},
  pages = {358--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176895},
  url = {http://www.nime.org/proceedings/2006/nime2006_358.pdf},
  keywords = {composition,electro-acoustic adaptation,emulation,illusion,improvisation,mapping,mute,trombone,ultrasonic},
  abstract = {Three electro-acoustic systems were devised for a newtrombone work, Rouse. This paper presents the technicalsystems and outlines their musical context and motivation. TheuSlide measures trombone slide-extension by a minimalhardware ultrasonic technique. An easy calibration proceduremaps linear extension to the slide "positions" of the player. TheeMouth is a driver that replaces the mouthpiece, with softwareemulation of trombone tone and algorithmic musical lines,allowing the trombone to appear to play itself. The eMute isbuilt around a loudspeaker unit, driven so that it affects stronglythe player's embouchure, allowing fine control of complex beatpatterns. eMouth and eMute, under control of the uSlide, set upimprovisatory worlds that are part of the composed architectureof Rouse.}
}

@inproceedings{MakiPatola2006,
  author = {Maki-Patola, Teemu and H\''{a}m\''{a}l\''{a}inen, Perttu and Kanerva, Aki},
  title = {The Augmented Djembe Drum --- Sculpting Rhythms},
  pages = {364--369},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176971},
  url = {http://www.nime.org/proceedings/2006/nime2006_364.pdf},
  keywords = {1,2,2 9,3897,39,425,43,7,8,9}
}

@inproceedings{Favilla2006,
  author = {Favilla, Stuart and Cannon, Joanne},
  title = {Children of Grainger: Leather Instruments for Free Music},
  pages = {370--375},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176897},
  url = {http://www.nime.org/proceedings/2006/nime2006_370.pdf}
}

@inproceedings{Hsu2006,
  author = {Hsu, William},
  title = {Managing Gesture and Timbre for Analysis and Instrument Control in an Interactive Environment},
  pages = {376--379},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176927},
  url = {http://www.nime.org/proceedings/2006/nime2006_376.pdf},
  keywords = {Interactive music systems, timbre analysis, instrument control. },
  abstract = {This paper describes recent enhancements in an interactive system designed to improvise with saxophonist John Butcher [1]. In addition to musical parameters such as pitch and loudness, our system is able to analyze timbral characteristics of the saxophone tone in real-time, and use timbral information to guide the generation of response material. We capture each saxophone gesture on the fly, extract a set of gestural and timbral contours, and store them in a repository. Improvising agents can consult the repository when generating responses. The gestural or timbral progression of a saxophone phrase can be remapped or transformed; this enables a variety of response material that also references audible contours of the original saxophone gestures. A single simple framework is used to manage gestural and timbral information extracted from analysis, and for expressive control of virtual instruments in a free improvisation context. }
}

@inproceedings{Hamel2006,
  author = {Hamel, Keith},
  title = {Integrated Interactive Music Performance Environment},
  pages = {380--383},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176917},
  url = {http://www.nime.org/proceedings/2006/nime2006_380.pdf}
}

@inproceedings{Ferguson2006,
  author = {Ferguson, Sam},
  title = {Learning Musical Instrument Skills Through Interactive Sonification},
  pages = {384--389},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176899},
  url = {http://www.nime.org/proceedings/2006/nime2006_384.pdf},
  keywords = {interactive sonification,music,sonification,sound visualization}
}

@inproceedings{Poepel2006,
  author = {Poepel, Cornelius and Overholt, Dan},
  title = {Recent Developments in Violin-related Digital Musical Instruments: Where Are We and Where Are We Going?},
  pages = {390--395},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176985},
  url = {http://www.nime.org/proceedings/2006/nime2006_390.pdf},
  keywords = {Violin, viola, cello, bass, digital, electronic, synthesis, controller. },
  abstract = {In this paper, some of the more recent developments in musical instruments related to the violin family are described, and analyzed according to several criteria adapted from other publications. While it is impossible to cover all such developments, we have tried to sample a variety of instruments from the last decade or so, with a greater focus on those published in the computer music literature. Experiences in the field of string players focusing on such developments are presented. Conclusions are drawn in which further research into violin-related digital instruments for string players may benefit from the presented criteria as well as the experiences. }
}

@inproceedings{Young2006,
  author = {Young, Diana and Nunn, Patrick and Vassiliev, Artem},
  title = {Composing for Hyperbow: A Collaboration Between {MIT} and the Royal Academy of Music},
  pages = {396--401},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177023},
  url = {http://www.nime.org/proceedings/2006/nime2006_396.pdf},
  keywords = {Cello, bow, controller, electroacoustic music, composition. },
  abstract = {In this paper we present progress of an ongoingcollaboration between researchers at the MIT MediaLaboratory and the Royal Academy of Music (RAM). The aimof this project is to further explore the expressive musicalpotential of the Hyperbow, a custom music controller firstdesigned for use in violin performance. Through the creationof new repertoire, we hope to stimulate the evolution of thisinterface, advancing its usability and refining itscapabilities. In preparation for this work, the Hyperbowsystem has been adapted for cello (acoustic and electric)performance. The structure of our collaboration is described,and two of the pieces currently in progress are presented.Feedback from the performers is also discussed, as well asfuture plans.}
}

@inproceedings{Bevilacqua2006,
  author = {Bevilacqua, Fr\'{e}d\'{e}ric and Rasamimanana, Nicolas and Fl\'{e}ty, Emmanuel and Lemouton, Serge and Baschet, Florence},
  title = {The Augmented Violin Project: Research, Composition and Performance Report},
  pages = {402--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176871},
  url = {http://www.nime.org/proceedings/2006/nime2006_402.pdf}
}

@inproceedings{Kimura2006,
  author = {Kimura, Mari and Risset, Jean-Claude},
  title = {Auditory Illusion and Violin: Demonstration of a Work by Jean-Claude Risset Written for Mari Kimura},
  pages = {407--408},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176939},
  url = {http://www.nime.org/proceedings/2006/nime2006_407.pdf},
  keywords = {Violin, psycho-acoustic phenomena, auditory illusions, sig- nal processing, subharmonics, Risset, Kimura. },
  abstract = {This is a description of a demonstration, regarding theuse of auditory illusions and psycho-acoustic phenomenonused in the interactive work of Jean-Claude Risset, writtenfor violinist Mari Kimura.}
}

@inproceedings{Freed2006a,
  author = {Freed, Adrian and Wessel, David and Zbyszynski, Michael and Uitti, Frances M.},
  title = {Augmenting the Cello},
  pages = {409--413},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2006},
  address = {Paris, France},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176905},
  url = {http://www.nime.org/proceedings/2006/nime2006_409.pdf},
  keywords = {Cello, chordophone, FSR, Rotary Absolute Position Encoder, Double Bowing, triple stops, double stops, convolution. },
  abstract = {Software and hardware enhancements to an electric 6-stringcello are described with a focus on a new mechanical tuningdevice, a novel rotary sensor for bow interaction and controlstrategies to leverage a suite of polyphonic soundprocessing effects.}
}

