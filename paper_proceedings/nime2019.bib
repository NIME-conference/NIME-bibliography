@inproceedings{Tomas2019,
  author = {Enrique Tomas and Thomas Gorbach and Hilda Tellioglu and Martin Kaltenbrunner},
  title = {Material embodiments of electroacoustic music: an experimental workshop study},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672842},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper001.pdf},
  abstract = {This paper reports on a workshop where participants produced physical mock-ups of musical interfaces directly after miming control of short electroacoustic music pieces. Our goal was understanding how people envision and materialize their own sound-producing gestures from spontaneous cognitive mappings. During the workshop, 50 participants from different creative backgrounds modeled more than 180 physical artifacts. Participants were filmed and interviewed for the later analysis of their different multimodal associations about music. Our initial hypothesis was that most of the physical mock-ups would be similar to the sound-producing objects that participants would identify in the musical pieces. Although the majority of artifacts clearly showed correlated design trajectories, our results indicate that a relevant number of participants intuitively decided to engineer alternative solutions emphasizing their personal design preferences. Therefore, in this paper we present and discuss the workshop format, its results and the possible applications for designing new musical interfaces.}
}

@inproceedings{Lu2019,
  author = {Yupu Lu and Yijie Wu and Shijie Zhu},
  title = {Collaborative Musical Performances with Automatic Harp Based on Image Recognition and Force Sensing Resistors},
  pages = {7--8},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672846},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper002.pdf},
  abstract = {In this paper, collaborative performance is defined as the performance of the piano by the performer and accompanied by an automatic harp. The automatic harp can play music based on the electronic score and change its speed according to the speed of the performer. We built a 32-channel automatic harp and designed a layered modular framework integrating both hardware and software, for experimental real-time control protocols. Considering that MIDI keyboard lacking information of force (acceleration) and fingering detection, both of which are important for expression, we designed force-sensor glove and achieved basic image recognition. They are used to accurately detect speed, force (corresponding to velocity in MIDI) and pitch when a performer plays the piano.}
}

@inproceedings{Arbel2019,
  author = {Lior Arbel and Yoav Y. Schechner and Noam Amir},
  title = {The Symbaline --- An Active Wine Glass Instrument with a Liquid Sloshing Vibrato Mechanism},
  pages = {9--14},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672848},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper003.pdf},
  abstract = {The Symbaline is an active instrument comprised of several partly-filled wine glasses excited by electromagnetic coils. This work describes an electromechanical system for incorporating frequency and amplitude modulation to the Symbaline's sound. A pendulum having a magnetic bob is suspended inside the liquid in the wine glass. The pendulum is put into oscillation by driving infra-sound signals through the coil. The pendulum's movement causes the liquid in the glass to slosh back and forth. Simultaneously, wine glass sounds are produced by driving audio-range signals through the coil, inducing vibrations in a small magnet attached to the glass surface and exciting glass vibrations. As the glass vibrates, the sloshing liquid periodically changes the glass's resonance frequencies and dampens the glass, thus modulating both wine glass pitch and sound intensity.}
}

@inproceedings{de-Souza-Nunes2019,
  author = {Helena de Souza Nunes and Federico Visi and Lydia Helena Wohl Coelho and Rodrigo Schramm},
  title = {SIBILIM: A low-cost customizable wireless musical interface},
  pages = {15--20},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672850},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper004.pdf},
  abstract = {This paper presents the SIBILIM, a low-cost musical interface composed of a resonance box made of cardboard containing customised push buttons that interact with a smartphone through its video camera. Each button can be mapped to a set of MIDI notes or control parameters. The sound is generated through synthesis or sample playback and can be amplified with the help of a transducer, which excites the resonance box. An essential contribution of this interface is the possibility of reconfiguration of the buttons layout without the need to hard rewire the system since it uses only the smartphone built-in camera. This features allow for quick instrument customisation for different use cases,
such as low cost projects for schools or instrument building  workshops. Our case study used the Sibilim for music education, where it was designed to develop the conscious of music perception and to stimulate creativity through exercises of short tonal musical compositions. We conducted a study with a group of twelve participants in an experimental workshop to verify its validity.}
}

@inproceedings{Bell2019,
  author = {Jonathan Bell},
  title = {The Risset Cycle, Recent Use Cases With SmartVox},
  pages = {21--24},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672852},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper005.pdf},
  abstract = {The combination of graphic/animated scores, acoustic signals (audio-scores) and Head-Mounted Display (HMD) technology offers promising potentials in the context of distributed notation, for live performances and concerts involving voices, instruments and electronics. After an explanation of what SmartVox is technically, and how it is used by composers and performers, this paper explains why this form of technology-aided performance might help musicians for synchronization to an electronic tape and (spectral) tuning. Then, from an exploration of the concepts of distributed notation and networked music performances, it proposes solutions (in conjunction with INScore, BabelScores and the Decibel Score Player) seeking for the expansion of distributed notation practice to a wider community. It finally presents findings relative to the use of SmartVox with HMDs.}
}

@inproceedings{Wang2019,
  author = {Johnty Wang and Axel Mulder and Marcelo Wanderley},
  title = {Practical Considerations for {MIDI} over Bluetooth Low Energy as a Wireless Interface},
  pages = {25--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672854},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper006.pdf},
  abstract = {This paper documents the key issues of performance and compatibility working with Musical Instrument Digital Interface (MIDI) via Bluetooth Low Energy (BLE) as a wireless interface for sensor or controller data and inter-module communication in the context of building interactive digital systems. An overview of BLE MIDI is presented along with a comparison of the protocol from the perspective of theoretical limits and interoperability, showing its widespread compatibility across platforms compared with other alternatives. Then we perform three complementary tests on BLE MIDI and alternative interfaces using prototype and commercial devices, showing that BLE MIDI has comparable performance with the tested WiFi implementations, with end-to-end (sensor input to audio output) latencies of under 10ms under certain conditions. Overall, BLE MIDI is an ideal choice for controllers and sensor interfaces that are designed to work on a wide variety of platforms.}
}

@inproceedings{Ramchurn2019,
  author = {Richard Ramchurn and Juan Pablo Martinez-Avila and Sarah Martindale and Alan Chamberlain and Max L Wilson and Steve Benford},
  title = {Improvising a Live Score to an Interactive Brain-Controlled Film},
  pages = {31--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672856},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper007.pdf},
  abstract = {We report on the design and deployment of systems for the performance of live score accompaniment to an interactive movie by a Networked Musical Ensemble. In this case, the audio-visual content of the movie is selected in real time based on user input to a Brain-Computer Interface (BCI). Our system supports musical improvisation between human performers and automated systems responding to the BCI. We explore the performers' roles during two performances when these socio-technical systems were implemented, in terms of coordination, problem-solving, managing uncertainty and musical responses to system constraints. This allows us to consider how features of these systems and practices might be incorporated into a general tool, aimed at any musician, which could scale for use in different performance settings involving interactive media. }
}

@inproceedings{Tom2019,
  author = {Ajin Jiji Tom and Harish Jayanth Venkatesan and Ivan Franco and Marcelo Wanderley},
  title = {Rebuilding and Reinterpreting a Digital Musical Instrument --- The Sponge},
  pages = {37--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672858},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper008.pdf},
  abstract = {Although several Digital Musical Instruments (DMIs) have been presented at NIME, very few of them remain accessible to the community. Rebuilding a DMI is often a necessary step to allow for performance with NIMEs. Rebuilding a DMI exactly similar to its original, however, might not be possible due to technology obsolescence, lack of documentation or other reasons. It might then be interesting to re-interpret a DMI and build an instrument inspired by the original one, creating novel performance opportunities. This paper presents the challenges and approaches involved in rebuilding and re-interpreting an existing DMI, The Sponge by Martin Marier. The rebuilt versions make use of newer/improved technology and customized design aspects like addition of vibrotactile feedback and implementation of different mapping strategies. It also discusses the implications of embedding sound synthesis within the DMI, by using the Prynth framework and further presents a comparison between this approach and the more traditional ground-up approach. As a result of the evaluation and comparison of the two rebuilt DMIs, we present a third version which combines the benefits and discuss performance issues with these devices.}
}

@inproceedings{Nishida2019,
  author = {Kiyu Nishida and Akishige Yuguchi and kazuhiro jo and Paul Modler and Markus Noisternig},
  title = {Border: A Live Performance Based on Web {AR} and a Gesture-Controlled Virtual Instrument},
  pages = {43--46},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672860},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper009.pdf},
  abstract = {Recent technological advances, such as increased CPU/GPU processing speed, along with the miniaturization of devices and sensors, have created new possibilities for integrating immersive technologies in music and performance art. Virtual and Augmented Reality (VR/AR) have become increasingly interesting as mobile device platforms, such as up-to-date smartphones, with necessary CPU resources entered the consumer market. In combination with recent web technologies, any mobile device can simply connect with a browser to a local server to access the latest technology. The web platform also eases the integration of collaborative situated media in participatory artwork. In this paper, we present the interactive music improvisation piece ‘Border,' premiered in 2018 at the Beyond Festival at the Center for Art and Media Karlsruhe (ZKM). This piece explores the interaction between a performer and the audience using web-based applications – including AR, real-time 3D audio/video streaming, advanced web audio, and gesture-controlled virtual instruments – on smart mobile devices.}
}

@inproceedings{Dahlstedt2019,
  author = {Palle Dahlstedt},
  title = {Taming and Tickling the Beast --- Multi-Touch Keyboard as Interface for a Physically Modelled Interconnected Resonating Super-Harp},
  pages = {47--52},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672862},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper010.pdf},
  abstract = {Libration Perturbed is a performance and an improvisation instrument, originally composed and designed for a multi-speaker dome. The performer controls a bank of 64 virtual inter-connected resonating strings, with individual and direct control of tuning and resonance characteristics through a multitouch-enhanced klavier interface (TouchKeys). It is a hybrid acoustic-electronic instrument, as all string vibrations originate from physical vibrations in the klavier and its casing, captured through contact microphones. In addition, there are gestural strings, called ropes, excited by performed musical gestures. All strings and ropes are connected, and inter-resonate together as a ”super-harp”, internally and through the performance space. With strong resonance, strings may go into chaotic motion or emergent quasi-periodic patterns, but custom adaptive leveling mechanisms keep loudness under the musician's control at all times. The hybrid digital/acoustic approach and the enhanced keyboard provide for an expressive and very physical interaction, and a strong multi-channel immersive experience. The paper describes the aesthetic choices behind the design of the system, as well as the technical implementation, and – primarily – the interaction design, as it emerges from mapping, sound design, physical modeling and integration of the acoustic, the gestural, and the virtual. The work is evaluated based on the experiences from a series of performances.}
}

@inproceedings{Cavdir2019,
  author = {Doga Cavdir and Juan Sierra and Ge Wang},
  title = {Taptop, Armtop, Blowtop: Evolving the Physical Laptop Instrument},
  pages = {53--58},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672864},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper011.pdf},
  abstract = {This research represents an evolution and evaluation of the embodied physical laptop instruments. Specifically, these are instruments that are physical in that they use bodily interaction, take advantage of the physical affordances of the laptop. They are embodied in the sense that instruments are played in such ways where the sound is embedded to be close to the instrument. Three distinct laptop instruments, Taptop, Armtop, and Blowtop, are introduced in this paper. We discuss the integrity of the design process with composing for laptop instruments and performing with them. In this process, our aim is to blur the boundaries of the composer and designer/engineer roles. How the physicality is achieved by leveraging musical gestures gained through traditional instrument practice is studied, as well as those inspired by body gestures. We aim to explore how using such interaction methods affects the communication between the ensemble and the audience. An aesthetic-first qualitative evaluation of these interfaces is discussed, through works and performances crafted specifically for these instruments and presented in the concert setting of the laptop orchestra. In so doing, we reflect on how such physical, embodied instrument design practices can inform a different kind of expressive and performance mindset.}
}

@inproceedings{Gomez-Jauregui2019,
  author = {David Antonio Gómez Jáuregui and Irvin Dongo and Nadine Couture},
  title = {Automatic Recognition of Soundpainting for the Generation of Electronic Music Sounds},
  pages = {59--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672866},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper012.pdf},
  abstract = {This work aims to explore the use of a new gesture-based interaction built on automatic recognition of Soundpainting structured gestural language. In the proposed approach, a composer (called Soundpainter) performs Soundpainting gestures facing a Kinect sensor. Then, a gesture recognition system captures gestures that are sent to a sound generator software. The proposed method was used to stage an artistic show in which a Soundpainter had to improvise with 6 different gestures to generate a musical composition from different sounds in real time. The accuracy of the gesture recognition system was evaluated as well as Soundpainter's user experience. In addition, a user evaluation study for using our proposed system in a learning context was also conducted. Current results open up perspectives for the design of new artistic expressions based on the use of automatic gestural recognition supported by Soundpainting language.}
}

@inproceedings{Morreale2019,
  author = {Fabio Morreale and Andrea Guidi and Andrew P. McPherson},
  title = {Magpick: an Augmented Guitar Pick for Nuanced Control},
  pages = {65--70},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672868},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper013.pdf},
  abstract = {This paper introduces the Magpick, an augmented pick for electric guitar that uses electromagnetic induction to sense the motion of the pick with respect to the permanent magnets in the guitar pickup. The Magpick provides the guitarist with nuanced control of the sound which coexists with traditional plucking-hand technique. The paper presents three ways that the signal from the pick can modulate the guitar sound, followed by a case study of its use in which 11 guitarists tested the Magpick for five days and composed a piece with it. Reflecting on their comments and experiences, we outline the innovative features of this technology from the point of view of performance practice. In particular, compared to other augmentations, the high temporal resolution, low latency, and large dynamic range of the Magpick support a highly nuanced control over the sound. Our discussion highlights the utility of having the locus of augmentation coincide with the locus of interaction.}
}

@inproceedings{Petit2019,
  author = {Bertrand Petit and manuel serrano},
  title = {Composing and executing Interactive music using the HipHop.js language},
  pages = {71--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672870},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper014.pdf},
  abstract = {Skini is a platform for composing and producing live performances with audience participating using connected devices (smartphones, tablets, PC, etc.). The music composer creates beforehand musical elements such as melodic patterns, sound patterns, instruments, group of instruments, and a dynamic score that governs the way the basic elements will behave according to events produced by the audience. During the concert or the performance, the audience, by interacting with the system, gives birth to an original music composition. Skini music scores are expressed in terms of constraints that establish relationships between instruments. A constraint maybe instantaneous, for instance one may disable violins while trumpets are playing. A constraint may also be temporal, for instance, the piano cannot play more than 30 consecutive seconds. The Skini platform is implemented in Hop.js and HipHop.js. HipHop.js, a synchronous reactive DLS, is used for implementing the music scores as its elementary constructs consisting of high level operators such as parallel executions, sequences, awaits, synchronization points, etc, form an ideal core language for implementing Skini constraints. This paper presents the Skini platform. It reports about live performances and an educational project. It briefly overviews the use of HipHop.js for representing score.}
}

@inproceedings{Rocha2019,
  author = {Gabriel Lopes Rocha and João Teixera Araújo and Flávio Luiz Schiavoni},
  title = {Ha Dou Ken Music: Different mappings to play music with joysticks},
  pages = {77--78},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672872},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper015.pdf},
  abstract = {Due to video game controls great presence in popular culture and its ease of access, even people who are not in the habit of playing electronic games possibly interacted with this kind of interface once in a lifetime. Thus, gestures like pressing a sequence of buttons, pressing them simultaneously or sliding your fingers through the control can be mapped for musical creation. This work aims the elaboration of a strategy in which several gestures performed in a joystick control can influence one or several parameters of the sound synthesis, making a mapping denominated many to many. Buttons combinations used to perform game actions that are common in fighting games, like Street Fighter, were mapped to the synthesizer to create a music. Experiments show that this mapping is capable of influencing the musical expression of a DMI making it closer to an acoustic instrument.}
}

@inproceedings{Næss2019,
  author = {Torgrim Rudland Næss and Charles Patrick Martin},
  title = {A Physical Intelligent Instrument using Recurrent Neural Networks},
  pages = {79--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672874},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper016.pdf},
  abstract = {This paper describes a new intelligent interactive instrument, based on an embedded computing platform, where deep neural networks are applied to interactive music generation. Even though using neural networks for music composition is not uncommon, a lot of these models tend to not support any form of user interaction. We introduce a self-contained intelligent instrument using generative models, with support for real-time interaction where the user can adjust high-level parameters to modify the music generated by the instrument. We describe the technical details of our generative model and discuss the experience of using the system as part of musical performance.}
}

@inproceedings{Fraietta2019,
  author = {Angelo Fraietta},
  title = {Creating Order and Progress},
  pages = {83--88},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672876},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper017.pdf},
  abstract = {This paper details the mapping strategy of the work Order and Progress: a sonic segue across A Auriverde, a composition based upon the skyscape represented on the Brazilian flag. This work uses the Stellarium planetarium software as a performance interface, blending the political symbology, scientific data and musical mapping of each star represented on the flag as a multimedia performance. The work is interfaced through the  Stellar Command module, a Java based program that converts the visible field of view from the Stellarium planetarium interface to astronomical data through the VizieR database of astronomical catalogues. This scientific data is then mapped to musical parameters through a Java based programming environment. I will discuss the strategies employed to create a work that was not only artistically novel, but also visually engaging and scientifically accurate.}
}

@inproceedings{Tragtenberg2019,
  author = {João Nogueira Tragtenberg and Filipe Calegario and Giordano Cabral and Geber L. Ramalho},
  title = {Towards the Concept of Digital Dance and Music Instruments},
  pages = {89--94},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672878},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper018.pdf},
  abstract = {This paper discusses the creation of instruments in which music is intentionally generated by dance. We introduce the conceptual framework of Digital Dance and Music Instruments (DDMI). Several DDMI have already been created, but they have been developed isolatedly, and there is still a lack of a common process of ideation and development. Knowledge about Digital Musical Instruments (DMIs) and Interactive Dance Systems (IDSs) can contribute to the design of DDMI, but the former brings few contributions to the body's expressiveness, and the latter brings few references to an instrumental relationship with music. Because of those different premises, the integration between both paradigms can be an arduous task for the designer of DDMI. The conceptual framework of DDMI can also serve as a bridge between DMIs and IDSs, serving as a lingua franca between both communities and facilitating the exchange of knowledge. The conceptual framework has shown to be a promising analytical tool for the design, development, and evaluation of new digital dance and music instrument.}
}

@inproceedings{Bomba2019,
  author = {Maros Suran Bomba and Palle Dahlstedt},
  title = {Somacoustics: Interactive Body-as-Instrument},
  pages = {95--100},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672880},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper019.pdf},
  abstract = {Visitors interact with a blindfolded artist's body, the motions of which are tracked and translated into synthesized four-channel sound, surrounding the participants. Through social-physical and aural interactions, they play his instrument-body, in a mutual dance. Crucial for this work has been the motion-to-sound mapping design, and the investigations of bodily interaction with normal lay-people and with professional contact-improvisation dancers. The extra layer of social-physical interaction both constrains and inspires the participant-artist relation and the sonic exploration, and through this, his body is transformed into an instrument, and physical space is transformed into a sound-space. The project aims to explore the experience of interaction between human and technology and its impact on one's bodily perception and embodiment, as well as the relation between body and space, departing from a set of existing theories on embodiment. In the paper, its underlying aesthetics are described and discussed, as well as the sensitive motion research process behind it, and the technical implementation of the work. It is evaluated based on participant behavior and experiences and analysis of its premiere exhibition in 2018.}
}

@inproceedings{Turczan2019,
  author = {Nathan Turczan and Ajay Kapur},
  title = {The Scale Navigator: A System for Networked Algorithmic Harmony},
  pages = {101--104},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672882},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper020.pdf},
  abstract = {The Scale Navigator is a graphical interface implementation of Dmitri Tymoczko's scale network designed to help generate algorithmic harmony and harmonically synchronize performers in a laptop or electro-acoustic orchestra. The user manipulates the Scale Navigator to direct harmony on a chord-to-chord level and on a scale-to-scale level. In a live performance setting, the interface broadcasts control data, MIDI, and real-time notation to an ensemble of live electronic performers, sight-reading improvisers, and musical generative algorithms. }
}

@inproceedings{Lucas2019,
  author = {Alex Michael Lucas and Miguel Ortiz and Dr. Franziska Schroeder},
  title = {Bespoke Design for Inclusive Music: The Challenges of Evaluation},
  pages = {105--109},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672884},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper021.pdf},
  abstract = {In this paper, the authors describe the evaluation of a collection of bespoke knob cap designs intended to improve the ease in which a specific musician with dyskinetic cerebral palsy can operate rotary controls in a musical context. The authors highlight the importance of the performers perspective when using design as a means for overcoming access barriers to music. Also, while the authors were not able to find an ideal solution for the musician within the confines of this study, several useful observations on the process of evaluating bespoke assistive music technology are described; observations which may prove useful to digital musical instrument designers working within the field of inclusive music.}
}

@inproceedings{Xiao2019,
  author = {Xiao Xiao and Grégoire Locqueville and Christophe d'Alessandro and Boris Doval},
  title = {T-Voks: the Singing and Speaking Theremin},
  pages = {110--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672886},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper022.pdf},
  abstract = {T-Voks is an augmented theremin that controls Voks, a performative singing synthesizer. Originally developed for control with a graphic tablet interface, Voks allows for real-time pitch and time scaling, vocal effort modification and syllable sequencing for pre-recorded voice utterances. For T-Voks the theremin's frequency antenna modifies the output pitch of the target utterance while the amplitude antenna controls not only volume as usual but also voice quality and vocal effort. Syllabic sequencing is handled by an additional pressure sensor attached to the player's volume-control hand. This paper presents the system architecture of T-Voks, the preparation procedure for a song, playing gestures, and practice techniques, along with musical and poetic examples across four different languages and styles.}
}

@inproceedings{Brown2019,
  author = {Hunter Brown and spencer topel},
  title = {{DRMMR}: An Augmented Percussion Implement},
  pages = {116--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672888},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper023.pdf},
  abstract = {Recent developments in music technology have enabled novel timbres to be acoustically synthesized using various actuation and excitation methods. Utilizing recent work in nonlinear acoustic synthesis, we propose a transducer based augmented percussion implement entitled DRMMR. This design enables the user to sustain computer sequencer-like drum rolls at faster speeds while also enabling the user to achieve nonlinear acoustic synthesis effects. Our acoustic evaluation shows drum rolls executed by DRMMR easily exhibit greater levels of regularity, speed, and precision than comparable transducer and electromagnetic-based actuation methods. DRMMR's nonlinear acoustic synthesis functionality also presents possibilities for new kinds of sonic interactions on the surface of drum membranes.}
}

@inproceedings{Lepri2019,
  author = {Giacomo Lepri and Andrew P. McPherson},
  title = {Fictional instruments, real values: discovering musical backgrounds with non-functional prototypes},
  pages = {122--127},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672890},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper024.pdf},
  abstract = {The emergence of a new technology can be considered as the result of social, cultural and technical process. Instrument designs are particularly influenced by cultural and aesthetic values linked to the specific contexts and communities that produced them. In previous work, we ran a design fiction workshop in which musicians created non-functional instrument mockups. In the current paper, we report on an online survey in which music technologists were asked to speculate on the background of the musicians who designed particular instruments. Our results showed several cues for the interpretation of the artefacts' origins, including physical features, body-instrument interactions, use of language and references to established music practices and tools. Tacit musical and cultural values were also identified based on intuitive and holistic judgments. Our discussion highlights the importance of cultural awareness and context-dependent values on the design and use of interactive musical systems.}
}

@inproceedings{Dewey2019,
  author = {Christopher Dewey and Jonathan P. Wakefield},
  title = {Exploring the Container Metaphor for Equalisation Manipulation},
  pages = {128--129},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672892},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper025.pdf},
  abstract = {This paper presents the first stage in the design and evaluation of a novel container metaphor interface for equalisation control.  The prototype system harnesses the Pepper's Ghost illusion to project mid-air a holographic data visualisation of an audio track's long-term average and real-time frequency content as a deformable shape manipulated directly via hand gestures. The system uses HTML 5, JavaScript and the Web Audio API in conjunction with a Leap Motion controller and bespoke low budget projection system. During subjective evaluation users commented that the novel system was simpler and more intuitive to use than commercially established equalisation interface paradigms and most suited to creative, expressive and explorative equalisation tasks.}
}

@inproceedings{Hofmann2019,
  author = {Alex Hofmann and Vasileios Chatziioannou and Sebastian Schmutzhard and Gökberk Erdogan and Alexander Mayer},
  title = {The Half-Physler: An oscillating real-time interface to a tube resonator model},
  pages = {130--133},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672896},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper026.pdf},
  abstract = {Physics-based sound synthesis allows to shape the sound by modifying parameters that reference to real world properties of acoustic instruments. This paper presents a hybrid physical modeling single reed instrument, where a virtual tube is coupled to a real mouthpiece with a sensor-equipped clarinet reed. The tube model is provided as an opcode for Csound which is running on the low-latency embedded audio-platform Bela. An actuator is connected to the audio output and the sensor-reed signal is fed back into the input of Bela. The performer can control the coupling between reed and actuator, and is also provided with a 3D-printed slider/knob interface to change parameters of the tube model in real-time.}
}

@inproceedings{Bussigel2019,
  author = {Peter Bussigel and Stephan Moore and Scott Smallwood},
  title = {Reanimating the Readymade},
  pages = {134--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672898},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper027.pdf},
  abstract = {There is rich history of using found or “readymade” objects in music performances and sound installations. John Cage's Water Walk, Carolee Schneeman's Noise Bodies, and David Tudor's Rainforest all lean on both the sonic and cultural affordances of found objects. Today, composers and sound artists continue to look at the everyday, combining readymades with microcontrollers and homemade electronics and repurposing known interfaces for their latent sonic potential. This paper gives a historical overview of work at the intersection of music and the readymade and then describes three recent sound installations/performances by the authors that further explore this space. The emphasis is on processes involved in working with found objects--the complex, practical, and playful explorations into sound and material culture.}
}

@inproceedings{Zhang2019,
  author = {Yian Zhang and Yinmiao Li and Daniel Chin and Gus Xia},
  title = {Adaptive Multimodal Music Learning via Interactive Haptic Instrument},
  pages = {140--145},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672900},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper028.pdf},
  abstract = {Haptic interfaces have untapped the sense of touch to assist multimodal music learning. We have recently seen various improvements of interface design on tactile feedback and force guidance aiming to make instrument learning more effective. However, most interfaces are still quite static; they cannot yet sense the learning progress and adjust the tutoring strategy accordingly. To solve this problem, we contribute an adaptive haptic interface based on the latest design of haptic flute. We first adopted a clutch mechanism to enable the interface to turn on and off the haptic control flexibly in real time. The interactive tutor is then able to follow human performances and apply the “teacher force” only when the software instructs so. Finally, we incorporated the adaptive interface with a step-by-step dynamic learning strategy. Experimental results showed that dynamic learning dramatically outperforms static learning, which boosts the learning rate by 45.3% and shrinks the forgetting chance by 86%.}
}

@inproceedings{Sguiglia2019,
  author = {Fabián Sguiglia and Pauli Coton and Fernando Toth},
  title = {El mapa no es el territorio: Sensor mapping for audiovisual performances},
  pages = {146--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672902},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper029.pdf},
  abstract = {We present El mapa no es el territorio (MNT), a set of open source tools that facilitate the design of visual and musical mappings for interactive installations and performance pieces. MNT is being developed by a multidisciplinary group that explores gestural control of audio-visual environments and virtual instruments. Along with these tools, this paper will present two projects in which they were used -interactive installation Memorias Migrantes and stage performance Recorte de Jorge Cárdenas Cayendo-, showing how MNT allows us to develop collaborative artworks that articulate body movement and generative audiovisual systems, and how its current version was influenced by these successive implementations.}
}

@inproceedings{Yaremchuk2019,
  author = {Vanessa Yaremchuk and Carolina Brum Medeiros and Marcelo Wanderley},
  title = {Small Dynamic Neural Networks for Gesture Classification with The Rulers (a Digital Musical Instrument)},
  pages = {150--155},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672904},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper030.pdf},
  abstract = {The Rulers is a Digital Musical Instrument with 7 metal beams, each of which is fixed at one end. It uses infrared sensors, Hall sensors, and strain gauges to estimate deflection. These sensors each perform better or worse depending on the class of gesture the user is making, motivating sensor fusion practices. Residuals between Kalman filters and sensor output are calculated and used as input to a recurrent neural network which outputs a classification that determines which processing parameters and sensor measurements are employed. Multiple instances (30) of layer recurrent neural networks with a single hidden layer varying in size from 1 to 10 processing units were trained, and tested on previously unseen data. The best performing neural network has only 3 hidden units and has a sufficiently low error rate to be good candidate for gesture classification. This paper demonstrates that: dynamic networks out-perform feedforward networks for this type of gesture classification, a small network can handle a problem of this level of complexity, recurrent networks of this size are fast enough for real-time applications of this type, and the importance of training multiple instances of each network architecture and selecting the best performing one from within that set.}
}

@inproceedings{Dahlstedt-b2019,
  author = {Palle Dahlstedt and Ami Skånberg Dahlstedt},
  title = {OtoKin: Mapping for Sound Space Exploration through Dance Improvisation},
  pages = {156--161},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672906},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper031.pdf},
  abstract = {We present a work where a space of realtime synthesized sounds is explored through ear (Oto) and movement (Kinesis) by one or two dancers. Movement is tracked and mapped through extensive pre-processing to a high-dimensional acoustic space, using a many-to-many mapping, so that every small body movement matters. Designed for improvised exploration, it works as both performance and installation. Through this re-translation of bodily action, position, and posture into infinite-dimensional sound texture and timbre, the performers are invited to re-think and re-learn position and posture as sound, effort as gesture, and timbre as a bodily construction. The sound space can be shared by two people, with added modes of presence, proximity and interaction. The aesthetic background and technical implementation of the system are described, and the system is evaluated based on a number of performances, workshops and installation exhibits. Finally, the aesthetic and choreographic motivations behind the performance narrative are explained, and discussed in the light of the design of the sonification.}
}

@inproceedings{Wright2019,
  author = {Joe Wright and James Dooley},
  title = {On the Inclusivity of Constraint: Creative Appropriation in Instruments for Neurodiverse Children and Young People},
  pages = {162--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672908},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper032.pdf},
  abstract = {Taking inspiration from research into deliberately constrained musical technologies and the emergence of neurodiverse, child-led musical groups such as the Artism Ensemble, the interplay between design-constraints, inclusivity and appro- priation is explored. A small scale review covers systems from two prominent UK-based companies, and two itera- tions of a new prototype system that were developed in collaboration with a small group of young people on the autistic spectrum. Amongst these technologies, the aspects of musical experience that are made accessible differ with re- spect to the extent and nature of each system's constraints. It is argued that the design-constraints of the new prototype system facilitated the diverse playing styles and techniques observed during its development. Based on these obser- vations, we propose that deliberately constrained musical instruments may be one way of providing more opportuni- ties for the emergence of personal practices and preferences in neurodiverse groups of children and young people, and that this is a fitting subject for further research.}
}

@inproceedings{Almeida2019,
  author = {Isabela Corintha Almeida and Giordano Cabral and Professor Gilberto Bernardes Almeida},
  title = {{AMIGO}: An Assistive Musical Instrument to Engage, Create and Learn Music},
  pages = {168--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672910},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper033.pdf},
  abstract = {We present AMIGO, a real-time computer music system that assists novice users in the composition process through guided musical improvisation. The system consists of 1) a computational analysis-generation algorithm, which not only formalizes musical principles from examples, but also guides the user in selecting note sequences; 2) a MIDI keyboard controller with an integrated LED stripe, which provides visual feedback to the user; and 3) a real-time music notation, which displays the generated output. Ultimately, AMIGO allows the intuitive creation of new musical structures and the acquisition of Western music formalisms, such as musical notation.}
}

@inproceedings{Figueiró2019,
  author = {Cristiano Figueiró and Guilherme Soares and Bruno Rohde},
  title = {{ESMERIL} --- An interactive audio player and composition system for collaborative experimental music netlabels},
  pages = {170--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672912},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper034.pdf},
  abstract = {ESMERIL is an application developed for Android with a toolchain based on Puredata and OpenFrameworks (with Ofelia library). The application enables music creation in a specific expanded format: four separate mono tracks, each one able to manipulate up to eight audio samples per channel. It works also as a performance instrument that stimulates collaborative remixings from compositions of scored interaction gestures called “scenes”. The interface also aims to be a platform to exchange those sample packs as artistic releases, a format similar to the popular idea of an “album”, but prepared to those four channel packs of samples and scores of interaction. It uses an adaptive audio slicing mechanism and it is based on interaction design for multi-touch screen features. A timing sequencer enhances the interaction between pre-set sequences (the “scenes”) and screen manipulation scratching, expanding and moving graphic sound waves. This paper describes the graphical interface features, some development decisions up to now and perspectives to its continuity.}
}

@inproceedings{Weber2019,
  author = {Aline Weber and Lucas Nunes Alegre and Jim Torresen and Bruno C. da Silva},
  title = {Parameterized Melody Generation with Autoencoders and Temporally-Consistent Noise},
  pages = {174--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672914},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper035.pdf},
  abstract = {We introduce a machine learning technique to autonomously generate novel melodies that are variations of an arbitrary base melody. These are produced by a neural network that ensures that (with high probability) the melodic and rhythmic structure of the new melody is consistent with a given set of sample songs. We train a Variational Autoencoder network to identify a low-dimensional set of variables that allows for the compression and representation of sample songs. By perturbing these variables with Perlin Noise---a temporally-consistent parameterized noise function---it is possible to generate smoothly-changing novel melodies. We show that (1) by regulating the amount of noise, one can specify how much of the base song will be preserved; and (2) there is a direct correlation between the noise signal and the differences between the statistical properties of novel melodies and the original one. Users can interpret the controllable noise as a type of "creativity knob": the higher it is, the more leeway the network has to generate significantly different melodies. We present a physical prototype that allows musicians to use a keyboard to provide base melodies and to adjust the network's "creativity knobs" to regulate in real-time the process that proposes new melody ideas.}
}

@inproceedings{Tanaka2019,
  author = {Atau Tanaka and Di Donato, Balandino and Michael Zbyszynski and Geert Roks},
  title = {Designing Gestures for Continuous Sonic Interaction},
  pages = {180--185},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672916},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper036.pdf},
  abstract = {This paper presents a system that allows users to quickly try different ways to train neural networks and temporal modeling techniques to associate arm gestures with time varying sound. We created a software framework for this, and designed three interactive sounds and presented them to participants in a workshop based study. We build upon previous work in sound-tracing and mapping-by-demonstration to ask the participants to design gestures with which to perform the given sounds using a multimodal, inertial measurement (IMU) and muscle sensing (EMG) device. We presented the user with four techniques for associating sensor input to synthesizer parameter output. Two were classical techniques from the literature, and two proposed different ways to capture dynamic gesture in a neural network. These four techniques were: 1.) A Static Position regression training procedure, 2.) A Hidden Markov based temporal modeler, 3.) Whole Gesture capture to a neural network, and 4.) a Windowed method using the position-based procedure on the fly during the performance of a dynamic gesture. Our results show trade-offs between accurate, predictable reproduction of the source sounds and exploration of the gesture-sound space. Several of the users were attracted to our new windowed method for capturing gesture anchor points on the fly as training data for neural network based regression. This paper will be of interest to musicians interested in going from sound design to gesture design and offers a workflow for quickly trying different mapping-by-demonstration techniques.}
}

@inproceedings{Erdem2019,
  author = {Cagri Erdem and Katja Henriksen Schia and Jensenius, Alexander Refsum},
  title = {Vrengt: A Shared Body-Machine Instrument for Music-Dance Performance},
  pages = {186--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672918},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper037.pdf},
  abstract = {This paper describes the process of developing a shared instrument for music--dance performance, with a particular focus on exploring the boundaries between standstill vs motion, and silence vs sound. The piece Vrengt grew from the idea of enabling a true partnership between a musician and a dancer, developing an instrument that would allow for active co-performance. Using a participatory design approach, we worked with sonification as a tool for systematically exploring the dancer's bodily expressions. The exploration used a "spatiotemporal matrix", with a particular focus on sonic microinteraction. In the final performance, two Myo armbands were used for capturing muscle activity of the arm and leg of the dancer, together with a wireless headset microphone capturing the sound of breathing. In the paper we reflect on multi-user instrument paradigms, discuss our approach to creating a shared instrument using sonification as a tool for the sound design, and reflect on the performers' subjective evaluation of the instrument.   }
}

@inproceedings{Parke-Wolfe2019,
  author = {Samuel Thompson Parke-Wolfe and Hugo Scurto and Rebecca Fiebrink},
  title = {Sound Control: Supporting Custom Musical Interface Design for Children with Disabilities},
  pages = {192--197},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672920},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper038.pdf},
  abstract = {We have built a new software toolkit that enables music therapists and teachers to create custom digital musical interfaces for children with diverse disabilities. It was designed in collaboration with music therapists, teachers, and children. It uses interactive machine learning to create new sensor- and vision-based musical interfaces using demonstrations of actions and sound, making interface building fast and accessible to people without programming or engineering expertise. Interviews with two music therapy and education professionals who have used the software extensively illustrate how richly customised, sensor-based interfaces can be used in music therapy contexts; they also reveal how properties of input devices, music-making approaches, and mapping techniques can support a variety of interaction styles and therapy goals.}
}

@inproceedings{Hödl2019,
  author = {Oliver Hödl},
  title = {'Blending Dimensions' when Composing for {DMI} and Symphonic Orchestra},
  pages = {198--203},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672922},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper039.pdf},
  abstract = {With a new digital music instrument (DMI), the interface itself, the sound generation, the composition, and the performance are often closely related and even intrinsically linked with each other. Similarly, the instrument designer, composer, and performer are often the same person. The Academic Festival Overture is a new piece of music for the DMI Trombosonic and symphonic orchestra written by a composer who had no prior experience with the instrument. The piece underwent the phases of a composition competition, rehearsals, a music video production, and a public live performance. This whole process was evaluated reflecting on the experience of three involved key stakeholder: the composer, the conductor, and the instrument designer as performer. `Blending dimensions' of these stakeholder and decoupling the composition from the instrument designer inspired the newly involved composer to completely rethink the DMI's interaction and sound concept. Thus, to deliberately avoid an early collaboration between a DMI designer and a composer bears the potential for new inspiration and at the same time the challenge to seek such a collaboration in the need of clarifying possible misunderstandings and improvement.}
}

@inproceedings{haki2019,
  author = {behzad haki and Sergi Jorda},
  title = {A Bassline Generation System Based on Sequence-to-Sequence Learning},
  pages = {204--209},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672928},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper040.pdf},
  abstract = {This paper presents a detailed explanation of a system generating basslines that are stylistically and rhythmically interlocked with a provided audio drum loop. The proposed system is based on a natural language processing technique: word-based sequence-to-sequence learning using LSTM units. The novelty of the proposed method lies in the fact that the system is not reliant on a voice-by-voice transcription of drums; instead, in this method, a drum representation is used as an input sequence from which a translated bassline is obtained at the output. The drum representation consists of fixed size sequences of onsets detected from a 2-bar audio drum loop in eight different frequency bands. The basslines generated by this method consist of pitched notes with different duration. The proposed system was trained on two distinct datasets compiled for this project by the authors. Each dataset contains a variety of 2-bar drum loops with annotated basslines from two different styles of dance music: House and Soca. A listening experiment designed based on the system revealed that the proposed system is capable of generating basslines that are interesting and are well rhythmically interlocked with the drum loops from which they were generated.}
}

@inproceedings{May2019,
  author = {Lloyd May and spencer topel},
  title = {{BLIKSEM}: An Acoustic Synthesis Fuzz Pedal},
  pages = {210--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672930},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper041.pdf},
  abstract = {This paper presents a novel physical fuzz pedal effect system named BLIKSEM. Our approach applies previous work in nonlinear acoustic synthesis via a driven cantilever soundboard configuration for the purpose of generating fuzz pedal-like effects as well as a variety of novel audio effects. Following a presentation of our pedal design, we compare the performance of our system with various various classic and contemporary fuzz pedals using an electric guitar. Our results show that BLIKSEM is capable of generating signals that approximate the timbre and dynamic behaviors of conventional fuzz pedals, as well as offer new mechanisms for expressive interactions and a range of new effects in different configurations.}
}

@inproceedings{Xambó2019,
  author = {Anna Xambó and Sigurd Saue and Alexander Refsum Jensenius and Robin Støckert and Oeyvind Brandtsegg},
  title = {{NIME} Prototyping in Teams: A Participatory Approach to Teaching Physical Computing},
  pages = {216--221},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672932},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper042.pdf},
  abstract = {In this paper, we present a workshop of physical computing applied to NIME design based on science, technology, engineering, arts, and mathematics (STEAM) education. The workshop is designed for master students with multidisciplinary backgrounds. They are encouraged to work in teams from two university campuses remotely connected through a portal space. The components of the workshop are prototyping, music improvisation and reflective practice. We report the results of this course, which show a positive impact on the students' confidence in prototyping and intention to continue in STEM fields. We also present the challenges and lessons learned on how to improve the teaching of hybrid technologies and programming skills in an interdisciplinary context across two locations, with the aim of satisfying both beginners and experts. We conclude with a broader discussion on how these new pedagogical perspectives can improve NIME-related courses.}
}

@inproceedings{Meneses2019,
  author = {Eduardo Meneses and Johnty Wang and Sergio Freire and Marcelo Wanderley},
  title = {A Comparison of Open-Source Linux Frameworks for an Augmented Musical Instrument Implementation},
  pages = {222--227},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672934},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper043.pdf},
  abstract = {The increasing availability of accessible sensor technologies, single board computers, and prototyping platforms have resulted in a growing number of frameworks explicitly geared towards the design and construction of Digital and Augmented Musical Instruments. Developing such instruments can be facilitated by choosing the most suitable framework for each project. In the process of selecting a framework for implementing an augmented guitar instrument, we have tested three Linux-based open-source platforms that have been designed for real-time sensor interfacing, audio processing, and synthesis. Factors such as acquisition latency, workload measurements, documentation, and software implementation are compared and discussed to determine the suitability of each environment for our particular project.}
}

@inproceedings{Matus-Lerner2019,
  author = {Lerner, Martin Matus},
  title = {Latin American {NIME}s: Electronic Musical Instruments and Experimental Sound Devices in the Twentieth Century},
  pages = {228--233},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672936},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper044.pdf},
  abstract = {During the twentieth century several Latin American nations (such as Argentina, Brazil, Chile, Cuba and Mexico) have originated relevant antecedents in the NIME field. Their innovative authors have interrelated musical composition, lutherie, electronics and computing. This paper provides a panoramic view of their original electronic instruments and experimental sound practices, as well as a perspective of them regarding other inventions around the World.}
}

@inproceedings{Reid2019,
  author = {Sarah Reid and Ryan Gaston and Ajay Kapur},
  title = {Perspectives on Time: performance practice, mapping strategies, \& composition with {MIGSI}},
  pages = {234--239},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672940},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper045.pdf},
  abstract = {This paper presents four years of development in performance and compositional practice on an electronically augmented trumpet called MIGSI. Discussion is focused on conceptual and technical approaches to data mapping, sonic interaction, and composition that are inspired by philosophical questions of time: what is now? Is time linear or multi-directional? Can we operate in multiple modes of temporal perception simultaneously? A number of mapping strategies are presented which explore these ideas through the manipulation of temporal separation between user input and sonic output. In addition to presenting technical progress, this paper will introduce a body of original repertoire composed for MIGSI, in order to illustrate how these tools and approaches have been utilized in live performance and how they may find use in other creative applications.}
}

@inproceedings{Lamounier2019,
  author = {Natacha Lamounier and Luiz Naveda and Adriana Bicalho},
  title = {The design of technological interfaces for interactions between music, dance and garment movements},
  pages = {240--245},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672942},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper046.pdf},
  abstract = {The present work explores the design of multimodal interfaces that capture hand gestures and promote interactions between dance, music and wearable technologic garment. We aim at studying the design strategies used to interface music to other domains of the performance, in special, the application of wearable technologies into music performances. The project describes the development of the music and wearable interfaces, which comprise a hand interface and a mechanical actuator attached to the dancer's dress. The performance resulted from the study is inspired in the butoh dances and attempts to add a technological poetic as music-dance-wearable interactions to the traditional dialogue between dance and music. }
}

@inproceedings{Alarcon-Diaz2019,
  author = {Ximena Alarcon Diaz and Victor Evaristo Gonzalez Sanchez and Cagri Erdem},
  title = {{INTIMAL}: Walking to Find Place, Breathing to Feel Presence},
  pages = {246--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672944},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper047.pdf},
  abstract = {INTIMAL is a physical virtual embodied system for relational listening that integrates body movement, oral archives, and voice expression through telematic improvisatory performance in migratory contexts. It has been informed by nine Colombian migrant women who express their migratory journeys through free body movement, voice and spoken word improvisation. These improvisations have been recorded using Motion Capture, in order to develop interfaces for co-located and telematic interactions for the sharing of narratives of migration. In this paper, using data from the Motion Capture experiments, we are exploring two specific movements from improvisers: displacements on space (walking, rotating), and breathing data. Here we envision how co-relations between walking and breathing, might be further studied to implement interfaces that help the making of connections between place, and the feeling of presence for people in-between distant locations.}
}

@inproceedings{Sardana2019,
  author = {Disha Sardana and Woohun Joo and Ivica Ico Bukvic and Greg Earle},
  title = {Introducing Locus: a {NIME} for Immersive Exocentric Aural Environments},
  pages = {250--255},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672946},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper048.pdf},
  abstract = {Locus is a NIME designed specifically for an interactive, immersive high density loudspeaker array environment. The system is based on a pointing mechanism to interact with a sound scene comprising 128 speakers. Users can point anywhere to interact with the system, and the spatial interaction utilizes motion capture, so it does not require a screen. Instead, it is completely controlled via hand gestures using a glove that is populated with motion-tracking markers.

The main purpose of this system is to offer intuitive physical interaction with the perimeter-based spatial sound sources. Further, its goal is to minimize user-worn technology and thereby enhance freedom of motion by utilizing environmental sensing devices, such as motion capture cameras or infrared sensors. The ensuing creativity enabling technology is applicable to a broad array of possible scenarios, from researching limits of human spatial hearing perception to facilitating learning and artistic performances, including dance. In this paper, we describe our NIME design and implementation, its preliminary assessment, and offer a Unity-based toolkit to facilitate its broader deployment and adoption.}
}

@inproceedings{Ho2019,
  author = {Echo Ho and Prof. Dr. Phil. Alberto de Campo and Hannes Hoelzl},
  title = {The SlowQin: An Interdisciplinary Approach to reinventing the Guqin},
  pages = {256--259},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672948},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper049.pdf},
  abstract = {This paper presents an ongoing process of examining and reinventing the Guqin, to forge a contemporary engagement with this unique traditional Chinese string instrument. The SlowQin is both a hybrid resemblance of the Guqin and a fully functioning wireless interface to interact with computer software. It has been developed and performed during the last decade. Instead of aiming for virtuosic perfection of playing the instrument, SlowQin emphasizes the openness for continuously rethinking and reinventing the Guqin's possibilities. Through a combination of conceptual work and practical production, Echo Ho's SlowQin project works as an experimental twist on Historically Informed Performance, with the motivation of conveying artistic gestures that tackle philosophical, ideological, and socio-political subjects embedded in our living environment in globalised conditions. In particular, this paper touches the history of the Guqin, gives an overview of the technical design concepts of the instrument, and discusses the aesthetical approaches of the SlowQin performances that have been realised so far.}
}

@inproceedings{Martin2019,
  author = {Charles Patrick Martin and Jim Torresen},
  title = {An Interactive Musical Prediction System with Mixture Density Recurrent Neural Networks},
  pages = {260--265},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672952},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper050.pdf},
  abstract = {This paper is about creating digital musical instruments where a predictive neural network model is integrated into the interactive system. Rather than predicting symbolic music (e.g., MIDI notes), we suggest that predicting future control data from the user and precise temporal information can lead to new and interesting interactive possibilities. We propose that a mixture density recurrent neural network (MDRNN) is an appropriate model for this task. The predictions can be used to fill-in control data when the user stops performing, or as a kind of filter on the user's own input. We present an interactive MDRNN prediction server that allows rapid prototyping of new NIMEs featuring predictive musical interaction by recording datasets, training MDRNN models, and experimenting with interaction modes. We illustrate our system with several example NIMEs applying this idea. Our evaluation shows that real-time predictive interaction is viable even on single-board computers and that small models are appropriate for small datasets.}
}

@inproceedings{Bazoge2019,
  author = {Nicolas Bazoge and Ronan Gaugne and Florian Nouviale and Valérie Gouranton and Bruno Bossis},
  title = {Expressive potentials of motion capture in musical performance},
  pages = {266--271},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672954},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper051.pdf},
  abstract = {The paper presents the electronic music performance project Vis Insita implementing the design of experimental instrumental interfaces based on optical motion capture technology with passive infrared markers (MoCap), and the analysis of their use in a real scenic presentation context. Because of MoCap's predisposition to capture the movements of the body, a lot of research and musical applications in the performing arts concern dance or the sonification of gesture. For our research, we wanted to move away from the capture of the human body to analyse the possibilities of a kinetic object handled by a performer, both in terms of musical expression, but also in the broader context of a multimodal scenic interpretation.}
}

@inproceedings{Van-Troyer2019,
  author = {Akito Van Troyer and Rebecca Kleinberger},
  title = {From Mondrian to Modular Synth: Rendering {NIME} using Generative Adversarial Networks},
  pages = {272--277},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672956},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper052.pdf},
  abstract = {This paper explores the potential of image-to-image translation techniques in aiding the design of new hardware-based musical interfaces such as MIDI keyboard, grid-based controller, drum machine, and analog modular synthesizers. We collected an extensive image database of such interfaces and implemented image-to-image translation techniques using variants of Generative Adversarial Networks. The created models learn the mapping between input and output images using a training set of either paired or unpaired images. We qualitatively assess the visual outcomes based on three image-to-image translation models: reconstructing interfaces from edge maps, and collection style transfers based on two image sets: visuals of mosaic tile patterns and geometric abstract two-dimensional arts. This paper aims to demonstrate that synthesizing interface layouts based on image-to-image translation techniques can yield insights for researchers, musicians, music technology industrial designers, and the broader NIME community.}
}

@inproceedings{Pardue2019,
  author = {Laurel Pardue and Kurijn Buys and Dan Overholt and Andrew P. McPherson and Michael Edinger},
  title = {Separating sound from source: sonic transformation of the violin through electrodynamic pickups and acoustic actuation},
  pages = {278--283},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672958},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper053.pdf},
  abstract = {When designing an augmented acoustic instrument, it is often of interest to retain an instrument's sound quality and nuanced response while leveraging the richness of digital synthesis.  Digital audio has traditionally been generated through speakers, separating sound generation from the instrument itself, or by adding an actuator within the instrument's resonating body, imparting new sounds along with the original.  We offer a third option, isolating the playing interface from the actuated resonating body, allowing us to rewrite the relationship between performance action and sound result while retaining the general form and feel of the acoustic instrument.  We present a hybrid acoustic-electronic violin based on a stick-body electric violin and an electrodynamic polyphonic pick-up capturing individual string displacements.  A conventional violin body acts as the resonator, actuated using digitally altered audio of the string inputs.  By attaching the electric violin above the body with acoustic isolation, we retain the physical playing experience of a normal violin along with some of the acoustic filtering and radiation of a traditional build.  We propose the use of the hybrid instrument with digitally automated pitch and tone correction to make an easy violin for use as a potential motivational tool for beginning violinists.}
}

@inproceedings{Advincula2019,
  author = {Gabriela Bila Advincula and Don Derek Haddad and Kent Larson},
  title = {Grain Prism: Hieroglyphic Interface for Granular Sampling},
  pages = {284--285},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672960},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper054.pdf},
  abstract = {This paper introduces the Grain Prism, a hybrid of a granular synthesizer and sampler that, through a capacitive sensing interface presented in obscure glyphs, invites users to create experimental sound textures with their own recorded voice. The capacitive sensing system, activated through skin contact over single glyphs or a combination of them, instigates the user to decipher the hidden sonic messages. The mysterious interface open space to aleatoricism in the act of conjuring sound, and therefore new discoveries. The users, when forced to abandon preconceived ways of playing a synthesizer, look at themselves in a different light, as their voice is the source material.}
}

@inproceedings{Bown2019,
  author = {Oliver Bown and Angelo Fraietta and Sam Ferguson and Lian Loke and Liam Bray},
  title = {Facilitating Creative Exploratory Search with Multiple Networked Audio Devices Using HappyBrackets},
  pages = {286--291},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672962},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper055.pdf},
  abstract = {We present an audio-focused creative coding toolkit for deploying music programs to remote networked devices. It is designed to support efficient creative exploratory search in the context of the Internet of Things (IoT), where one or more devices must be configured, programmed and interact over a network, with applications in digital musical instruments, networked music performance and other digital experiences. Users can easily monitor and hack what multiple devices are doing on the fly, enhancing their ability to perform ``exploratory search'' in a creative workflow. We present two creative case studies using the system: the creation of a dance performance and the creation of a distributed musical installation. Analysing different activities within the production process, with a particular focus on the trade-off between more creative exploratory tasks and more standard configuring and problem-solving tasks, we show how the system supports creative exploratory search for multiple networked devices. }
}

@inproceedings{Fernandes-Santos2019,
  author = {Thais Fernandes Santos},
  title = {The reciprocity between ancillary gesture and music structure performed by expert musicians},
  pages = {292--297},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672966},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper056.pdf},
  abstract = {During the musical performance, expert musicians consciously manipulate acoustical parameters expressing their interpretative choices. Also, players make physical motions and, in many cases, these gestures are related to the musicians' artistic intentions. However, it's not clear if the sound manipulation reflects in physical motions. The understanding of the musical structure of the work being performed in its many levels may impact the projection of artistic intentions, and performers alter it in micro and macro-sections, such as in musical motifs, phrases and sessions. Therefore, this paper investigates the timing manipulation and how such variations may reflect in physical gestures. The study involved musicians (flute, clarinet, and bassoon players) performing a unison excerpt by G. Rossini. We analyzed the relationship between timing variation (the Inter Onsets Interval deviations) and physical motion based on the traveled distance of the flute under different conditions. The flutists were asked to play the musical excerpt in three experimental conditions: (1) playing solo and playing in duets with previous recordings by other instrumentalists, (2) clarinetist and (3) bassoonist. The finding suggests that: 1) the movements, which seem to be related to the sense of pulse, are recurrent and stable, 2) the timing variability in micro or macro sections reflects in gestures' amplitude performed by flutists.}
}

@inproceedings{Paisa2019,
  author = {Razvan Paisa and Dan Overholt},
  title = {Enhancing the Expressivity of the Sensel Morph via Audio-rate Sensing},
  pages = {298--302},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672968},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper057.pdf},
  abstract = {This project describes a novel approach to hybrid electro-acoustical instruments by augmenting the Sensel Morph, with real-time audio sensing capabilities. The actual action-sounds are captured with a piezoelectric transducer and processed in Max 8 to extend the sonic range existing in the acoustical domain alone. The control parameters are captured by the Morph and mapped to audio algorithm proprieties like filter cutoff frequency, frequency shift or overdrive. The instrument opens up the possibility for a large selection of different interaction techniques that have a direct impact on the output sound. The instrument is evaluated from a sound designer's perspective, encouraging exploration in the materials used as well as techniques. The contribution are two-fold. First, the use of a piezo transducer to augment the Sensel Morph affords an extra dimension of control on top of the offerings. Second, the use of acoustic sounds from physical interactions as a source for excitation and manipulation of an audio processing system offers a large variety of new sounds to be discovered. The methodology involved an exploratory process of iterative instrument making, interspersed with observations gathered via improvisatory trials, focusing on the new interactions made possible through the fusion of audio-rate inputs with the Morph's default interaction methods.}
}

@inproceedings{Ramos2019,
  author = {Juan Mariano Ramos},
  title = {Eolos: a wireless {MIDI} wind controller},
  pages = {303--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672972},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper058.pdf},
  abstract = {This paper presents a description of the design and usage of Eolos, a wireless MIDI wind controller. The main goal of Eolos is to provide an interface that facilitates the production of music for any individual, regardless of their playing skills or previous musical knowledge. Its features are: open design, lower cost than commercial alternatives, wireless MIDI operation, rechargeable battery power, graphical user interface, tactile keys, sensitivity to air pressure, left-right reversible design and two FSR sensors. There is also a mention about its participation in the 1st Collaborative Concert over the Internet between Argentina and Cuba "Tradición y Nuevas Sonoridades".}
}

@inproceedings{Yang2019,
  author = {Ruihan Yang and Tianyao Chen and Yiyi Zhang and gus xia},
  title = {Inspecting and Interacting with Meaningful Music Representations using {VAE}},
  pages = {307--312},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672974},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper059.pdf},
  abstract = {Variational Autoencoder has already achieved great results on image generation  and recently made promising progress on music sequence generation. However, the model is still quite difficult to  control in the sense that the learned latent representations lack meaningful music semantics. What users really need is to interact with certain  music features, such as rhythm and pitch contour, in the creation process so that they can easily test different composition ideas. In this paper, we propose a disentanglement by augmentation method to inspect the pitch and rhythm interpretations of the latent representations. Based on the interpretable representations, an intuitive graphical user interface demo is designed for users to better direct the music creation process by manipulating the pitch contours and rhythmic complexity.}
}

@inproceedings{Roma2019,
  author = {Gerard Roma and Owen Green and Pierre Alexandre Tremblay},
  title = {Adaptive Mapping of Sound Collections for Data-driven Musical Interfaces},
  pages = {313--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672976},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper060.pdf},
  abstract = {Descriptor spaces have become an ubiquitous interaction paradigm for music based on collections of audio samples. However, most systems rely on a small predefined set of descriptors, which the user is often required to understand and choose from. There is no guarantee that the chosen descriptors are relevant for a given collection. In addition, this method does not scale to longer samples that require higher-dimensional descriptions, which biases systems towards the use of short samples. In this paper we propose novel framework for automatic creation of interactive sound spaces from sound collections using feature learning and dimensionality reduction. The framework is implemented as a software library using the SuperCollider language. We compare several algorithms and describe some example interfaces for interacting with the resulting spaces. Our experiments signal the potential of unsupervised algorithms for creating data-driven musical interfaces.}
}

@inproceedings{Norilo2019,
  author = {Vesa Petri Norilo},
  title = {Veneer: Visual and Touch-based Programming for Audio},
  pages = {319--324},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672978},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper061.pdf},
  abstract = {This paper presents Veneer, a visual, touch-ready programming interface for the Kronos programming language. The challenges of representing high-level data flow abstractions, including higher order functions, are described. The tension between abstraction and spontaneity in programming is addressed, and gradual abstraction in live programming is proposed as a potential solution. Several novel user interactions for patching on a touch device are shown. In addition, the paper describes some of the current issues of web audio music applications and offers strategies for integrating a web-based presentation layer with a low-latency native processing backend.}
}

@inproceedings{Faitas2019,
  author = {Andrei Faitas and Synne Engdahl Baumann and Torgrim Rudland Næss and Jim Torresen and Charles Patrick Martin},
  title = {Generating Convincing Harmony Parts with Simple Long Short-Term Memory Networks},
  pages = {325--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672980},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper062.pdf},
  abstract = {Generating convincing music via deep neural networks is a challenging problem that shows promise for many applications including interactive musical creation. One part of this challenge is the problem of generating convincing accompaniment parts to a given melody, as could be used in an automatic accompaniment system. Despite much progress in this area, systems that can automatically learn to generate interesting sounding, as well as harmonically plausible, accompanying melodies remain somewhat elusive. In this paper we explore the problem of sequence to sequence music generation where a human user provides a sequence of notes, and a neural network model responds with a harmonically suitable sequence of equal length. We consider two sequence-to-sequence models; one featuring standard unidirectional long short-term memory (LSTM) architecture, and the other featuring bidirectional LSTM, both successfully trained to produce a sequence based on the given input. Both of these are fairly dated models, as part of the investigation is to see what can be achieved with such models. These are evaluated and compared via a qualitative study that features 106 respondents listening to eight random samples from our set of generated music, as well as two human samples. From the results we see a preference for the sequences generated by the bidirectional model as well as an indication that these sequences sound more human.}
}

@inproceedings{Marasco2019,
  author = {Anthony T. Marasco and Edgar Berdahl and Jesse Allison},
  title = {{Bendit\_I/O}: A System for Networked Performance of Circuit-Bent Devices},
  pages = {331--334},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672982},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper063.pdf},
  abstract = {Bendit\_I/O is a system that allows for wireless, networked performance of circuit-bent devices, giving artists a new outlet for performing with repurposed technology. In a typical setup, a user pre-bends a device using the Bendit\_I/O board as an intermediary, replacing physical switches and potentiometers with the board's reed relays, motor driver, and digital potentiometer signals. Bendit\_I/O brings the networking techniques of distributed music performances to the hardware hacking realm, opening the door for creative implementation of multiple circuit-bent devices in audiovisual experiences. Consisting of a Wi-Fi- enabled I/O board and a Node-based server, the system provides performers with a variety of interaction and control possibilities between connected users and hacked devices. Moreover, it is user-friendly, low-cost, and modular, making it a flexible toolset for artists of diverse experience levels.}
}

@inproceedings{Macionis2019,
  author = {McLean J Macionis and Ajay Kapur},
  title = {Where Is The Quiet: Immersive Experience Design Using the Brain, Mechatronics, and Machine Learning},
  pages = {335--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672984},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper064.pdf},
  abstract = {'Where Is The Quiet?' is a mixed-media installation that utilizes immersive experience design, mechatronics, and machine learning in order to enhance wellness and increase connectivity to the natural world. Individuals interact with the installation by wearing a brainwave interface that measures the strength of the alpha wave signal. The interface then transmits the data to a computer that uses it in order to determine the individual's overall state of relaxation. As the individual achieves higher states of relaxation, mechatronic instruments respond and provide feedback. This feedback not only encourages self-awareness but also it motivates the individual to relax further. Visitors without the headset experience the installation by watching a film and listening to an original musical score. Through the novel arrangement of technologies and features, 'Where Is The Quiet?' demonstrates that mediated technological experiences are capable of evoking meditative states of consciousness, facilitating individual and group connectivity, and deepening awareness of the natural world. As such, this installation opens the door to future research regarding the possibility of immersive experiences supporting humanitarian needs.}
}

@inproceedings{Carson2019,
  author = {Tate Carson},
  title = {Mesh Garden: A creative-based musical game for participatory musical performance },
  pages = {339--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672986},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper065.pdf},
  abstract = {Mesh Garden explores participatory music-making with smart- phones using an audio sequencer game made up of a distributed smartphone speaker system. The piece allows a group of people in a relaxed situation to create a piece of ambient music using their smartphones networked through the internet. The players' interactions with the music are derived from the orientations of their phones. The work also has a gameplay aspect; if two players' phones match in orientation, one player has the option to take the other player's note, building up a bank of notes that will be used to form a melody.}
}

@inproceedings{Rossmy2019,
  author = {Beat Rossmy and Alexander Wiethoff},
  title = {The Modular Backward Evolution --- Why to Use Outdated Technologies},
  pages = {343--348},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672988},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper066.pdf},
  abstract = {In this paper we draw a picture that captures the increasing interest in the format of modular synthesizers today. We therefore provide a historical summary, which includes the origins, the fall and the rediscovery of that technology. Further an empirical analysis is performed based on statements given by artists and manufacturers taken from published interviews. These statements were aggregated, objectified and later reviewed by an expert group consisting of modular synthesizer vendors. Their responses provide the basis for the discussion on how emerging trends in synthesizer interface design reveal challenges and opportunities for the NIME community. }
}

@inproceedings{Goudard2019,
  author = {Vincent Goudard},
  title = {Ephemeral instruments},
  pages = {349--354},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672990},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper067.pdf},
  abstract = {This article questions the notion of ephemerality of digital musical instruments (DMI). Longevity is generally regarded as a valuable quality that good design criteria should help to achieve. However, the nature of the tools, of the performance conditions and of the music itself may lead to think of ephemerality as an intrinsic modality of the existence of DMIs. In particular, the conditions of contemporary musical production suggest that contextual adaptations of instrumental devices beyond the monolithic unity of classical instruments should be considered. The first two parts of this article analyse various reasons to reassess the issue of longevity and ephemerality. The last two sections attempt to propose an articulation of these two aspects to inform both the design of the DMI and their learning.}
}

@inproceedings{Jaramillo2019,
  author = {Julian Jaramillo and Fernando Iazzetta},
  title = {{PICO}: A portable audio effect box for traditional plucked-string instruments},
  pages = {355--360},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672992},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper068.pdf},
  abstract = {This paper reports the conception, design, implementation and evaluation processes of PICO, a portable audio effect system created with Pure Data and the Raspberry Pi, which augments traditional plucked string instruments such as the Brazilian Cavaquinho, the Venezuelan Cuatro, the Colombian Tiple and the Peruvian/Bolivian Charango. A fabric soft case fixed to the instrument`s body holds the PICO modules: the touchscreen, the single board computer, the sound card, the speaker system and the DC power bank. The device audio specifications arose from musicological insights about the social role of performers in their musical contexts and the instruments' playing techniques. They were taken as design challenges in the creation process of PICO`s first prototype, which was submitted to a short evaluation. Along with the construction of PICO, we reflected over the design of an interactive audio interface as a mode of research. Therefore, the paper will also discuss methodological aspects of audio hardware design.}
}

@inproceedings{Bertissolo2019,
  author = {Guilherme Bertissolo},
  title = {Composing Understandings: music, motion, gesture and embodied cognition},
  pages = {361--364},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672994},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper069.pdf},
  abstract = { This paper focuses on ongoing research in music composition based on   the study of cognitive research in musical meaning. As a method and result at the same time, we propose the creation of experiments related to key issues in composition and music cognition, such as music and movement, memory, expectation and metaphor in creative process. The theoretical reference approached is linked to the embodied cognition, with unfolding related to the cognitive semantics and the enactivist current of cognitive sciences, among other domains of contemporary sciences of mind and neuroscience. The experiments involve the relationship between music and movement, based on prior research using as a reference context in which it is not possible to establish a clear distinction between them: the Capoeira. Finally, we proposes a discussion about the application of the theoretical approach in two compositions: Boreal IV, for Steel Drums and real time electronics, and Converse, collaborative multimedia piece for piano, real-time audio (Puredata) and video processing (GEM and live video) and a dancer.}
}

@inproceedings{Ramos-Flores2019,
  author = {Cristohper Ramos Flores and Jim Murphy and Michael Norris},
  title = {HypeSax: Saxophone acoustic augmentation},
  pages = {365--370},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672996},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper070.pdf},
  abstract = {New interfaces allow performers to access new possibilities of musical expression. Even though interfaces are often designed to be adaptable to different software, most of them rely on external speakers or similar transducers. This often results on disembodiment and acoustic disengagement from the interface, and in the case of augmented instruments, from the instruments themselves. This paper describes a project in which a hybrid system allows an acoustic integration between the sound of acoustic saxophone and electronics.}
}

@inproceedings{Chwalek2019,
  author = {Patrick Chwalek and Joe Paradiso},
  title = {CD-Synth: a Rotating, Untethered, Digital Synthesizer},
  pages = {371--374},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672998},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper071.pdf},
  abstract = {We describe the design of an untethered digital synthesizer that can be held and manipulated while broadcasting audio data to a receiving off-the-shelf Bluetooth receiver. The synthesizer allows the user to freely rotate and reorient the instrument while exploiting non-contact light sensing for a truly expressive performance. The system consists of a suite of sensors that convert rotation, orientation, touch, and user proximity into various audio filters and effects operated on preset wave tables, while offering a persistence of vision display for input visualization. This paper discusses the  design of the system, including the circuit, mechanics, and software layout, as well as how this device may be incorporated into a performance. }
}

@inproceedings{Granieri2019,
  author = {Niccolò Granieri and James Dooley},
  title = {Reach: a keyboard-based gesture recognition system for live piano sound modulation},
  pages = {375--376},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673000},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper072.pdf},
  abstract = {This paper presents Reach, a keyboard-based gesture recog- nition system for live piano sound modulation. Reach is a system built using the Leap Motion Orion SDK, Pure Data and a custom C++ OSC mapper1. It provides control over the sound modulation of an acoustic piano using the pi- anist's ancillary gestures. The system was developed using an iterative design pro- cess, incorporating research findings from two user studies and several case studies. The results that emerged show the potential of recognising and utilising the pianist's existing technique when designing keyboard-based DMIs, reducing the requirement to learn additional techniques.}
}

@inproceedings{schedel2019,
  author = {margaret schedel and Jocelyn Ho and Matthew Blessing},
  title = {Women's Labor: Creating {NIME}s from Domestic Tools },
  pages = {377--380},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3672729},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper073.pdf},
  abstract = {This paper describes the creation of a NIME created from an iron and wooden ironing board. The ironing board acts as a resonator for the system which includes sensors embedded in the iron such as pressure, and piezo microphones. The iron has LEDs wired to the sides and  at either end of the board are CCDs; using machine learning we can identify what kind of fabric is being ironed, and the position of the iron along the x and y-axes as well as its rotation and tilt. This instrument is part of a larger project, Women's Labor, that juxtaposes traditional musical instruments such as spinets and virginals designated for “ladies” with new interfaces for musical expression that repurpose older tools of women's work. Using embedded technologies, we reimagine domestic tools as musical interfaces, creating expressive instruments from the appliances of women's chores.}
}

@inproceedings{Rauber-Du-Bois2019,
  author = {Andre Rauber Du Bois and Rodrigo Geraldo Ribeiro},
  title = {HMusic: A domain specific language for music programming and live coding},
  pages = {381--386},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673003},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper074.pdf},
  abstract = {This paper presents HMusic, a domain specific language based on music patterns that can be used to write music and live coding. The main abstractions provided by the language are patterns and tracks. Code written in HMusic looks like patterns and multi-tracks available in music sequencers and drum machines. HMusic provides primitives to design and compose patterns generating new patterns. The basic abstractions provided by the language have an inductive definition and HMusic is embedded in the Haskell functional programming language,  programmers can design functions to manipulate music on the fly.}
}

@inproceedings{Fraietta-b2019,
  author = {Angelo Fraietta},
  title = {Stellar Command: a planetarium software based cosmic performance interface},
  pages = {387--392},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673005},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper075.pdf},
  abstract = {This paper presents the use of Stellarium planetarium software coupled with the VizieR database of astronomical catalogues as an interface mechanism for creating astronomy based multimedia performances, and as a music composition interface. The celestial display from Stellarium is used to query VizieR, which then obtains scienti
c astronomical data from the stars displayed--including colour, celestial position, magnitude and distance--and sends it as input data for music composition or performance. Stellarium and VizieR are controlled through Stellar Command, a software library that couples the two systems and can be used as both a standalone command line utility using Open Sound Control, and as a software library.}
}

@inproceedings{Müller2019,
  author = {Patrick Müller and Johannes Michael Schuett},
  title = {Towards a Telematic Dimension Space},
  pages = {393--400},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673007},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper076.pdf},
  abstract = {Telematic performances connect two or more locations so that participants are able to interact in real time. Such practices blend a variety of dimensions, insofar as the representation of remote performers on a local stage intrinsically occurs on auditory, as well as visual and scenic, levels. Due to their multimodal nature, the analysis or creation of such performances can quickly descend into a house of mirrors wherein certain intensely interdependent dimensions come to the fore, while others are multiplied, seem hidden or are made invisible. In order to have a better understanding of such performances, Dimension Space Analysis, with its capacity to review multifaceted components of a system, can be applied to telematic performances, understood here as (a bundle of) NIMEs. In the second part of the paper, some telematic works from the practices of the authors are described with the toolset developed.}
}

@inproceedings{Lucas-b2019,
  author = {Pedro Pablo Lucas},
  title = {A {MIDI} Controller Mapper for the Built-in Audio Mixer in the Unity Game Engine},
  pages = {401--404},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673009},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper077.pdf},
  abstract = {Unity is one of the most used engines in the game industry and several extensions have been implemented to increase its features in order to create multimedia products in a more effective and efficient way. From the point of view of audio development, Unity has included an Audio Mixer from version 5 which facilitates the organization of sounds, effects, and the mixing process in general; however, this module can be manipulated only through its graphical interface. This work describes the design and implementation of an extension tool to map parameters from the Audio Mixer to MIDI external devices, like controllers with sliders and knobs, such way the developer can easily mix a game with the feeling of a physical interface. }
}

@inproceedings{Lucas-c2019,
  author = {Pedro Pablo Lucas},
  title = {AuSynthAR: A simple low-cost modular synthesizer based on Augmented Reality},
  pages = {405--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673011},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper078.pdf},
  abstract = {AuSynthAR is a digital instrument based on Augmented Reality (AR), which allows sound synthesis modules to create simple sound networks. It only requires a mobile device, a set of tokens, a sound output device and, optionally, a MIDI controller, which makes it an affordable instrument. An application running on the device generates the sounds and the graphical augmentations over the tokens.}
}

@inproceedings{Haddad2019,
  author = {Don Derek Haddad and Joe Paradiso},
  title = {The World Wide Web in an Analog Patchbay},
  pages = {407--410},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673013},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper079.pdf},
  abstract = {This paper introduces a versatile module for Eurorack synthesizers that allows multiple modular synthesizers to be patched together remotely through the world wide web. The module is configured from a read-eval-print-loop environment running in the web browser, that can be used to send signals to the modular synthesizer from a live coding interface or from various data sources on the internet.}
}

@inproceedings{Yoshimura2019,
  author = {Fou Yoshimura and kazuhiro jo},
  title = {A "voice" instrument based on vocal tract models by using soft material for a 3D printer and an electrolarynx},
  pages = {411--412},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673015},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper080.pdf},
  abstract = {In this paper, we propose a “voice” instrument based on vocal tract models with a soft material for a 3D printer and an electrolarynx. In our practice, we explore the incongruity of the voice instrument through the accompanying music production and performance. With the instrument, we aim to return to the fact that the “Machine speaks out.” With the production of a song “Vocalise (Incomplete),” and performances, we reveal how the instrument could work with the audiences and explore the uncultivated field of voices.}
}

@inproceedings{Yepez-Placencia2019,
  author = {Juan Pablo Yepez Placencia and Jim Murphy and Dale Carnegie},
  title = {Exploring Dynamic Variations for Expressive Mechatronic Chordophones},
  pages = {413--418},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673017},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper081.pdf},
  abstract = {Mechatronic chordophones have become increasingly common in mechatronic music. As expressive instruments, they offer multiple techniques to create and manipulate sounds using their actuation mechanisms. Chordophone designs have taken multiple forms, from frames that play a guitar-like instrument, to machines that integrate strings and actuators as part of their frame. However, few of these instruments have taken advantage of dynamics, which have been largely unexplored. This paper details the design and construction of a new picking mechanism prototype which enables expressive techniques through fast and precise movement and actuation. We have adopted iterative design and rapid prototyping strategies to develop and refine a compact picker capable of creating dynamic variations reliably. Finally, a quantitative evaluation process demonstrates that this system offers the speed and consistency of previously existing picking mechanisms, while providing increased control over musical dynamics and articulations.}
}

@inproceedings{Chauhan2019,
  author = {Dhruv Chauhan and Peter Bennett},
  title = {Searching for the Perfect Instrument: Increased Telepresence through Interactive Evolutionary Instrument Design},
  pages = {419--422},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673019},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper082.pdf},
  abstract = {In this paper, we introduce and explore a novel Virtual Reality musical interaction system (named REVOLVE) that utilises a user-guided evolutionary algorithm to personalise musical instruments to users' individual preferences. REVOLVE is designed towards being an `endlessly entertaining' experience through the potentially infinite number of sounds that can be produced. Our hypothesis is that using evolutionary algorithms with VR for musical interactions will lead to increased user telepresence. In addition to this, REVOLVE was designed to inform novel research into this unexplored area. Think aloud trials and thematic analysis revealed 5 main themes: control, comparison to the real world, immersion, general usability and limitations, in addition to practical improvements. Overall, it was found that this combination of technologies did improve telepresence levels, proving the original hypothesis correct.}
}

@inproceedings{Savery2019,
  author = {Richard J Savery and Benjamin Genchel and Jason Brent Smith and Anthony Caulkins and Molly E Jones and Anna Savery},
  title = {Learning from History: Recreating and Repurposing Harriet Padberg's Computer Composed Canon and Free Fugue},
  pages = {423--428},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673021},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper083.pdf},
  abstract = {Harriet Padberg wrote Computer-Composed Canon and Free Fugue as part of her 1964 dissertation in Mathematics and Music at Saint Louis University. This program is one of the earliest examples of text-to-music software and algorithmic composition, which are areas of great interest in the present-day field of music technology. This paper aims to analyze the technological innovation, aesthetic design process, and impact of Harriet Padberg's original 1964 thesis as well as the design of a modern recreation and utilization, in order to gain insight to the nature of revisiting older works. Here, we present our open source recreation of Padberg's program with a modern interface and, through its use as an artistic tool by three composers, show how historical works can be effectively used for new creative purposes in contemporary contexts. Not Even One by Molly Jones draws on the historical and social significance of Harriet Padberg through using her program in a piece about the lack of representation of women judges in composition competitions. Brevity by Anna Savery utilizes the original software design as a composition tool, and The Padberg Piano by Anthony Caulkins uses the melodic generation of the original to create a software instrument.}
}

@inproceedings{Berdahl2019,
  author = {Edgar Berdahl and Austin Franklin and Eric Sheffield},
  title = {A Spatially Distributed Vibrotactile Actuator Array for the Fingertips},
  pages = {429--430},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673023},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper084.pdf},
  abstract = {The design of a Spatially Distributed Vibrotactile Actuator Array (SDVAA) for the fingertips is presented.  It provides high-fidelity vibrotactile stimulation at the audio sampling rate.  Prior works are discussed, and the system is demonstrated using two music compositions by the authors.}
}

@inproceedings{Gregorio2019,
  author = {Jeff Gregorio and Youngmoo Kim},
  title = {Augmenting Parametric Synthesis with Learned Timbral Controllers},
  pages = {431--436},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673025},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper085.pdf},
  abstract = {Feature-based synthesis applies machine learning and signal processing methods to the development of alternative interfaces for controlling parametric synthesis algorithms. One approach, geared toward real-time control, uses low dimensional gestural controllers and learned mappings from control spaces to parameter spaces, making use of an intermediate latent timbre distribution, such that the control space affords a spatially-intuitive arrangement of sonic possibilities. Whereas many existing systems present alternatives to the traditional parametric interfaces, the proposed system explores ways in which feature-based synthesis can augment one-to-one parameter control, made possible by fully invertible mappings between control and parameter spaces.}
}

@inproceedings{Leigh2019,
  author = {Sang-won Leigh and Abhinandan Jain and Pattie Maes},
  title = {Exploring Human-Machine Synergy and Interaction on a Robotic Instrument},
  pages = {437--442},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673027},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper086.pdf},
  abstract = {This paper introduces studies conducted with musicians that aim to understand modes of human-robot interaction, situated between automation and human augmentation. Our robotic guitar system used for the study consists of various sound generating mechanisms, either driven by software or by a musician directly. The control mechanism allows the musician to have a varying degree of agency over the overall musical direction. We present interviews and discussions on open-ended experiments conducted with music students and musicians. The outcome of this research includes new modes of playing the guitar given the robotic capabilities, and an understanding of how automation can be integrated into instrument-playing processes. The results present insights into how a human-machine hybrid system can increase the efficacy of training or exploration, without compromising human engagement with a task.}
}

@inproceedings{Lee2019,
  author = {Sang Won Lee},
  title = {Show Them My Screen: Mirroring a Laptop Screen as an Expressive and Communicative Means in Computer Music},
  pages = {443--448},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673029},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper087.pdf},
  abstract = {Modern computer music performances often involve a musical instrument that is primarily digital; software runs on a computer, and the physical form of the instrument is the computer. In such a practice, the performance interface is rendered on a computer screen for the performer. There has been a concern in using a laptop as a musical instrument from the audience's perspective, in that having ``a laptop performer sitting behind the screen'' makes it difficult for the audience to understand how the performer is creating music.  Mirroring a computer screen on a projection screen has been one way to address the concern and reveal the performer's instrument. This paper introduces and discusses the author's computer music practice, in which a performer actively considers screen mirroring as an essential part of the performance, beyond visualization of music. In this case, screen mirroring is not complementary, but inevitable from the inception of the performance. The related works listed within explore various roles of screen mirroring in computer music performance and helps us understand empirical and logistical findings in such practices.}
}

@inproceedings{Davis2019,
  author = {Josh Urban Davis},
  title = {IllumiWear: A Fiber-Optic eTextile for MultiMedia Interactions},
  pages = {449--454},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Marcelo Queiroz and Anna Xambó Sedó},
  year = {2019},
  month = {June},
  publisher = {UFRGS},
  address = {Porto Alegre, Brazil},
  issn = {2220-4806},
  doi = {10.5281/zenodo.3673033},
  url = {http://www.nime.org/proceedings/2019/nime2019_paper088.pdf},
  abstract = {We present IllumiWear, a novel eTextile prototype that uses fiber optics as interactive input and visual output. Fiber optic cables are separated into bundles and then woven like a basket into a bendable glowing fabric. By equipping light emitting diodes to one side of these bundles and photodiode light intensity sensors to the other, loss of light intensity can be measured when the fabric is bent. The sensing technique of IllumiWear is not only able to discriminate between discreet touch, slight bends, and harsh bends, but also recover the location of deformation. In this way, our computational fabric prototype uses its intrinsic means of visual output (light) as a tool for interactive input. We provide design and implementation details for our prototype as well as a technical evaluation of its effectiveness and limitations as an interactive computational textile. In addition, we examine the potential of this prototype's interactive capabilities by extending our eTextile to create a tangible user interface for audio and visual manipulation.}
}

