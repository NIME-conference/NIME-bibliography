@inproceedings{Jones2007,
  author = {Jones, Randy and Schloss, Andrew},
  title = {Controlling a Physical Model with a {2D} Force Matrix},
  pages = {27--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177131},
  url = {http://www.nime.org/proceedings/2007/nime2007_027.pdf},
  keywords = {Physical modeling, instrument design, expressive control, multi-touch, performance },
  abstract = {Physical modeling has proven to be a successful method ofsynthesizing highly expressive sounds. However, providingdeep methods of real time musical control remains a majorchallenge. In this paper we describe our work towards aninstrument for percussion synthesis, in which a waveguidemesh is both excited and damped by a 2D matrix of forcesfrom a sensor. By emulating a drum skin both as controllerand sound generator, our instrument has reproduced someof the expressive qualities of hand drumming. Details of ourimplementation are discussed, as well as qualitative resultsand experience gleaned from live performances.}
}

@inproceedings{B2007,
  author = {Bottcher, Niels and Gelineck, Steven and Serafin, Stefania},
  title = {{PHY}SMISM : A Control Interface for Creative Exploration of Physical Models},
  pages = {31--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177051},
  url = {http://www.nime.org/proceedings/2007/nime2007_031.pdf},
  keywords = {Physical models, hybrid instruments, excitation, resonator. },
  abstract = {In this paper we describe the design and implementation of the PHYSMISM: an interface for exploring the possibilities for improving the creative use of physical modelling sound synthesis. The PHYSMISM is implemented in a software and hardware version. Moreover, four different physical modelling techniques are implemented, to explore the implications of using and combining different techniques. In order to evaluate the creative use of physical models, a test was performed using 11 experienced musicians as test subjects. Results show that the capability of combining the physical models and the use of a physical interface engaged the musicians in creative exploration of physical models.}
}

@inproceedings{Chuchacz2007,
  author = {Chuchacz, Katarzyna and O'Modhrain, Sile and Woods, Roger},
  title = {Physical Models and Musical Controllers -- Designing a Novel Electronic Percussion Instrument},
  pages = {37--40},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177071},
  url = {http://www.nime.org/proceedings/2007/nime2007_037.pdf},
  keywords = {Physical Model, Electronic Percussion Instrument, FPGA. },
  abstract = {A novel electronic percussion synthesizer prototype is presented. Our ambition is to design an instrument that will produce a high quality, realistic sound based on a physical modelling sound synthesis algorithm. This is achieved using a real-time Field Programmable Gate Array (FPGA) implementation of the model coupled to an interface that aims to make efficient use of all the subtle nuanced gestures of the instrumentalist. It is based on a complex physical model of the vibrating plate --- the source of sound in the majority of percussion instruments. A Xilinx Virtex II pro FPGA core handles the sound synthesis computations with an 8 billion operations per second performance and has been designed in such a way to allow a high level of control and flexibility. Strategies are also presented to that allow the parametric space of the model to be mapped to the playing gestures of the percussionist.}
}

@inproceedings{Wessel2007,
  author = {Wessel, David and Avizienis, Rimas and Freed, Adrian and Wright, Matthew},
  title = {A Force Sensitive Multi-Touch Array Supporting Multiple {2-D} Musical Control Structures},
  pages = {41--45},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179479},
  url = {http://www.nime.org/proceedings/2007/nime2007_041.pdf},
  keywords = {Pressure and force sensing, High-resolution gestural signals, Touchpad, VersaPad.},
  abstract = {We describe the design, implementation, and evaluation with musical applications of force sensitive multi-touch arrays of touchpads. Each of the touchpads supports a three dimensional representation of musical material: two spatial dimensions plus a force measurement we typically use to control dynamics. We have developed two pad systems, one with 24 pads and a second with 2 arrays of 16 pads each. We emphasize the treatment of gestures as sub-sampled audio signals. This tight coupling of gesture with audio provides for a high degree of control intimacy. Our experiments with the pad arrays demonstrate that we can efficiently deal with large numbers of audio encoded gesture channels – 72 for the 24 pad array and 96 for the two 16 pad arrays.
}
}

@inproceedings{Chang2007,
  author = {Chang, Angela and Ishii, Hiroshi},
  title = {Zstretch : A Stretchy Fabric Music Controller},
  pages = {46--49},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177067},
  url = {http://www.nime.org/proceedings/2007/nime2007_046.pdf},
  keywords = {Tangible interfaces, textiles, tactile design, musical expressivity },
  abstract = {FigureWe present Zstretch, a textile music controller that supports expressive haptic interactions. The musical controller takes advantage of the fabric's topological constraints to enable proportional control of musical parameters. This novel interface explores ways in which one might treat music as a sheet of cloth. This paper proposes an approach to engage simple technologies for supporting ordinary hand interactions. We show that this combination of basic technology with general tactile movements can result in an expressive musical interface. a}
}

@inproceedings{Kim2007,
  author = {Kim, Juno and Schiemer, Greg and Narushima, Terumi},
  title = {Oculog : Playing with Eye Movements},
  pages = {50--55},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177145},
  url = {http://www.nime.org/proceedings/2007/nime2007_050.pdf},
  keywords = {1,algorithmic composition,expressive control interfaces,eye movement recording,microtonal tuning,midi,nime07,pure data,video},
  abstract = {In this paper, we describe the musical development of a new system for performing electronic music where a video-based eye movement recording system, known as Oculog, is used to control sound. Its development is discussed against a background that includes a brief history of biologically based interfaces for performing music, together with a survey of various recording systems currently in use for monitoring eye movement in clinical applications. Oculog is discussed with specific reference to its implementation as a performance interface for electronic music. A new work features algorithms driven by eye movement response and allows the user to interact with audio synthesis and introduces new possibilities for microtonal performance. Discussion reflects an earlier technological paradigm and concludes by reviewing possibilities for future development.}
}

@inproceedings{Camurri2007,
  author = {Camurri, Antonio and Canepa, Corrado and Volpe, Gualtiero},
  title = {Active Listening to a Virtual Orchestra Through an Expressive Gestural Interface : The Orchestra Explorer},
  pages = {56--61},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177059},
  url = {http://www.nime.org/proceedings/2007/nime2007_056.pdf},
  keywords = {Active listening of music, expressive interfaces, full-body motion analysis and expressive gesture processing, multimodal interactive systems for music and performing arts applications. },
  abstract = {In this paper, we present a new system, the Orchestra Explorer, enabling a novel paradigm for active fruition of sound and music content. The Orchestra Explorer allows users to physically navigate inside a virtual orchestra, to actively explore the music piece the orchestra is playing, to modify and mold the sound and music content in real-time through their expressive full-body movement and gesture. An implementation of the Orchestra Explorer was developed and presented in the framework of the science exhibition {Cimenti di Invenzione e Armonia}, held at Casa Paganini, Genova, from October 2006 to January 2007. }
}

@inproceedings{Bell2007,
  author = {Bell, Bo and Kleban, Jim and Overholt, Dan and Putnam, Lance and Thompson, John and Morin-Kuchera, JoAnn},
  title = {The Multimodal Music Stand},
  pages = {62--65},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177039},
  url = {http://www.nime.org/proceedings/2007/nime2007_062.pdf},
  keywords = {Multimodal, interactivity, computer vision, e-field sensing, untethered control. },
  abstract = {We present the Multimodal Music Stand (MMMS) for the untethered sensing of performance gestures and the interactive control of music. Using e-field sensing, audio analysis, and computer vision, the MMMS captures a performer's continuous expressive gestures and robustly identifies discrete cues in a musical performance. Continuous and discrete gestures are sent to an interactive music system featuring custom designed software that performs real-time spectral transformation of audio. }
}

@inproceedings{Malloch2007,
  author = {Malloch, Joseph and Wanderley, Marcelo M.},
  title = {The T-Stick : From Musical Interface to Musical Instrument},
  pages = {66--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177175},
  url = {http://www.nime.org/proceedings/2007/nime2007_066.pdf},
  keywords = {gestural controller, digital musical instrument, families of instruments },
  abstract = {This paper describes the T-Stick, a new family of digitalmusical instruments. It presents the motivation behind theproject, hardware and software design, and presents insightsgained through collaboration with performers who have collectively practised and performed with the T-Stick for hundreds of hours, and with composers who have written piecesfor the instrument in the context of McGill University's Digital Orchestra project. Each of the T-Sticks is based on thesame general structure and sensing platform, but each alsodiffers from its siblings in size, weight, timbre and range.}
}

@inproceedings{Paine2007,
  author = {Paine, Garth and Stevenson, Ian and Pearce, Angela},
  title = {The Thummer Mapping Project (ThuMP)},
  pages = {70--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177217},
  url = {http://www.nime.org/proceedings/2007/nime2007_070.pdf},
  keywords = {Musical Instrument Design, Mapping, Musicianship, evaluation, testing. },
  abstract = {This paper presents the Thummer Mapping Project (ThuMP), an industry partnership project between ThumMotion P/L and The University of Western Sydney (UWS). ThuMP sought to developing mapping strategies for new interfaces for musical expression (NIME), specifically the ThummerTM, which provides thirteen simultaneous degrees of freedom. This research presents a new approach to the mapping problem resulting from a primary design research phase and a prototype testing and evaluation phase. In order to establish an underlying design approach for the ThummerTM mapping strategies, a number of interviews were carried out with high-level acoustic instrumental performers, the majority of whom play with the Sydney Symphony Orchestra, Sydney, Australia. Mapping strategies were developed from analysis of these interviews and then evaluated in trial usability testing.}
}

@inproceedings{dAlessandro2007,
  author = {d'Alessandro, Nicolas and Dutoit, Thierry},
  title = {HandSketch Bi-Manual Controller Investigation on Expressive Control Issues of an Augmented Tablet},
  pages = {78--81},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177027},
  url = {http://www.nime.org/proceedings/2007/nime2007_078.pdf},
  keywords = {Pen tablet, FSR, bi-manual gestural control. },
  abstract = {In this paper, we present a new bi-manual gestural controller, called HandSketch, composed of purchasable devices : pen tablet and pressure-sensing surfaces. It aims at achieving real-time manipulation of several continuous and articulated aspects of pitched sounds synthesis, with a focus on expressive voice. Both prefered and non-prefered hand issues are discussed. Concrete playing diagrams and mapping strategies are described. These results are integrated and a compact controller is proposed.}
}

@inproceedings{Takegawa2007,
  author = {Takegawa, Yoshinari and Terada, Tsutomu},
  title = {Mobile Clavier : New Music Keyboard for Flexible Key Transpose},
  pages = {82--87},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177255},
  url = {http://www.nime.org/proceedings/2007/nime2007_082.pdf},
  keywords = {Portable keyboard, Additional black keys, Diapason change },
  abstract = {Musical performers need to show off their virtuosity for selfexpression and communicate with other people. Therefore, they are prepared to perform at any time and anywhere. However, a musical keyboard of 88 keys is too large and too heavy to carry around. When a portable keyboard that is suitable for carrying around is played over a wide range, the notes being played frequently cause the diapason of the keyboard to protrude. It is common to use Key Transpose in conventional portable keyboards, which shifts the diapason of the keyboard. However, this function creates several problems such as the feeling of discomfort from the misalignment between the keying positions and their output sounds. Therefore, the goal of our study is to construct Mobile Clavier, which enables the diapason to be changed smoothly. Mobile Clavier resolves the problems with Key Transpose by having black keys inserted between any two side-by-side white keys. This paper also discusses how effective Mobile Clavier was in an experiment conducted using professional pianists. We can play music at any time and anywhere with Mobile Clavier.}
}

@inproceedings{Ojanen2007,
  author = {Ojanen, Mikko and Suominen, Jari and Kallio, Titti and Lassfolk, Kai},
  title = {Design Principles and User Interfaces of Erkki Kurenniemi's Electronic Musical Instruments of the 1960's and 1970's},
  pages = {88--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177211},
  url = {http://www.nime.org/proceedings/2007/nime2007_088.pdf},
  keywords = {Erkki Kurenniemi, Dimi, Synthesizer, Digital electronics, User interface design },
  abstract = {This paper presents a line of historic electronic musical instruments designed by Erkki Kurenniemi in the 1960's and1970's. Kurenniemi's instruments were influenced by digitallogic and an experimental attitude towards user interfacedesign. The paper presents an overview of Kurenniemi'sinstruments and a detailed description of selected devices.Emphasis is put on user interface issues such as unconventional interactive real-time control and programming methods.}
}

@inproceedings{Magnusson2007,
  author = {Magnusson, Thor and Mendieta, Enrike H.},
  title = {The Acoustic, the Digital and the Body : A Survey on Musical Instruments},
  pages = {94--99},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177171},
  url = {http://www.nime.org/proceedings/2007/nime2007_094.pdf},
  keywords = {Survey, musical instruments, usability, ergonomics, embodiment, mapping, affordances, constraints, instrumental entropy, audio programming. },
  abstract = {This paper reports on a survey conducted in the autumn of 2006 with the objective to understand people's relationship to their musical tools. The survey focused on the question of embodiment and its different modalities in the fields of acoustic and digital instruments. The questions of control, instrumental entropy, limitations and creativity were addressed in relation to people's activities of playing, creating or modifying their instruments. The approach used in the survey was phenomenological, i.e. we were concerned with the experience of playing, composing for and designing digital or acoustic instruments. At the time of analysis, we had 209 replies from musicians, composers, engineers, designers, artists and others interested in this topic. The survey was mainly aimed at instrumentalists and people who create their own instruments or compositions in flexible audio programming environments such as SuperCollider, Pure Data, ChucK, Max/MSP, CSound, etc. }
}

@inproceedings{Zbyszynski2007,
  author = {Zbyszynski, Michael and Wright, Matthew and Momeni, Ali and Cullen, Daniel},
  title = {Ten Years of Tablet Musical Interfaces at CNMAT},
  pages = {100--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179483},
  url = {http://www.nime.org/proceedings/2007/nime2007_100.pdf},
  keywords = {1,algorithmic composition,digitizing tablet,expressivity,gesture,mapping,nime07,position sensing,wacom tablet,why the wacom tablet},
  abstract = {We summarize a decade of musical projects and research employing Wacom digitizing tablets as musical controllers, discussing general implementation schemes using Max/MSP and OpenSoundControl, and specific implementations in musical improvisation, interactive sound installation, interactive multimedia performance, and as a compositional assistant. We examine two-handed sensing strategies and schemes for gestural mapping. }
}

@inproceedings{Gurevich2007,
  author = {Gurevich, Michael and Trevi\~{n}o, Jeffrey},
  title = {Expression and Its Discontents : Toward an Ecology of Musical Creation},
  pages = {106--111},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177107},
  url = {http://www.nime.org/proceedings/2007/nime2007_106.pdf},
  keywords = {Expression, expressivity, non-expressive, emotion, discipline, model, construct, discourse, aesthetic goal, experience, transparency, evaluation, communication },
  abstract = {We describe the prevailing model of musical expression, which assumes a binary formulation of "the text" and "the act", along with its implied roles of composer and performer. We argue that this model not only excludes some contemporary aesthetic values but also limits the communicative ability of new music interfaces. As an alternative, an ecology of musical creation accounts for both a diversity of aesthetic goals and the complex interrelation of human and non-human agents. An ecological perspective on several approaches to musical creation with interactive technologies reveals an expanded, more inclusive view of artistic interaction that facilitates novel, compelling ways to use technology for music. This paper is fundamentally a call to consider the role of aesthetic values in the analysis of artistic processes and technologies. }
}

@inproceedings{Nilson2007,
  author = {Nilson, Click},
  title = {Live Coding Practice},
  pages = {112--117},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177209},
  url = {http://www.nime.org/proceedings/2007/nime2007_112.pdf},
  keywords = {Practice, practising, live coding },
  abstract = {Live coding is almost the antithesis of immediate physical musicianship, and yet, has attracted the attentions of a number of computer-literate musicians, as well as the music-savvy programmers that might be more expected. It is within the context of live coding that I seek to explore the question of practising a contemporary digital musical instrument, which is often raised as an aside but more rarely carried out in research (though see [12]). At what stage of expertise are the members of the live coding movement, and what practice regimes might help them to find their true potential?}
}

@inproceedings{Mann2007,
  author = {Mann, Steve},
  title = {Natural Interfaces for Musical Expression : Physiphones and a Physics-Based Organology},
  pages = {118--123},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177181},
  url = {http://www.nime.org/proceedings/2007/nime2007_118.pdf},
  keywords = {all or part of,ethnomusicology,hydraulophone,is granted without fee,nime07,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,tangible user interface,this work for},
  abstract = {This paper presents two main ideas: (1) Various newly invented liquid-based or underwater musical instruments are proposed that function like woodwind instruments but use water instead of air. These “woodwater” instruments expand the space of known instruments to include all three states of matter: solid (strings, percussion); liquid (the proposed instruments); and gas (brass and woodwinds). Instruments that use the fourth state of matter (plasma) are also proposed. (2) Although the current trend in musical interfaces has been to expand versatililty and generality by separating the interface from the sound-producing medium, this paper identifies an opposite trend in musical interface design inspired by instruments such as the harp, the acoustic or electric guitar, the tin whistle, and the Neanderthal flute, that have a directness of user-interface, where the fingers of the musician are in direct physical contact with the sound-producing medium. The newly invented instruments are thus designed to have this sensually tempting intimacy not be lost behind layers of abstraction, while also allowing for the high degree of virtuosity. Examples presented include the poseidophone, an instrument made from an array of ripple tanks, each tuned for a particular note, and the hydraulophone, an instrument in which sound is produced by pressurized hydraulic fluid that is in direct physical contact with the fingers of the player. Instruments based on these primordial media tend to fall outside existing classifications and taxonomies of known musical instruments which only consider instruments that make sound with solid or gaseous states of matter. To better understand and contextualize some of the new primordial user interfaces, a broader concept of musical instrument classification is proposed that considers the states of matter of both the user-interface and the sound production medium.}
}

@inproceedings{Bevilacqua2007,
  author = {Bevilacqua, Fr\'{e}d\'{e}ric and Gu\'{e}dy, Fabrice and Schnell, Norbert and Fl\'{e}ty, Emmanuel and Leroy, Nicolas},
  title = {Wireless Sensor Interface and Gesture-Follower for Music Pedagogy},
  pages = {124--129},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177045},
  url = {http://www.nime.org/proceedings/2007/nime2007_124.pdf},
  keywords = {Technology-enhanced learning, music pedagogy, wireless interface, gesture-follower, gesture recognition },
  abstract = {We present in this paper a complete gestural interface built to support music pedagogy. The development of this prototype concerned both hardware and software components: a small wireless sensor interface including accelerometers and gyroscopes, and an analysis system enabling gesture following and recognition. A first set of experiments was conducted with teenagers in a music theory class. The preliminary results were encouraging concerning the suitability of these developments in music education. }
}

@inproceedings{Dannenberg2007,
  author = {Dannenberg, Roger B.},
  title = {New Interfaces for Popular Music Performance},
  pages = {130--135},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177081},
  url = {http://www.nime.org/proceedings/2007/nime2007_130.pdf},
  keywords = {accompaniment,beat,conducting,intelligent,music synchronization,nime07,synthetic performer,tracking,virtual orchestra},
  abstract = {Augmenting performances of live popular music with computer systems poses many new challenges. Here, "popular music" is taken to mean music with a mostly steady tempo, some improvisational elements, and largely predetermined melodies, harmonies, and other parts. The overall problem is studied by developing a framework consisting of constraints and subproblems that any solution should address. These problems include beat acquisition, beat phase, score location, sound synthesis, data preparation, and adaptation. A prototype system is described that offers a set of solutions to the problems posed by the framework, and future work is suggested. }
}

@inproceedings{Lee2007,
  author = {Lee, Eric and Enke, Urs and Borchers, Jan and de Jong, Leo},
  title = {Towards Rhythmic Analysis of Human Motion Using Acceleration-Onset Times},
  pages = {136--141},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177159},
  url = {http://www.nime.org/proceedings/2007/nime2007_136.pdf},
  keywords = {rhythm analysis, dance movement analysis, onset analysis },
  abstract = {We present a system for rhythmic analysis of human motion inreal-time. Using a combination of both spectral (Fourier) andspatial analysis of onsets, we are able to extract repeating rhythmic patterns from data collected using accelerometers. These extracted rhythmic patterns show the relative magnitudes of accentuated movements and their spacing in time. Inspired by previouswork in automatic beat detection of audio recordings, we designedour algorithms to be robust to changes in timing using multipleanalysis techniques and methods for sensor fusion, filtering andclustering. We tested our system using a limited set of movements,as well as dance movements collected from a professional, bothwith promising results.}
}

@inproceedings{Bouillot2007,
  author = {Bouillot, Nicolas},
  title = {nJam User Experiments : Enabling Remote Musical Interaction from Milliseconds to Seconds},
  pages = {142--147},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177055},
  url = {http://www.nime.org/proceedings/2007/nime2007_142.pdf},
  keywords = {Remote real-time musical interaction, end-to-end delays, syn- chronization, user experiments, distributed metronome, NMP. },
  abstract = {Remote real-time musical interaction is a domain where endto-end latency is a well known problem. Today, the mainexplored approach aims to keep it below the musicians perception threshold. In this paper, we explore another approach, where end-to-end delays rise to several seconds, butcomputed in a controlled (and synchronized) way dependingon the structure of the musical pieces. Thanks to our fullydistributed prototype called nJam, we perform user experiments to show how this new kind of interactivity breaks theactual end-to-end latency bounds.}
}

@inproceedings{Moody2007,
  author = {Moody, Niall and Fells, Nick and Bailey, Nicholas},
  title = {Ashitaka : An Audiovisual Instrument},
  pages = {148--153},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177199},
  url = {http://www.nime.org/proceedings/2007/nime2007_148.pdf},
  keywords = {audiovisual,instrument,mappings,nime07,synchresis,x3d},
  abstract = {This paper describes the Ashitaka audiovisual instrumentand the process used to develop it. The main idea guidingthe design of the instrument is that motion can be used toconnect audio and visuals, and the first part of the paperconsists of an exploration of this idea. The issue of mappings is raised, discussing both audio-visual mappings andthe mappings between the interface and synthesis methods.The paper concludes with a detailed look at the instrumentitself, including the interface, synthesis methods, and mappings used.}
}

@inproceedings{Aimi2007,
  author = {Aimi, Roberto},
  title = {Percussion Instruments Using Realtime Convolution : Physical Controllers},
  pages = {154--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177033},
  url = {http://www.nime.org/proceedings/2007/nime2007_154.pdf},
  keywords = {Musical controllers, extended acoustic instruments },
  abstract = {This paper describes several example hybrid acoustic / electronic percussion instruments using realtime convolution toaugment and modify the apparent acoustics of damped physical objects. Examples of cymbal, frame drum, practice pad,brush, and bass drum controllers are described.}
}

@inproceedings{Rohs2007,
  author = {Rohs, Michael and Essl, Georg},
  title = {CaMus 2 -- Optical Flow and Collaboration in Camera Phone Music Performance},
  pages = {160--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177233},
  url = {http://www.nime.org/proceedings/2007/nime2007_160.pdf},
  keywords = {Camera phone, mobile phone, music performance, mobile sound generation, sensing-based interaction, collaboration },
  abstract = {CaMus2 allows collaborative performance with mobile camera phones. The original CaMus project was extended tosupport multiple phones performing in the same space andgenerating MIDI signals to control sound generation andmanipulation software or hardware. Through an opticalflow technology the system can be used without a referencemarker grid. When using a marker grid, the use of dynamicdigital zoom extends the range of performance. Semanticinformation display helps guide the performer visually.}
}

@inproceedings{Fiebrink2007,
  author = {Fiebrink, Rebecca and Wang, Ge and Cook, Perry R.},
  title = {Don't Forget the Laptop : Using Native Input Capabilities for Expressive Musical Control},
  pages = {164--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177087},
  url = {http://www.nime.org/proceedings/2007/nime2007_164.pdf},
  keywords = {Mapping strategies. Laptop-based physical interfaces. Collaborative laptop performance.},
  abstract = {We draw on our experiences with the Princeton Laptop Orchestra to discuss novel uses of the laptop’s native physical inputs for flexible and expressive control. We argue that instruments designed using these built-in inputs offer benefits over custom standalone controllers, particularly in certain group performance settings; creatively thinking about native capabilities can lead to interesting and unique new interfaces. We discuss a variety of example instruments that use the laptop’s native capabilities and suggest avenues for future work. We also describe a new toolkit for rapidly experimenting with these capabilities.}
}

@inproceedings{Moriwaki2007,
  author = {Moriwaki, Katherine and Brucken-Cohen, Jonah},
  title = {MIDI Scrapyard Challenge Workshops},
  pages = {168--172},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177201},
  url = {http://www.nime.org/proceedings/2007/nime2007_168.pdf},
  keywords = {Workshop, MIDI, Interaction Design, Creativity, Performance},
  abstract = {In this paper the authors present the MIDI Scrapyard Challenge (MSC) workshop, a one-day hands-on experience which asks participants to create musical controllers out of cast-off electronics, found materials and junk. The workshop experience, principles, and considerations are detailed, along with sample projects which have been created in various MSC workshops. Observations and implications as well as future developments for the workshop are discussed.}
}

@inproceedings{Lee2007a,
  author = {Lee, Eric and Wolf, Marius and Jansen, Yvonne and Borchers, Jan},
  title = {REXband : A Multi-User Interactive Exhibit for Exploring Medieval Music},
  pages = {172--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177163},
  url = {http://www.nime.org/proceedings/2007/nime2007_172.pdf},
  keywords = {interactive music exhibits, medieval music, augmented instruments, e-learning, education },
  abstract = {We present REXband, an interactive music exhibit for collaborative improvisation to medieval music. This audio-only system consists of three digitally augmented medieval instrument replicas: thehurdy gurdy, harp, and frame drum. The instruments communicate with software that provides users with both musical support and feedback on their performance using a "virtual audience" set in a medieval tavern. REXband builds upon previous work in interactive music exhibits by incorporating aspects of e-learning to educate, in addition to interaction design patterns to entertain; care was also taken to ensure historic authenticity. Feedback from user testing in both controlled (laboratory) and public (museum) environments has been extremely positive. REXband is part of the Regensburg Experience, an exhibition scheduled to open in July 2007 to showcase the rich history of Regensburg, Germany.}
}

@inproceedings{Baalman2007,
  author = {Baalman, Marije A. and Moody-Grigsby, Daniel and Salter, Christopher L.},
  title = {Schwelle : Sensor Augmented, Adaptive Sound Design for Live Theatrical Performance},
  pages = {178--184},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177035},
  url = {http://www.nime.org/proceedings/2007/nime2007_178.pdf},
  keywords = {Interactive performance, dynamical systems, wireless sens- ing, adaptive audio scenography, audio dramaturgy, situated computing, sound design },
  abstract = {This paper describes work on a newly created large-scale interactive theater performance entitled Schwelle (Thresholds). The authors discuss an innovative approach towards the conception, development and implementation of dynamic and responsive audio scenography: a constantly evolving, multi-layered sound design generated by continuous input from a series of distributed wireless sensors deployed both on the body of a performer and placed within the physical stage environment. The paper is divided into conceptual and technological parts. We first describe the project’s dramaturgical and conceptual context in order to situate the artistic framework that has guided the technological system design. Specifically, this framework discusses the team’s approach in combining techniques from situated computing, theatrical sound design practice and dynamical systems in order to create a new kind of adaptive audio scenographic environment augmented by wireless, distributed sensing for use in live theatrical performance. The goal of this adaptive sound design is to move beyond both existing playback models used in theatre sound as well as the purely humancentered, controller-instrument approach used in much current interactive performance practice.}
}

@inproceedings{Jakovich2007,
  author = {Jakovich, Joanne and Beilharz, Kirsty},
  title = {ParticleTecture : Interactive Granular Soundspaces for Architectural Design},
  pages = {185--190},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177127},
  url = {http://www.nime.org/proceedings/2007/nime2007_185.pdf},
  keywords = {Architecture, installation, interaction, granular synthesis, adaptation, engagement. },
  abstract = {Architectural space is a key contributor to the perceptual world we experience daily. We present ‘ParticleTecture’, a soundspace installation system that extends spatial perception of ordinary architectural space through gestural interaction with sound in space. ParticleTecture employs a particle metaphor to produce granular synthesis soundspaces in response to video-tracking of human movement. It incorporates an adaptive mechanism that utilizes a measure of engagement to inform ongoing audio patterns in response to human activity. By identifying engaging features in its response, the system is able to predict, pre-empt and shape its evolving responses in accordance with the most engaging, compelling, interesting attributes of the active environment. An implementation of ParticleTecture for gallery installation is presented and discussed as one form of architectural space.}
}

@inproceedings{Franinovic2007,
  author = {Franinovic, Karmen and Visell, Yon},
  title = {New Musical Interfaces in Context : Sonic Interaction Design in the Urban Setting},
  pages = {191--196},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177093},
  url = {http://www.nime.org/proceedings/2007/nime2007_191.pdf},
  keywords = {architecture,interaction,music,nime07,sound in-,urban design},
  abstract = {The distinctive features of interactive sound installations in public space are considered, with special attention to the rich, if undoubtedly difficult, environments in which they exist. It is argued that such environments, and the social contexts that they imply, are among the most valuable features of these works for the approach that we have adopted to creation as research practice. The discussion is articulated through case studies drawn from two of our installations, Recycled Soundscapes (2004) and Skyhooks (2006). Implications for the broader design of new musical instruments are presented.}
}

@inproceedings{Gimenes2007,
  author = {Gimenes, Marcelo and Miranda, Eduardo and Johnson, Chris},
  title = {Musicianship for Robots with Style},
  pages = {197--202},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177099},
  url = {http://www.nime.org/proceedings/2007/nime2007_197.pdf},
  keywords = {artificial life,musical style,musicianship,nime07},
  abstract = {In this paper we introduce a System conceived to serve as the "musical brain" of autonomous musical robots or agent-based software simulations of robotic systems. Our research goal is to provide robots with the ability to integrate with the musical culture of their surroundings. In a multi-agent configuration, the System can simulate an environment in which autonomous agents interact with each other as well as with external agents (e.g., robots, human beings or other systems). The main outcome of these interactions is the transformation and development of their musical styles as well as the musical style of the environment in which they live. }
}

@inproceedings{Topper2007,
  author = {Topper, David},
  title = {Extended Applications of the Wireless Sensor Array (WISEAR)},
  pages = {203--204},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177261},
  url = {http://www.nime.org/proceedings/2007/nime2007_203.pdf},
  keywords = {Wireless, sensors, embedded devices, linux, real-time audio, real- time video },
  abstract = { WISEAR (Wireless Sensor Array)8, provides a robust andscalable platform for virtually limitless types of data input tosoftware synthesis engines. It is essentially a Linux based SBC(Single Board Computer) with 802.11a/b/g wireless capability.The device, with batteries, only weighs a few pounds and can beworn by a dancer or other live performer. Past work has focusedon connecting "conventional" sensors (eg., bend sensors,accelerometers, FSRs, etc...) to the board and using it as a datarelay, sending the data as real time control messages to synthesisengines like Max/MSP and RTcmix1. Current research hasextended the abilities of the device to take real-time audio andvideo data from USB cameras and audio devices, as well asrunning synthesis engines on board the device itself. Given itsgeneric network ability (eg., being an 802.11a/b/g device) there istheoretically no limit to the number of WISEAR boxes that canbe used simultaneously in a performance, facilitating multiperformer compositions. This paper will present the basic design philosophy behindWISEAR, explain some of the basic concepts and methods, aswell as provide a live demonstration of the running device, wornby the author.}
}

@inproceedings{Fernstrom2007,
  author = {Torre, Giuseppe and Fernstr\''{o}m, Mikael and O'Flynn, Brendan and Angove, Philip},
  title = {Celeritas : Wearable Wireless System},
  pages = {205--208},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179463},
  url = {http://www.nime.org/proceedings/2007/nime2007_205.pdf},
  keywords = {Inertial Measurement Unit, IMU, Position Tracking, Interactive Dance Performance, Graphical Object, Mapping. },
  abstract = {In this paper, we describe a new wearable wireless sensor system for solo or group dance performances. The system consists of a number of 25mm Wireless Inertial Measurement Unit (WIMU) nodes designed at the Tyndall National Institute. Each sensor node has two dual-axis accelerometers, three single axis gyroscopes and two dual axis magnetometers, providing 6 Degrees of Freedom (DOF) movement tracking. All sensors transmit data wirelessly to a basestation at a frequency band and power that does not require licensing. The interface process has been developed at the Interaction Design Center of the University of Limerick (Ireland). The data are acquired and manipulated in well-know real-time software like pd and Max/MSP. This paper presents the new system, describes the interface design and outlines the main achievements of this collaborative research, which has been named ‘Celeritas’.}
}

@inproceedings{Sinclair2007,
  author = {Sinclair, Stephen and Wanderley, Marcelo M.},
  title = {Defining a Control Standard for Easily Integrating Haptic Virtual Environments with Existing Audio / Visual Systems},
  pages = {209--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177245},
  url = {http://www.nime.org/proceedings/2007/nime2007_209.pdf},
  keywords = {Haptics, control, multi-modal, audio, force-feedback },
  abstract = {This paper presents an approach to audio-haptic integration that utilizes Open Sound Control, an increasingly wellsupported standard for audio communication, to initializeand communicate with dynamic virtual environments thatwork with off-the-shelf force-feedback devices.}
}

@inproceedings{Donaldson2007,
  author = {Donaldson, Justin and Knopke, Ian and Raphael, Chris},
  title = {Chroma Palette : Chromatic Maps of Sound As Granular Synthesis Interface},
  pages = {213--219},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177085},
  url = {http://www.nime.org/proceedings/2007/nime2007_213.pdf},
  keywords = {Chroma, granular synthesis, dimensionality reduction },
  abstract = {Chroma based representations of acoustic phenomenon are representations of sound as pitched acoustic energy. A framewise chroma distribution over an entire musical piece is a useful and straightforward representation of its musical pitch over time. This paper examines a method of condensing the block-wise chroma information of a musical piece into a two dimensional embedding. Such an embedding is a representation or map of the different pitched energies in a song, and how these energies relate to each other in the context of the song. The paper presents an interactive version of this representation as an exploratory analytical tool or instrument for granular synthesis. Pointing and clicking on the interactive map recreates the acoustical energy present in the chroma blocks at that location, providing an effective way of both exploring the relationships between sounds in the original piece, and recreating a synthesized approximation of these sounds in an instrumental fashion. }
}

@inproceedings{Collins2007,
  author = {Collins, Nick},
  title = {Matching Parts : Inner Voice Led Control for Symbolic and Audio Accompaniment},
  pages = {220--223},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177075},
  url = {http://www.nime.org/proceedings/2007/nime2007_220.pdf},
  keywords = {accompaniment,concatenative sound syn-,feature matching,inner parts,interactive mu-,melodic similarity,nime07,thesis}
}

@inproceedings{Cartwright2007,
  author = {Cartwright, Mark and Jones, Matt and Terasawa, Hiroko},
  title = {Rage in Conjunction with the Machine},
  pages = {224--227},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177063},
  url = {http://www.nime.org/proceedings/2007/nime2007_224.pdf},
  keywords = {audience participation,inflatable,instrume nt design,instrume nt size,mapping,musical,new musical instrument,nime07,physical systems,sound scultpure},
  abstract = {This report presents the design and construct ion of Rage in Conjunction with the Machine, a simple but novel pairing of musical interface and sound sculpture. The ,
,
authors discuss the design and creation of this instrument , focusing on the unique aspects of it, including the use of physical systems, large gestural input, scale, and the electronic coupling of a physical input to a physical output.}
}

@inproceedings{Weinberg2007,
  author = {Weinberg, Gil and Driscoll, Scott},
  title = {The Design of a Robotic Marimba Player -- Introducing Pitch into Robotic Musicianship},
  pages = {228--233},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179477},
  url = {http://www.nime.org/proceedings/2007/nime2007_228.pdf},
  keywords = {human-machine interaction,improvisation,nime07,perceptual modeling,robotic musicianship},
  abstract = {The paper presents the theoretical background and the design scheme for a perceptual and improvisational robotic marimba player that interacts with human musicians in a visual and acoustic manner. Informed by an evaluation of a previously developed robotic percussionist, we present the extension of our work to melodic and harmonic realms with the design of a robotic player that listens to, analyzes and improvises pitch-based musical materials. After a presentation of the motivation for the project, theoretical background and related work, we present a set of research questions followed by a description of hardware and software approaches that address these questions. The paper concludes with a description of our plans to implement and embed these approaches in a robotic marimba player that will be used in workshops and concerts.}
}

@inproceedings{Robertson2007,
  author = {Robertson, Andrew and Plumbley, Mark D.},
  title = {B-Keeper : A Beat-Tracker for Live Performance},
  pages = {234--237},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177231},
  url = {http://www.nime.org/proceedings/2007/nime2007_234.pdf},
  keywords = {Human-Computer Interaction, Automatic Accompaniment, Performance },
  abstract = {This paper describes the development of B-Keeper, a reatime beat tracking system implemented in Java and Max/MSP,which is capable of maintaining synchronisation between anelectronic sequencer and a drummer. This enables musicians to interact with electronic parts which are triggeredautomatically by the computer from performance information. We describe an implementation which functions withthe sequencer Ableton Live.}
}

@inproceedings{Kapur2007,
  author = {Kapur, Ajay and Singer, Eric and Benning, Manjinder S. and Tzanetakis, George and Trimpin, Trimpin},
  title = {Integrating HyperInstruments , Musical Robots \& Machine Musicianship for North {India}n Classical Music},
  pages = {238--241},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177137},
  url = {http://www.nime.org/proceedings/2007/nime2007_238.pdf},
  keywords = {Musical Robotics, Electronic Sitar, Hyperinstruments, Music Information Retrieval (MIR). },
  abstract = {This paper describes a system enabling a human to perform music with a robot in real-time, in the context of North Indian classical music. We modify a traditional acoustic sitar into a hyperinstrument in order to capture performance gestures for musical analysis. A custom built four-armed robotic Indian drummer was built using a microchip, solenoids, aluminum and folk frame drums. Algorithms written towards "intelligent" machine musicianship are described. The final goal of this research is to have a robotic drummer accompany a professional human sitar player live in performance. }
}

@inproceedings{Clay2007,
  author = {Clay, Arthur and Majoe, Dennis},
  title = {The Wrist-Conductor},
  pages = {242--245},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177073},
  url = {http://www.nime.org/proceedings/2007/nime2007_242.pdf},
  keywords = {Mobile Music, GPS, Controller, Collaborative Performance },
  abstract = {The starting point for this project is the want to produce a music controller that could be employed in such a manner that even lay public could enjoy the possibilities of mobile art. All of the works that are discussed here are in relation to a new GPS-based controller, the Wrist-Conductor. The works are technically based around the synchronizing possibilities using the GPS Time Mark and are aesthetically rooted in works that function in an open public space such as a city or a forest. One of the works intended for the controller, China Gates, is discussed here in detail in order to describe how the GPS Wrist-Controller is actually used in a public art context. The other works, CitySonics, The Enchanted Forest and Get a Pot \& a Spoon are described briefly in order to demonstrate that even a simple controller can be used to create a body of works. This paper also addresses the breaking of the media bubble via the concept of the “open audience”, or how mobile art can engage pedestrians as viewers or listeners within public space and not remain an isolated experience for performers only.}
}

@inproceedings{Hollinger2007,
  author = {Hollinger, Avrum and Steele, Christopher and Penhune, Virginia and Zatorre, Robert and Wanderley, Marcelo M.},
  title = {fMRI-Compatible Electronic Controllers},
  pages = {246--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177119},
  url = {http://www.nime.org/proceedings/2007/nime2007_246.pdf},
  keywords = {Input device, MRI-compatible, fMRI, motor learning, optical sensing. },
  abstract = {This paper presents an electronic piano keyboard and computer mouse designed for use in a magnetic resonance imaging scanner. The interface allows neuroscientists studying motor learning of musical tasks to perform functional scans of a subject's brain while synchronizing the scanner, auditory and visual stimuli, and auditory feedback with the onset, offset, and velocity of the piano keys. The design of the initial prototype and environment-specific issues are described, as well as prior work in the field. Preliminary results are positive and were unable to show the existence of image artifacts caused by the interface. Recommendations to improve the optical assembly are provided in order to increase the robustness of the design. }
}

@inproceedings{Nagashima2007,
  author = {Nagashima, Yoichi},
  title = {GHI project and "Cyber Kendang"},
  pages = {250--253},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177205},
  url = {http://www.nime.org/proceedings/2007/nime2007_250.pdf},
  keywords = {kendang, media arts, new instruments, sound and light},
  abstract = {This is a report of research project about developing novel musical instruments for interactive computer music. The project's name - "GHI project" means that "It might be good that musical instrument shines, isn't it?" in Japanese. I examined the essences of musical instruments again on proverb "Taking a lesson from the past". At the first step, my project targeted and chose "Kendang" - the traditional musical instrument of Indonesia.}
}

@inproceedings{Toyoda2007,
  author = {Toyoda, Shinichiro},
  title = {Sensillum : An Improvisational Approach to Composition},
  pages = {254--255},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179465},
  url = {http://www.nime.org/proceedings/2007/nime2007_254.pdf},
  keywords = {Interactive systems, improvisation, gesture, composition INTRODUCTION Though music related research focusing on the interaction between people and computers is currently experiencing wide range development, the history of approaches wherein the creation of new musical expression is made possible via the active },
  abstract = {This study proposes new possibilities for interaction design pertaining to music piece creation. Specifically, the study created an environment wherein a wide range of users are able to easily experience new musical expressions via a combination of newly developed software and the Nintendo Wii Remote controller. }
}

@inproceedings{Gruenbaum2007,
  author = {Gruenbaum, Leon},
  title = {The Samchillian Tip Tip Tip Cheeepeeeee : A Relativistic Keyboard Instrument},
  pages = {256--259},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177103},
  url = {http://www.nime.org/proceedings/2007/nime2007_256.pdf},
  keywords = {samchillian, keyboard, MIDI controller, relative, interval, microtonal, computer keyboard, pitch, musical instrument },
  abstract = {Almost all traditional musical instruments have a one-to-one correspondence between a given fingering and the pitch that sounds for that fingering. The Samchillian Tip Tip Tip Cheeepeeeee does not --- it is a keyboard MIDI controller that is based on intervals rather than fixed pitches. That is, a given keypress will sound a pitch a number of steps away from the last note sounded (within the key signature and scale selected) according to the 'delta' value assigned to that key. The advantages of such a system are convenience, speed, and the ability to play difficult, unusual and/or unintended passages extemporaneously. }
}

@inproceedings{Freeman2007,
  author = {Freeman, Jason},
  title = {Graph Theory : Interfacing Audiences Into the Compositional Process},
  pages = {260--263},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177095},
  url = {http://www.nime.org/proceedings/2007/nime2007_260.pdf},
  keywords = {Music, Composition, Residency, Audience Interaction, Collaboration, Violin, Graph, Flash, Internet, Traveling Salesman. },
  abstract = {Graph Theory links the creative music-making activities of web site visitors to the dynamic generation of an instrumental score for solo violin. Participants use a web-based interface to navigate among short, looping musical fragments to create their own unique path through the open-form composition. Before each concert performance, the violinist prints out a new copy of the score that orders the fragments based on the decisions made by web visitors. }
}

@inproceedings{Villar2007,
  author = {Villar, Nicolas and Gellersen, Hans and Jervis, Matt and Lang, Alexander},
  title = {The ColorDex DJ System : A New Interface for Live Music Mixing},
  pages = {264--269},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179475},
  url = {http://www.nime.org/proceedings/2007/nime2007_264.pdf},
  keywords = {Novel interfaces, live music-mixing, cube-based interfaces, crossfading, repurposing HDDs, accelerometer-based cubic control },
  abstract = {This paper describes the design and implementation of a new interface prototype for live music mixing. The ColorDex system employs a completely new operational metaphor which allows the mix DJ to prepare up to six tracks at once, and perform mixes between up to three of those at a time. The basic premises of the design are: 1) Build a performance tool that multiplies the possible choices a DJ has in respect in how and when tracks are prepared and mixed; 2) Design the system in such a way that the tool does not overload the performer with unnecessary complexity, and 3) Make use of novel technology to make the performance of live music mixing more engaging for both the performer and the audience. The core components of the system are: A software program to load, visualize and playback digitally encoded tracks; the HDDJ device (built chiefly out of a repurposed hard disk drive), which provides tactile manipulation of the playback speed and position of tracks; and the Cubic Crossfader, a wireless sensor cube that controls of the volume of individual tracks, and allows the DJ to mix these in interesting ways. }
}

@inproceedings{Dahl2007,
  author = {Dahl, Luke and Whetsell, Nathan and Van Stoecker, John},
  title = {The WaveSaw : A Flexible Instrument for Direct Timbral Manipulation},
  pages = {270--272},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177079},
  url = {http://www.nime.org/proceedings/2007/nime2007_270.pdf},
  keywords = {Musical controller, Puredata, scanned synthesis, flex sensors. },
  abstract = {In this paper, we describe a musical controller – the WaveSaw – for directly manipulating a wavetable. The WaveSaw consists of a long, flexible metal strip with handles on either end, somewhat analogous to a saw. The user plays the WaveSaw by holding the handles and bending the metal strip. We use sensors to measure the strip’s curvature and reconstruct its shape as a wavetable stored in a computer. This provides a direct gestural mapping from the shape of the WaveSaw to the timbral characteristics of the computer-generated sound. Additional sensors provide control of pitch, amplitude, and other musical parameters.}
}

@inproceedings{Bennett2007,
  author = {Bennett, Peter and Ward, Nicholas and O'Modhrain, Sile and Rebelo, Pedro},
  title = {DAMPER : A Platform for Effortful Interface Development},
  pages = {273--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177041},
  url = {http://www.nime.org/proceedings/2007/nime2007_273.pdf},
  keywords = {Effortful Interaction. Haptics. Laban Analysis. Physicality. HCI. },
  abstract = {This paper proposes that the physicality of an instrument be considered an important aspect in the design of new interfaces for musical expression. The use of Laban's theory of effort in the design of new effortful interfaces, in particular looking at effortspace modulation, is investigated, and a platform for effortful interface development (named the DAMPER) is described. Finally, future work is described and further areas of research are highlighted. }
}

@inproceedings{Francois2007,
  author = {Fran\c{c}ois, Alexandre R. and Chew, Elaine and Thurmond, Dennis},
  title = {Visual Feedback in Performer-Machine Interaction for Musical Improvisation},
  pages = {277--280},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177091},
  url = {http://www.nime.org/proceedings/2007/nime2007_277.pdf},
  keywords = {Performer-machine interaction, visualization design, machine improvisation },
  abstract = {This paper describes the design of Mimi, a multi-modal interactive musical improvisation system that explores the potential and powerful impact of visual feedback in performermachine interaction. Mimi is a performer-centric tool designed for use in performance and teaching. Its key andnovel component is its visual interface, designed to providethe performer with instantaneous and continuous information on the state of the system. For human improvisation,in which context and planning are paramount, the relevantstate of the system extends to the near future and recentpast. Mimi's visual interface allows for a peculiar blendof raw reflex typically associated with improvisation, andpreparation and timing more closely affiliated with scorebased reading. Mimi is not only an effective improvisationpartner, it has also proven itself to be an invaluable platformthrough which to interrogate the mental models necessaryfor successful improvisation.}
}

@inproceedings{Poepel2007,
  author = {Poepel, Cornelius and Marx, G\''{u}nter},
  title = {>hot\_strings SIG},
  pages = {281--284},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177221},
  url = {http://www.nime.org/proceedings/2007/nime2007_281.pdf},
  keywords = {Interdisciplinary user group, electronic bowed string instrument, evaluation of computer based musical instruments },
  abstract = {Many fascinating new developments in the area bowed stringed instruments have been developed in recent years. However, the majority of these new applications are either not well known, used orconsidered in a broader context by their target users. The necessaryexchange between the world of developers and the players is ratherlimited. A group of performers, researchers, instrument developersand composers was founded in order to share expertise and experiences and to give each other feedback on the work done to developnew instruments. Instruments incorporating new interfaces, synthesis methods, sensor technology, new materials like carbon fiber andwood composites as well as composite materials and research outcome are presented and discussed in the group. This paper gives anintroduction to the group and reports about activities and outcomesin the last two years.}
}

@inproceedings{Cook2007,
  author = {Cook, Andrew A. and Pullin, Graham},
  title = {Tactophonics : Your Favourite Thing Wants to Sing},
  pages = {285--288},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177077},
  url = {http://www.nime.org/proceedings/2007/nime2007_285.pdf},
  keywords = {1,affordance,background and problem space,cultural probes,design research,improvisation,interaction design,nime07,performance},
  abstract = {Description of a project, inspired by the theory of affordance, exploring the issues of visceral expression and audience engagement in the realm of computer performance. Describes interaction design research techniques in novel application, used to engage and gain insight into the culture and mindset of the improvising musician. This research leads to the design and implementation of a prototype system that allows musicians to play an object of their choice as a musical instrument.}
}

@inproceedings{Perez2007,
  author = {P\'{e}rez, Miguel A. and Knapp, Benjamin and Alcorn, Michael},
  title = {D\'{\i}amair : Composing for Choir and Integral Music Controller},
  pages = {289--292},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177215},
  url = {http://www.nime.org/proceedings/2007/nime2007_289.pdf},
  keywords = {Composition, Integral Music Controller, Emotion measurement, Physiological Measurement, Spatialisation. },
  abstract = {In this paper, we describe the composition of a piece for choir and Integral Music Controller. We focus more on the aesthetic, conceptual, and practical aspects of the interface and less on the technological details. We especially stress the influence that the designed interface poses on the compositional process and how we approach the expressive organisation of musical materials during the composition of the piece, as well as the addition of nuances (personal real-time expression) by the musicians at performance time. }
}

@inproceedings{Fornari2007,
  author = {Fornari, Jose and Maia, Adolfo Jr. and Manzolli, Jonatas},
  title = {Interactive Spatialization and Sound Design using an Evolutionary System},
  pages = {293--298},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177089},
  url = {http://www.nime.org/proceedings/2007/nime2007_293.pdf},
  keywords = {interactive, sound, spatialization, evolutionary, adaptation. },
  abstract = {We present an interactive sound spatialization and synthesis system based on Interaural Time Difference (ITD) model and Evolutionary Computation. We define a Sonic Localization Field using sound attenuation and ITD azimuth angle parameters and, in order to control an adaptive algorithm, we used pairs of these parameters as Spatial Sound Genotypes (SSG). They are extracted from waveforms which are considered individuals of a Population Set. A user-interface receives input from a generic gesture interface (such as a NIME device) and interprets them as ITD cues. Trajectories provided by these signals are used as Target Sets of an evolutionary algorithm. A Fitness procedure optimizes locally the distance between the Target Set and the SSG pairs. Through a parametric score the user controls dynamic changes in the sound output. }
}

@inproceedings{Hornof2007,
  author = {Hornof, Anthony J. and Rogers, Troy and Halverson, Tim},
  title = {EyeMusic : Performing Live Music and Multimedia Compositions with Eye Movements},
  pages = {299--300},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177121},
  url = {http://www.nime.org/proceedings/2007/nime2007_299.pdf},
  keywords = {H.5.2 [Information Interfaces and Presentation] User Interfaces --- input devices and strategies, interaction styles. J.5 [Arts and Humanities] Fine arts, performing arts. },
  abstract = {In this project, eye tracking researchers and computer music composers collaborate to create musical compositions that are played with the eyes. A commercial eye tracker (LC Technologies Eyegaze) is connected to a music and multimedia authoring environment (Max/MSP/Jitter). The project addresses issues of both noise and control: How will the performance benefit from the noise inherent in eye trackers and eye movements, and to what extent should the composition encourage the performer to try to control a specific musical outcome? Providing one set of answers to these two questions, the authors create an eye-controlled composition, EyeMusic v1.0, which was selected by juries for live performance at computer music conferences.}
}

@inproceedings{Kirk2007,
  author = {Kirk, Turner and Leider, Colby},
  title = {The FrankenPipe : A Novel Bagpipe Controller},
  pages = {301--304},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177151},
  url = {http://www.nime.org/proceedings/2007/nime2007_301.pdf},
  keywords = {FrankenPipe, alternate controller, MIDI, bagpipe, photoresistor, chanter. },
  abstract = {The FrankenPipe project is an attempt to convert a traditionalHighland Bagpipe into a controller capable of driving both realtime synthesis on a laptop as well as a radio-controlled (RC) car.Doing so engages musical creativity while enabling novel, oftenhumorous, performance art. The chanter is outfitted withphotoresistors (CdS photoconductive cells) underneath each hole,allowing a full range of MIDI values to be produced with eachfinger and giving the player a natural feel. An air-pressure sensoris also deployed in the bag to provide another element of controlwhile capturing a fundamental element of bagpipe performance.The final product navigates the realm of both musical instrumentand toy, allowing the performer to create a novel yet richperformance experience for the audience.}
}

@inproceedings{Camurri2007a,
  author = {Camurri, Antonio and Coletta, Paolo and Varni, Giovanna and Ghisio, Simone},
  title = {Developing Multimodal Interactive Systems with EyesWeb XMI},
  pages = {305--308},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177061},
  url = {http://www.nime.org/proceedings/2007/nime2007_305.pdf},
  keywords = {EyesWeb, multimodal interactive systems, performing arts. },
  abstract = {EyesWeb XMI (for eXtended Multimodal Interaction) is the new version of the well-known EyesWeb platform. It has a main focus on multimodality and the main design target of this new release has been to improve the ability to process and correlate several streams of data. It has been used extensively to build a set of interactive systems for performing arts applications for Festival della Scienza 2006, Genoa, Italy. The purpose of this paper is to describe the developed installations as well as the new EyesWeb features that helped in their development.}
}

@inproceedings{Hoffman2007,
  author = {Hoffman, Matt and Cook, Perry R.},
  title = {Real-Time Feature-Based Synthesis for Live Musical Performance},
  pages = {309--312},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177117},
  url = {http://www.nime.org/proceedings/2007/nime2007_309.pdf},
  keywords = {Feature, Synthesis, Analysis, Mapping, Real-time. },
  abstract = {A crucial set of decisions in digital musical instrument design deals with choosing mappings between parameters controlled by the performer and the synthesis algorithms that actually generate sound. Feature-based synthesis offers a way to parameterize audio synthesis in terms of the quantifiable perceptual characteristics, or features, the performer wishes the sound to take on. Techniques for accomplishing such mappings and enabling feature-based synthesis to be performed in real time are discussed. An example is given of how a real-time performance system might be designed to take advantage of feature-based synthesis's ability to provide perceptually meaningful control over a large number of synthesis parameters. }
}

@inproceedings{Hashida2007,
  author = {Hashida, Mitsuyo and Nagata, Noriko and Katayose, Haruhiro},
  title = {jPop-E : An Assistant System for Performance Rendering of Ensemble Music},
  pages = {313--316},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177111},
  url = {http://www.nime.org/proceedings/2007/nime2007_313.pdf},
  keywords = {Performance Rendering, User Interface, Ensemble Music Ex- pression },
  abstract = {This paper introduces jPop-E (java-based PolyPhrase Ensemble), an assistant system for the Pop-E performancerendering system. Using this assistant system, MIDI dataincluding expressive tempo changes or velocity control canbe created based on the user's musical intention. Pop-E(PolyPhrase Ensemble) is one of the few machine systemsdevoted to creating expressive musical performances thatcan deal with the structure of polyphonic music and theuser's interpretation of the music. A well-designed graphical user interface is required to make full use of the potential ability of Pop-E. In this paper, we discuss the necessaryelements of the user interface for Pop-E, and describe theimplemented system, jPop-E.}
}

@inproceedings{Sarkar2007,
  author = {Sarkar, Mihir and Vercoe, Barry},
  title = {Recognition and Prediction in a Network Music Performance System for {India}n Percussion},
  pages = {317--320},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177239},
  url = {http://www.nime.org/proceedings/2007/nime2007_317.pdf},
  keywords = {network music performance, real-time online musical collab- oration, Indian percussions, tabla bols, strokes recognition, music prediction },
  abstract = {Playing music over the Internet, whether for real-time jamming, network performance or distance education, is constrained by the speed of light which introduces, over long distances, time delays unsuitable for musical applications. Current musical collaboration systems generally transmit compressed audio streams over low-latency and high-bandwidthnetworks to optimize musician synchronization. This paperproposes an alternative approach based on pattern recognition and music prediction. Trained for a particular typeof music, here the Indian tabla drum, the system calledTablaNet identifies rhythmic patterns by recognizing individual strokes played by a musician and mapping them dynamically to known musical constructs. Symbols representing these musical structures are sent over the network toa corresponding computer system. The computer at thereceiving end anticipates incoming events by analyzing previous phrases and synthesizes an estimated audio output.Although such a system may introduce variants due to prediction approximations, resulting in a slightly different musical experience at both ends, we find that it demonstratesa high level of playability with an immediacy not present inother systems, and functions well as an educational tool.}
}

@inproceedings{Vigoda2007,
  author = {Vigoda, Benjamin and Merrill, David},
  title = {JamiOki-PureJoy : A Game Engine and Instrument for Electronically-Mediated Musical Improvisation},
  pages = {321--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179473},
  url = {http://www.nime.org/proceedings/2007/nime2007_321.pdf},
  keywords = {JamiOki, PureJoy, collaborative performance, structured im- provisation, electronically-mediated performance, found sound },
  abstract = {JamiOki-PureJoy is a novel electronically mediated musical performance system. PureJoy is a musical instrument; A highly flexible looper, sampler, effects processor and sound manipulation interface based on Pure Data, with input from a joystick controller and headset microphone. PureJoy allows the player to essentially sculpt their voice with their hands. JamiOki is an engine for running group-player musical game pieces. JamiOki helps each player by ‘whispering instructions’ in their ear. Players track and control their progress through the game using a graphical display and a touch-sensitive footpad. JamiOki is an architecture for bringing groups of players together to express themselves musically in a way that is both spontaneous and formally satisfying. The flexibility of the PureJoy instrument offers to JamiOki the ability for any player to play any requested role in the music at any time. The musical structure provided by JamiOki helps PureJoy players create more complex pieces of music on the fly with spontaneous sounds, silences, themes, recapitulation, tight transitions, structural hierarchy, interesting interactions, and even friendly competition. As a combined system JamiOki-PureJoy is exciting and fun to play.}
}

@inproceedings{Gomez2007,
  author = {G\'{o}mez, Daniel and Donner, Tjebbe and Posada, Andr\'{e}s},
  title = {A Look at the Design and Creation of a Graphically Controlled Digital Musical Instrument},
  pages = {327--329},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177097},
  url = {http://www.nime.org/proceedings/2007/nime2007_327.pdf},
  keywords = {nime07},
  abstract = {In this article we want to show how graphical languages can be used successfully for monitoring and controlling a digital musical instrument. An overview of the design and development stages of this instrument shows how we can create models which will simplify the control and use of different kinds of musical algorithms for synthesis and sequencing.}
}

@inproceedings{Vanegas2007,
  author = {Vanegas, Roy},
  title = {The {MIDI} Pick : Trigger Serial Data , Samples, and {MIDI} from a Guitar Pick},
  pages = {330--333},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179471},
  url = {http://www.nime.org/proceedings/2007/nime2007_330.pdf},
  keywords = {guitar, MIDI, pick, plectrum, wireless, bluetooth, ZigBee, Arduino, NIME, ITP },
  abstract = {The guitar pick has traditionally been used to strike or rakethe strings of a guitar or bass, and in rarer instances, ashamisen, lute, or other stringed instrument. The pressure exerted on it, however, has until now been ignored.The MIDI Pick, an enhanced guitar pick, embraces this dimension, acting as a trigger for serial data, audio samples,MIDI messages 1, Max/MSP patches, and on/off messages.This added scope expands greatly the stringed instrumentplayer's musical dynamic in the studio or on stage.}
}

@inproceedings{Benning2007,
  author = {Benning, Manjinder S. and McGuire, Michael and Driessen, Peter},
  title = {Improved Position Tracking of a {3-D} Gesture-Based Musical Controller Using a {Kalman} Filter},
  pages = {334--337},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177043},
  url = {http://www.nime.org/proceedings/2007/nime2007_334.pdf},
  keywords = {Kalman Filtering, Radiodrum, Gesture Tracking, Interacting Multiple Model INTRODUCTION Intention is a key aspect of traditional music performance. The ability for an artist to reliably reproduce sound, pitch, rhythms, and emotion is paramount to the design of any instrument. With the },
  abstract = {This paper describes the design and experimentation of a Kalman Filter used to improve position tracking of a 3-D gesture-based musical controller known as the Radiodrum. The Singer dynamic model for target tracking is used to describe the evolution of a Radiodrum's stick position in time. The autocorrelation time constant of a gesture's acceleration and the variance of the gesture acceleration are used to tune the model to various performance modes. Multiple Kalman Filters tuned to each gesture type are run in parallel and an Interacting Multiple Model (IMM) is implemented to decide on the best combination of filter outputs to track the current gesture. Our goal is to accurately track Radiodrum gestures through noisy measurement signals. }
}

@inproceedings{Keating2007,
  author = {Keating, Noah H.},
  title = {The Lambent Reactive : An Audiovisual Environment for Kinesthetic Playforms},
  pages = {338--343},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177141},
  url = {http://www.nime.org/proceedings/2007/nime2007_338.pdf},
  keywords = {Responsive Environments, Audiovisual Play, Kinetic Games, Movement Rich Game Play, Immersive Dance, Smart Floor },
  abstract = {In this paper, design scenarios made possible by the use of an interactive illuminated floor as the basis of an audiovisual environment are presented. By interfacing a network of pressure sensitive, light-emitting tiles with a 7.1 channel speaker system and requisite audio software, many avenues for collaborative expression emerge, as do heretofore unexplored modes of multiplayer music and dance gaming. By giving users light and sound cues that both guide and respond to their movement, a rich environment is created that playfully integrates the auditory, the visual, and the kinesthetic into a unified interactive experience.}
}

@inproceedings{Stark2007,
  author = {Stark, Adam M. and Plumbley, Mark D. and Davies, Matthew E.},
  title = {Real-Time Beat-Synchronous Audio Effects},
  pages = {344--345},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177249},
  url = {http://www.nime.org/proceedings/2007/nime2007_344.pdf},
  keywords = {a beat-synchronous tremolo effect,audio effects,beat tracking,figure 1,im-,nime07,plemented as a vst,plug-in,real-time,the rate is controlled,vst plug-in},
  abstract = {We present a new group of audio effects that use beat tracking, the detection of beats in an audio signal, to relate effectparameters to the beats in an input signal. Conventional audio effects are augmented so that their operation is related tothe output of a beat tracking system. We present a temposynchronous delay effect and a set of beat synchronous lowfrequency oscillator effects including tremolo, vibrato andauto-wah. All effects are implemented in real-time as VSTplug-ins to allow for their use in live performance.}
}

@inproceedings{Wulfson2007,
  author = {Wulfson, Harris and Barrett, G. Douglas and Winter, Michael},
  title = {Automatic Notation Generators},
  pages = {346--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176861},
  url = {http://www.nime.org/proceedings/2007/nime2007_346.pdf},
  keywords = {nime07},
  abstract = {This article presents various custom software tools called Automatic Notation Generators (ANG's) developed by the authors to aid in the creation of algorithmic instrumental compositions. The unique possibilities afforded by ANG software are described, along with relevant examples of their compositional output. These avenues of exploration include: mappings of spectral data directly into notated music, the creation of software transcribers that enable users to generate multiple realizations of algorithmic compositions, and new types of spontaneous performance with live generated screen-based music notation. The authors present their existing software tools along with suggestions for future research and artistic inquiry. }
}

@inproceedings{Young2007,
  author = {Young, Diana and Deshmane, Anagha},
  title = {Bowstroke Database : A Web-Accessible Archive of Violin Bowing Data},
  pages = {352--357},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179481},
  url = {http://www.nime.org/proceedings/2007/nime2007_352.pdf},
  keywords = {violin, bowed string, bowstroke, bowing, bowing parameters, technique, gesture, audio },
  abstract = {This paper presents a newly created database containing calibrated gesture and audio data corresponding to various violin bowstrokes, as well as video and motion capture data in some cases. The database is web-accessible and searchable by keywords and subject. It also has several important features designed to improve accessibility to the data and to foster collaboration between researchers in fields related to bowed string synthesis, acoustics, and gesture.}
}

@inproceedings{Schacher2007,
  author = {Schacher, Jan C.},
  title = {Gesture Control of Sounds in {3D} Space},
  pages = {358--362},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177241},
  url = {http://www.nime.org/proceedings/2007/nime2007_358.pdf},
  keywords = {Gesture, Surround Sound, Mapping, Trajectory, Transform Matrix, Tree Hierarchy, Emergent Structures. },
  abstract = {This paper presents a methodology and a set of tools for gesture control of sources in 3D surround sound. The techniques for rendering acoustic events on multi-speaker or headphone-based surround systems have evolved considerably, making it possible to use them in real-time performances on light equipment. Controlling the placement of sound sources is usually done in idiosyncratic ways and has not yet been fully explored and formalized. This issue is addressed here with the proposition of a methodical approach. The mapping of gestures to source motion is implemented by giving the sources physical object properties and manipulating these characteristics with standard geometrical transforms through hierarchical or emergent relationships. }
}

@inproceedings{Porres2007,
  author = {Porres, Alexandre T. and Manzolli, Jonatas},
  title = {Adaptive Tuning Using Theremin as Gestural Controller},
  pages = {363--366},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177223},
  url = {http://www.nime.org/proceedings/2007/nime2007_363.pdf},
  keywords = {Interaction, adaptive tuning, theremin, sensorial dissonance, synthesis. },
  abstract = {This work presents an interactive device to control an adaptive tuning and synthesis system. The gestural controller is based on the theremin concept in which only an antenna is used as a proximity sensor. This interactive process is guided by sensorial consonance curves and adaptive tuning related to psychoacoustical studies. We used an algorithm to calculate the dissonance values according to amplitudes and frequencies of a given sound spectrum. The theoretical background is presented followed by interactive composition strategies and sound results. }
}

@inproceedings{Hsu2007,
  author = {Hsu, William},
  title = {Design Issues in Interaction Modeling for Free Improvisation},
  pages = {367--370},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177123},
  url = {http://www.nime.org/proceedings/2007/nime2007_367.pdf},
  keywords = {Interactive music systems, timbral analysis, free improvisation. },
  abstract = {In previous publications (see for example [2] and [3]), we described an interactive music system, designed to improvise with saxophonist John Butcher; our system analyzes timbral and gestural features in real-time, and uses this information to guide response generation. This paper overviews our recent work with the system's interaction management component (IMC). We explore several options for characterizing improvisation at a higher level, and managing decisions for interactive performance in a rich timbral environment. We developed a simple, efficient framework using a small number of features suggested by recent work in mood modeling in music. We describe and evaluate the first version of the IMC, which was used in performance at the Live Algorithms for Music (LAM) conference in December 2006. We touch on developments on the system since LAM, and discuss future plans to address perceived shortcomings in responsiveness, and the ability of the system to make long-term adaptations. }
}

@inproceedings{Groux2007,
  author = {le Groux, Sylvain and Manzolli, Jonatas and Verschure, Paul F.},
  title = {VR-RoBoser : Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior},
  pages = {371--374},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177101},
  url = {http://www.nime.org/proceedings/2007/nime2007_371.pdf},
  keywords = {Real-time Composition, Interactive Sonification, Real-time Neural Processing, Multimedia, Virtual Environment, Avatar. },
  abstract = {Until recently, the sonification of Virtual Environments had often been reduced to its simplest expression. Too often soundscapes and background music are predetermined, repetitive and somewhat predictable. Yet, there is room for more complex and interesting sonification schemes that can improve the sensation of presence in a Virtual Environment. In this paper we propose a system that automatically generates original background music in real-time called VR-RoBoser. As a test case we present the application of VR-RoBoser to a dynamic avatar that explores its environment. We show that the musical events are directly and continuously generated and influenced by the behavior of the avatar in three-dimensional virtual space, generating a context dependent sonification. }
}

@inproceedings{Steiner2007,
  author = {Steiner, Hans-Christoph and Merrill, David and Matthes, Olaf},
  title = {A Unified Toolkit for Accessing Human Interface Devices in Pure Data and Max / MSP},
  pages = {375--378},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177251},
  url = {http://www.nime.org/proceedings/2007/nime2007_375.pdf},
  keywords = {nime07},
  abstract = {In this paper we discuss our progress on the HID toolkit, a collection of software modules for the Pure Data and Max/MSP programming environments that provide unified, user-friendly and cross-platform access to human interface devices (HIDs) such as joysticks, digitizer tablets, and stomp-pads. These HIDs are ubiquitous, inexpensive and capable of sensing a wide range of human gesture, making them appealing interfaces for interactive media control. However, it is difficult to utilize many of these devices for custom-made applications, particularly for novices. The modules we discuss in this paper are [hidio], which handles incoming and outgoing data between a patch and a HID, and [input noticer], which monitors HID plug/unplug events. The goal in creating these modules is to preserve maximal flexibility in accessing the input and output capabilities of HIDs, in a manner that is ap- proachable for both sophisticated and beginning designers. This paper documents our design notes and implementa- tion considerations, current progress, and ideas for future extensions to the HID toolkit.}
}

@inproceedings{Nort2007,
  author = {Van Nort, Doug and Wanderley, Marcelo M.},
  title = {Control Strategies for Navigation of Complex Sonic Spaces Transformation of Resonant Models},
  pages = {379--383},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179469},
  url = {http://www.nime.org/proceedings/2007/nime2007_379.pdf},
  keywords = {Mapping, Control, Sound Texture, Musical Gestures },
  abstract = {This paper describes musical experiments aimed at designing control structures for navigating complex and continuous sonic spaces. The focus is on sound processing techniques which contain a high number of control parameters,and which exhibit subtle and interesting micro-variationsand textural qualities when controlled properly. The examples all use a simple low-dimensional controller --- a standard graphics tablet --- and the task of initimate and subtle textural manipulations is left to the design of proper mappings,created using a custom toolbox of mapping functions. Thiswork further acts to contextualize past theoretical results bythe given musical presentations, and arrives at some conclusions about the interplay between musical intention, controlstrategies and the process of their design.}
}

@inproceedings{Knorig2007,
  author = {Kn\''{o}rig, Andr\'{e} and M\''{u}ller, Boris and Wettach, Reto},
  title = {Articulated Paint : Musical Expression for Non-Musicians},
  pages = {384--385},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177155},
  url = {http://www.nime.org/proceedings/2007/nime2007_384.pdf},
  keywords = {musical interface, musical expression, expressive gesture, musical education, natural interface },
  abstract = {In this paper we present the concept and prototype of a new musical interface that utilizes the close relationship between gestural expression in the act of painting and that of playing a musical instrument in order to provide non-musicians the opportunity to create musical expression. A physical brush on a canvas acts as the instrument. The characteristics of its stroke are intuitively mapped to a conductor program, defining expressive parameters of the tone in real-time. Two different interaction modes highlight the importance of bodily expression in making music as well as the value of a metaphorical visual representation.}
}

@inproceedings{Baba2007,
  author = {Baba, Tetsuaki and Ushiama, Taketoshi and Tomimatsu, Kiyoshi},
  title = {Freqtric Drums : A Musical Instrument that Uses Skin Contact as an Interface},
  pages = {386--387},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177037},
  url = {http://www.nime.org/proceedings/2007/nime2007_386.pdf},
  keywords = {interpersonal communication, musical instrument, interaction design, skin contact, touch },
  abstract = {Freqtric Drums is a new musical, corporal electronic instrument that allows us not only to recover face-to-face communication, but also makes possible body-to-body communication so that a self image based on the sense of being a separate body can be signicant altered through an openness toand even a sense of becoming part of another body. FreqtricDrums is a device that turns audiences surrounding a performer into drums so that the performer, as a drummer, cancommunicate with audience members as if they were a setof drums. We describe our concept and the implementationand process of evolution of Freqtric Drums.}
}

@inproceedings{Han2007,
  author = {Han, Chang Min},
  title = {Project Scriabin v.3},
  pages = {388--389},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177109},
  url = {http://www.nime.org/proceedings/2007/nime2007_388.pdf},
  keywords = {Synaesthesia, Sonification, Touch Screen},
  abstract = {Project Scriabin is an interactive implementation of Alexander Scriabin’s experimentation with “opposite mapping direction”, that is, mapping from hue (colour) to pitch (sound). Main colour to sound coding was implemented by Scriabin’s colour scale.}
}

@inproceedings{Castellano2007,
  author = {Castellano, Ginevra and Bresin, Roberto and Camurri, Antonio and Volpe, Gualtiero},
  title = {Expressive Control of Music and Visual Media by Full-Body Movement},
  pages = {390--391},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177065},
  url = {http://www.nime.org/proceedings/2007/nime2007_390.pdf},
  keywords = {Expressive interaction; multimodal environments; interactive music systems },
  abstract = {In this paper we describe a system which allows users to use their full-body for controlling in real-time the generation of an expressive audio-visual feedback. The system extracts expressive motion features from the user's full-body movements and gestures. The values of these motion features are mapped both onto acoustic parameters for the real-time expressive rendering of a piece of music, and onto real-time generated visual feedback projected on a screen in front of the user. }
}

@inproceedings{Tindale2007,
  author = {Tindale, Adam R.},
  title = {A Hybrid Method for Extended Percussive Gesture},
  pages = {392--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177259},
  url = {http://www.nime.org/proceedings/2007/nime2007_392.pdf},
  keywords = {electronic percussion,nime07,physical modeling,timbre recognition},
  abstract = {This paper describes a hybrid method to allow drummers to expressively utilize electronics. Commercial electronic drum hardware is made more expressive by replacing the sample playback “drum brain” with a physical modeling algorithm implemented in Max/MSP. Timbre recognition techniques identify striking implement and location as symbolic data that can be used to modify the parameters of the physical model.}
}

@inproceedings{Bottoni2007,
  author = {Bottoni, Paolo and Caporali, Riccardo and Capuano, Daniele and Faralli, Stefano and Labella, Anna and Pierro, Mario},
  title = {Use of a Dual-Core {DSP} in a Low-Cost, Touch-Screen Based Musical Instrument},
  pages = {394--395},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177053},
  url = {http://www.nime.org/proceedings/2007/nime2007_394.pdf},
  keywords = {dual-core, DSP, touch-screen, synthesizer, controller },
  abstract = {This paper reports our experiments on using a dual-coreDSP processor in the construction of a user-programmablemusical instrument and controller called the TouchBox.}
}

@inproceedings{Kanebako2007,
  author = {Kanebako, Junichi and Gibson, James and Mignonneau, Laurent},
  title = {Mountain Guitar : a Musical Instrument for Everyone},
  pages = {396--398},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177133},
  url = {http://www.nime.org/proceedings/2007/nime2007_396.pdf},
  keywords = {Musical Expression, Guitar Instrument, MIDI to sensor mapping, Physical Computing, Intuitive Interaction},
  abstract = {This instrument is a part of the “Gangu Project” at IAMAS, which aim to develop digital toys for improving children’s social behavior in the future. It was further developed as part of the IAMAS-Interface Cultures exchange program. “Mountain Guitar” is a new musical instrument that enables musical expression through a custom-made sensor technology, which captures and transforms the height at which the instrument is held to the musical outcome during the playing session. One of the goals of “Mountain Guitar” is to let untrained users easily and intuitively play guitar through their body movements. In addition to capturing the users’ body movements, “Mountain Guitar” also simulates standard guitar playing techniques such as vibrato, choking, and mute. “Mountain Guitar’s” goal is to provide playing pleasure for guitar training sessions. This poster describes the “Mountain Guitar’s” fundamental principles and its mode of operation.}
}

@inproceedings{Sirguy2007,
  author = {Sirguy, Marc and Gallin, Emmanuelle},
  title = {Eobody2 : A Follow-up to Eobody's Technology},
  pages = {401--402},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177247},
  url = {http://www.nime.org/proceedings/2007/nime2007_401.pdf},
  keywords = {Gestural controller, Sensor, MIDI, USB, Computer music, Relays, Motors, Robots, Wireless. },
  abstract = {Eowave and Ircam have been deeply involved into gestureanalysis and sensing for a few years by now, as severalartistic projects demonstrate (1). In 2004, Eowave has beenworking with Ircam on the development of the Eobodysensor system, and since that, Eowave's range of sensors hasbeen increased with new sensors sometimes developed innarrow collaboration with artists for custom sensor systemsfor installations and performances. This demo-paperdescribes the recent design of a new USB/MIDI-to-sensorinterface called Eobody2.}
}

@inproceedings{Till2007,
  author = {Till, Bernie C. and Benning, Manjinder S. and Livingston, Nigel},
  title = {Wireless Inertial Sensor Package (WISP)},
  pages = {403--404},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177257},
  url = {http://www.nime.org/proceedings/2007/nime2007_403.pdf},
  keywords = {Music Controller, Human-Computer Interaction, Wireless Sensing, Inertial Sensing. },
  abstract = {The WISP is a novel wireless sensor that uses 3 axis magnetometers, accelerometers, and rate gyroscopes to provide a real-time measurement of its own orientation in space. Orientation data are transmitted via the Open Sound Control protocol (OSC) to a synthesis engine for interactive live dance performance. }
}

@inproceedings{Loewenstein2007,
  author = {Loewenstein, Stefan},
  title = {"Acoustic Map" -- An Interactive Cityportrait},
  pages = {405--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177167},
  url = {http://www.nime.org/proceedings/2007/nime2007_405.pdf},
  keywords = {nime07},
  abstract = {The ”Acoustic Map“ is an interactive soundinstallation developed for the “Hallakustika” Festival in Hall (Tyrolia, Austria) using the motion tracking software Eyes-Web and Max-MSP. For the NIME 07 a simulation of the motion tracking part of the original work will be shown. Its aim was to create an interactive city portrait of the city of Hall and to offer the possibility to enhance six sites of the city on an acoustical basis with what I called an “acoustic zoom”.}
}

@inproceedings{Hashida2007a,
  author = {Hashida, Tomoko and Naemura, Takeshi and Sato, Takao},
  title = {A System for Improvisational Musical Expression Based on Player's Sense of Tempo},
  pages = {407--408},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177113},
  url = {http://www.nime.org/proceedings/2007/nime2007_407.pdf},
  keywords = {Improvisation, interactive music, a sense of tempo },
  abstract = {This paper introduces a system for improvisational musical expression that enables all users, novice and experienced, to perform intuitively and expressively. Users can generate musically consistent results through intuitive action, inputting rhythm in a decent tempo. We demonstrate novel mapping ways that reflect user’s input information more interactively and effectively in generating the music. We also present various input devices that allow users more creative liberty.}
}

@inproceedings{Nakamoto2007,
  author = {Nakamoto, Misako and Kuhara, Yasuo},
  title = {Circle Canon Chorus System Used To Enjoy A Musical Ensemble Singing "Frog Round"},
  pages = {409--410},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177207},
  url = {http://www.nime.org/proceedings/2007/nime2007_409.pdf},
  keywords = {Circle canon, Chorus, Song, Frog round, Ensemble, Internet, Max/MSP, MySQL database. },
  abstract = {We proposed a circle canon system for enjoying a musical ensemble supported by a computer and network. Using the song {Frog round}, which is a popular circle canon chorus originated from a German folk song, we produced a singing ensemble opportunity where everyone plays the music together at the same time. The aim of our system is that anyone can experience the joyful feeling of actually playing the music as well as sharing it with others. }
}

@inproceedings{Estrada2007,
  author = {Pereira, Rui},
  title = {Loop-R : Real-Time Video Interface},
  pages = {411--414},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177219},
  url = {http://www.nime.org/proceedings/2007/nime2007_411.pdf},
  keywords = {Real-time; video; interface; live-visuals; loop; },
  abstract = {Loop-R is a real-time video performance tool, based in the exploration of low-tech, used technology and human engineering research. With this tool its author is giving a shout to industry, using existing and mistreated technology in innovative ways, combining concepts and interfaces: blending segregated interfaces (GUI and Physical) into one. After graspable interfaces and the “end” of WIMP interfaces, hardware and software blend themselves in a new genre providing free control of video-loops in an expressive hybrid tool.}
}

@inproceedings{Rigler2007,
  author = {Rigler, Jane and Seldess, Zachary},
  title = {The Music Cre8tor : an Interactive System for Musical Exploration and Education},
  pages = {415--416},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177227},
  url = {http://www.nime.org/proceedings/2007/nime2007_415.pdf},
  keywords = {Music Education, disabilities, special education, motion sensors, music composition, interactive performance. },
  abstract = {The Music Cre8tor is an interactive music composition system controlled by motion sensors specifically designed for children with disabilities although not exclusively for this population. The player(s) of the Music Cre8tor can either hold or attach accelerometer sensors to trigger a variety of computer-generated sounds, MIDI instruments and/or pre-recorded sound files. The sensitivity of the sensors can be modified for each unique individual so that even the smallest movement can control a sound. The flexibility of the system is such that either four people can play simultaneously and/or one or more players can use up to four sensors. The original goal of this program was to empower students with disabilities to create music and encourage them to perform with other musicians, however this same goal has expanded to include other populations.}
}

@inproceedings{Guedes2007,
  author = {Guedes, Carlos},
  title = {Establishing a Musical Channel of Communication between Dancers and Musicians in Computer-Mediated Collaborations in Dance Performance},
  pages = {417--419},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177105},
  url = {http://www.nime.org/proceedings/2007/nime2007_417.pdf},
  keywords = {dance,in dance,interaction between music and,interactive,interactive dance,interactive performance,musical rhythm and rhythm,nime07,performance systems},
  abstract = {In this demonstration, I exemplify how a musical channel ofcommunication can be established in computer-mediatedinteraction between musicians and dancers in real time. Thischannel of communication uses a software libraryimplemented as a library of external objects for Max/MSP[1],that processes data from an object or library that performsframe-differencing analysis of a video stream in real time inthis programming environment.}
}

@inproceedings{Bull2007,
  author = {Bull, Steve and Gresham-Lancaster, Scot and Mintchev, Kalin and Svoboda, Terese},
  title = {Cellphonia : WET},
  pages = {420--420},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177057},
  url = {http://www.nime.org/proceedings/2007/nime2007_420.pdf},
  keywords = {nime07}
}

@inproceedings{Court2007,
  author = {Collective Dearraindrop},
  title = {Miller},
  pages = {421--421},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177083},
  url = {http://www.nime.org/proceedings/2007/nime2007_421.pdf},
  keywords = {nime07}
}

@inproceedings{Hauert2007,
  author = {Hauert, Sibylle and Reichmuth, Daniel and B\"{o}hm, Volker},
  title = {Instant City, a Music Building Game Table},
  pages = {422--422},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177115},
  url = {http://www.nime.org/proceedings/2007/nime2007_422.pdf},
  keywords = {nime07}
}

@inproceedings{Milmoe2007,
  author = {Milmoe, Andrew},
  title = {NIME Performance \& Installation : Sonic Pong V3.0},
  pages = {423--423},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177197},
  url = {http://www.nime.org/proceedings/2007/nime2007_423.pdf},
  keywords = {nime07}
}

@inproceedings{Biggs2007,
  author = {Biggs, Betsey},
  title = {The Tipping Point},
  pages = {424--424},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177047},
  url = {http://www.nime.org/proceedings/2007/nime2007_424.pdf},
  keywords = {nime07}
}

@inproceedings{Morris2007,
  author = {Morris, Simon},
  title = {Musique Concrete : Transforming Space , Sound and the City Through Skateboarding},
  pages = {425--425},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177203},
  url = {http://www.nime.org/proceedings/2007/nime2007_425.pdf},
  keywords = {nime07}
}

@inproceedings{Uozumi2007,
  author = {Uozumi, Yuta and Takahashi, Masato and Kobayashi, Ryoho},
  title = {Bd : A Sound Installation with Swarming Robots},
  pages = {426--426},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179467},
  url = {http://www.nime.org/proceedings/2007/nime2007_426.pdf},
  keywords = {nime07}
}

@inproceedings{Stanza2007,
  author = {, Stanza},
  title = {Sensity},
  pages = {427--427},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177029},
  url = {http://www.nime.org/proceedings/2007/nime2007_427.pdf},
  keywords = {nime07}
}

@inproceedings{Sa2007,
  author = {Sa, Adriana},
  title = {Thresholds},
  pages = {428--428},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177235},
  url = {http://www.nime.org/proceedings/2007/nime2007_428.pdf},
  keywords = {nime07}
}

@inproceedings{Takahashi2007,
  author = {Takahashi, Masato and Tanaka, Hiroya},
  title = {bog : Instrumental Aliens},
  pages = {429--429},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177253},
  url = {http://www.nime.org/proceedings/2007/nime2007_429.pdf},
  keywords = {nime07}
}

@inproceedings{Oliver2007,
  author = {Oliver, Julian and Pickles, Steven},
  title = {Fijuu2 : A Game-Based Audio-Visual Performance and Composition Engine},
  pages = {430--430},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177213},
  url = {http://www.nime.org/proceedings/2007/nime2007_430.pdf},
  keywords = {nime07}
}

@inproceedings{Corness2007,
  author = {Seo, Jinsil and Corness, Greg},
  title = {nite\_aura : An Audio-Visual Interactive Immersive Installation},
  pages = {431--431},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177243},
  url = {http://www.nime.org/proceedings/2007/nime2007_431.pdf},
  keywords = {nime07}
}

@inproceedings{Historia2007,
  author = {\'{A}lvarez-Fern\'{a}ndez, Miguel and Kersten, Stefan and Piascik, Asia},
  title = {Soundanism},
  pages = {432--432},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177031},
  url = {http://www.nime.org/proceedings/2007/nime2007_432.pdf},
  keywords = {nime07}
}

@inproceedings{Quessy2007,
  author = {Quessy, Alexandre},
  title = {Human Sequencer},
  pages = {433--433},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2007},
  address = {New York City, NY, United States},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177225},
  url = {http://www.nime.org/proceedings/2007/nime2007_433.pdf},
  keywords = {nime07}
}

