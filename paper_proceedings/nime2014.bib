@inproceedings{lpereira2014,
  author = {Luisa Pereira Hors},
  title = {The Well-Sequenced Synthesizer},
  pages = {88--89},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178806},
  url = {http://www.nime.org/proceedings/2014/nime2014_2.pdf},
  abstract = {The Well--Sequenced Synthesizer is a series of sequencers that create music in dialog with the user. Through the sequencers' physical interfaces, users can control music theory-based generative algorithms. This series --a work-in-progress-is composed by three sequencers at this time. The first one, called The Counterpointer, takes a melody input from the user and responds by generating voices based on the rules of eighteenth--century counterpoint. The second one is based on a recent treatise on harmony and counterpoint by music theorist Dmitri Tymoczco: El Ordenador lets users explore a set of features of tonality by constraining randomly generated music according to one or more of them. El Ordenador gives the user less control than The Counterpointer, but more than La Mec{\'a}nica, the third sequencer in the series. La Mec{\'a}nica plays back the sequences generated by El Ordenador using a punch-card reading music box mechanism. It makes the digital patterns visible and tactile, and links them back to the physical world.}
}

@inproceedings{ptimothy2014,
  author = {Timothy Polashek and Brad Meyer},
  title = {Engravings for Prepared Snare Drum, iPad, and Computer},
  pages = {82--83},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178907},
  url = {http://www.nime.org/proceedings/2014/nime2014_254.pdf},
  abstract = {This paper describes the technologies, collaborative processes, and artistic intents of the musical composition Engravings for Prepared Snare Drum, iPad, and Computer, which was composed by Timothy Polashek for percussionist Brad Meyer using a jointly created electroacoustic and interactive musical instrument. During performance, the percussionist equally manipulates and expresses through two surfaces, an iPad displaying an interactive touch screen and a snare drum augmented with various foreign objects, including a contact microphone adhered to the drumhead's surface. A computer program created for this composition runs on a laptop computer in front of the percussionist. The software captures sound from the contact microphone and transforms this sound through audio signal processing controlled by the performer's gestures on the iPad. The computer screen displays an animated graphic score, as well as the current states of iPad controls and audio signal processing, for the performer. Many compositional and technological approaches used in this project pay tribute to composer John Cage, since the premiere performance of Engravings for Prepared Snare Drum, iPad, and Computer took place in 2012, the centennial celebration of Cage's birth year.}
}

@inproceedings{mzareei2014,
  author = {Mo Zareei and Ajay Kapur and Dale A. Carnegie},
  title = {Rasper: a Mechatronic Noise-Intoner},
  pages = {473--478},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178995},
  url = {http://www.nime.org/proceedings/2014/nime2014_268.pdf},
  abstract = {Over the past few decades, there has been an increasing number of musical instruments and works of sound art that incorporate robotics and mechatronics. This paper proposes a new approach in classification of such works and focuses on those whose ideological roots can be sought in Luigi Russolo's noise-intoners (intonarumori). It presents a discussion on works in which mechatronics is used to investigate new and traditionally perceived as ``extra-musical'' sonic territories, and introduces Rasper: a new mechatronic noise-intoner that features an electromechanical apparatus to create noise physically, while regulating it rhythmically and timbrally.}
}

@inproceedings{cudell2014,
  author = {Chet Udell and James Paul Sain},
  title = {eMersion | Sensor-controlled Electronic Music Modules \& Digital Data Workstation},
  pages = {130--133},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178971},
  url = {http://www.nime.org/proceedings/2014/nime2014_272.pdf},
  abstract = {In our current era, where smartphones are commonplace and buzzwords like ``the internet of things,'' ``wearable tech,'' and ``augmented reality'' are ubiquitous, translating performance gestures into data and intuitively mapping it to control musical/visual parameters in the realm of computing should be trivial; but it isn't. Technical barriers still persist that limit this activity to exclusive groups capable of learning skillsets far removed from one's musical craft. These skills include programming, soldering, microprocessors, wireless protocols, and circuit design. Those of us whose creative activity is centered in NIME have to become polyglots of many disciplines to achieve our work. In the NIME community, it's unclear that we should even draw distinctions between 'artist' and 'technician', because these skillsets have become integral to our creative practice. However, what about the vast communities of musicians, composers, and artists who want to leverage sensing to take their craft into new territory with no background in circuits, soldering, embedded programming, and sensor function? eMersion, a plug-and-play, modular, wireless alternative solution for creating NIMEs will be presented. It enables one to bypass the technical hurdles listed above in favor of immediate experimentation with musical practice and wireless sensing. A unique software architecture will also be unveiled that enables one to quickly and intuitively process and map unpredictable numbers and types of wireless data streams, the Digital Data Workstation.}
}

@inproceedings{tmurraybrowne2014,
  author = {Tim Murray-Browne and Mark Plumbley},
  title = {Harmonic Motion: A Toolkit for Processing Gestural Data for Interactive Sound},
  pages = {213--216},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178887},
  url = {http://www.nime.org/proceedings/2014/nime2014_273.pdf},
  abstract = {We introduce Harmonic Motion, a free open source toolkit for artists, musicians and designers working with gestural data. Extracting musically useful features from captured gesture data can be challenging, with projects often requiring bespoke processing techniques developed through iterations of tweaking equations involving a number of constant values -sometimes referred to as `magic numbers'. Harmonic Motion provides a robust interface for rapid prototyping of patches to process gestural data and a framework through which approaches may be encapsulated, reused and shared with others. In addition, we describe our design process in which both personal experience and a survey of potential users informed a set of specific goals for the software.}
}

@inproceedings{slui2014,
  author = {Simon Lui},
  title = {A Real Time Common Chord Progression Guide on the Smartphone for Jamming Pop Song on the Music Keyboard},
  pages = {98--101},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178855},
  url = {http://www.nime.org/proceedings/2014/nime2014_275.pdf},
  abstract = {Pop music jamming on the keyboard requires massive music knowledge. Musician needs to understand and memorize the behavior of each chord in different keys. However, most simple pop music follows a common chord progression pattern. This pattern applies to most simple pop music on all the 12 keys. We designed an app that can reduce the difficulty of music jamming on the keyboard by using this pattern. The app displays the current chord in the Roman numeral and suggests the expected next chord in an easy to understand way on a smartphone. This work investigates into the human computer interaction perspective of music performance. We use a smartphone app as a bridge, which assists musician to react faster in music jamming by transforming the complex music knowledge into a simple, unified and easy to understand format. Experiment result shows that this app can help the non-keyboardist musician to learn pop music jamming. It also shows that the app is useful to assist keyboardist in making key transpose and playing music in the key with many sharps and flats. We will use the same interface design to guide user on playing other chord progressions such as the jazz chord progression.}
}

@inproceedings{tmagnusson2014,
  author = {Thor Magnusson},
  title = {Improvising with the Threnoscope: Integrating Code, Hardware, GUI, Network, and Graphic Scores},
  pages = {19--22},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178857},
  url = {http://www.nime.org/proceedings/2014/nime2014_276.pdf},
  abstract = {Live coding emphasises improvisation. It is an art practice that merges the act of musical composition and performance into a public act of projected writing. This paper introduces the Threnoscope system, which includes a live coding micro-language for drone-based microtonal composition. The paper discusses the aims and objectives of the system, elucidates the design decisions, and introduces in particular the code score feature present in the Threnoscope. The code score is a novel element in the design of live coding systems allowing for improvisation through a graphic score, rendering a visual representation of past and future events in a real-time performance. The paper demonstrates how the system's methods can be mapped ad hoc to GUIor hardware-based control.}
}

@inproceedings{strump2014,
  author = {Sebastian Trump and Jamie Bullock},
  title = {Orphion: A Gestural Multi-Touch Instrument for the iPad},
  pages = {159--162},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178963},
  url = {http://www.nime.org/proceedings/2014/nime2014_277.pdf},
  abstract = {This paper describes the concept and design of Orphion, a new digital musical instrument based on the Apple iPad. We begin by outlining primary challenges associated with DMI design, focussing on the specific problems Orphion seeks to address such as requirements for haptic feedback from the device. Orphion achieves this by incorporating an interaction model based on tonally tuned virtual ``pads'' in user-configurable layouts, where the pitch and timbre associated with each pad depends on the initial point of touch, touch point size and size variation, and position after the initial touch. These parameters control a physical model for sound generation with visual feedback provided via the iPad display. We present findings from the research and development process including design revisions made in response to user testing. Finally, conclusions are made about the effectiveness of the instrument based on large-scale user feedback.}
}

@inproceedings{mkrzyzaniak2014,
  author = {Michael Krzyzaniak and Julie Akerly and Matthew Mosher and Muharrem Yildirim and Garth Paine},
  title = {Separation: Short Range Repulsion. Implementation of an Automated Aesthetic Synchronization System for a Dance Performance.},
  pages = {303--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178841},
  url = {http://www.nime.org/proceedings/2014/nime2014_279.pdf},
  abstract = {This paper describes the implementation of a digital audio / visual feedback system for an extemporaneous dance performance. The system was designed to automatically synchronize aesthetically with the dancers. The performance was premiered at the Slingshot festival in Athens Georgia on March 9, 2013.}
}

@inproceedings{ylim2014,
  author = {Yang Kyu Lim and Woon Seung Yeo},
  title = {Smartphone-based Music Conducting},
  pages = {573--576},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178851},
  url = {http://www.nime.org/proceedings/2014/nime2014_281.pdf},
  abstract = {Smartphone-based music conducting is a convenient and effective approach to conducting practice that aims to overcome the practical limitations of traditional conducting practice and provide enhanced user experience compared to those of previous virtual conducting examples. This work introduces the v-Maestro, a smartphone application for music conducting. Powered by the Gyroscope of the device, the v-Maestro analyzes conducting motions that allows the user to not only control the tempo but also simulate ``cueing'' for different instruments. Results from user tests show that, in spite of certain ergonomic problems, new conducting practice with the v-Maestro is more satisfactory than traditional methods and has a strong potential as a conducting practice tool.}
}

@inproceedings{jdeng2014,
  author = {Jun-qi Deng and Francis Chi Moon Lau and Ho-Cheung Ng and Yu-Kwong Kwok and Hung-Kwan Chen and Yu-heng Liu},
  title = {WIJAM: A Mobile Collaborative Improvisation Platform under Master-players Paradigm},
  pages = {407--410},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178746},
  url = {http://www.nime.org/proceedings/2014/nime2014_284.pdf},
  abstract = {Music jamming is an extremely difficult task for musical novices. Trying to extend this meaningful activity, which can be highly enjoyable, to a larger recipient group, we present WIJAM, a mobile application for an ad-hoc group of musical novices to perform improvisation along with a music master. In this ``master-players'' paradigm, the master offers a music backing, orchestrates the musical flow, and gives feedbacks to the players; the players improvise by tapping and sketching on their smartphones. We believe that this paradigm can be a significant contribution to the possibility of music playing by a group of novices with no instrumental training leading to decent musical results.}
}

@inproceedings{tmurraybrowne12014,
  author = {Tim Murray-Browne and Dom Aversano and Susanna Garcia and Wallace Hobbes and Daniel Lopez and Tadeo Sendon and Panagiotis Tigas and Kacper Ziemianin and Duncan Chapman},
  title = {The Cave of Sounds: An Interactive Installation Exploring How We Create Music Together},
  pages = {307--310},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178885},
  url = {http://www.nime.org/proceedings/2014/nime2014_288.pdf},
  abstract = {The Cave of Sounds is an interactive sound installation made up of new musical instruments. Exploring what it means to create instruments together within the context of NIME and the maker scene, each instrument was created by an individual but with the aim of forming a part of this new ensemble over ten months, with the final installation debuting at the Barbican in London in August 2013. In this paper, we describe how ideas of prehistoric collective music making inspired and guided this participatory musical work, both in terms of how it was created and the audience experience of musical collaboration we aimed to create in the final installation. Following a detailed description of the installation itself, we reflect on the successes, lessons and future challenges of encouraging creative musical collaboration among members of an audience.}
}

@inproceedings{knymoen12014,
  author = {Kristian Nymoen and Sichao Song and Yngve Hafting and Jim Torresen},
  title = {Funky Sole Music: Gait Recognition and Adaptive Mapping},
  pages = {299--302},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178895},
  url = {http://www.nime.org/proceedings/2014/nime2014_289.pdf},
  abstract = {We present Funky Sole Music, a musical interface employing a sole embedded with three force sensitive resistors in combination with a novel algorithm for continuous movement classification. A heuristics-based music engine has been implemented, allowing users to control high-level parameters of the musical output. This provides a greater degree of control to users without musical expertise compared to what they get with traditional media playes. By using the movement classification result not as a direct control action in itself, but as a way to change mapping spaces and musical sections, the control possibilities offered by the simple interface are greatly increased.}
}

@inproceedings{fheller2014,
  author = {Florian Heller and Jan Borchers},
  title = {Visualizing Song Structure on Timecode Vinyls},
  pages = {66--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178796},
  url = {http://www.nime.org/proceedings/2014/nime2014_290.pdf},
  abstract = {Although an analog technology, many DJs still value the turntable as an irreplaceable performance tool. Digital vinyl systems combine the distinct haptic nature of the analog turntable with the advantages of digital media. They use special records containing a digital timecode which is then processed by a computer and mapped to properties like playback speed and direction. These records, however, are generic and, in contrast to traditional vinyl, do not provide visual cues representing the structure of the track. We present a system that augments the timecode record with a visualization of song information such as artist, title, and track length, but also with a waveform that allows to visually navigate to a certain beat. We conducted a survey examining the acceptance of such tools in the DJ community and conducted a user study with professional DJs. The system was widely accepted as a tool in the DJ community and received very positive feedback during observational mixing sessions with four professional DJs.}
}

@inproceedings{mmarier2014,
  author = {Martin Marier},
  title = {Designing Mappings for the Sponge: Towards Spongistic Music},
  pages = {525--528},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178863},
  url = {http://www.nime.org/proceedings/2014/nime2014_292.pdf},
  abstract = {The development of the cushion-like musical interface called the sponge started about seven years ago. Since then, it was extensively used to perform in various settings. The sponge itself is described, but the main focus is on the evolution of the mapping strategies that are used. The author reviews the guidelines proposed by other researchers and explains how they were concretely applied with the sponge. He concludes that no single strategy constitutes a solution to the issue of mapping and that musical compositions are complex entities that require the use of a multitude of mapping strategies in parallel. It is hoped that the mappings described combined with new strategies will eventually lead to the emergence of a musical language that is idiomatic to the sponge.}
}

@inproceedings{mneupert2014,
  author = {Joachim Go{\ss}mann and Max Neupert},
  title = {Musical Interface to Audiovisual Corpora of Arbitrary Instruments},
  pages = {151--154},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178772},
  url = {http://www.nime.org/proceedings/2014/nime2014_296.pdf},
  abstract = {We present an instrument for audio-visual performance that allows to recombine sounds from a collection of sampled media through concatenative synthesis. A three-dimensional distribution derived from feature-analysis becomes accessible through a theremin-inspired interface, allowing the player to shift from exploration and intuitive navigation toward embodied performance on a granular level. In our example we illustrate this concept by using the audiovisual recording of an instrumental performance as a source. Our system provides an alternative interface to the musical instrument's audiovisual corpus: as the instrument's sound and behavior is accessed in ways that are not possible on the instrument itself, the resulting non-linear playback of the grains generates an instant remix in a cut-up aesthetic. The presented instrument is a human-computer interface that employs the structural outcome of machine analysis accessing audiovisual corpora in the context of a musical performance.}
}

@inproceedings{ibergstrom2014,
  author = {Ilias Bergstrom and Joan Llobera},
  title = {OSC-Namespace and OSC-State: Schemata for Describing the Namespace and State of OSC-Enabled Systems},
  pages = {311--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178712},
  url = {http://www.nime.org/proceedings/2014/nime2014_300.pdf},
  abstract = {We introduce two complementary OSC schemata for two contexts of use. The first is for the complete description of an OSC namespace: detailing the full set of messages each OSC-enabled system can receive or send, alongside choice metadata we deem necessary to make full use of each system's description. The second context of use is a snapshot (partial or full) of the system's state. We also relate our proposed schemata to the current state of the art, and how using these resolves issues that were left pending with previous research.}
}

@inproceedings{ebertelli2014,
  author = {Emily Robertson and Enrico Bertelli},
  title = {Conductive Music: Teaching Innovative Interface Design and Composition Techniques with Open-Source Hardware},
  pages = {517--520},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178921},
  url = {http://www.nime.org/proceedings/2014/nime2014_301.pdf},
  abstract = {Through examining the decisions and sequences of presenting a multi-media instrument fabrication program to students, this paper seeks to uncover practical elements of best practice and possible improvements in science and music education. The Conductive Music program incorporates public engagement principles, open-source hardware, DIY ethos, contemporary composition techniques, and educational activities for creative and analytical thinking. These activities impart positive skills through multi-media content delivery for all learning types. The program is designed to test practices for engaging at-risk young people from urban areas in the construction and performance of new electronic instruments. The goal is to open up the world of electronic music performance to a new generation of young digital artists and to replace negative social behaviours with creative outlets for expression through technology and performance. This paper highlights the key elements designed to deliver the program's agenda and examines the ways in which these aims were realised or tested in the classroom.}
}

@inproceedings{tmudd2014,
  author = {Tom Mudd and Simon Holland and Paul Mulholland and Nick Dalton},
  title = {Dynamical Interactions with Electronic Instruments},
  pages = {126--129},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178881},
  url = {http://www.nime.org/proceedings/2014/nime2014_302.pdf},
  abstract = {This paper examines electronic instruments that are based on dynamical systems, where the behaviour of the instrument depends not only upon the immediate input to the instrument, but also on the past input. Five instruments are presented as case studies: Michel Waisvisz' Cracklebox, Dylan Menzies' Spiro, no-input mixing desk, the author's Feedback Joypad, and microphone-loudspeaker feedback. Links are suggested between the sonic affordances of each instrument and the dynamical mechanisms embedded in them. This is discussed in the context of contemporary, materialoriented approaches to composition and particularly to free improvisation where elements such as unpredictability and instability are often of interest, and the process of exploration and discovery is an important part of the practice. Links are also made with the use of dynamical interactions in computer games to produce situations in which slight variations in the timing and ordering of inputs can lead to very different outcomes, encouraging similarly explorative approaches.}
}

@inproceedings{mbretan2014,
  author = {Mason Bretan and Gil Weinberg},
  title = {Chronicles of a Robotic Musical Companion},
  pages = {315--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178724},
  url = {http://www.nime.org/proceedings/2014/nime2014_303.pdf},
  abstract = {As robots become more pervasive in the world we think about how this might influence the way in which people experience music. We introduce the concept of a "robotic musical companion" (RMC) in the form of Shimi, a smart-phone enabled five degree-of-freedom (DoF) robotic platform. We discuss experiences individuals tend to have with music as consumers and performers and explore how these experiences can be modified, aided, or improved by the inherent synergies between a human and robot. An overview of several applications developed for Shimi is provided. These applications place Shimi in various roles and enable human-robotic interactions (HRIs) that are highlighted by more personable social communications using natural language and other forms of communication.}
}

@inproceedings{sserafin2014,
  author = {Stefania Serafin and Stefano Trento and Francesco Grani and Hannah Perner-Wilson and Seb Madgwick and Tom Mitchell},
  title = {Controlling Physically Based Virtual Musical Instruments Using The Gloves},
  pages = {521--524},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178937},
  url = {http://www.nime.org/proceedings/2014/nime2014_307.pdf},
  abstract = {In this paper we propose an empirical method to develop mapping strategies between a gestural based interface (the Gloves) and physically based sound synthesis models. An experiment was performed in order to investigate which kind of gestures listeners associate to synthesised sounds produced using physical models, corresponding to three categories of sound: sustained, iterative and impulsive. The results of the experiment show that listeners perform similar gestures when controlling sounds from the different categories. We used such gestures in order to create the mapping strategy between the Gloves and the physically based synthesis engine.}
}

@inproceedings{cgnegy12014,
  author = {Chet Gnegy},
  title = {CollideFx: A Physics-Based Audio Effects Processor},
  pages = {427--430},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178770},
  url = {http://www.nime.org/proceedings/2014/nime2014_308.pdf},
  abstract = {CollideFx is a real-time audio effects processor that integrates the physics of real objects into the parameter space of the signal chain. Much like a traditional signal chain, the user can choose a series of effects and offer realtime control to their various parameters. In this work, we introduce a means of creating tree-like signal graphs that dynamically change their routing in response to changes in the location of the unit generators in a virtual space. Signals are rerouted using a crossfading scheme that avoids the harsh clicks and pops associated with amplitude discontinuities. The unit generators are easily controllable using a click and drag interface that responds using familiar physics. CollideFx brings the interactivity of a video game together with the purpose of creating interesting and complex audio effects. With little difficulty, users can craft custom effects, or alternatively, can fling a unit generator into a cluster of several others to obtain more surprising results, letting the physics engine do the decision making.}
}

@inproceedings{tbarraclough2014,
  author = {Timothy J Barraclough and Jim Murphy and Ajay Kapur},
  title = {New Open-Source Interfaces for Group Based Participatory Performance of Live Electronic Music},
  pages = {155--158},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178708},
  url = {http://www.nime.org/proceedings/2014/nime2014_309.pdf},
  abstract = {This paper describes the Modulome System, a new hardware interface set for group-based electronic music performance and installation. Taking influence from a variety of established interfaces, the Modulome is a modular controller with application dependant use cases.}
}

@inproceedings{olahdeoja2014,
  author = {Otso L\''ahdeoja},
  title = {Structure-Borne Sound and Aurally Active Spaces},
  pages = {319--322},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178843},
  url = {http://www.nime.org/proceedings/2014/nime2014_310.pdf},
  abstract = {This paper provides a report of a research effort to transform architectural and scenographic surfaces into sound sources and use them in artistic creation. Structure-borne sound drivers are employed to induce sound into the solid surfaces, making them vibrate and emit sound. The sound waves can be perceived both via the aural (airborne diffusion) as well as the tactile (structure-borne diffusion) senses. The paper describes the main challenges encountered in the use of structure-borne sound technology, as well as the current results in overcoming them. Two completed artistic projects are presented in order to illustrate the creative possibilities enabled by the research.}
}

@inproceedings{dwikstrom2014,
  author = {D. J. Valtteri Wikstr\''om},
  title = {Musical Composition by Regressional Mapping of Physiological Responses to Acoustic Features},
  pages = {549--552},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178981},
  url = {http://www.nime.org/proceedings/2014/nime2014_311.pdf},
  abstract = {In this paper an emotionally justified approach for controlling sound with physiology is presented. Measurements of listeners' physiology, while they are listening to recorded music of their own choosing, are used to create a regression model that predicts features extracted from music with the help of the listeners' physiological response patterns. This information can be used as a control signal to drive musical composition and synthesis of new sounds an approach involving concatenative sound synthesis is suggested. An evaluation study was conducted to test the feasibility of the model. A multiple linear regression model and an artificial neural network model were evaluated against a constant regressor, or dummy model. The dummy model outperformed the other models in prediction accuracy, but the artificial neural network model achieved significant correlations between predictions and target values for many acoustic features.}
}

@inproceedings{jlong2014,
  author = {Jason Long},
  title = {The Robotic Taishogoto: A New Plug 'n Play Desktop Performance Instrument},
  pages = {479--482},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178853},
  url = {http://www.nime.org/proceedings/2014/nime2014_313.pdf},
  abstract = {This paper describes the Robotic Taishogoto, a new robotic musical instrument for performance, musical installations, and educational purposes. The primary goals of its creation is to provide an easy to use, cost effective, compact and integrated acoustic instrument which is fully automated and controllable via standard MIDI commands. This paper describes the technical details of its design and implementation including the mechanics, electronics and firmware. It also outlines various control methodologies and use cases for the instrument.}
}

@inproceedings{pmathews2014,
  author = {Paul Mathews and Ness Morris and Jim Murphy and Ajay Kapur and Dale Carnegie},
  title = {Tangle: a Flexible Framework for Performance with Advanced Robotic Musical Instruments},
  pages = {187--190},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178867},
  url = {http://www.nime.org/proceedings/2014/nime2014_314.pdf},
  abstract = {Networked musical performance using networks of computers for live performance of electronic music has evolved over a number of decades but has tended to rely upon customized and highly specialized software designed specifically for particular artistic goals. This paper presents Tangle, a flexible software framework designed to provide a basis for performance on any number of distinct instruments. The network includes features to simplify the control of robotic instruments, such as automated latency compensation and self-testing, while being simple to extend in order to implement device-specific logic and failsafes. Tangle has been tested on two diverse systems incorporating a number of unique and complex mechatronic instruments.}
}

@inproceedings{ofried2014,
  author = {Ohad Fried and Zeyu Jin and Reid Oda and Adam Finkelstein},
  title = {AudioQuilt: {2D} Arrangements of Audio Samples using Metric Learning and Kernelized Sorting},
  pages = {281--286},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178766},
  url = {http://www.nime.org/proceedings/2014/nime2014_315.pdf},
  abstract = {The modern musician enjoys access to a staggering number of audio samples. Composition software can ship with many gigabytes of data, and there are many more to be found online. However, conventional methods for navigating these libraries are still quite rudimentary, and often involve scrolling through alphabetical lists. We present a system for sample exploration that allows audio clips to be sorted according to user taste, and arranged in any desired 2D formation such that similar samples are located near each other. Our method relies on two advances in machine learning. First, metric learning allows the user to shape the audio feature space to match their own preferences. Second, kernelized sorting finds an optimal arrangement for the samples in 2D. We demonstrate our system with two new interfaces for exploring audio samples, and evaluate the technology qualitatively and quantitatively via a pair of user studies.}
}

@inproceedings{dgabanaarellano2014,
  author = {Daniel G\'abana Arellano and Andrew McPherson},
  title = {Radear: A Tangible Spinning Music Sequencer},
  pages = {84--85},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178704},
  url = {http://www.nime.org/proceedings/2014/nime2014_324.pdf},
  abstract = {This paper presents a new circular tangible interface where one or multiple users can collaborate and interact in real time by placing and moving passive wooden pucks on a transparent tabletop in order to create music. The design encourages physical intuition and visual feedback on the music being created. An arm with six optical sensors rotates beneath a transparent surface, triggering sounds based on the objects placed above. The interface's simplicity and tangibility make it easy to learn and suitable for a broad range of users.}
}

@inproceedings{aberndt2014,
  author = {Axel Berndt and Nadia Al-Kassab and Raimund Dachselt},
  title = {TouchNoise: A Particle-based Multitouch Noise Modulation Interface},
  pages = {323--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178714},
  url = {http://www.nime.org/proceedings/2014/nime2014_325.pdf},
  abstract = {We present the digital musical instrument TouchNoise that is based on multitouch interaction with a particle system. It implements a novel interface concept for modulating noise spectra. Each particle represents a sine oscillator that moves through the two-dimensional frequency and stereo panning domain via Brownian motion. Its behavior can be affected by multitouch gestures allowing the shaping of the resulting sound in many different ways. Particles can be dragged, attracted, repelled, accentuated, and their autonomous behavior can be manipulated. In this paper we introduce the concepts behind this instrument, describe its implementation and discuss the sonic design space emerging from it.}
}

@inproceedings{ynakanishi2014,
  author = {Yoshihito Nakanishi and Seiichiro Matsumura and Chuichi Arakawa},
  title = {B.O.M.B. -Beat Of Magic Box -: Stand-Alone Synthesizer Using Wireless Synchronization System For Musical Session and Performance},
  pages = {80--81},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178889},
  url = {http://www.nime.org/proceedings/2014/nime2014_327.pdf},
  abstract = {In this paper, the authors introduce a stand-alone synthesizer, ``B.O.M.B. -Beat Of Magic Box --'' for electronic music sessions and live performances. ``B.O.M.B.'' has a wireless communication system that synchronizes musical scale and tempo (BPM) between multiple devices. In addition, participants can change master/slave role between performers immediately. Our primary motivation is to provide musicians and nonmusicians with opportunities to experience a collaborative electronic music performance. Here, the hardware and interaction design of the device is presented. To date, numerous collaborative musical instruments have been developed in electronic music field [1][2][3]. The authors are interested in formations of musical sessions using stand-alone devices and leader/follower relationship in musical sessions. The authors specify three important requirements of instrument design for musical session. They are as follows: (1) Simple Interface: Interface that enables performers to control three sound elements (pitch, timbre, and amplitude) with simple interaction. (2) Portable Stand-alone System: System that runs standalone (with sound generators, speakers, and butteries). Because musical sessions can be improvised at any place and time, the authors consider that portability is essential in designing musical instruments for sessions. (3) Wireless Synchronization: System that supports ensembles by automatically synchronizing tempo (BPM) and tonality between multiple devices by air because of portability. In addition, performers can switch master/slave roles smoothly such as leader/follower relationship during a musical session. The authors gave ten live performances using this device at domestic and international events. In these events, the authors confirmed that our proposed wireless synchronization system worked stable. It is suggested that our system demonstrate the practicality of wireless synchronization. In future, the authors will evaluate the device in terms of its stability in multi-performer musical sessions.}
}

@inproceedings{gwakefield2014,
  author = {Graham Wakefield and Charlie Roberts and Matthew Wright and Timothy Wood and Karl Yerkes},
  title = {Collaborative Live-Coding with an Immersive Instrument},
  pages = {505--508},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178975},
  url = {http://www.nime.org/proceedings/2014/nime2014_328.pdf},
  abstract = {We discuss live coding audio-visual worlds for large-scale virtual reality environments. We describe Alive, an instrument allowing multiple users to develop sonic and visual behaviors of agents in a virtual world, through a browserbased collaborative code interface, accessible while being immersed through spatialized audio and stereoscopic display. The interface adds terse syntax for query-based precise or stochastic selections and declarative agent manipulations, lazily-evaluated expressions for synthesis and behavior, event handling, and flexible scheduling.}
}

@inproceedings{ssuh2014,
  author = {Sangwon Suh and Jeong-seob Lee and Woon Seung Yeo},
  title = {A Gesture Detection with Guitar Pickup and Earphones},
  pages = {90--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178949},
  url = {http://www.nime.org/proceedings/2014/nime2014_333.pdf},
  abstract = {For the electric guitar, which takes a large proportion in modern pop music, effects unit (or effector) is no longer optional. Many guitarists already `play' their effects with their instrument. However, it is not easy to control these effects during the play, so lots of new controllers and interfaces have been devised; one example is a pedal type effects that helps players to control effects with a foot while their hands are busy. Some players put a controller on their guitars. However, our instruments are so precious to drill a hole, and the stage is too big for the player who is just kneeling behind the pedals and turning the knobs. In this paper, we designed a new control system for electric guitar and bass. This paper is about a gesture-based sound control system that controls the electric guitar effects (like delay time, reverberation or pitch) with the player's hand gesture. This system utilizes TAPIR signal to trace player's hand motion. TAPIR signal is an acoustic signal that can rarely be received by most people, because its frequency exists between 18 kHz to 22 kHz [TAPIR article]. This system consists of a signal generator, an electric guitar and a sound processor. From the generator that is attached on the player's hand, the TAPIR signal transfers to the magnetic pickup equipped on the electric guitar. Player's gesture is captured as a Doppler shift and the processor calculates the value as the sound effect parameter. In this paper, we focused on the demonstration of the signal transfer on aforementioned system.}
}

@inproceedings{fberthaut2014,
  author = {Florent Berthaut and Jarrod Knibbe},
  title = {Wubbles: A Collaborative Ephemeral Musical Instrument},
  pages = {499--500},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178716},
  url = {http://www.nime.org/proceedings/2014/nime2014_334.pdf},
  abstract = {This paper presents a collaborative digital musical instrument that uses the ephemeral and physical properties of soap bubbles to explore the complexity layers and oscillating parameters of electronic (bass) music. This instrument, called Wubbles, aims at encouraging both individual and collaborative musical manipulations.}
}

@inproceedings{lpardue2014,
  author = {Laurel Pardue and Dongjuan Nian and Christopher Harte and Andrew McPherson},
  title = {Low-Latency Audio Pitch Tracking: A Multi-Modal Sensor-Assisted Approach},
  pages = {54--59},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178899},
  url = {http://www.nime.org/proceedings/2014/nime2014_336.pdf},
  abstract = {This paper presents a multi-modal approach to musical instrument pitch tracking combining audio and position sensor data. Finger location on a violin fingerboard is measured using resistive sensors, allowing rapid detection of approximate pitch. The initial pitch estimate is then used to restrict the search space of an audio pitch tracking algorithm. Most audio-only pitch tracking algorithms face a fundamental tradeoff between accuracy and latency, with longer analysis windows producing better pitch estimates at the cost of noticeable lag in a live performance environment. Conversely, sensor-only strategies struggle to achieve the fine pitch accuracy a human listener would expect. By combining the two approaches, high accuracy and low latency can be simultaneously achieved.}
}

@inproceedings{nklugel12014,
  author = {Niklas Kl\''ugel and Timo Becker and Georg Groh},
  title = {Designing Sound Collaboratively Perceptually Motivated Audio Synthesis},
  pages = {327--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178833},
  url = {http://www.nime.org/proceedings/2014/nime2014_339.pdf},
  abstract = {In this contribution, we will discuss a prototype that allows a group of users to design sound collaboratively in real time using a multi-touch tabletop. We make use of a machine learning method to generate a mapping from perceptual audio features to synthesis parameters. This mapping is then used for visualization and interaction. Finally, we discuss the results of a comparative evaluation study.}
}

@inproceedings{seloul2014,
  author = {Yehiel Amo and Gil Zissu and Shaltiel Eloul and Eran Shlomi and Dima Schukin and Almog Kalifa},
  title = {A Max/MSP Approach for Incorporating Digital Music via Laptops in Live Performances of Music Bands},
  pages = {94--97},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178700},
  url = {http://www.nime.org/proceedings/2014/nime2014_340.pdf},
  abstract = {We use Max/MSP framework to create a reliable but flexible approach for managing live performances of music bands who rely on live playing with digital music. This approach utilizes Max/MSP to allow any player an easy and low cost way to apply and experiment innovative music interfaces for live performance, without losing the professionalism required on stage. In that approach, every 1-3 players is plugged to a unit consisting of a standard sound-card and laptop. This unit is controlled by an interface that schedules and manages all the digital sounds made by each player (VST effects, VST instruments and 'home-made' interactive interfaces). All the player's units are then remotely controlled by a conductor patch which is in charge of the synchronization of all the players and background samples in real time, as well as providing sensitive metronome and scheduling visual enhancement. Moreover, and not less important, we can take the advantage of using virtual instruments and virtual effects in Max environment to manage the mix, and routing the audio. This providing monitors and metronome to the players ears, and virtual mixing via Max/MSP patch. This privilege almost eliminates the dependency in the venue's equipment, and in that way, the sound quality and music ideas can be taken out directly from the studio to the stage.}
}

@inproceedings{asa2014,
  author = {Adriana Sa},
  title = {Repurposing Video Game Software for Musical Expression: A Perceptual Approach},
  pages = {331--334},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178925},
  url = {http://www.nime.org/proceedings/2014/nime2014_343.pdf},
  abstract = {The text exposes a perceptual approach to instrument design and composition, and it introduces an instrument that outputs acoustic sound, digital sound, and digital image. We explore disparities between human perception and digital analysis as creative material. Because the instrument repurposes software intended to create video games, we establish a distinction between the notion of ``flow'' in music and gaming, questioning how it may substantiate in interaction design. Furthermore, we extrapolate from cognition/attention research to describe how the projected image creates a reactive stage scene without deviating attention from the music.}
}

@inproceedings{jmurphy2014,
  author = {Jim Murphy and Paul Mathews and Ajay Kapur and Dale Carnegie},
  title = {Robot: Tune Yourself! Automatic Tuning for Musical Robotics},
  pages = {565--568},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178883},
  url = {http://www.nime.org/proceedings/2014/nime2014_345.pdf},
  abstract = {This paper presents a method for a self-tuning procedure for musical robots capable of continuous pitch-shifting. Such a technique is useful for robots consisting of many strings: the ability to self-tune allows for long-term installation without human intervention as well as on-the-fly tuning scheme changes. The presented method consists of comparing a detuned string's pitch at runtime to a pre-compiled table of string responses at varying tensions. The behavior of the current detuned string is interpolated from the two nearest pre-characterized neighbors, and the desired virtual fret positions are added to the interpolated model. This method allows for rapid tuning at runtime, requiring only a single string actuation to determine the pitch. After a detailed description of the self-tuning technique and implementation, the results will be evaluated on the new Swivel 2 robotic slide guitar. The paper concludes with a discussion of performance applications and ideas for subsequent work on self-tuning musical robotic systems.}
}

@inproceedings{astark2014,
  author = {Adam Stark},
  title = {Sound Analyser: A Plug-In for Real-Time Audio Analysis in Live Performances and Installations},
  pages = {183--186},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178945},
  url = {http://www.nime.org/proceedings/2014/nime2014_348.pdf},
  abstract = {Real-time audio analysis has great potential for being used to create musically responsive applications in live performances. There have been many examples of such use, including sound-responsive visualisations, adaptive audio effects and machine musicianship. However, at present, using audio analysis algorithms in live performance requires either some detailed knowledge about the algorithms themselves, or programming or both. Those wishing to use audio analysis in live performances may not have either of these as their strengths. Rather, they may instead wish to focus upon systems that respond to audio analysis data, such as visual projections or sound generators. In response, this paper introduces the Sound Analyser an audio plug-in allowing users to a) select a custom set of audio analyses to be performed in real-time and b) send that information via OSC so that it can easily be used by other systems to develop responsive applications for live performances and installations. A description of the system architecture and audio analysis algorithms implemented in the plug-in is presented before moving on to two case studies where the plug-in has been used in the field with artists.}
}

@inproceedings{bjohnson2014,
  author = {Bridget Johnson and Michael Norris and Ajay Kapur},
  title = {The Development Of Physical Spatial Controllers},
  pages = {335--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178820},
  url = {http://www.nime.org/proceedings/2014/nime2014_349.pdf},
  abstract = {This paper introduces recent developments in the Chronus series, a family of custom controllers that afford a performer gestural interaction with surround sound systems that can be easily integrated into their personal performance systems. The controllers are built with the goal of encouraging more electronic musicians to include the creation of dynamic pantophonic fields in performance. The paper focuses on technical advances of the Chronus 2.0 prototype that extend the interface to control both radial and angular positional data, and the controllers' ease of integration into electronic performance configurations, both for diffusion and for performance from the wider electronic music community.}
}

@inproceedings{ajensenius2014,
  author = {Alexander Refsum Jensenius},
  title = {To gesture or Not? {A}n Analysis of Terminology in {NIME} Proceedings 2001--2013},
  pages = {217--220},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178816},
  url = {http://www.nime.org/proceedings/2014/nime2014_351.pdf},
  abstract = {The term 'gesture' has represented a buzzword in the NIME community since the beginning of its conference series. But how often is it actually used, what is it used to describe, and how does its usage here differ from its usage in other fields of study? This paper presents a linguistic analysis of the motion-related terminology used in all of the papers published in the NIME conference proceedings to date (2001-2013). The results show that 'gesture' is in fact used in 62 % of all NIME papers, which is a significantly higher percentage than in other music conferences (ICMC and SMC), and much more frequently than it is used in the HCI and biomechanics communities. The results from a collocation analysis support the claim that 'gesture' is used broadly in the NIME community, and indicate that it ranges from the description of concrete human motion and system control to quite metaphorical applications.}
}

@inproceedings{etomas12014,
  author = {Enrique Tom\'as and Martin Kaltenbrunner},
  title = {Tangible Scores: Shaping the Inherent Instrument Score},
  pages = {609--614},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178953},
  url = {http://www.nime.org/proceedings/2014/nime2014_352.pdf},
  abstract = {Tangible Scores are a new paradigm for musical instrument design with a physical configuration inspired by graphic scores. In this paper we will focus on the design aspects of this new interface as well as on some of the related technical details. Creating an intuitive, modular and expressive instrument for textural music was the primary driving force. Following these criteria, we literally incorporated a musical score onto the surface of the instrument as a way of continuously controlling several parameters of the sound synthesis. Tangible Scores are played with both hands and they can adopt multiple physical forms. Complex and expressive sound textures can be easily played over a variety of timbres, enabling precise control in a natural manner.}
}

@inproceedings{emorgan2014,
  author = {Evan Morgan and Hatice Gunes and Nick Bryan-Kinns},
  title = {Instrumenting the Interaction: Affective and Psychophysiological Features of Live Collaborative Musical Improvisation},
  pages = {23--28},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178877},
  url = {http://www.nime.org/proceedings/2014/nime2014_353.pdf},
  abstract = {New technologies have led to the design of exciting interfaces for collaborative music making. However we still have very little understanding of the underlying affective and communicative processes which occur during such interactions. To address this issue, we carried out a pilot study where we collected continuous behavioural, physiological, and performance related measures from pairs of improvising drummers. This paper presents preliminary findings, which could be useful for the evaluation and design of user-centred collaborative interfaces for musical creativity and expression.}
}

@inproceedings{bjohnston2014,
  author = {Blake Johnston and Henry Dengate Thrush and Ajay Kapur and Jim Murphy and Tane Moleta},
  title = {Polus: The Design and Development of a New, Mechanically Bowed String Instrument Ensemble},
  pages = {557--560},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178822},
  url = {http://www.nime.org/proceedings/2014/nime2014_355.pdf},
  abstract = {This paper details the creation, design, implementation and uses of a series of new mechanically bowed string instruments. These instruments have been designed with the objective of allowing for multiple parameters of musical expressivity, as well as including the physical and spatial features of the instruments to be integral aspects of their perception as instruments and sonic objects. This paper focuses on the hardware design, software implementation, and present musical uses of the ensemble.}
}

@inproceedings{oizmirli2014,
  author = {Ozgur Izmirli and Jake Faris},
  title = {Imitation Framework for Percussion},
  pages = {483--486},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178814},
  url = {http://www.nime.org/proceedings/2014/nime2014_360.pdf},
  abstract = {We present a framework for imitation of percussion performances with parameter-based learning for accurate reproduction. We constructed a robotic setup involving pull-solenoids attached to drum sticks which communicate with a computer through an Arduino microcontroller. The imitation framework allows for parameter adaptation to different mechanical constructions by learning the capabilities of the overall system being used. For the rhythmic vocabulary, we have considered regular stroke, flam and drag styles. A learning and calibration system was developed to efficiently perform grace notes for the drag rudiment as well as the single stroke and the flam rudiment. A second pre-performance process is introduced to minimize the latency difference between individual drum sticks in our mechanical setup. We also developed an off-line onset detection method to reliably recognize onsets from the microphone input. Once these pre-performance steps are taken, our setup will then listen to a human drummer's performance pattern, analyze for onsets, loudness, and rudiment pattern, and then play back using the learned parameters for the particular system. We conducted three different evaluations of our constructed system.}
}

@inproceedings{ailsar2014,
  author = {Alon Ilsar and Mark Havryliv and Andrew Johnston},
  title = {Evaluating the Performance of a New Gestural Instrument Within an Ensemble},
  pages = {339--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178812},
  url = {http://www.nime.org/proceedings/2014/nime2014_363.pdf},
  abstract = {This paper discusses one particular mapping for a new gestural instrument called the AirSticks. This mapping was designed to be used for improvised or rehearsed duos and restricts the performer to only utilising the sound source of one other musician playing an acoustic instrument. Several pieces with different musicians were performed and documented, musicians were observed and interviews with these musicians were transcribed. In this paper we will examine the thoughts of these musicians to gather a better understanding of how to design effective ensemble instruments of this type.}
}

@inproceedings{abarenca2014,
  author = {Adri{\'a}n Barenca and Milos Corak},
  title = {The Manipuller II: Strings within a Force Sensing Ring},
  pages = {589--592},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178706},
  url = {http://www.nime.org/proceedings/2014/nime2014_364.pdf},
  abstract = {The Manipuller is a musical interface based on strings and multi-dimensional force sensing. This paper presents a new architectural approach to the original interface design which has been materialized with the implementation of the Manipuller II system prototype. Besides the short paper we would like to do a poster presentation plus a demo of the new prototype where the public will be invited to play with the new musical interface.}
}

@inproceedings{osarier2014,
  author = {Ozan Sarier},
  title = {Rub Synth : A Study of Implementing Intentional Physical Difficulty Into Touch Screen Music Controllers},
  pages = {179--182},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178931},
  url = {http://www.nime.org/proceedings/2014/nime2014_367.pdf},
  abstract = {In the recent years many touch screen interfaces have been designed and used for musical control. When compared with their physical counterparts, current control paradigms employed in touch screen musical interfaces do not require the same level of physical labor and this negatively affects the user experience in terms of expressivity, engagement and enjoyment. This lack of physicality can be remedied by using interaction elements, which are designed for the exertion of the user. Employing intentionally difficult and inefficient interaction design can enhance the user experience by allowing greater bodily expression, kinesthetic feedback, more apparent skill acquisition, and performer satisfaction. Rub Synth is a touch screen musical instrument with an exertion interface. It was made for creating and testing exertion strategies that are possible by only using 2d touch coordinates as input and evaluating the outcomes of implementing intentional difficulty. This paper discusses the strategies that can be employed to model effort on touch screens, the benefits of having physical difficulty, Rub Synth's interaction design, and user experience results of using such an interface.}
}

@inproceedings{lfyfe2014,
  author = {Lawrence Fyfe and Adam Tindale and Sheelagh Carpendale},
  title = {Extending the Nexus Data Exchange Format (NDEF) Specification},
  pages = {343--346},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178768},
  url = {http://www.nime.org/proceedings/2014/nime2014_368.pdf},
  abstract = {The Nexus Data Exchange Format (NDEF) is an Open Sound Control (OSC) namespace specification designed to make connection and message management tasks easier for OSC-based networked performance systems. New extensions to the NDEF namespace improve both connection and message management between OSC client and server nodes. Connection management between nodes now features human-readable labels for connections and a new message exchange for pinging connections to determine their status. Message management now has improved namespace synchronization via a message count exchange and by the ability to add, remove, and replace messages on connected nodes.}
}

@inproceedings{ihattwick12014,
  author = {Ian Hattwick and Preston Beebe and Zachary Hale and Marcelo Wanderley and Philippe Leroux and Fabrice Marandola},
  title = {Unsounding Objects: Audio Feature Extraction for the Control of Sound Synthesis},
  pages = {597--600},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178790},
  url = {http://www.nime.org/proceedings/2014/nime2014_369.pdf},
  abstract = {This paper presents results from the development of a digital musical instrument which uses audio feature extraction for the control of sound synthesis. Our implementation utilizes multi-band audio analysis to generate control signals. This technique is well-suited to instruments for which the gestural interface is intentionally weakly defined. We present a percussion instrument utilizing this technique in which the timbral characteristics of found objects are the primary source of audio for analysis.}
}

@inproceedings{ihattwick2014,
  author = {Ian Hattwick and Joseph Malloch and Marcelo Wanderley},
  title = {Forming Shapes to Bodies: Design for Manufacturing in the Prosthetic Instruments},
  pages = {443--448},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178792},
  url = {http://www.nime.org/proceedings/2014/nime2014_370.pdf},
  abstract = {Moving new DMIs from the research lab to professional artistic contexts places new demands on both their design and manufacturing. Through a discussion of the Prosthetic Instruments, a family of digital musical instruments we designed for use in an interactive dance performance, we discuss four different approaches to manufacturing -artisanal, building block, rapid prototyping, and industrial. We discuss our use of these different approaches as we strove to reconcile the many conflicting constraints placed upon the instruments' design due to their use as hypothetical prosthetic extensions to dancers' bodies, as aesthetic objects, and as instruments used in a professional touring context. Experiences and lessons learned during the design and manufacturing process are discussed in relation both to these manufacturing approaches as well as to Bill Buxton's concept of artist-spec design.}
}

@inproceedings{cnash2014,
  author = {Chris Nash},
  title = {Manhattan: End-User Programming for Music},
  pages = {221--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178891},
  url = {http://www.nime.org/proceedings/2014/nime2014_371.pdf},
  abstract = {This paper explores the concept of end-user programming languages in music composition, and introduces the Manhattan system, which integrates formulas with a grid-based style of music sequencer. Following the paradigm of spreadsheets, an established model of end-user programming, Manhattan is designed to bridge the gap between traditional music editing methods (such as MIDI sequencing and typesetting) and generative and algorithmic music -seeking both to reduce the learning threshold of programming and support flexible integration of static and dynamic musical elements in a single work. Interaction draws on rudimentary knowledge of mathematics and spreadsheets to augment the sequencer notation with programming concepts such as expressions, built-in functions, variables, pointers and arrays, iteration (for loops), branching (goto), and conditional statements (if-then-else). In contrast to other programming tools, formulas emphasise the visibility of musical data (e.g. notes), rather than code, but also allow composers to interact with notated music from a more abstract perspective of musical processes. To illustrate the function and use cases of the system, several examples of traditional and generative music are provided, the latter drawing on minimalism (process-based music) as an accessible introduction to algorithmic composition. Throughout, the system and approach are evaluated using the cognitive dimensions of notations framework, together with early feedback for use by artists.}
}

@inproceedings{croberts2014,
  author = {Charlie Roberts and Matthew Wright and JoAnn Kuchera-Morin and Tobias H{\''o}llerer},
  title = {Rapid Creation and Publication of Digital Musical Instruments},
  pages = {239--242},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178919},
  url = {http://www.nime.org/proceedings/2014/nime2014_373.pdf},
  abstract = {We describe research enabling the rapid creation of digital musical instruments and their publication to the Internet. This research comprises both high-level abstractions for making continuous mappings between audio, interactive, and graphical elements, as well as a centralized database for storing and accessing instruments. Published instruments run in most devices capable of running a modern web browser. Notation of instrument design is optimized for readability and expressivity.}
}

@inproceedings{ibukvic2014,
  author = {Ivica Bukvic},
  title = {Pd-L2Ork Raspberry Pi Toolkit as a Comprehensive Arduino Alternative in K-12 and Production Scenarios},
  pages = {163--166},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178726},
  url = {http://www.nime.org/proceedings/2014/nime2014_377.pdf},
  abstract = {The following paper showcases new integrated Pd-L2Ork system and its K12 educational counterpart running on Raspberry Pi hardware. A collection of new externals and abstractions in conjunction with the Modern Device LOP shield transforms Raspberry Pi into a cost-efficient sensing hub providing Arduino-like connectivity with 10 digital I/O pins (including both software and hardware implementations of pulse width modulation) and 8 analog inputs, while offering a number of integrated features, including audio I/O, USB and Ethernet connectivity and video output.}
}

@inproceedings{rkleinberger2014,
  author = {Charles Holbrow and Elena Jessop and Rebecca Kleinberger},
  title = {Vocal Vibrations: A Multisensory Experience of the Voice},
  pages = {431--434},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178800},
  url = {http://www.nime.org/proceedings/2014/nime2014_378.pdf},
  abstract = {Vocal Vibrations is a new project by the Opera of the Future group at the MIT Media Lab that seeks to engage the public in thoughtful singing and vocalizing, while exploring the relationship between human physiology and the resonant vibrations of the voice. This paper describes the motivations, the technical implementation, and the experience design of the Vocal Vibrations public installation. This installation consists of a space for reflective listening to a vocal composition (the Chapel) and an interactive space for personal vocal exploration (the Cocoon). In the interactive experience, the participant also experiences a tangible exteriorization of his voice by holding the ORB, a handheld device that translates his voice and singing into tactile vibrations. This installation encourages visitors to explore the physicality and expressivity of their voices in a rich musical context.}
}

@inproceedings{gdublon2014,
  author = {Gershon Dublon and Joseph A. Paradiso},
  title = {FingerSynth: Wearable Transducers for Exploring the Environment and Playing Music Everywhere},
  pages = {134--135},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178754},
  url = {http://www.nime.org/proceedings/2014/nime2014_379.pdf},
  abstract = {We present the FingerSynth, a wearable musical instrument made up of a bracelet and set of rings that enable its player to produce sound by touching nearly any surface in their environment. Each ring contains a small, independently controlled exciter transducer commonly used for auditory bone conduction. The rings sound loudly when they touch a hard object, and are practically silent otherwise. When a wearer touches their own (or someone else's) head, the contacted person hears the sound through bone conduction, inaudible to others. The bracelet contains a microcontroller, a set of FET transistors, an accelerometer, and a battery. The microcontroller generates a separate audio signal for each ring, switched through the FETs, and can take user input through the accelerometer in the form of taps, flicks, and other gestures. The player controls the envelope and timbre of the sound by varying the physical pressure and the angle of their finger on the surface, or by touching differently resonant surfaces. Because its sound is shaped by direct, physical contact with objects and people, the FingerSynth encourages players to experiment with the materials around them and with one another, making music with everything they touch.}
}

@inproceedings{fhashimoto2014,
  author = {Fumito Hashimoto and Motoki Miura},
  title = {Operating Sound Parameters Using {Markov} Model and {Bayes}ian Filters in Automated Music Performance},
  pages = {347--350},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178788},
  url = {http://www.nime.org/proceedings/2014/nime2014_380.pdf},
  abstract = {In recent years, there has been an increase in the number of artists who make use of automated music performances in their music and live concerts. Automated music performance is a form of music production using programmed musical notes. Some artists who introduce automated music performance operate parameters of the sound in their performance for production of their music. In this paper, we focus on the music production aspects and describe a method that realizes operation of the sound parameters via computer. Further, in this study, the probability distribution of the action (i.e., variation of parameters) is obtained within the music, using Bayesian filters. The probability distribution of each piece of music is transformed by passing through a Markov model. After the probability distribution is obtained, sound parameters can be automatically controlled. We have developed a system to reproduce the musical expressions of humans and confirmed the possibilities of our method.}
}

@inproceedings{rgupfinger2014,
  author = {Reinhard Gupfinger and Martin Kaltenbrunner},
  title = {SOUND TOSSING Audio Devices in the Context of Street Art},
  pages = {577--580},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178778},
  url = {http://www.nime.org/proceedings/2014/nime2014_385.pdf},
  abstract = {Street art opens a new, broad research field in the context of urban communication and sound aesthetics in public space. The primary focus of this article is the relevance and effects of using sound technologies and audio devices to shape urban landscape and soundscape. This paper examines the process of developing an alternative type of street art that uses sound as its medium. It represents multiple audio device prototypes, which encourage new chances for street artists and activists to contribute their messages and signs in public spaces. Furthermore, it documents different approaches to establishing this alternative urban practice within the street art and new media art field. The findings also expose a research space for sound and technical interventions in the context of street art.}
}

@inproceedings{tmitchell2014,
  author = {Thomas Mitchell and Sebastian Madgwick and Simon Rankine and Geoffrey Hilton and Adrian Freed and Andrew Nix},
  title = {Making the Most of Wi-Fi: Optimisations for Robust Wireless Live Music Performance},
  pages = {251--256},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178875},
  url = {http://www.nime.org/proceedings/2014/nime2014_386.pdf},
  abstract = {Wireless technology is growing increasingly prevalent in the development of new interfaces for live music performance. However, with a number of different wireless technologies operating in the 2.4 GHz band, there is a high risk of interference and congestion, which has the potential to severely disrupt live performances. With its high transmission power, channel bandwidth and throughput, Wi-Fi (IEEE 802.11) presents an opportunity for highly robust wireless communications. This paper presents our preliminary work optimising the components of a Wi-Fi system for live performance scenarios. We summarise the manufacture and testing of a prototype directional antenna that is designed to maximise sensitivity to a performer's signal while suppressing interference from elsewhere. We also propose a set of recommended Wi-Fi configurations to reduce latency and increase throughput. Practical investigations utilising these arrangements demonstrate a single x-OSC device achieving a latency of <3 ms and a distributed network of 15 devices achieving a net throughput of ~4800 packets per second (~320 per device); where each packet is a 104-byte OSC message containing 16 analogue input channels acquired by the device.}
}

@inproceedings{mmainsbridge2014,
  author = {Mary Mainsbridge and Kirsty Beilharz},
  title = {Body As Instrument: Performing with Gestural Interfaces},
  pages = {110--113},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178859},
  url = {http://www.nime.org/proceedings/2014/nime2014_393.pdf},
  abstract = {This paper explores the challenge of achieving nuanced control and physical engagement with gestural interfaces in performance. Performances with a prototype gestural performance system, Gestate, provide the basis for insights into the application of gestural systems in live contexts. These reflections stem from a performer's perspective, outlining the experience of prototyping and performing with augmented instruments that extend vocal or instrumental technique through ancillary gestures. Successful implementation of rapidly evolving gestural technologies in real-time performance calls for new approaches to performing and musicianship, centred around a growing understanding of the body's physical and creative potential. For musicians hoping to incorporate gestural control seamlessly into their performance practice a balance of technical mastery and kinaesthetic awareness is needed to adapt existing systems to their own purposes. Within non-tactile systems, visual feedback mechanisms can support this process by providing explicit visual cues that compensate for the absence of haptic or tangible feedback. Experience gained through prototyping and performance can yield a deeper understanding of the broader nature of gestural control and the way in which performers inhabit their own bodies.}
}

@inproceedings{hportner2014,
  author = {Hanspeter Portner},
  title = {CHIMAERA The Poly-Magneto-Phonic Theremin An Expressive Touch-Less Hall-Effect Sensor Array},
  pages = {501--504},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178909},
  url = {http://www.nime.org/proceedings/2014/nime2014_397.pdf},
  abstract = {The Chimaera is a touch-less, expressive, polyphonic and electronic music controller based on magnetic field sensing. An array of hall-effect sensors and their vicinity make up a continuous 2D interaction space. The sensors are excited with Neodymium magnets worn on fingers. The device continuously tracks position and vicinity of multiple present magnets along the sensor array to produce event signals accordingly. Apart from the two positional signals, an event also carries the magnetic field polarization, a unique identifier and group association. We like to think of it as a mixed analog/digital offspring of theremin and trautonium. These general-purpose event signals are transmitted and eventually translated into musical events according to custom mappings on a host system. With its touch-less control (no friction), high update rates (2-4kHz), its quasi-continuous spatial resolution and its low-latency (<1 ms), the Chimaera can react to most subtle motions instantaneously and allows for a highly dynamic and expressive play. Its open source design additionally gives the user all possibilities to further tune hardware and firmware to his or her needs. The Chimaera is network-oriented and configured with and communicated by OSC (Open Sound Control), which makes it straight-forward to integrate into any setup.}
}

@inproceedings{twebster12014,
  author = {Thomas Webster and Guillaume LeNost and Martin Klang},
  title = {The OWL programmable stage effects pedal: Revising the concept of the on-stage computer for live music performance},
  pages = {621--624},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178979},
  url = {http://www.nime.org/proceedings/2014/nime2014_399.pdf},
  abstract = {This paper introduces the OWL stage effects pedal and aims to present the device within the context of Human Computer Interaction (HCI) research. The OWL is a dedicated, programmable audio device designed to provide an alternative to the use of laptop computers for bespoke audio processing on stage for music performance. By creating a software framework that allows the user to program their own code for the hardware in C++, the OWL project makes it possible to use homemade audio processing on stage without the need for a laptop running a computer music environment such as Pure Data or Supercollider. Moving away from the general-purpose computer to a dedicated audio device means that some of the potential problems and technical complexity of performing with a laptop computer onstage can be avoided, allowing the user to focus more of their attention on the musical performance. Within the format of a traditional guitar 'stomp box', the OWL aims to integrate seamlessly into a guitarist's existing pedal board setup, and in this way presents as an example of a ubiquitous and tangible computing device -a programmable computer designed to fit into an existing mode of musical performance whilst being transparent in use.}
}

@inproceedings{gtorre2014,
  author = {Nicholas Ward and Giuseppe Torre},
  title = {Constraining Movement as a Basis for {DMI} Design and Performance.},
  pages = {449--454},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178977},
  url = {http://www.nime.org/proceedings/2014/nime2014_404.pdf},
  abstract = {In this paper we describe the application of a movement-based design process for digital musical instruments which led to the development of a prototype DMI named the Twister. The development is described in two parts. Firstly, we consider the design of the interface or physical controller. Following this we describe the development of a specific sonic character, mapping approach and performance. In both these parts an explicit consideration of the type of movement we would like the device to engender in performance drove the design choices. By considering these two parts separately we draw attention to two different levels at which movement might be considered in the design of DMIs; at a general level of ranges of movement in the creation of the controller and a more specific, but still quite open, level in the creation of the final instrument and a particular performance. In light of the results of this process the limitations of existing representations of movement within the DMI design discourse is discussed. Further, the utility of a movement focused design approach is discussed.}
}

@inproceedings{mdavies2014,
  author = {Matthew Davies and Adam Stark and Fabien Gouyon and Masataka Goto},
  title = {Improvasher: A Real-Time Mashup System for Live Musical Input},
  pages = {541--544},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178744},
  url = {http://www.nime.org/proceedings/2014/nime2014_405.pdf},
  abstract = {In this paper we present Improvasher a real-time musical accompaniment system which creates an automatic mashup to accompany live musical input. Improvasher is built around two music processing modules, the first, a performance following technique, makes beat-synchronous predictions of chroma features from a live musical input. The second, a music mashup system, determines the compatibility between beat-synchronous chromagrams from different pieces of music. Through the combination of these two techniques, a real-time time predict mashup can be generated towards a new form of automatic accompaniment for interactive musical performance.}
}

@inproceedings{operrotin2014,
  author = {Olivier Perrotin and Christophe d'Alessandro},
  title = {Visualizing Gestures in the Control of a Digital Musical Instrument},
  pages = {605--608},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178901},
  url = {http://www.nime.org/proceedings/2014/nime2014_406.pdf},
  abstract = {Conceiving digital musical instruments might be challenging in terms of spectator accessibility. Depending on the interface and the complexity of the software used as a transition between the controller and sound, a musician performance can be totally opaque for the audience and loose its interest. This paper examines the possibility of adding a visual feedback to help the public understanding, and add expressivity to the performance. It explores the various mapping organizations between controller and sound, giving different spaces of representation for the visual feedback. It can be either an amplification of the controller parameters, or a representation of the related musical parameters. Different examples of visualization are presented and evaluated, taking the Cantor Digitalis as a support. It appears the representation of musical parameters, little used compared to the representation of controllers, received a good opinion from the audience, highlighting the musical intention of the performers.}
}

@inproceedings{ldonovan2014,
  author = {Liam Donovan and Andrew McPherson},
  title = {The Talking Guitar: Headstock Tracking and Mapping Strategies},
  pages = {351--354},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178752},
  url = {http://www.nime.org/proceedings/2014/nime2014_407.pdf},
  abstract = {This paper presents the Talking Guitar, an electric guitar augmented with a system which tracks the position of the headstock in real time and uses that data to control the parameters of a formant-filtering effect which impresses upon the guitar sound a sense of speech. A user study is conducted with the device to establish an indication of the practicality of using headstock tracking to control effect parameters and to suggest natural and useful mapping strategies. Individual movements and gestures are evaluated in order to guide further development of the system.}
}

@inproceedings{vzappi2014,
  author = {Victor Zappi and Andrew McPherson},
  title = {Dimensionality and Appropriation in Digital Musical Instrument Design},
  pages = {455--460},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178993},
  url = {http://www.nime.org/proceedings/2014/nime2014_409.pdf},
  abstract = {This paper investigates the process of appropriation in digital musical instrument performance, examining the effect of instrument complexity on the emergence of personal playing styles. Ten musicians of varying background were given a deliberately constrained musical instrument, a wooden cube containing a touch/force sensor, speaker and embedded computer. Each cube was identical in construction, but half the instruments were configured for two degrees of freedom while the other half allowed only a single degree. Each musician practiced at home and presented two performances, in which their techniques and reactions were assessed through video, sensor data logs, questionnaires and interviews. Results show that the addition of a second degree of freedom had the counterintuitive effect of reducing the exploration of the instrument's affordances; this suggested the presence of a dominant constraint in one of the two configurations which strongly differentiated the process of appropriation across the two groups of participants.}
}

@inproceedings{mrodrigues2014,
  author = {Clayton Mamedes and Mailis Rodrigues and Marcelo M. Wanderley and J{\^o}natas Manzolli and Denise H. L. Garcia and Paulo Ferreira-Lopes},
  title = {Composing for {DMI}s Entoa, a Dedicate Piece for Intonaspacio},
  pages = {509--512},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178861},
  url = {http://www.nime.org/proceedings/2014/nime2014_411.pdf},
  abstract = {Digital Musical Instruments (DMIs) have difficulties establishing themselves after their creation. A huge number of DMIs is presented every year and few of them actually remain in use. Several causes could explain this reality, among them the lack of a proper instrumental technique, inadequacy of the traditional musical notation and the non-existence of a repertoire dedicated to the instrument. In this paper we present Entoa, the first written music for Intonaspacio, a DMI we designed in our research project. We propose some strategies for mapping data from sensors to sound processing, in order to accomplish an expressive performance. Entoa is divided in five different sections that corresponds to five movements. For each, a different mapping is designed, introducing subtle alterations that progressively explore the ensemble of features of the instrument. The performer is then required to adapt his repertoire of gestures along the piece. Indications are expressed through a gestural notation, where freedom is give to performer to control certain parameters at specific moments in the music.}
}

@inproceedings{dmazzanti2014,
  author = {Dario Mazzanti and Victor Zappi and Darwin Caldwell and Andrea Brogni},
  title = {Augmented Stage for Participatory Performances},
  pages = {29--34},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178871},
  url = {http://www.nime.org/proceedings/2014/nime2014_413.pdf},
  abstract = {Designing a collaborative performance requires the use of paradigms and technologies which can deeply influence the whole piece experience. In this paper we define a set of six variables, and use them to describe and evaluate a number of platforms for participatory performances. Based on this evaluation, the Augmented Stage is introduced. Such concept describes how Augmented Reality techniques can be used to superimpose a performance stage with a virtual environment, populated with interactive elements. The manipulation of these objects allows spectators to contribute to the visual and sonic outcome of the performance through their mobile devices, while keeping their freedom to focus on the stage. An interactive acoustic rock performance based on this concept was staged. Questionnaires distributed to the audience and performers' comments have been analyzed, contributing to an evaluation of the presented concept and platform done through the defined variables.}
}

@inproceedings{rtubb2014,
  author = {Robert Tubb and Simon Dixon},
  title = {The Divergent Interface: Supporting Creative Exploration of Parameter Spaces},
  pages = {227--232},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178967},
  url = {http://www.nime.org/proceedings/2014/nime2014_415.pdf},
  abstract = {This paper outlines a theoretical framework for creative technology based on two contrasting processes: divergent exploration and convergent optimisation. We claim that these two cases require different gesture-to-parameter mapping properties. Results are presented from a user experiment that motivates this theory. The experiment was conducted using a publicly available iPad app: ``Sonic Zoom''. Participants were encouraged to conduct an open ended exploration of synthesis timbre using a combination of two different interfaces. The first was a standard interface with ten sliders, hypothesised to be suited to the ``convergent'' stage of creation. The second was a mapping of the entire 10-D combinatorial space to a 2-D surface using a space filling curve. This novel interface was intended to support the ``divergent'' aspect of creativity. The paths of around 250 users through both 2-D and 10-D space were logged and analysed. Both the interaction data and questionnaire results show that the different interfaces tended to be used for different aspects of sound creation, and a combination of these two navigation styles was deemed to be more useful than either individually. The study indicates that the predictable, separate parameters found in most music technology are more appropriate for convergent tasks.}
}

@inproceedings{sfavilla2014,
  author = {Stu Favilla and Sonja Pedell},
  title = {Touch Screen Collaborative Music: Designing NIME for Older People with Dementia},
  pages = {35--39},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178760},
  url = {http://www.nime.org/proceedings/2014/nime2014_417.pdf},
  abstract = {This paper presents new touch-screen collaborative music interaction for people with dementia. The authors argue that dementia technology has yet to focus on collaborative multi-user group musical interactions. The project aims to contribute to dementia care while addressing a significant gap in current literature. Two trials explore contrasting musical scenarios: the performance of abstract electronic music and the distributed performance of J.S. Bach's Goldberg Variations. Findings presented in this paper; demonstrate that people with dementia can successfully perform and engage in collaborative music performance activities with little or no scaffolded instruction. Further findings suggest that people with dementia can develop and retain musical performance skill over time. This paper proposes a number of guidelines and design solutions.}
}

@inproceedings{jeaton2014,
  author = {Joel Eaton and Weiwei Jin and Eduardo Miranda},
  title = {The Space Between Us. A Live Performance with Musical Score Generated via Emotional Levels Measured in {EEG} of One Performer and an Audience Member},
  pages = {593--596},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178756},
  url = {http://www.nime.org/proceedings/2014/nime2014_418.pdf},
  abstract = {The Space Between Us is a live performance piece for vocals, piano and live electronics using a Brain-Computer Music Interface system for emotional control of the score. The system not only aims to reflect emotional states but to direct and induce emotional states through the real-time generation of the score, highlighting the potential of direct neural-emotional manipulation in live performance. The EEG of the vocalist and one audience member is measured throughout the performance and the system generates a real-time score based on mapping the emotional features within the EEG. We measure the two emotional descriptors, valence and arousal, within EEG and map the two-dimensional correlate of averaged windows to musical phrases. These pre-composed phrases contain associated emotional content based on the KTH Performance Rules System (Director Musices). The piece is in three movements, the first two are led by the emotions of each subject respectively, whilst the third movement interpolates the combined response of the performer and audience member. The system not only aims to reflect the individuals' emotional states but also attempts to induce a shared emotional experience by drawing the two responses together. This work highlights the potential available in affecting neural-emotional manipulation within live performance and demonstrates a new approach to real-time, affectively-driven composition.}
}

@inproceedings{jmathew2014,
  author = {Justin Mathew and St{\'e}phane Huot and Alan Blum},
  title = {A Morphological Analysis of Audio-Objects and their Control Methods for {3D} Audio},
  pages = {415--420},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178865},
  url = {http://www.nime.org/proceedings/2014/nime2014_420.pdf},
  abstract = {Recent technological improvements in audio reproduction systems increased the possibilities to spatialize sources in a listening environment. The spatialization of reproduced audio is however highly dependent on the recording technique, the rendering method, and the loudspeaker configuration. While object-based audio production has proven to reduce the dependency on loudspeaker configurations, authoring tools are still considered to be difficult to interact with in current production environments. In this paper, we investigate the issues of spatialization techniques for object-based audio production and introduce the Spatial Audio Design Spaces (SpADS) framework, that provides insights into the spatial manipulation of object-based audio. Based on interviews with professional sound engineers, this morphological analysis clarifies the relationships between recording and rendering techniques that define audio-objects for 3D speaker configurations, allowing the analysis and the design of advanced object-based controllers as well.}
}

@inproceedings{rcanning2014,
  author = {Rob Canning},
  title = {Interactive Parallax Scrolling Score Interface for Composed Networked Improvisation},
  pages = {144--146},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178728},
  url = {http://www.nime.org/proceedings/2014/nime2014_421.pdf},
  abstract = {This paper describes the Parallaxis Score System, part of the authors ongoing research into to the development of technological tools that foster creative interactions between improvising musicians and predefined instructional texts. The Parallaxis platform places these texts within a networked, interactive environment with a generalised set of controls in order to explore and devise ontologies of network performance. As an interactive tool involved in music production the score system itself undergoes a functional transformation and becomes a distributed meta-instrument in its own right, independent from, yet intrinsically connected to those instruments held by the performers.}
}

@inproceedings{arenaud2014,
  author = {Alain Renaud and Caecilia Charbonnier and Sylvain Chagu\'e},
  title = {{3D}inMotion A Mocap Based Interface for Real Time Visualisation and Sonification of Multi-User Interactions},
  pages = {495--496},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178915},
  url = {http://www.nime.org/proceedings/2014/nime2014_423.pdf},
  abstract = {This paper provides an overview of a proposed demonstration of 3DinMotion, a system using real time motion capture of one or several subjects, which can be used in interactive audiovisual pieces and network performances. The skeleton of a subject is analyzed in real time and displayed as an abstract avatar as well as sonified based on mappings and rules to make the interplay experience lively and rewarding. A series of musical pieces have been composed for the interface following cueing strategies. In addition a second display, ``the prompter'' guides the users through the piece. 3DinMotion has been developed from scratch and natively, leading to a system with a very low latency, making it suitable for real time music interactions. In addition, 3DinMotion is fully compatible with the OpenSoundControl (OSC) protocol, allowing expansion to commonly used musical and sound design applications.}
}

@inproceedings{tkelkar2014,
  author = {Udit Roy and Tejaswinee Kelkar and Bipin Indurkhya},
  title = {TrAP: An Interactive System to Generate Valid Raga Phrases from Sound-Tracings},
  pages = {243--246},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178923},
  url = {http://www.nime.org/proceedings/2014/nime2014_424.pdf},
  abstract = {We propose a new musical interface, TrAP (TRace-A-Phrase) for generating phrases of Hindustani Classical Music (HCM). In this system the user traces melodic phrases on a tablet interface to create phrases in a raga. We begin by analyzing tracings drawn by 28 participants, and train a classifier to categorize them into one of four melodic categories from the theory of Hindustani Music. Then we create a model based on note transitions from the raga grammar for the notes used in the singable octaves in HCM. Upon being given a new tracing, the system segments the tracing and computes a final phrase that best approximates the tracing.}
}

@inproceedings{ncollins2014,
  author = {Nick Collins and Alex McLean},
  title = {Algorave: Live Performance of Algorithmic Electronic Dance Music},
  pages = {355--358},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178734},
  url = {http://www.nime.org/proceedings/2014/nime2014_426.pdf},
  abstract = {The algorave movement has received reasonable international exposure in the last two years, including a series of concerts in Europe and beyond, and press coverage in a number of media. This paper seeks to illuminate some of the historical precedents to the scene, its primary aesthetic goals, and the divergent technological and musical approaches of representative participants. We keep in mind the novel possibilities in musical expression explored by algoravers. The scene is by no means homogeneous, and the very lack of uniformity of technique, from new live coding languages through code DJing to plug-in combination, with or without visual extension, is indicative of the flexibility of computers themselves as general information processors.}
}

@inproceedings{jbowers12014,
  author = {John Bowers and Tim Shaw},
  title = {Reappropriating Museum Collections: Performing Geology Specimens and Meterology Data as New Instruments for Musical Expression},
  pages = {175--178},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178720},
  url = {http://www.nime.org/proceedings/2014/nime2014_429.pdf},
  abstract = {In this paper we describe an artistic response to a collection of natural history museum artefacts, developed as part of a residency organised around a public participatory workshop. Drawing on a critical literature in studies of material culture, the work incorporated data sonification, image audification, field recordings and created a number of instruments for exploring geological artefacts and meterological data as aesthetic material. The residency culminated in an exhibition presented as a 'sensorium' for the sensory exploration of museum objects. In describing the methods and thinking behind the project this paper presents an alternative approach to engaging artists and audiences with local heritage and museum archives, which draws on research in NIME and allied literatures, and which is devoted to enlivening collections as occasions for varied interpretation, appropriation and aesthetic response.}
}

@inproceedings{ahadjakos12014,
  author = {Aristotelis Hadjakos and Simon Waloschek},
  title = {SPINE: A TUI Toolkit and Physical Computing Hybrid},
  pages = {625--628},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178782},
  url = {http://www.nime.org/proceedings/2014/nime2014_430.pdf},
  abstract = {Physical computing platforms such as the Arduino have significantly simplified developing physical musical interfaces. However, those platforms typically target everyday programmers rather than composers and media artists. On the other hand, tangible user interface (TUI) toolkits, which provide an integrated, easy-to-use solution have not gained momentum in modern music creation. We propose a concept that hybridizes physical computing and TUI toolkit approaches. This helps to tackle typical TUI toolkit weaknesses, namely quick sensor obsolescence and limited choices. We developed a physical realization based on the idea of "universal pins", which can be configured to perform a variety of duties, making it possible to connect different sensor breakouts and modules. We evaluated our prototype by making performance measurements and conducting a user study demonstrating the feasibility of our approach.}
}

@inproceedings{ogreen2014,
  author = {Owen Green},
  title = {NIME, Musicality and Practice-led Methods},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178776},
  url = {http://www.nime.org/proceedings/2014/nime2014_434.pdf},
  abstract = {To engage with questions of musicality is to invite into consideration a complex network of topics beyond the mechanics of soundful interaction with our interfaces. Drawing on the work of Born, I sketch an outline of the reach of these topics. I suggest that practice-led methods, by dint of focussing on the lived experience where many of these topics converge, may be able to serve as a useful methodological `glue' for NIME by helping stimulate useful agonistic discussion on our objects of study, and map the untidy contours of contemporary practices. I contextualise this discussion by presenting two recently developed improvisation systems and drawing from these some starting suggestions for how attention to the grain of lived practice could usefully contribute to considerations for designers in terms of the pursuit of musicality and the care required in considering performances in evaluation.}
}

@inproceedings{jsokolovskis2014,
  author = {Janis Sokolovskis and Andrew McPherson},
  title = {Optical Measurement of Acoustic Drum Strike Locations},
  pages = {70--73},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178943},
  url = {http://www.nime.org/proceedings/2014/nime2014_436.pdf},
  abstract = {This paper presents a method for locating the position of a strike on an acoustic drumhead. Near-field optical sensors were installed underneath the drumhead of a commercially available snare drum. By implementing time difference of arrival (TDOA) algorithm accuracy within 2cm was achieved in approximating the location of strikes. The system can be used for drum performance analysis, timbre analysis and can form a basis for an augmented drum performance system.}
}

@inproceedings{fmorreale2014,
  author = {Fabio Morreale and Antonella De Angeli and Sile O'Modhrain},
  title = {Musical Interface Design: An Experience-oriented Framework},
  pages = {467--472},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178879},
  url = {http://www.nime.org/proceedings/2014/nime2014_437.pdf},
  abstract = {This paper presents MINUET, a framework for musical interface design grounded in the experience of the player. MINUET aims to provide new perspectives on the design of musical interfaces, referred to as a general term that comprises digital musical instruments and interactive installations. The ultimate purpose is to reduce the complexity of the design space emphasizing the experience of the player. MINUET is structured as a design process consisting of two stages: goal and specifications. The reliability of MINUET is tested through a systematic comparison with the related work and through a case study. To this end, we present the design and prototyping of Hexagon, a new musical interface with learning purposes.}
}

@inproceedings{jbowers2014,
  author = {John Bowers and Annika Haas},
  title = {Hybrid Resonant Assemblages: Rethinking Instruments, Touch and Performance in New Interfaces for Musical Expression},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178718},
  url = {http://www.nime.org/proceedings/2014/nime2014_438.pdf},
  abstract = {This paper outlines a concept of hybrid resonant assemblages, combinations of varied materials excited by sound transducers, feeding back to themselves via digital signal processing. We ground our concept as an extension of work by David Tudor, Nicolas Collins and Bowers and Archer [NIME 2005] and draw on a variety of critical perspectives in the social sciences and philosophy to explore such assemblages as an alternative to more familiar ideas of instruments and interfaces. We lay out a conceptual framework for the exploration of hybrid resonant assemblages and describe how we have approached implementing them. Our performance experience is presented and implications for work are discussed. In the light of our work, we urge a reconsideration of the implicit norms of performance which underlie much research in NIME. In particular, drawing on the philosophical work of Jean-Luc Nancy, we commend a wider notion of touch that also recognises the performative value of withholding contact.}
}

@inproceedings{cgeiger2014,
  author = {Cornelius Pöpel and Jochen Feitsch and Marco Strobel and Christian Geiger},
  title = {Design and Evaluation of a Gesture Controlled Singing Voice Installation},
  pages = {359--362},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178905},
  url = {http://www.nime.org/proceedings/2014/nime2014_439.pdf},
  abstract = {We present a system that allows users to experience singing without singing using gesture-based interaction techniques. We designed a set of body-related interaction and multi-modal feedback techniques and developed a singing voice synthesizer system that is controlled by the user's mouth shapes and arm gestures. Based on the adaption of a number of digital media-related techniques such as face and body tracking, 3D rendering, singing voice synthesis and physical computing, we developed a media installation that allows users to perform an aria without real singing and provide the look and feel from a 20th century performance of an opera singer. We evaluated this system preliminarily with users.}
}

@inproceedings{dwilliams2014,
  author = {Duncan Williams and Peter Randall-Page and Eduardo Miranda},
  title = {Timbre morphing: near real-time hybrid synthesis in a musical installation},
  pages = {435--438},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178983},
  url = {http://www.nime.org/proceedings/2014/nime2014_440.pdf},
  abstract = {This paper presents an implementation of a near real-time timbre morphing signal processing system, designed to facilitate an element of `liveness' and unpredictability in a musical installation. The timbre morpher is a hybrid analysis and synthesis technique based on Spectral Modeling Synthesis (an additive and subtractive modeling technique). The musical installation forms an interactive soundtrack in response to the series of Rosso Luana marble sculptures Shapes in the Clouds, I, II, III, IV & V by artist Peter Randall-Page, exhibited at the Peninsula Arts Gallery in Devon, UK, from 1 February to 29 March 2014. The timbre morphing system is used to transform live input captured at each sculpture with a discrete microphone array, by morphing towards noisy source signals that have been associated with each sculpture as part of a pre-determined musical structure. The resulting morphed audio is then fed-back to the gallery via a five-channel speaker array. Visitors are encouraged to walk freely through the installation and interact with the sound world, creating unique audio morphs based on their own movements, voices, and incidental sounds.}
}

@inproceedings{pvandertorren12014,
  author = {Piers Titus van der Torren},
  title = {Striso, a Compact Expressive Instrument Based on a New Isomorphic Note Layout},
  pages = {615--620},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178957},
  url = {http://www.nime.org/proceedings/2014/nime2014_442.pdf},
  abstract = {The Striso is a new expressive music instrument with an acoustic feel, which is designed to be intuitive to play and playable everywhere. The sound of every note can be precisely controlled using the direction and pressure sensitive buttons, combined with instrument motion like tilting or shaking. It works standalone, with an internal speaker and battery, and is meant as a self contained instrument with its own distinct sound, but can also be connected to a computer to control other synthesizers. The notes are arranged in an easy and systematic way, according to the new DCompose note layout that is also presented in this paper. The DCompose note layout is designed to be compact, ergonomic, easy to learn, and closely bound to the harmonic properties of the notes.}
}

@inproceedings{mbugge2014,
  author = {Anders Tveit and Hans Wilmers and Notto Thelle and Magnus Bugge and Thom Johansen and Eskil Muan S{\ae}ther},
  title = {{Reunion}2012: A Novel Interface for Sound Producing Actions Through the Game of Chess},
  pages = {561--564},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178969},
  url = {http://www.nime.org/proceedings/2014/nime2014_443.pdf},
  abstract = {Reunion2012 is a work for electronically modified chessboard, chess players and electronic instruments. The work is based on---but also departs from---John Cage's Reunion, which premiered at the Sightsoundsystems Festival, Toronto, 1968. In the original performance, Cage and Marcel Duchamp played chess on an electronic board constructed by Lowell Cross. The board `conducted' various electronic sound sources played by Cross, Gordon Mumma, David Tudor, and David Behrman, using photoresistors fitted under the squares [1]. Reunion2012, on the other hand, utilises magnet sensors via an Arduino. Like in Cage's Variations V, this resulted in a musical situation where the improvising musicians had full control over their own sound, but no control regarding when their sound may be heard. In addition to a concert version, this paper also describes an interactive installation based on the same hardware.}
}

@inproceedings{avantroyer2014,
  author = {Akito van Troyer},
  title = {Composing Embodied Sonic Play Experiences: Towards Acoustic Feedback Ecology},
  pages = {118--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178961},
  url = {http://www.nime.org/proceedings/2014/nime2014_444.pdf},
  abstract = {Acoustic feedback controllers (AFCs) are typically applied to solve feedback problems evident in applications such as public address (PA) systems, hearing aids, and speech applications. Applying the techniques of AFCs to different contexts, such as musical performance, sound installations, and product design, presents a unique insight into the research of embodied sonic interfaces and environments. This paper presents techniques that use digital acoustic feedback control algorithms to augment the sonic properties of environments and discusses approaches to the design of sonically playful experiences that apply such techniques. Three experimental prototypes are described to illustrate how the techniques can be applied to versatile environments and continuous coupling of users' audible actions with sonically augmented environments. The knowledge obtained from these prototypes has led to Acoustic Feedback Ecology System (AFES) design patterns. The paper concludes with some future research directions based on the prototypes and proposes several other potentially useful applications ranging from musical performance to everyday contexts.}
}

@inproceedings{mcartwright2014,
  author = {Mark Cartwright and Bryan Pardo},
  title = {SynthAssist: Querying an Audio Synthesizer by Vocal Imitation},
  pages = {363--366},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178730},
  url = {http://www.nime.org/proceedings/2014/nime2014_446.pdf},
  abstract = {Programming an audio synthesizer can be a difficult task for many. However, if a user has a general idea of the sound they are trying to program, they may be able to imitate it with their voice. This paper presents SynthAssist, a system for interactively searching the synthesis space of an audio synthesizer. In this work, we present how to use the system for querying a database of audio synthesizer patches (i.e. settings/parameters) by vocal imitation and user feedback. To account for the limitations of the human voice, it uses both absolute and relative time series representations of features and relevance feedback on both the feature weights and time series to refine the query. The method presented in this paper can be used to search through large databases of previously existing ``factory presets'' or program a synthesizer using the data-driven approach to automatic synthesizer programming.}
}

@inproceedings{chutchins2014,
  author = {Charles Hutchins and Holger Ballweg and Shelly Knotts and Jonas Hummel and Antonio Roberts},
  title = {Soundbeam: A Platform for Sonyfing Web Tracking},
  pages = {497--498},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178810},
  url = {http://www.nime.org/proceedings/2014/nime2014_447.pdf},
  abstract = {Government spying on internet traffic has seemingly become ubiquitous. Not to be left out, the private sector tracks our online footprint via our ISP or with a little help from facebook. Web services, such as advertisement servers and Google track our progress as we surf the net and click on links. The Mozilla plugin, Lightbeam (formerly Collusion), shows the user a visual map of every site a surfer sends data to. A interconnected web of advertisers and other (otherwise) invisible data-gatherers quickly builds during normal usage. We propose modifying this plugin so that as the graph builds, its state is broadcast visa OSC. Members of BiLE will receive and interpret those OSC messages in SuperCollider and PD. We will act as a translational object in a process of live-sonification. The collected data is the material with which we will develop a set of music tracks based on patterns we may discover. The findings of our data collection and the developed music will be presented in the form of an audiovisual live performance. Snippets of collected text and URLs will both form the basis of our audio interpretation, but also be projected on to a screen, so an audience can voyeuristically experience the actions taken on their behalf by governments and advertisers. After the concert, all of the scripts and documentation related to the data collection and sharing in the piece will be posted to github under a GPL license.}
}

@inproceedings{jcomajuncosas2014,
  author = {Josep Comajuncosas and Enric Guaus},
  title = {Conducting Collective Instruments : A Case Study},
  pages = {513--516},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178736},
  url = {http://www.nime.org/proceedings/2014/nime2014_448.pdf},
  abstract = {According to the tradition, music ensembles are usually lead by a conductor who is the responsible to coordinate and guide the group under a specific musical criteria. Similarly, computer ensembles resort to a conductor to keep the synchronization and structural coordination of the performance, often with the assistance of software. Achieving integration and coherence in a networked performance, however, can be challenging in certain scenarios. This is the case for configurations with a high degree of mutual interdependence and shared control. This paper focuses on the design strategies for developing a software based conductor assistant for collective instruments. We propose a novel conductor dimension space representation for collective instruments, which takes into account both its social and structural features. We present a case study of a collective instrument implementing a software conductor. Finally, we discuss the implications of human and machine conduction schemes in the context of the proposed dimension space.}
}

@inproceedings{mgurevich12014,
  author = {Michael Gurevich},
  title = {Distributed Control in a Mechatronic Musical Instrument},
  pages = {487--490},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178780},
  url = {http://www.nime.org/proceedings/2014/nime2014_449.pdf},
  abstract = {Drawing on concepts from systemics, cybernetics, and musical automata, this paper proposes a mechatronic, electroacoustic instrument that allows for shared control between programmed, mechanized motion and a human interactor. We suggest that such an instrument, situated somewhere between a robotic musical instrument and a passive controller, will foster the emergence of new, complex, and meaningful modes of musical interaction. In line with the methodological principles of practice as research, we describe the development and design of one such instrument-Stringtrees. The design process also reflects the notion of ambiguity as a resource in design: The instrument was endowed with a collection of sensors, controls, and actuators without a highly specific or prescriptive model for how a musician would interact with it.}
}

@inproceedings{dschwarz12014,
  author = {Diemo Schwarz and Pierre Alexandre Tremblay and Alex Harker},
  title = {Rich Contacts: Corpus-Based Convolution of Contact Interaction Sound for Enhanced Musical Expression},
  pages = {247--250},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178935},
  url = {http://www.nime.org/proceedings/2014/nime2014_451.pdf},
  abstract = {We propose ways of enriching the timbral potential of gestural sonic material captured via piezo or contact microphones, through latency-free convolution of the microphone signal with grains from a sound corpus. This creates a new way to combine the sonic richness of large sound corpora, easily accessible via navigation through a timbral descriptor space, with the intuitive gestural interaction with a surface, captured by any contact microphone. We use convolution to excite the grains from the corpus via the microphone input, capturing the contact interaction sounds, which allows articulation of the corpus by hitting, scratching, or strumming a surface with various parts of the hands or objects. We also show how changes of grains have to be carefully handled, how one can smoothly interpolate between neighbouring grains, and finally evaluate the system against previous attempts.}
}

@inproceedings{fvisi2014,
  author = {Federico Visi and Rodrigo Schramm and Eduardo Miranda},
  title = {Use of Body Motion to Enhance Traditional Musical Instruments},
  pages = {601--604},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178973},
  url = {http://www.nime.org/proceedings/2014/nime2014_460.pdf},
  abstract = {This work describes a new approach to gesture mapping in a performance with a traditional musical instrument and live electronics based upon theories of embodied music cognition (EMC) and musical gestures. Considerations on EMC and how gestures affect the experience of music inform different mapping strategies. Our intent is to enhance the expressiveness and the liveness of performance by tracking gestures via a multimodal motion capture system and to use motion data to control several features of the music. After a review of recent research in the field, a proposed application of such theories to a performance with electric guitar and live electronics will follow, focusing both on aspects of meaning formation and motion capturing.}
}

@inproceedings{jjeon2014,
  author = {Jimin Jeon and Gunho Chae and Edward Jangwon Lee and Woon Seung Yeo},
  title = {TAPIR Sound Tag: An Enhanced Sonic Communication Framework for Audience Participatory Performance},
  pages = {367--370},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178818},
  url = {http://www.nime.org/proceedings/2014/nime2014_461.pdf},
  abstract = {This paper presents an enhanced sonic data communication method using TAPIR (Theoretically Audible, but Practically Inaudible Range: frequencies above 18kHz) sound and a software toolkit as its implementation. Using inaudible sound as a data medium, a digital data network among the audience and performer can be easily built with microphones and speakers, without requiring any additional hardware. ``TAPIR Sound Tag'' is a smart device framework for inaudible data communication that can be easily embedded in audience participatory performances and interactive arts. With a bandwidth of 900 Hz, a high transmission rate of 200 bps can be achieved, enabling peer-to-peer or broadcasting real-time data communication among smart devices. This system can be used without any advanced knowledge in signal processing and communication system theory; simply specifying carrier frequency and bandwidth with a few lines of code can start data communication. Several usage scenarios of the system are also presented, such as participating in an interactive performance by adding and controlling sound, and collaborative completion of an artist's work by audience. We expect this framework to provide a new way of audience interaction to artists, as well as further promoting audience participation by simplifying the process: using personal smart devices as a medium and not requiring additional hardware or complex settings.}
}

@inproceedings{asarasua2014,
  author = {Alvaro Saras\'ua and Enric Guaus},
  title = {Dynamics in Music Conducting: A Computational Comparative Study Among Subjects},
  pages = {195--200},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178929},
  url = {http://www.nime.org/proceedings/2014/nime2014_464.pdf},
  abstract = {Many musical interfaces have used the musical conductor metaphor, allowing users to control the expressive aspects of a performance by imitating the gestures of conductors. In most of them, the rules to control these expressive aspects are predefined and users have to adapt to them. Other works have studied conductors' gestures in relation to the performance of the orchestra. The goal of this study is to analyze, following the path initiated by this latter kind of works, how simple motion capture descriptors can explain the relationship between the loudness of a given performance and the way in which different subjects move when asked to impersonate the conductor of that performance. Twenty-five subjects were asked to impersonate the conductor of three classical music fragments while listening to them. The results of different linear regression models with motion capture descriptors as explanatory variables show that, by studying how descriptors correlate to loudness differently among subjects, different tendencies can be found and exploited to design models that better adjust to their expectations.}
}

@inproceedings{kkeatch2014,
  author = {Kirsty Keatch},
  title = {An Exploration of Peg Solitaire as a Compositional Tool},
  pages = {102--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178827},
  url = {http://www.nime.org/proceedings/2014/nime2014_466.pdf},
  abstract = {Sounds of Solitaire is a novel interface for musical expression based on an extended peg solitaire board as a generator of live musical composition. The classic puzzle game, for one person, is extended by mapping the moves of the game through a self contained system using Arduino and Raspberry Pi, triggering both analogue and digital sound. The solitaire board, as instrument, is presented as a wood and Perspex box with the hardware inside. Ball bearings function as both solitaire pegs and switches, while a purpose built solenoid controlled monochord and ball bearing run provide the analogue sound source, which is digitally manipulated in real-time, according to the sequences of game moves. The creative intention of Sounds of Solitaire is that the playful approach to participation in a musical experience, provided by the material for music making in real-time, demonstrates an integrated approach to concepts of composing, performing and listening.}
}

@inproceedings{xxiao2014,
  author = {Xiao Xiao and Basheer Tome and Hiroshi Ishii},
  title = {Andante: Walking Figures on the Piano Keyboard to Visualize Musical Motion},
  pages = {629--632},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178987},
  url = {http://www.nime.org/proceedings/2014/nime2014_467.pdf},
  abstract = {We present Andante, a representation of music as animated characters walking along the piano keyboard that appear to play the physical keys with each step. Based on a view of music pedagogy that emphasizes expressive, full-body communication early in the learning process, Andante promotes an understanding of the music rooted in the body, taking advantage of walking as one of the most fundamental human rhythms. We describe three example visualizations on a preliminary prototype as well as applications extending our examples for practice feedback, improvisation and composition. Through our project, we reflect on some high level considerations for the NIME community.}
}

@inproceedings{avantklooster2014,
  author = {Adinda van 't Klooster and Nick Collins},
  title = {In A State: Live Emotion Detection and Visualisation for Music Performance},
  pages = {545--548},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178837},
  url = {http://www.nime.org/proceedings/2014/nime2014_469.pdf},
  abstract = {Emotion is a complex topic much studied in music and arguably equally central to the visual arts where this is usually referred to with the overarching label of aesthetics. This paper explores how music and the arts have incorporated the study of emotion. We then introduce the development of a live audio visual interface entitled In A State that detects emotion from live audio (in this case a piano performance) and generates visuals and electro acoustic music in response.}
}

@inproceedings{doverholt2014,
  author = {Dan Overholt and Steven Gelineck},
  title = {Design \& Evaluation of an Accessible Hybrid Violin Platform},
  pages = {122--125},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178897},
  url = {http://www.nime.org/proceedings/2014/nime2014_470.pdf},
  abstract = {We introduce and describe the initial evaluation of a new low-cost augmented violin prototype, with research focused on the user experience when playing such hybrid physical-digital instruments, and the exploration of novel interactive performance techniques. Another goal of this work is wider platform accessibility for players, via a simple `do-it-yourself' approach described by the design herein. While the hardware and software elements are open source, the build process can nonetheless require non-insignificant investments of time and money, as well as basic electronics construction skills. These have been kept to a minimum wherever possible. Our initial prototype is based upon an inexpensive electric violin that is widely available online for approximately $200 USD. This serves as the starting point for construction, to which the design adds local Digital Signal Processing (DSP), gestural sensing, and sound output. Real-time DSP algorithms are running on a mobile device, which also incorporates orientation/gesture sensors for parameter mapping, with the resulting sound amplified and rendered via small loudspeakers mounted on the instrument. The platform combines all necessary elements for digitally-mediated interactive performance; the need for a traditional computer only arises when developing new DSP algorithms for the platform. An initial exploratory evaluation with users is presented, in which performers explore different possibilities with the proposed platform (various DSP implementations, mapping schemes, physical setups, etc.) in order to better establish the needs of the performing artist. Based on these results, future work is outlined leading towards the development of a complete quartet of instruments.}
}

@inproceedings{axambo2014,
  author = {Anna Xamb\'o and Gerard Roma and Robin Laney and Chris Dobbyn and Sergi Jord\`a},
  title = {SoundXY4: Supporting Tabletop Collaboration and Awareness with Ambisonics Spatialisation},
  pages = {40--45},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178985},
  url = {http://www.nime.org/proceedings/2014/nime2014_471.pdf},
  abstract = {Co-located tabletop tangible user interfaces (TUIs) for music performance are known for promoting multi-player collaboration with a shared interface, yet it is still unclear how to best support the awareness of the workspace in terms of understanding individual actions and the other group members actions, in parallel. In this paper, we investigate the effects of providing auditory feedback using ambisonics spatialisation, aimed at informing users about the location of the tangibles on the tabletop surface, with groups of mixed musical backgrounds. Participants were asked to improvise music on {SoundXY4: The Art of Noise}, a tabletop system that includes sound samples inspired by Russolo's taxonomy of noises. We compared spatialisation vs. no-spatialisation conditions, and findings suggest that, when using spatialisation, there was a clearer workspace awareness, and a greater engagement in the musical activity as an immersive experience.}
}

@inproceedings{smealla2014,
  author = {Sergi Jord\`a and Sebastian Mealla},
  title = {A Methodological Framework for Teaching, Evaluating and Informing NIME Design with a Focus on Mapping and Expressiveness},
  pages = {233--238},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178824},
  url = {http://www.nime.org/proceedings/2014/nime2014_472.pdf},
  abstract = {The maturation process of the NIME field has brought a growing interest in teaching the design and implementation of Digital Music Instruments (DMI) as well as in finding objective evaluation methods to assess the suitability of these outcomes. In this paper we propose a methodology for teaching NIME design and a set of tools meant to inform the design process. This approach has been applied in a master course focused on the exploration of expressiveness and on the role of the mapping component in the NIME creation chain, through hands-on and self-reflective approach based on a restrictive setup consisting of smart-phones and the Pd programming language. Working Groups were formed, and a 2-step DMI design process was applied, including 2 performance stages. The evaluation tools assessed both System and Performance aspects of each project, according to Listeners' impressions after each performance. Listeners' previous music knowledge was also considered. Through this methodology, students with different backgrounds were able to effectively engage in the NIME design processes, developing working DMI prototypes according to the demanded requirements; the assessment tools proved to be consistent for evaluating NIMEs systems and performances, and the fact of informing the design processes with the outcome of the evaluation, showed a traceable progress in the students outcomes.}
}

@inproceedings{btaylor2014,
  author = {Benjamin Taylor and Jesse Allison and William Conlin and Yemin Oh and Daniel Holmes},
  title = {Simplified Expressive Mobile Development with NexusUI, NexusUp, and NexusDrop},
  pages = {257--262},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178951},
  url = {http://www.nime.org/proceedings/2014/nime2014_480.pdf},
  abstract = {Developing for mobile and multimodal platforms is more important now than ever, as smartphones and tablets proliferate and mobile device orchestras become commonplace. We detail NexusUI, a JavaScript framework that enables rapid prototyping and development of expressive multitouch electronic instrument interfaces within a web browser. Extensions of this project assist in easily creating dynamic user interfaces. NexusUI contains several novel encapsulations of creative interface objects, each accessible with one line of code. NexusUp assists in one-button duplication of Max interfaces into mobile-friendly web pages that transmit to Max automatically via Open Sound Control. NexusDrop enables drag-and-drop interface building and saves interfaces to a central Nexus database. Finally, we provide an overview of several projects made with NexusUI, including mobile instruments, art installations, sound diffusion tools, and iOS games, and describe Nexus' possibilities as an architecture for our future Mobile App Orchestra.}
}

@inproceedings{mfunk2014,
  author = {Bastiaan van Hout and Luca Giacolini and Bart Hengeveld and Mathias Funk and Joep Frens},
  title = {Experio: a Design for Novel Audience Participation in Club Settings},
  pages = {46--49},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178808},
  url = {http://www.nime.org/proceedings/2014/nime2014_481.pdf},
  abstract = {When looking at modern music club settings, especially in the area of electronic music, music is consumed in a unidirectional way -from DJ or producer to the audience -with little direct means to influence and participate. In this paper we challenge this phenomenon and aim for a new bond between the audience and the DJ through the creation of an interactive dance concept: Experio. Experio allows for multiple audience participants influencing the musical performance through dance, facilitated by a musical moderator using a tailored interface. This co-creation of electronic music on both novice and expert levels is a new participatory live performance approach, which is evaluated on the basis of thousands of visitors who interacted with Experio during several international exhibitions.}
}

@inproceedings{jfrancoise12014,
  author = {Jules Fran\c{c}oise and Norbert Schnell and Riccardo Borghesi and Fr\'ed\'eric Bevilacqua},
  title = {Probabilistic Models for Designing Motion and Sound Relationships},
  pages = {287--292},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178764},
  url = {http://www.nime.org/proceedings/2014/nime2014_482.pdf},
  abstract = {We present a set of probabilistic models that support the design of movement and sound relationships in interactive sonic systems. We focus on a mapping--by--demonstration approach in which the relationships between motion and sound are defined by a machine learning model that learns from a set of user examples. We describe four probabilistic models with complementary characteristics in terms of multimodality and temporality. We illustrate the practical use of each of the four models with a prototype application for sound control built using our Max implementation.}
}

@inproceedings{atsiros12014,
  author = {Augoustinos Tsiros},
  title = {Evaluating the Perceived Similarity Between Audio-Visual Features Using Corpus-Based Concatenative Synthesis},
  pages = {421--426},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178965},
  url = {http://www.nime.org/proceedings/2014/nime2014_484.pdf},
  abstract = {This paper presents the findings of two exploratory studies. In these studies participants performed a series of image-sound association tasks. The aim of the studies was to investigate the perceived similarity and the efficacy of two multidimensional mappings each consisting of three audio-visual associations. The purpose of the mappings is to enable visual control of corpus-based concatenative synthesis. More specifically the stimuli in the first study was designed to test the perceived similarity of six audio-visual associations, between the two mappings using three corpora resulting in 18 audio-visual stimuli. The corpora differ in terms of two sound characteristics: harmonic contain and continuity. Data analysis revealed no significant differences in the participant's responses between the three corpora, or between the two mappings. However highly significant differences were revealed between the individual audio-visual association pairs. The second study investigates the affects of the mapping and the corpus in the ability of the participants to detect which image out of three similar images was used to generate six audio stimuli. The data analysis revealed significant differences in the ability of the participants' to detect the correct image depending on which corpus was used. Less significant was the effect of the mapping in the success rate of the participant responses.}
}

@inproceedings{ngold2014,
  author = {Jihyun Han and Nicolas Gold},
  title = {Lessons Learned in Exploring the Leap Motion(TM) Sensor for Gesture-based Instrument Design},
  pages = {371--374},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178784},
  url = {http://www.nime.org/proceedings/2014/nime2014_485.pdf},
  abstract = {The Leap Motion(TM) sensor offers fine-grained gesture-recognition and hand tracking. Since its release, there have been several uses of the device for instrument design, musical interaction and expression control, documented through online video. However, there has been little formal documented investigation of the potential and challenges of the platform in this context. This paper presents lessons learned from work-in-progress on the development of musical instruments and control applications using the Leap Motion(TM) sensor. Two instruments are presented: Air-Keys and Air-Pads and the potential for augmentation of a traditional keyboard is explored. The results show that the platform is promising in this context but requires various challenges, both physical and logical, to be overcome.}
}

@inproceedings{jlarsen2014,
  author = {Jeppe Larsen and Dan Overholt and Thomas Moeslund},
  title = {The Actuated guitar: Implementation and user test on children with Hemiplegia},
  pages = {60--65},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178845},
  url = {http://www.nime.org/proceedings/2014/nime2014_486.pdf},
  abstract = {People with a physical handicap are often not able to engage and embrace the world of music on the same terms as normal functioning people. Musical instruments have been refined the last centuries which makes them highly specialized instruments that nearly all requires at least two functioning hands. In this study we try to enable people with hemiplegia to play a real electrical guitar by modifying it in a way that make people with hemiplegia able to actually play the guitar. We developed the guitar platform to utilize sensors to capture the rhythmic motion of alternative fully functioning limbs, such as a foot, knee or the head to activate a motorized fader moving a pick back and forth across the strings. The approach employs the flexibility of a programmable digital system which allows us to scale and map different ranges of data from various sensors to the motion of the actuator and thereby making it easier adapt to individual users. To validate and test the instrument platform we collaborated with the Helena Elsass Center during their 2013 Summer Camp to see if we actually succeeded in creating an electrical guitar that children with hemiplegia could actually play. The initial user studies showed that children with hemiplegia were able to play the actuated guitar by producing rhythmical movement across the strings that enables them to enter a world of music they so often see as closed.}
}

@inproceedings{tresch2014,
  author = {Thomas Resch and Matthias Krebs},
  title = {A Simple Architecture for Server-based (Indoor) Audio Walks},
  pages = {269--272},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178917},
  url = {http://www.nime.org/proceedings/2014/nime2014_491.pdf},
  abstract = {This paper proposes a simple architecture for creating (indoor) audio walks by using a server running Max/MSP together with the external object fhnw.audiowalk.state and smartphone clients running either under Android or iOS using LibPd. Server and smartphone clients communicate over WLAN by exchanging OSC messages. Server and client have been designed in a way that allows artists with only little programming skills to create position-based audio walks.}
}

@inproceedings{strail2014,
  author = {Shawn Trail and Duncan MacConnell and Leo Jenkins and Jeff Snyder and George Tzanetakis and Peter Driessen},
  title = {El-Lamellophone A Low-cost, DIY, Open Framework for Acoustic Lemellophone Based Hyperinstruments},
  pages = {537--540},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178959},
  url = {http://www.nime.org/proceedings/2014/nime2014_492.pdf},
  abstract = {The El-Lamellophone (El-La) is a Lamellophone hyperinstrument incorporating electronic sensors and integrated DSP. Initial investigations have been made into digitallycontrolled physical actuation of the acoustic tines. An embedded Linux micro-computer supplants the laptop. A piezoelectric pickup is mounted to the underside of the body of the instrument for direct audio acquisition providing a robust signal with little interference. The signal is used for electric sound-reinforcement, creative signal processing and audio analysis developed in Puredata (Pd). This signal inputs and outputs the micro computer via stereo 1/8th inch phono jacks. Sensors provide gesture recognition affording the performer a broader, more dynamic range of musical human computer interaction (MHCI) over specific DSP functions. Work has been done toward electromagnetic actuation of the tines, aiming to allow performer control and sensation via both traditional Lamellophone techniques, as well as extended playing techniques that incorporate shared human/computer control of the resulting sound. The goal is to achieve this without compromising the traditional sound production methods of the acoustic instrument while leveraging inherent performance gestures with embedded continuous controller values essential to MHCI. The result is an intuitive, performer designed, hybrid electro-acoustic instrument, idiomatic computer interface, and robotic acoustic instrument in one framework.}
}

@inproceedings{nklugel2014,
  author = {Niklas Kl\''ugel and Gerhard Hagerer and Georg Groh},
  title = {TreeQuencer: Collaborative Rhythm Sequencing A Comparative Study},
  pages = {50--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178835},
  url = {http://www.nime.org/proceedings/2014/nime2014_498.pdf},
  abstract = {In this contribution we will show three prototypical applications that allow users to collaboratively create rhythmic structures with successively more degrees of freedom to generate rhythmic complexity. By means of a user study we analyze the impact of this on the users' satisfaction and further compare it to data logged during the experiments that allow us to measure the rhythmic complexity created.}
}

@inproceedings{dschlienger2014,
  author = {Dominik Schlienger and Sakari Tervo},
  title = {Acoustic Localisation as an Alternative to Positioning Principles in Applications presented at NIME 2001-2013},
  pages = {439--442},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178933},
  url = {http://www.nime.org/proceedings/2014/nime2014_501.pdf},
  abstract = {This paper provides a rationale for choosing acoustic localisation techniques as an alternative to other principles to provide spatial positions in interactive locative audio applications (ILAA). By comparing positioning technology in existing ILAAs to the expected performance of acoustic positioning systems (APS), we can evaluate if APS would perform equivalently in a particular application. In this paper, the titles of NIME conference proceedings from 2001 to 2013 were searched for presentations on ILAA using positioning technology. Over 80 relevant articles were found. For each of the systems we evaluated if and why APS would be a contender or not. The results showed that for over 73 percent of the reviewed applications, APS could possibly provide competitive alternatives and at very low cost.}
}

@inproceedings{cfaubel12014,
  author = {Christian Faubel},
  title = {Rhythm Apparatus on Overhead},
  pages = {491--494},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1180950},
  url = {http://www.nime.org/proceedings/2014/nime2014_503.pdf},
  abstract = {In the paper I present a robotic device that offers new ways of interaction for producing rhythmic patterns. The apparatus is placed on an overhead projector and a visual presentation of these rhythmic patterns is delivered as a shadow play. The rhythmic patterns can be manipulated by modifying the environment of the robot, through direct physical interaction with the robot, by rewiring the internal connectivity, and by adjusting internal parameters. The theory of embodied cognition provides the theoretical basis of this device. The core postulate of embodied cognition is that biological behavior can only be understood through an understanding of the real-time interactions of an organism's nervous system, the organism's body and the environment. One the one hand the device illustrates this theory because the patterns that are created equally depend on the real-time interactions of the electronics, the physical structure of the device and the environment. On the other hand the device presents a synthesis of these ideas and it is effectively possible to play with it at all the three levels, the electronics, the physical configuration of the robot and the environment.}
}

@inproceedings{ahazzard2014,
  author = {Adrian Hazzard and Steve Benford and Gary Burnett},
  title = {You'll Never Walk Alone: Composing Location-Based Soundtracks},
  pages = {411--414},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178794},
  url = {http://www.nime.org/proceedings/2014/nime2014_506.pdf},
  abstract = {Music plays a vital role in accompanying all manner of our experiences. Soundtracks within films, video games and ceremonies possess a unique ability to enhance a narrative, suggest emotional content and mark key transitions. Moreover, soundtracks often achieve all of this without being the primary focus, on the contrary they typically assume a supporting role. The proliferation of mobile devices increasingly leads us to listen to music while on the move and musicians are seizing on locative technologies as a tool for creating new kinds of music that directly respond to people's movements through space. In light of these trends, we consider the interesting question of how composers might set about creating musical soundtracks to accompany mobile experiences. What we have in mind are experiences such as guided walks, tours and even pervasive games. The novelty of our research here is in the music serving as an accompaniment to enhance a location specific activity, much as a soundtrack does for a film. This calls for composers to take into account the key features of the experience, and its setting, to gently complement them through the music. We examine this process from a composer's perspective by presenting `from the field' an account of how they address the multifaceted challenges of designing a soundtrack for public sculpture park. We chart a composer's rationale as they developed a soundtrack for this site over multiple iterations of design, testing and refinement. We expose key relationships between the raw materials of music (melody, harmony, timbre, rhythm and dynamics) and those of the physical setting, that enable the composer to gracefully mesh the music into the fabric of the space. The result is to propose a set of recommendations to inform the composition of mobile soundtracks that we intend to guide future practice and research.}
}

@inproceedings{kyerkes2014,
  author = {Karl Yerkes and Matthew Wright},
  title = {Twkyr: a Multitouch Waveform Looper},
  pages = {375--378},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178989},
  url = {http://www.nime.org/proceedings/2014/nime2014_508.pdf},
  abstract = {Twkyr is a new interface for musical expression that emphasizes realtime manipulation, audification, and visualization of waveforms with a multitouch surface, offering different interactivity at different time scales, within the same waveform. The interactive audiovisual design of Tweakyr is motivated by the need for increased parsimony and transparency in electronic musical instruments and draws from the work of Curtis Roads on time scales as qualitative musical parameters, and Edward Tufte's ``data-ink'' principles for the improvement of data graphics.}
}

@inproceedings{tmays2014,
  author = {Tom Mays and Francis Faber},
  title = {A Notation System for the Karlax Controller},
  pages = {553--556},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178869},
  url = {http://www.nime.org/proceedings/2014/nime2014_509.pdf},
  abstract = {In this paper we expose the need to go beyond the composer/performer model of electronic instrument design and programming to encourage the transmission of compositions and the creation of a repertory via notation of repeatable performance practice. Drawing on 4 years of practice using the Karlax controller (Da Fact) as a base for new digital musical instruments, we present our notation system in detail and cite some mapping strategies and examples from to pieces in a growing repertory of chamber music compositions for electronic and acoustic instruments}
}

@inproceedings{avanzandt2014,
  author = {Alejandro Van Zandt-Escobar and Baptiste Caramiaux and Atau Tanaka},
  title = {PiaF: A Tool for Augmented Piano Performance Using Gesture Variation Following},
  pages = {167--170},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178991},
  url = {http://www.nime.org/proceedings/2014/nime2014_511.pdf},
  abstract = {When performing a piece, a pianist's interpretation is communicated both through the sound produced and through body gestures. We present PiaF (Piano Follower), a prototype for augmenting piano performance by measuring gesture variations. We survey other augmented piano projects, several of which focus on gestural recognition, and present our prototype which uses machine learning techniques for gesture classification and estimation of gesture variations in real-time. Our implementation uses the Kinect depth sensor to track body motion in space, which is used as input data. During an initial learning phase, the system is taught a set of reference gestures, or templates. During performance, the live gesture is classified in real-time, and variations with respect to the recognized template are computed. These values can then be mapped to audio processing parameters, to control digital effects which are applied to the acoustic output of the piano in real-time. We discuss initial tests using PiaF with a pianist, as well as potential applications beyond live performance, including pedagogy and embodiment of recorded performance.}
}

@inproceedings{pdahlstedt12014,
  author = {Palle Dahlstedt and Patrik Karlsson and Katarina Widell and Tony Blomdahl},
  title = {YouHero Making an Expressive Concert Instrument from the GuitarHero Controller},
  pages = {403--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178742},
  url = {http://www.nime.org/proceedings/2014/nime2014_513.pdf},
  abstract = {The idea behind the YouHero was two-fold. First, to make an expressive instrument out of the computer game toy guitar controller from the famous game GuitarHero. With its limited amount of control parameters, this was a challenge. Second, through this instrument we wanted to provide an alternative to the view that you become a hero by perfect imitation of your idols. Instead, play yourself. You are the hero. In this paper, we describe the design of the instrument, including its novel mapping approach based on switched timbre vectors scaled by accellerometer data, unconventional sound engines and the sound and mapping editing features, including manual editing of individual vectors. The instrument is evaluated through its practical applications during the whole project, with workshops with teenagers, a set of state-funded commissions from professional composers, and the development of considerable skill by the key performers. We have also submitted a performance proposal for this project.}
}

@inproceedings{ldahl2014,
  author = {Luke Dahl},
  title = {Triggering Sounds from Discrete Air Gestures: What Movement Feature Has the Best Timing?},
  pages = {201--206},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178738},
  url = {http://www.nime.org/proceedings/2014/nime2014_514.pdf},
  abstract = {Motion sensing technologies enable musical interfaces where a performer moves their body "in the air" without manipulating or contacting a physical object. These interfaces work well when the movement and sound are smooth and continuous, but it has proven difficult to design a system which triggers discrete sounds with precision that allows for complex rhythmic performance. We conducted a study where participants perform ``air-drumming'' gestures in time to rhythmic sounds. These movements are recorded, and the timing of various movement features with respect to the onset of audio events is analyzed. A novel algorithm for detecting sudden changes in direction is used to find the end of the strike gesture. We find that these occur on average after the audio onset and that this timing varies with the tempo of the movement. Sharp peaks in magnitude acceleration occur before the audio onset and do not vary with tempo. These results suggest that detecting peaks in acceleration will lead to more naturally responsive air gesture instruments.}
}

@inproceedings{chonigman2014,
  author = {Colin Honigman and Jordan Hochenbaum and Ajay Kapur},
  title = {Techniques in Swept Frequency Capacitive Sensing: An Open Source Approach},
  pages = {74--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178802},
  url = {http://www.nime.org/proceedings/2014/nime2014_515.pdf},
  abstract = {This paper introduces a new technique for creating Swept Frequency Capacitive Sensing with open source technology for use in creating richer and more complex musical gestures. This new style of capacitive touch sensing is extremely robust compared to older versions and will allow greater implementation of gesture recognition and touch control in the development of NIMEs. Inspired by the Touch{\'e} project, this paper discusses how to implement this technique using the community standard hardware Arduino instead of custom designed electronics. The technique requires only passive components and can be used to enhance the touch sensitivity of many everyday objects and even biological materials and substances such as plants, which this paper will focus on as a case study through the project known as Cultivating Frequencies. This paper will discuss different techniques of filtering data captured by this system, different methods for creating gesture recognition unique to the object being used, and the implications of this technology as it pertains to the goal of ubiquitous sensing. Furthermore, this paper will introduce a new Arduino Library, SweepingCapSense, which simplifies the coding required to implement this technique.}
}

@inproceedings{hdiao2014,
  author = {Haojing Diao and Yanchao Zhou and Christopher Andrew Harte and Nick Bryan-Kinns},
  title = {Sketch-Based Musical Composition and Performance},
  pages = {569--572},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178748},
  url = {http://www.nime.org/proceedings/2014/nime2014_517.pdf},
  abstract = {Sketching is a natural way for one person to convey their thoughts and intentions to another. With the recent rise of tablet-based computing, the use of sketching as a control and interaction paradigm is one that deserves exploration. In this paper we present an interactive sketch-based music composition and performance system called Drawchestra. The aim of the system is to give users an intuitive way to convey their musical ideas to a computer system with the minimum of technical training thus enabling them to focus on the creative tasks of composition and performance. The system provides the user with a canvas upon which they may create their own instruments by sketching shapes on the tablet screen. The system recognises a certain set of shapes which it treats as virtual instruments or effects. Once recognised, these virtual instruments can then be played by the user in real time. The size of a sketched instrument shape is used to control certain parameters of the sound so the user can build complex orchestras containing many different shapes of different sizes. The sketched shapes may also be moved and resized as desired making it possible to customise and edit the virtual orchestra as the user goes along. The system has been implemented in Python and user tests conducted using an iPad as the control surface. We report the results of the user study at the end of the paper before briefly discussing the outcome and outlining the next steps for the system design.}
}

@inproceedings{jratcliffe2014,
  author = {Jarrod Ratcliffe},
  title = {Hand and Finger Motion-Controlled Audio Mixing Interface},
  pages = {136--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178911},
  url = {http://www.nime.org/proceedings/2014/nime2014_518.pdf},
  abstract = {This paper presents a control surface interface for music mixing using real time computer vision. Two input sensors are considered: the Leap Motion and the Microsoft Kinect. The author presents significant design considerations, including improving of the user's sense of depth and panorama, maintaining broad accessibility by integrating the system with Digital Audio Workstation (DAW) software, and implementing a system that is portable and affordable. To provide the user with a heightened sense of sound spatialization over the traditional channel strip, the concept of depth is addressed directly using the stage metaphor. Sound sources are represented as colored spheres in a graphical user interface to provide the user with visual feedback. Moving sources back and forward controls volume, while left to right controls panning. To provide broader accessibility, the interface is configured to control mixing within the Ableton Live DAW. The author also discusses future plans to expand functionality and evaluate the system.}
}

@inproceedings{cmckinney2014,
  author = {Chad McKinney},
  title = {Quick Live Coding Collaboration In The Web Browser},
  pages = {379--382},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178873},
  url = {http://www.nime.org/proceedings/2014/nime2014_519.pdf},
  abstract = {With the growing adoption of internet connectivity across the world, online collaboration is still a difficult and slow endeavor. Many amazing languages and tools such as SuperCollider, ChucK, and Max/MSP all facilitate networking and collaboration, however these languages and tools were not created explicitly to make group performances simple and intuitive. New web standards such as Web Audio and Web GL introduce the capability for web browsers to duplicate many of the features in computer music tools. This paper introduces Lich.js, an effort to bring musicians together over the internet with minimal effort by leveraging web technologies.}
}

@inproceedings{sknotts2014,
  author = {Shelly Knotts and Nick Collins},
  title = {The Politics of Laptop Ensembles: A Survey of 160 Laptop Ensembles and their Organisational Structures},
  pages = {191--194},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178839},
  url = {http://www.nime.org/proceedings/2014/nime2014_521.pdf},
  abstract = {This paper reports the results of an online survey of 160 laptop ensembles and the relative democracy of their organisational and social structures. For the purposes of this research a laptop ensemble is defined as a performing group of three or more musicians for whom the laptop is the main sound generating source and who typically perform together in the same room. The concept of democracy (i.e. governance by members of the group) has been used as a starting point to assess firstly what types of organisational structures are currently used in laptop ensembles and secondarily to what extent laptop ensembles consider the implications of organisational and social structure on their musical output. To assess this I recorded a number of data points including ensemble size, whether the group has a director or conductor, use of homogenous vs. heterogenous hardware and software, whether they perform composed pieces or mainly improvise, the level of network interaction and whether or not the ensemble has an academic affiliation. The survey allowed me to define a scale of democracy in laptop ensembles and typical features of the most and least democratic groups. Some examples are given of democratic and autocratic activity in existing laptop ensembles. This work is part of a larger scale project investigating the effect of social structures on the musical output of laptop ensembles.}
}

@inproceedings{lfeugere2014,
  author = {Lionel Feug\`ere and Christophe d'Alessandro},
  title = {Rule-Based Performative Synthesis of Sung Syllables},
  pages = {86--87},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178762},
  url = {http://www.nime.org/proceedings/2014/nime2014_522.pdf},
  abstract = {In this demonstration, the mapping and the gestural control strategy developed in the Digitartic are presented. Digitartic is a musical instrument able to control sung syllables. Performative rule-based synthesis allows for controlling semi-consonants, plosive, fricative and nasal consonants with a same gesture, despite the structural differences in natural production of such vocal segments. A graphic pen tablet is used for capturing the gesture with a high sampling rate and resolution. This system alows for both performing various manners of articulation and having a continuous control over the articulation.}
}

@inproceedings{jharriman2014,
  author = {Jiffer Harriman and Michael Theodore and Nikolaus Correll and Hunter Ewen},
  title = {endo/exo Making Art and Music with Distributed Computing},
  pages = {383--386},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178786},
  url = {http://www.nime.org/proceedings/2014/nime2014_523.pdf},
  abstract = {What do new possibilities for music and art making look like in a world in which the biological and mechanical are increasingly entangled? Can a contrived environment envelope the senses to the point that one feel fully immersed in it? It was with these questions in mind that the interactive mechanical sound art installation endo/exo came into being. Through the use of networked technology the system becomes more like a self-aware organism, passing messages from node to node as cells communicate through chemical signals with their neighbors. In an artistic context, the communication network resembles, but differs from, other mechanical systems. Issues such as latency are often considered negative factors, yet they can contribute a touch of personality in this context. This paper is a reflection on these and other considerations gained from the experience of designing and constructing endo/exo as well as future implications for the Honeycomb platform as a tool for creating musical interactions within a new paradigm which allows for emergent behavior across vast physical spaces. The use of swarming and self-organization, as well as playful interaction, creates an ``aliveness'' in the mechanism, and renders its exploration pleasurable, intriguing and uncanny.}
}

@inproceedings{rgraham2014,
  author = {Ricky Graham and Brian Bridges},
  title = {Gesture and Embodied Metaphor in Spatial Music Performance Systems Design.},
  pages = {581--584},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178774},
  url = {http://www.nime.org/proceedings/2014/nime2014_526.pdf},
  abstract = {This paper describes the theoretical underpinnings, design, and development of a hyper--instrumental performance system driven by gestural data obtained from an electric guitar. The system combines a multichannel audio feed from the guitar (which is parsed for its pitch, spectral content and note inter--onset time data to provide abstractions of sounded performance gestures) with motion tracking of the performer's larger--scale bodily movements using a Microsoft Xbox Kinect sensor. These gestural materials are used to provide the basis for the structures of relational mappings, informed by the embodied image schema structures of Lakoff and Johnson. These theoretical perspectives are refined via larger-scale ecological-embodied structural relationships in electroacoustic music outlined in Smalley's theory of spectromorphology, alongside the incorporation of an additional active-agential response structure through the use of the boids flocking algorithm by Reynolds to control the spatialization of outputs and other textural processes. The paper aims to advance a broadly-applicable 'performance gesture ecology', providing a shared spatial-relational mapping (a 'basic gestural space') which allows for creative (but still coherent) mappings from the performance gestures to the control of textural and spatial structures.}
}

@inproceedings{ckiefer2014,
  author = {Chris Kiefer},
  title = {Musical Instrument Mapping Design with Echo State Networks},
  pages = {293--298},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178829},
  url = {http://www.nime.org/proceedings/2014/nime2014_530.pdf},
  abstract = {Echo State Networks (ESNs), a form of recurrent neural network developed in the field of Reservoir Computing, show significant potential for use as a tool in the design of mappings for digital musical instruments. They have, however, seldom been used in this area, so this paper explores their possible uses. This project contributes a new open source library, which was developed to allow ESNs to run in the Pure Data dataflow environment. Several use cases were explored, focusing on addressing current issues in mapping research. ESNs were found to work successfully in scenarios of pattern classification, multiparametric control, explorative mapping and the design of nonlinearities and uncontrol. Un-trained behaviours are proposed, as augmentations to the conventional reservoir system that allow the player to introduce potentially interesting non-linearities and uncontrol into the reservoir. Interactive evolution style controls are proposed as strategies to help design these behaviours, which are otherwise dependent on arbitrary parameters. A study on sound classification shows that ESNs can reliably differentiate between two drum sounds, and also generalise to other similar input. Following evaluation of the use cases, heuristics are proposed to aid the use of ESNs in computer music scenarios.}
}

@inproceedings{jallison2014,
  author = {Bradley Strylowski and Jesse Allison and Jesse Guessford},
  title = {Pitch Canvas: Touchscreen Based Mobile Music Instrument},
  pages = {171--174},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178947},
  url = {http://www.nime.org/proceedings/2014/nime2014_533.pdf},
  abstract = {Mobile music applications are typically quite limiting to musicians, as they either attempt to mimic non-touch screen interfaces or do not offer enough control. Pitch Canvas is a musical interface that was built specifically for the touchscreen. Pitches are laid out in a hexagonal pattern that allow for easy scale, chord, and arpeggiation patterns. Notes are played by touch, but are sustained through continuous movement. Pitch bends can be achieved by passing through the space between the notes. Its current implementation runs only on Apple iPad tablet computers using a libPd to convert user interaction into audio. An iPad overlay offers physical feedback for the circles as well as the pitch bend area between the circles. A performable version of the application has been built, though several active developments allow alternative sonic interpretation of the gestures, enhanced visual response to user interaction, and the ability to control the instrument with multiple devices.}
}

@inproceedings{pdahlstedt2014,
  author = {Palle Dahlstedt},
  title = {Circle Squared and Circle Keys Performing on and with an Unstable Live Algorithm for the Disklavier},
  pages = {114--117},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178740},
  url = {http://www.nime.org/proceedings/2014/nime2014_534.pdf},
  abstract = {Two related versions of an unstable live algorithm for the Disklavier player piano are presented. The underlying generative feedback system consists of four virtual musicians, listening to each other in a circular configuration. There is no temporal form, and all parameters of the system are controlled by the performer through an intricate but direct mapping, in an attempt to combine the experienced musician's physical control of gesture and phrasing, with the structural complexities and richness of generative music. In the first version, Circle Squared, the interface is an array of pressure sensors, and the performer performs on the system without participating directly, like a puppet master. In the second version, control parameters are derived directly from playing on the same piano that performs the output of the system. Here, the performer both plays with and on the system in an intricate dance with the unpredictable output of the unstable virtual ensemble. The underlying mapping strategies are presented, together with the structure of the generative system. Experiences from a series of performances are discussed, primarily from the perspective of the improvising musician.}
}

@inproceedings{fthalmann2014,
  author = {Daniel Tormoen and Florian Thalmann and Guerino Mazzola},
  title = {The Composing Hand: Musical Creation with Leap Motion and the BigBang Rubette},
  pages = {207--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178955},
  url = {http://www.nime.org/proceedings/2014/nime2014_536.pdf},
  abstract = {This paper introduces an extension of the Rubato Composer software's BigBang rubette module for gestural composition. The extension enables composers and improvisers to operate BigBang using the Leap Motion controller, which uses two cameras to detect hand motions in three-dimensional space. The low latency and high precision of the device make it a good fit for BigBang's functionality, which is based on immediate visual and auditive feedback. With the new extensions, users can define an infinite variety of musical objects, such as oscillators, pitches, chord progressions, or frequency modulators, in real-time and transform them in order to generate more complex musical structures on any level of abstraction.}
}

@inproceedings{obown2014,
  author = {Oliver Bown and Renick Bell and Adam Parkinson},
  title = {Examining the Perception of Liveness and Activity in Laptop Music: Listeners' Inference about what the Performer is Doing from the Audio Alone},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178722},
  url = {http://www.nime.org/proceedings/2014/nime2014_538.pdf},
  abstract = {Audiences of live laptop music frequently express dismay at the opacity of performer activity and question how ``live'' performances actually are. Yet motionless laptop performers endure as musical spectacles from clubs to concert halls, suggesting that for many this is a non-issue. Understanding these perceptions might help performers better achieve their intentions, inform interface design within the NIME field and help develop theories of liveness and performance. To this end, a study of listeners' perception of liveness and performer control in laptop performance was carried out, in which listeners examined several short audio-only excerpts of laptop performances and answered questions about their perception of the performance: what they thought was happening and its sense of liveness. Our results suggest that audiences are likely to associate liveness with perceived performer activity such as improvisation and the audibility of gestures, whereas perceptions of generative material, backing tracks, or other preconceived material do not appear to inhibit perceptions of liveness.}
}

@inproceedings{jsnyder12014,
  author = {Jeff Snyder and Danny Ryan},
  title = {The Birl: An Electronic Wind Instrument Based on an Artificial Neural Network Parameter Mapping Structure},
  pages = {585--588},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178939},
  url = {http://www.nime.org/proceedings/2014/nime2014_540.pdf},
  abstract = {This paper discusses the Birl, an electronic wind instrument developed by the authors. It uses artificial neural nets to apply machine learning to the mapping of fingering systems and embouchure position. The design features of the instrument are described, and the machine learning mapping strategy is discussed.}
}

@inproceedings{ahindle12014,
  author = {Abram Hindle},
  title = {CloudOrch: A Portable SoundCard in the Cloud},
  pages = {277--280},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178798},
  url = {http://www.nime.org/proceedings/2014/nime2014_541.pdf},
  abstract = {One problem with live computer music performance is the transport of computers to a venue and the following setup of the computers used in playing and rendering music. The more computers involved the longer the setup and tear-down of a performance. Each computer adds power and cabling requirements that the venue must accommodate. Cloud computing can change of all this by simplifying the setup of many (10s, 100s) of machines at the click of a button. But there's a catch, the cloud is not physically near you, you cannot run an audio cable to the cloud. The audio from a computer music instrument in the cloud needs to streamed back to the performer and listeners. There are many solutions for streaming audio over networks and the internet, most of them suffer from high latency, heavy buffering, or proprietary/non-portable clients. In this paper we propose a portable cloud-friendly method of streaming, almost a cloud soundcard, whereby performers can use mobile devices (Android, iOS, laptops) to stream audio from the cloud with far lower latency than technologies like icecast. This technology enables near-realtime control over power computer music networks enabling performers to travel light and perform live with more computers than ever before.}
}

@inproceedings{jsnyder2014,
  author = {Jeff Snyder and Avneesh Sarwate},
  title = {Mobile Device Percussion Parade},
  pages = {147--150},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178941},
  url = {http://www.nime.org/proceedings/2014/nime2014_542.pdf},
  abstract = {In this paper, we present the ``Mobile Marching Band'' (MMB) as a new mode of musical performance with mobile computing devices. We define an MMB to be, at its most general, any ensemble utilizing mobile computation that can travel as it performs, with the performance being independent of its location. We will discuss the affordances and limitations of mobile-based instrument design and performance, specifically within the context of a ``moving'' ensemble. We will also discuss the use of a Mobile Marching Band as an educational tool. Finally, we will explore our implementation of a Mobile Parade, a digital Brazilian samba ensemble.}
}

@inproceedings{dvannort2014,
  author = {Navid Navab and Doug Van Nort and Sha Xin Wei},
  title = {A Material Computation Perspective on Audio Mosaicing and Gestural Conditioning},
  pages = {387--390},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178893},
  url = {http://www.nime.org/proceedings/2014/nime2014_544.pdf},
  abstract = {This paper discusses an approach to instrument conception that is based on a careful consideration of the coupling of tactile and sonic gestural action both into and out of the performance system. To this end we propose a design approach that not only considers the materiality of the instrument, but that leverages it as a central part of the conception of the sonic quality, the control structuring and what generally falls under the umbrella of "mapping" design. As we will discuss, this extended computational matter-centric view is of benefit towards holistically understanding an ``instrument'' gestural engagement, as it is realized through physical material, sonic gestural matter and felt human engagement. We present instrumental systems that have arisen as a result of this approach to instrument design.}
}

@inproceedings{slee12014,
  author = {Sang Won Lee and Georg Essl and Z. Morley Mao},
  title = {Distributing Mobile Music Applications for Audience Participation Using Mobile Ad-hoc Network ({MANET})},
  pages = {533--536},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178849},
  url = {http://www.nime.org/proceedings/2014/nime2014_546.pdf},
  abstract = {This work introduces a way to distribute mobile applications using mobile ad-hoc network in the context of audience participation. The goal is to minimize user configuration so that the process is highly accessible for casual smartphone users. The prototype mobile applications utilize WiFiDirect and Service Discovery Protocol to distribute code. With the aid of these two technologies, the prototype system requires no infrastructure and minimum user configuration.}
}

@inproceedings{jherrera2014,
  author = {Hyung Suk Kim and Jorge Herrera and Ge Wang},
  title = {Ping-Pong: Musically Discovering Locations},
  pages = {273--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178831},
  url = {http://www.nime.org/proceedings/2014/nime2014_550.pdf},
  abstract = {A recently developed system that uses pitched sounds to discover relative 3D positions of a group of devices located in the same physical space is described. The measurements are coordinated over an IP network in a decentralized manner, while the actual measurements are carried out measuring the time-of-flight of the notes played by different devices. Approaches to sonify the discovery process are discussed. A specific instantiation of the system is described in detail. The melody is specified in the form of a score, available to every device in the network. The system performs the melody by playing different notes consecutively on different devices, keeping a consistent timing, while carrying out the inter-device measurements necessary to discover the geometrical configuration of the devices in the physical space.}
}

@inproceedings{eberdahl2014,
  author = {Edgar Berdahl},
  title = {How to Make Embedded Acoustic Instruments},
  pages = {140--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178710},
  url = {http://www.nime.org/proceedings/2014/nime2014_551.pdf},
  abstract = {An embedded acoustic instrument is an embedded musical instrument that provides a direct acoustic output. This paper describes how to make embedded acoustic instruments using laser cutting for digital fabrication. Several tips are given for improving the acoustic quality including: employing maximally stiff material, placing loudspeaker drivers in the corners of enclosure faces, increasing the stiffness of ``loudspeaker'' faces by doubling their thickness, choosing side-lengths with non-integer ratios, and incorporating bracing. Various versions of an open design of the ``LapBox'' are provided to help community members replicate and extend the work. A procedure is suggested for testing and optimizing the acoustic quality.}
}

@inproceedings{cdominguez2014,
  author = {Carlos Dominguez},
  title = {16-{CdS}: A Surface Controller for the Simultaneous Manipulation of Multiple Analog Components},
  pages = {78--79},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178750},
  url = {http://www.nime.org/proceedings/2014/nime2014_552.pdf},
  abstract = {This paper presents a project that discusses a brief history of artistic systems that use photoresistors (light-dependent resistors) and results in the construction of an interface and performance controller. The controller combines an Arduino microcontroller with a grid of photoresistors set into a slab of wood covered with a thin acrylic sheet. A brief background on past uses of these components for music and film composition and instrument-building introduces a few different implementations and performance contexts for the controller. Topics such as implementation, construction, and performance possibilities (including electroacoustic and audio-visual performance) of the controller are also discussed.}
}

@inproceedings{slee2014,
  author = {Sang Won Lee and Georg Essl},
  title = {Communication, Control, and State Sharing in Collaborative Live Coding},
  pages = {263--268},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178847},
  url = {http://www.nime.org/proceedings/2014/nime2014_554.pdf},
  abstract = {In the setting of collaborative live coding, a number of issues emerge: (1) need for communication, (2) issues of conflicts in sharing program state space, and (3) remote control of code execution. In this paper, we propose solutions to these problems. In the recent extension of UrMus, a programming environment for mobile music application development, we introduce a paradigm of shared and individual namespaces safeguard against conflicts in parallel coding activities. We also develop live variable view that communicates live changes in state among live coders, networked performers, and the audience. Lastly, we integrate collaborative aspects of programming execution into built-in live chat, which enables not only communication with others, but also distributed execution of code.}
}

@inproceedings{rcollecchia2014,
  author = {Regina Collecchia and Dan Somen and Kevin McElroy},
  title = {The Siren Organ},
  pages = {391--394},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178732},
  url = {http://www.nime.org/proceedings/2014/nime2014_558.pdf},
  abstract = {Sirens evoke images of alarm, public service, war, and forthcoming air raid. Outside of the music of Edgard Varese, sirens have rarely been framed as musical instruments. By connecting air hoses to spinning disks with evenly-spaced perforations, the siren timbre is translated musically. Polyphony gives our instrument an organ-like personality: keys are mapped to different frequencies and the pressure applied to them determines volume. The siren organ can produce a large range of sounds both timbrally and dynamically. In addition to a siren timbre, the instrument produces similar sounds to a harmonica. Portability, robustness, and electronic stability are all areas for improvement.}
}

@inproceedings{drector2014,
  author = {David Rector and Spencer Topel},
  title = {Internally Actuated Drums for Expressive Performance},
  pages = {395--398},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178913},
  url = {http://www.nime.org/proceedings/2014/nime2014_559.pdf},
  abstract = {Actuated instruments is a growing area of activity for research and composition, yet there has been little focus on membrane-based instruments. This paper describes a novel design for an internally actuated drum based on the mechanical principles of a loudspeaker. Implementation is described in detail; in particular, two modes of actuation, a moving-coil electromagnet and a moving-magnet design, are described. We evaluate the drum using a synthesized frequency sweep, and find that the instrument has a broad frequency response and exhibits qualities of both a drum and speaker.}
}

@inproceedings{ssalazar2014,
  author = {Spencer Salazar and Ge Wang},
  title = {Auraglyph: Handwritten Computer Music Composition and Design},
  pages = {106--109},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178927},
  url = {http://www.nime.org/proceedings/2014/nime2014_560.pdf},
  abstract = {Effective software interaction design must consider all of the capabilities and limitations of the platform for which it is developed. To this end, we propose a new model for computer music system design on touchscreen devices, combining both pen/stylus input and multitouch gestures. Such a model surpasses the barrier of touchscreen-based keyboard input, preserving the primary interaction of touch and direct manipulation throughout the development of a complex musical program. We have implemented an iPad software application utilizing these principles, called ``Auraglyph.'' Auraglyph offers a number of fundamental audio processing and control operators, as well as facilities for structured input and output. All of these software objects are created, parameterized, and interconnected via stylus and touch input. Underlying this application is an advanced handwriting recognition framework, LipiTk, which can be trained to recognize both alphanumeric characters and arbitrary figures, shapes, and patterns.}
}

@inproceedings{ahornof2014,
  author = {Anthony Hornof},
  title = {The Prospects For Eye-Controlled Musical Performance},
  pages = {461--466},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178804},
  url = {http://www.nime.org/proceedings/2014/nime2014_562.pdf},
  abstract = {Although new sensor devices and data streams are increasingly used for musical expression, and although eye-tracking devices have become increasingly cost-effective and prevalent in research and as a means of communication for people with severe motor impairments, eye-controlled musical expression nonetheless remains somewhat elusive and minimally explored. This paper (a) identifies a number of fundamental human eye movement capabilities and constraints which determine in part what can and cannot be musically expressed with eye movements, (b) reviews prior work on eye-controlled musical expression, and (c) analyzes and provides a taxonomy of what has been done, and what will need to be addressed in future eye-controlled musical instruments. The fundamental human constraints and processes that govern eye movements create a challenge for eye-controlled music in that the instrument needs to be designed to motivate or at least permit specific unique visual goals, each of which when accomplished must then be mapped, using the eye tracker and some sort of sound generator, to different musical outcomes. The control of the musical instrument is less direct than if it were played with muscles that can be controlled in a more direct manner, such as the muscles in the hands.}
}

@inproceedings{aplace2014,
  author = {Adam Place and Liam Lacey and Thomas Mitchell},
  title = {AlphaSphere from Prototype to Product},
  pages = {399--402},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178903},
  url = {http://www.nime.org/proceedings/2014/nime2014_568.pdf},
  abstract = {This paper explores the design process of the AlphaSphere, an experimental new musical instrument that has transitioned into scale production and international distribution. Initially, the design intentions and engineering processes are covered. The paper continues by briefly evaluating the user testing process and outlining the ergonomics, communication protocol and software of the device. The paper closes by questioning what it takes to evaluate success as a musical instrument.}
}

@inproceedings{aandersson2014,
  author = {Anders-Petter Andersson and Birgitta Cappelen and Fredrik Olofsson},
  title = {Designing Sound for Recreation and Well-Being},
  pages = {529--532},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2014},
  month = {June},
  publisher = {Goldsmiths, University of London},
  address = {London, United Kingdom},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178702},
  url = {http://www.nime.org/proceedings/2014/nime2014_572.pdf},
  abstract = {In this paper we explore how we compose sound for an interactive tangible and mobile interface; where the goal is to improve health and well-being for families with children with disabilities. We describe the composition process from how we decompose a linear beat-based and vocal sound material; recompose it with real-time audio synthesis and composition rules into interactive Scenes. Scenes that make it possible for the user to select, explore and recreate different ``sound worlds'' with the tangible interface as an instrument; create and play with it as a friend; improvise and create; or relax with it as an ambient sounding furniture. We continue discussing a user story, how the Scenes are recreated by amateur users, persons with severe disabilities and family members; improvising with the mobile tangibles. We discuss composition techniques for mixing sound, tangible-physical and lighting elements in the Scenes. Based on observations we explore how a diverse audience in the family and at school can recreate and improvise their own sound experience and play together with others. We conclude by discussing the possible impact of our findings for the NIME-community; how the techniques of decomposing, recomposing and recreating sound, based on a relational perspective, could contribute to the design of new instruments for musical expression.}
}

