@inproceedings{Buchla2005,
  author = {Buchla, Don},
  title = {A History of Buchla's Musical Instruments},
  pages = {1--1},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176715},
  url = {http://www.nime.org/proceedings/2005/nime2005_001.pdf}
}

@inproceedings{Levin2005,
  author = {Levin, Golan},
  title = {A Personal Chronology of Audiovisual Systems Research},
  pages = {2--3},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176770},
  url = {http://www.nime.org/proceedings/2005/nime2005_002.pdf}
}

@inproceedings{Buxton2005,
  author = {Buxton, Bill},
  title = {Causality and Striking the Right Note},
  pages = {4--4},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176717},
  url = {http://www.nime.org/proceedings/2005/nime2005_004.pdf}
}

@inproceedings{Bowers2005,
  author = {Bowers, John and Archer, Phil},
  title = {Not Hyper, Not Meta, Not Cyber but Infra-Instruments},
  pages = {5--10},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176713},
  url = {http://www.nime.org/proceedings/2005/nime2005_005.pdf},
  keywords = {Infra-instruments, hyperinstruments, meta-instruments, virtual instruments, design concepts and principles. },
  abstract = {As a response to a number of notable contemporary aesthetic tendencies, this paper introduces the notion of an infra-instrument as a kind of ‘new interface for musical expression’ worthy of study and systematic design. In contrast to hyper-, meta- and virtual instruments, we propose infra-instruments as devices of restricted interactive potential, with little sensor enhancement, which engender simple musics with scarce opportunity for conventional virtuosity. After presenting numerous examples from our work, we argue that it is precisely such interactionally and sonically challenged designs that leave requisite space for computer-generated augmentations in hybrid, multi-device performance settings.}
}

@inproceedings{Maki-patola2005,
  author = {M\''{a}ki-patola, Teemu and Laitinen, Juha and Kanerva, Aki and Takala, Tapio},
  title = {Experiments with Virtual Reality Instruments},
  pages = {11--16},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176780},
  url = {http://www.nime.org/proceedings/2005/nime2005_011.pdf},
  keywords = {Musical instrument design, virtual instrument, gesture, widgets, physical sound modeling, control mapping.},
  abstract = {In this paper, we introduce and analyze four gesture-controlled musical instruments. We briefly discuss the test platform designed to allow for rapid experimentation of new interfaces and control mappings. We describe our design experiences and discuss the effects of system features such as latency, resolution and lack of tactile feedback. The instruments use virtual reality hardware and computer vision for user input, and three-dimensional stereo vision as well as simple desktop displays for providing visual feedback. The instrument sounds are synthesized in real-time using physical sound modeling. }
}

@inproceedings{Weinberg2005,
  author = {Weinberg, Gil and Driscoll, Scott},
  title = {iltur -- Connecting Novices and Experts Through Collaborative Improvisation},
  pages = {17--22},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176840},
  url = {http://www.nime.org/proceedings/2005/nime2005_017.pdf},
  keywords = {Collaboration, improvisation, gestrual handheld controllers, novices, mapping},
  abstract = {The iltur system features a novel method of interaction between expert and novice musicians through a set of musical controllers called Beatbugs. Beatbug players can record live musical input from MIDI and acoustic instruments and respond by transforming the recorded material in real-time, creating motif-and-variation call-and-response routines on the fly. A central computer system analyzes MIDI and audio played by expert players and allows novice Beatbug players to personalize the analyzed material using a variety of transformation algorithms. This paper presents the motivation for developing the iltur system, followed by a brief survey of pervious and related work that guided the definition of the project’s goals. We then present the hardware and software approaches that were taken to address these goals, as well as a couple of compositions that were written for the system. The paper ends with a discussion based on observations of players using the iltur system and a number of suggestions for future work.}
}

@inproceedings{Jorda2005,
  author = {Jord\`{a}, Sergi},
  title = {Multi-user Instruments: Models, Examples and Promises},
  pages = {23--26},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176760},
  url = {http://www.nime.org/proceedings/2005/nime2005_023.pdf},
  keywords = {Multi-user instruments, collaborative music, new instruments design guidelines. },
  abstract = {In this paper we study the potential and the challenges posed by multi-user instruments, as tools that can facilitate interaction and responsiveness not only between performers and their instrument but also between performers as well. Several previous studies and taxonomies are mentioned, after what different paradigms exposed with examples based on traditional mechanical acoustic instruments. In the final part, several existing systems and implementations, now in the digital domain, are described and identified according to the models and paradigms previously introduced. }
}

@inproceedings{Blaine2005,
  author = {Blaine, Tina},
  title = {The Convergence of Alternate Controllers and Musical Interfaces in Interactive Entertainment},
  pages = {27--33},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176709},
  url = {http://www.nime.org/proceedings/2005/nime2005_027.pdf},
  keywords = {Alternate controllers, musical interaction, interactive entertainment, video game industry, arcades, rhythm action, collaborative gameplay, musical performance games},
  abstract = {This paper will investigate a variety of alternate controllers that are making an impact in interactive entertainment, particularly in the video game industry. Since the late 1990's, the surging popularity of rhythmic and musical performance games in Japanese arcades has led to the development of new interfaces and alternate controllers for the consumer market worldwide. Rhythm action games such as Dance Dance Revolution, Taiko No Tatsujin (Taiko: Drum Master), and Donkey Konga are stimulating collaborative gameplay and exposing consumers to custom controllers designed specifically for musical and physical interaction. We are witnessing the emergence and acceptance of these breakthrough controllers and models for gameplay as an international cultural phenomenon penetrating the video game and toy markets in record numbers. Therefore, it is worth considering the potential benefits to developers of musical interfaces, electronic devices and alternate controllers in light of these new and emerging opportunities, particularly in the realm of video gaming, toy development, arcades, and other interactive entertainment experiences. }
}

@inproceedings{Overholt2005,
  author = {Overholt, Dan},
  title = {The Overtone Violin},
  pages = {34--37},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176796},
  url = {http://www.nime.org/proceedings/2005/nime2005_034.pdf}
}

@inproceedings{Caceres2005,
  author = {C\'{a}ceres, Juan Pablo and Mysore, Gautham J. and Trevi\~{n}o, Jeffrey},
  title = {{SC}UBA: The Self-Contained Unified Bass Augmenter},
  pages = {38--41},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176719},
  url = {http://www.nime.org/proceedings/2005/nime2005_038.pdf},
  keywords = {Interactive music, electro-acoustic musical instruments, musical instrument design, human computer interface, signal processing, Open Sound Control (OSC) },
  abstract = {The Self-Contained Unified Bass Augmenter (SCUBA) is a new augmentative OSC (Open Sound Control) [5] controller for the tuba. SCUBA adds new expressive possibilities to the existing tuba interface through onboard sensors. These sensors provide continuous and discrete user-controlled parametric data to be mapped at will to signal processing parameters, virtual instrument control parameters, sound playback, and various other functions. In its current manifestation, control data is mapped to change the processing of the instrument's natural sound in Pd (Pure Data) [3]. SCUBA preserves the unity of the solo instrument interface by acoustically mixing direct and processed sound in the instrument's bell via mounted satellite speakers, which are driven by a subwoofer below the performer's chair. The end result augments the existing interface while preserving its original unity and functionality. }
}

@inproceedings{Sinyor2005,
  author = {Sinyor, Elliot and Wanderley, Marcelo M.},
  title = {Gyrotyre : A dynamic hand-held computer-music controller based on a spinning wheel},
  pages = {42--45},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176820},
  url = {http://www.nime.org/proceedings/2005/nime2005_042.pdf},
  keywords = {HCI, Digital Musical Instruments, Gyroscopic Precession, Rotational Inertia, Open Sound Control },
  abstract = {This paper presents a novel controller built to exploit thephysical behaviour of a simple dynamical system, namely aspinning wheel. The phenomenon of gyroscopic precessioncauses the instrument to slowly oscillate when it is spunquickly, providing the performer with proprioceptive feedback. Also, due to the mass of the wheel and tire and theresulting rotational inertia, it maintains a relatively constant angular velocity once it is set in motion. Various sensors were used to measure continuous and discrete quantitiessuch as the the angular frequency of the wheel, its spatialorientation, and the performer's finger pressure. In addition, optical and hall-effect sensors detect the passing of aspoke-mounted photodiode and two magnets. A base software layer was developed in Max/MSP and various patcheswere written with the goal of mapping the dynamic behaviorof the wheel to varied musical processes.}
}

@inproceedings{Fraietta2005a,
  author = {Fraietta, Angelo},
  title = {The Smart Controller Workbench},
  pages = {46--49},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176745},
  url = {http://www.nime.org/proceedings/2005/nime2005_046.pdf},
  keywords = {Control Voltage, Open Sound Control, Algorithmic Composition, MIDI, Sound Installations, programmable logic control, synthesizers, electronic music, Sensors, Actuators, Interaction. },
  abstract = {The Smart Controller is a portable hardware device that responds to input control voltage, OSC, and MIDI messages; producing output control voltage, OSC, and MIDI messages (depending upon the loaded custom patch). The Smart Controller is a stand alone device; a powerful, reliable, and compact instrument capable of reducing the number of electronic modules required in a live performance or installation, particularly the requirement of a laptop computer. More powerful, however, is the Smart Controller Workbench, a complete interactive development environment. In addition to enabling the composer to create and debug their patches, the Smart Controller Workbench accurately simulates the behaviour of the hardware, and functions as an incircuit debugger that enables the performer to remotely monitor, modify, and tune patches running in an installation without the requirement of stopping or interrupting the live performance. }
}

@inproceedings{Singer2005,
  author = {Singer, Eric and Feddersen, Jeff and Bowen, Bil},
  title = {A Large-Scale Networked Robotic Musical Instrument Installation},
  pages = {50--55},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176818},
  url = {http://www.nime.org/proceedings/2005/nime2005_050.pdf},
  keywords = {Robotics, music, instruments, MIDI, video, interactive, networked, streaming.},
  abstract = {This paper describes an installation created by LEMUR(League of Electronic Musical Urban Robots) in January, 2005.The installation included over 30 robotic musical instrumentsand a multi-projector real-time video projection and wascontrollable and programmable over a MIDI network. Theinstallation was also controllable remotely via the Internet andcould be heard and viewed via room mics and a robotic webcam connected to a streaming server.}
}

@inproceedings{Allison2005,
  author = {Allison, Jesse T. and Place, Timothy},
  title = {Teabox: A Sensor Data Interface System},
  pages = {56--59},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176693},
  url = {http://www.nime.org/proceedings/2005/nime2005_056.pdf},
  keywords = {Teabox, Electrotap, Sensor Interface, High Speed, High Resolution, Sensors, S/PDIF},
  abstract = {Artists have long sought after alternative controllers, sensors, and other means for controlling computer-based musical performance in real-time. Traditional techniques for transmitting the data generated by such devices typically employ the use of MIDI as the transport protocol. Recently, several devices have been developed using alternatives to MIDI, including Ethernet-based and USB-based sensor interfaces. We have designed and produced a system that uses S/PDIF as the transport mechanism for a sensor interface. This provides robust performance, together with extremely low latency and high resolution. In our system, data from all sensors is multiplexed onto the digital audio line and demultiplexed in software on the computer using standard techniques. We have written demultiplexer objects and plugins for Max/MSP and Jade, as well as a MIDI Conversion program for interapplicaton uses, while others are in the works for PD, SuperCollider, and AudioUnits.}
}

@inproceedings{Oore2005,
  author = {Oore, Sageev},
  title = {Learning Advanced Skills on New Instruments},
  pages = {60--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176794},
  url = {http://www.nime.org/proceedings/2005/nime2005_060.pdf},
  keywords = {performance, learning new instruments },
  abstract = {When learning a classical instrument, people often either take lessons in which an existing body of “technique” is de- livered, evolved over generations of performers, or in some cases people will “teach themselves” by watching people play and listening to existing recordings. What does one do with a complex new digital instrument? In this paper I address this question drawing on my expe- rience in learning several very different types of sophisticated instruments: the Glove Talk II real-time gesture-to-speech interface, the Digital Marionette controller for virtual 3D puppets, and pianos and keyboards. As the primary user of the first two systems, I have spent hundreds of hours with Digital Marionette and Glove-Talk II, and thousands of hours with pianos and keyboards (I continue to work as a professional musician). I will identify some of the under- lying principles and approaches that I have observed during my learning and playing experience common to these instru- ments. While typical accounts of users learning new inter- faces generally focus on reporting beginner’s experiences, for various practical reasons, this is fundamentally different by focusing on the expert’s learning experience.}
}

@inproceedings{Livingstone2005,
  author = {Livingstone, Dan and Miranda, Eduardo},
  title = {Orb3 -- Adaptive Interface Design for Real time Sound Synthesis \& Diffusion within Socially Mediated Spaces},
  pages = {65--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176774},
  url = {http://www.nime.org/proceedings/2005/nime2005_065.pdf},
  keywords = {Adaptive System, Sound Installation, Smart Interfaces, Music Robots, Spatial Music, Conscious Subconscious Interaction.},
  abstract = {Haptic and Gestural interfaces offer new and novel ways of interacting with and creating new musical forms. Increasingly it is the integration of these interfaces with more complex adaptive systems or dynamically variable social contexts that provide significant opportunities for socially mediated composition through conscious and subconscious interaction. This paper includes a brief comparative survey of related works and articulates the design process and interaction modes or ‘play states’ for the Orb3 interface – 3 wireless mobile globes that collect and share environmental data and user interactions to synthesize and diffuse sound material in real time, a ‘social’ group of composer and listener objects. The physical interfaces are integrated into a portable 8 channel auditory sphere for collaborative interaction but can also be integrated with large-scale social environments, such as atria and other public spaces with embedded sound systems.}
}

@inproceedings{Essl2005,
  author = {Essl, Georg and O'Modhrain, Sile},
  title = {Scrubber: An Interface for Friction-induced Sounds},
  pages = {70--75},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176737},
  url = {http://www.nime.org/proceedings/2005/nime2005_070.pdf},
  abstract = {The Scrubber is a general controller for friction-induced sound. Allowing the user to engage in familiar gestures and feel- ing actual friction, the synthesized sound gains an evocative nature for the performer and a meaningful relationship between gesture and sound for the audience. It can control a variety of sound synthesis algorithms of which we demonstrate examples based on granular synthesis, wave-table synthesis and physically informed modeling.} 
}

@inproceedings{Topper2005,
  author = {Topper, David and Swendsen, Peter V.},
  title = {Wireless Dance Control : PAIR and WISEAR},
  pages = {76--79},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176830},
  url = {http://www.nime.org/proceedings/2005/nime2005_076.pdf},
  abstract = {WISEAR (Wireless Sensor Array) is a Linux based Embeddedx86 TS-5600 SBC (Single Board Computer) specifically configured for use with music, dance and video performance technologies. The device offers a general purpose solution to many sensor and gestural controller problems. Much like the general purpose CPU, which resolved many issues of its predecessor (ie., the special purpose DSP chip), the WISEAR box attempts to move beyond custom made BASIC stamp projects that are often created on a per-performance basis and rely heavily on MIDI. WISEAR is both lightweight and wireless. Unlike several commercial alternatives, it is also a completely open source project.  PAIR (Partnering Analysis in Real Time) exploits the power of WISEAR and revisits the potential of hardware-based systems for real-time measurement of bodily movement. Our goal was to create a robust yet adaptable system that could attend to both general and precise aspects of performer interaction. Though certain commonalities with existing hardware systems exist, our PAIR system takes a fundamentally different approach by focusing specifically on the interaction of two or more dancers.}
}

@inproceedings{Dannenberg2005,
  author = {Dannenberg, Roger B. and Brown, Ben and Zeglin, Garth and Lupish, Ron},
  title = {McBlare: A Robotic Bagpipe Player},
  pages = {80--84},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176729},
  url = {http://www.nime.org/proceedings/2005/nime2005_080.pdf},
  keywords = {bagpipes, robot, music, instrument, MIDI },
  abstract = {McBlare is a robotic bagpipe player developed by the Robotics Institute at Carnegie Mellon University. McBlare plays a standard set of bagpipes, using a custom air compressor to supply air and electromechanical ``fingers'' to control the chanter. McBlare is MIDI controlled, allowing for simple interfacing to a keyboard, computer, or hardware sequencer. The control mechanism exceeds the measured speed of expert human performers. On the other hand, human performers surpass McBlare in their ability to compensate for limitations and imperfections in reeds, and we discuss future enhancements to address these problems. McBlare has been used to perform traditional bagpipe music as well as experimental computer generated music. }
}

@inproceedings{Bevilacqua2005,
  author = {Bevilacqua, Fr\'{e}d\'{e}ric and M\''{u}ller, R\'{e}my and Schnell, Norbert},
  title = {MnM: a Max/MSP mapping toolbox},
  pages = {85--88},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176703},
  url = {http://www.nime.org/proceedings/2005/nime2005_085.pdf},
  keywords = {Mapping, interface design, matrix, Max/MSP. },
  abstract = {In this report, we describe our development on the Max/MSPtoolbox MnM dedicated to mapping between gesture andsound, and more generally to statistical and machine learningmethods. This library is built on top of the FTM library, whichenables the efficient use of matrices and other data structuresin Max/MSP. Mapping examples are described based onvarious matrix manipulations such as Single ValueDecomposition. The FTM and MnM libraries are freelyavailable.}
}

@inproceedings{Pelletier2005,
  author = {Pelletier, Jean-Marc},
  title = {A Graphical Interface for Real-Time Signal Routing},
  pages = {89--92},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176800},
  url = {http://www.nime.org/proceedings/2005/nime2005_089.pdf},
  keywords = {Graphical user interface, real-time performance, map, dynamic routing },
  abstract = {This paper describes DspMap, a graphical user interface (GUI)designed to assist the dynamic routing of signal generators andmodifiers currently being developed at the International Academy of Media Arts \& Sciences. Instead of relying on traditional boxand-line approaches, DspMap proposes a design paradigm whereconnections are determined by the relative positions of the variouselements in a single virtual space.}
}

@inproceedings{Scavone2005,
  author = {Scavone, Gary and Silva, Andrey R.},
  title = {Frequency Content of Breath Pressure and Implications for Use in Control},
  pages = {93--96},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176810},
  url = {http://www.nime.org/proceedings/2005/nime2005_093.pdf},
  keywords = {Breath Control, Wind Controller, Breath Sensors },
  abstract = {The breath pressure signal applied to wind music instruments is generally considered to be a slowly varying function of time. In a context of music control, this assumptionimplies that a relatively low digital sample rate (100-200Hz) is sufficient to capture and/or reproduce this signal.We tested this assumption by evaluating the frequency content in breath pressure, particularly during the use of extended performance techniques such as growling, humming,and flutter tonguing. Our results indicate frequency contentin a breath pressure signal up to about 10 kHz, with especially significant energy within the first 1000 Hz. We furtherinvestigated the frequency response of several commerciallyavailable pressure sensors to assess their responsiveness tohigher frequency breath signals. Though results were mixed,some devices were found capable of sensing frequencies upto at least 1.5 kHz. Finally, similar measurements were conducted with Yamaha WX11 and WX5 wind controllers andresults suggest that their breath pressure outputs are sampled at about 320 Hz and 280 Hz, respectively.}
}

@inproceedings{Crevoisier2005,
  author = {Crevoisier, Alain and Polotti, Pietro},
  title = {Tangible Acoustic Interfaces and their Applications for the Design of New Musical Instruments},
  pages = {97--100},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176727},
  url = {http://www.nime.org/proceedings/2005/nime2005_097.pdf},
  keywords = {Tangible interfaces, new musical instruments design. },
  abstract = {Tangible Acoustic Interfaces (TAI) rely on various acousticsensing technologies, such as sound source location and acoustic imaging, to detect the position of contact of users interacting with the surface of solid materials. With their ability to transform almost any physical objects, flat or curved surfaces and walls into interactive interfaces, acoustic sensing technologies show a promising way to bring the sense of touch into the realm of computer interaction. Because music making has been closely related to this sense during centuries, an application of particular interest is the use of TAI's for the design of new musical instruments that matches the physicality and expressiveness of classical instruments. This paper gives an overview of the various acoustic-sensing technologies involved in the realisation of TAI's and develops on the motivation underlying their use for the design of new musical instruments. }
}

@inproceedings{Bencina2005,
  author = {Bencina, Ross},
  title = {The Metasurface -- Applying Natural Neighbour Interpolation to Two-to-Many Mapping},
  pages = {101--104},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176701},
  url = {http://www.nime.org/proceedings/2005/nime2005_101.pdf},
  keywords = {computational geometry,design,design support,high-level control,interpolation,mapping,of interpo-,this section reviews related,user interface,work in the field},
  abstract = {This report describes The Metasurface – a mapping interface supporting interactive design of two-to-many mappings through the placement and interpolation of parameter snapshots on a plane. The Metasurface employs natural neighbour interpolation, a local interpolation method based on Voronoi tessellation, to interpolate between parameter snapshots. Compared to global field based methods, natural neighbour interpolation offers increased predictability and the ability to represent multi-scale surfaces. An implementation of the Metasurface in the AudioMulch software environment is presented and key architectural features of AudioMulch which facilitate this implementation are discussed.}
}

@inproceedings{Silva2005,
  author = {Silva, Andrey R. and Wanderley, Marcelo M. and Scavone, Gary},
  title = {On the Use of Flute Air Jet as A Musical Control Variable},
  pages = {105--108},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176814},
  url = {http://www.nime.org/proceedings/2005/nime2005_105.pdf},
  keywords = {Embouchure, air pressure sensors, hot wires, mapping, augmented flute. },
  abstract = {This paper aims to present some perspectives on mappingembouchure gestures of flute players and their use as controlvariables. For this purpose, we have analyzed several typesof sensors, in terms of sensitivity, dimension, accuracy andprice, which can be used to implement a system capable ofmapping embouchure parameters such as air jet velocity andair jet direction. Finally, we describe the implementationof a sensor system used to map embouchure gestures of aclassical Boehm flute.}
}

@inproceedings{Rodet2005,
  author = {Rodet, Xavier and Lambert, Jean-Philippe and Cahen, Roland and Gaudy, Thomas and Guedy, Fabrice and Gosselin, Florian and Mobuchon, Pascal},
  title = {Study of haptic and visual interaction for sound and music control in the Phase project},
  pages = {109--114},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176804},
  url = {http://www.nime.org/proceedings/2005/nime2005_109.pdf},
  keywords = {Haptic, interaction, sound, music, control, installation. },
  abstract = {The PHASE project is a research project devoted to the study and the realization of systems of multi-modal interaction for generation, handling and control of sound and music. Supported by the network RIAM (Recherche et Innovation en Audiovisuel et Multim\'{e}dia), it was carried out by the CEA-LIST for haptic research, Haption for the realization of the haptic device, Ondim for integration and visual realization and Ircam for research and realization about sound, music and the metaphors for interaction. The integration of the three modalities offers completely innovative capacities for interaction. The objectives are scientific, cultural and educational. Finally, an additional objective was to test such a prototype system, including its haptic arm, in real conditions for general public and over a long duration in order to measure its solidity, its reliability and its interest for users. Thus, during the last three months of the project, a demonstrator was presented and evaluated in a museum in Paris, in the form of an interactive installation offering the public a musical game. Different from a video game, the aim is not to animate the pixels on the screen but to play music and to incite musical awareness.}
}

@inproceedings{Levin2005a,
  author = {Levin, Golan and Lieberman, Zachary},
  title = {Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours in The Manual Input Sessions},
  pages = {115--120},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176772},
  url = {http://www.nime.org/proceedings/2005/nime2005_115.pdf},
  keywords = {Audiovisual performance, hand silhouettes, computer vision, contour analysis, sound-image relationships, augmented reality. },
  abstract = {We report on The Manual Input Sessions, a series of audiovisual vignettes which probe the expressive possibilities of free-form hand gestures. Performed on a hybrid projection system which combines a traditional analog overhead projector and a digital PC video projector, our vision-based software instruments generate dynamic sounds and graphics solely in response to the forms and movements of the silhouette contours of the user's hands. Interactions and audiovisual mappings which make use of both positive (exterior) and negative (interior) contours are discussed. }
}

@inproceedings{Yonezawa2005,
  author = {Yonezawa, Tomoko and Suzuki, Takahiko and Mase, Kenji and Kogure, Kiyoshi},
  title = {HandySinger : Expressive Singing Voice Morphing using Personified Hand-puppet Interface},
  pages = {121--126},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176844},
  url = {http://www.nime.org/proceedings/2005/nime2005_121.pdf},
  keywords = {Personified Expression, Singing Voice Morphing, Voice Ex- pressivity, Hand-puppet Interface },
  abstract = {The HandySinger system is a personified tool developed to naturally express a singing voice controlled by the gestures of a hand puppet. Assuming that a singing voice is a kind of musical expression, natural expressions of the singing voice are important for personification. We adopt a singing voice morphing algorithm that effectively smoothes out the strength of expressions delivered with a singing voice. The system’s hand puppet consists of a glove with seven bend sensors and two pressure sensors. It sensitively captures the user’s motion as a personified puppet’s gesture. To synthesize the different expressional strengths of a singing voice, the “normal” (without expression) voice of a particular singer is used as the base of morphing, and three different expressions, “dark,” “whisper” and “wet,” are used as the target. This configuration provides musically expressed controls that are intuitive to users. In the experiment, we evaluate whether 1) the morphing algorithm interpolates expressional strength in a perceptual sense, 2) the handpuppet interface provides gesture data at sufficient resolution, and 3) the gestural mapping of the current system works as planned.}
}

@inproceedings{Funk2005,
  author = {Funk, Mathias and Kuwabara, Kazuhiro and Lyons, Michael J.},
  title = {Sonification of Facial Actions for Musical Expression},
  pages = {127--131},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176750},
  url = {http://www.nime.org/proceedings/2005/nime2005_127.pdf},
  keywords = {Video-based musical interface; gesture-based interaction; facial expression; facial therapy interface. },
  abstract = {The central role of the face in social interaction and non-verbal communication suggest we explore facial action as a means of musical expression. This paper presents the design, implementation, and preliminary studies of a novel system utilizing face detection and optic flow algorithms to associate facial movements with sound synthesis in a topographically specific fashion. We report on our experience with various gesture-to-sound mappings and applications, and describe our preliminary experiments at musical performance using the system. }
}

@inproceedings{Janer2005,
  author = {Janer, Jordi},
  title = {Voice-controlled plucked bass guitar through two synthesis techniques},
  pages = {132--135},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176758},
  url = {http://www.nime.org/proceedings/2005/nime2005_132.pdf},
  keywords = {Singing voice, musical controller, sound synthesis, spectral processing. },
  abstract = {In this paper we present an example of the use of the singingvoice as a controller for digital music synthesis. The analysis of the voice with spectral processing techniques, derivedfrom the Short-Time Fourier Transform, provides ways ofdetermining a performer's vocal intentions. We demonstratea prototype, in which the extracted vocal features drive thesynthesis of a plucked bass guitar. The sound synthesis stageincludes two different synthesis techniques, Physical Modelsand Spectral Morph.}
}

@inproceedings{Lehrman2005,
  author = {Lehrman, Paul D. and Ryan, Todd M.},
  title = {Bridging the Gap Between Art and Science Education Through Teaching Electronic Musical Instrument Design},
  pages = {136--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176768},
  url = {http://www.nime.org/proceedings/2005/nime2005_136.pdf},
  keywords = {Science education, music education, engineering, electronic music, gesture controllers, MIDI. },
  abstract = {Electronic Musical Instrument Design is an excellent vehiclefor bringing students from multiple disciplines together towork on projects, and help bridge the perennial gap betweenthe arts and the sciences. This paper describes how at TuftsUniversity, a school with no music technology program,students from the engineering (electrical, mechanical, andcomputer), music, performing arts, and visual arts areas usetheir complementary skills, and teach each other, to developnew devices and systems for music performance and control.}
}

@inproceedings{Steiner2005,
  author = {Steiner, Hans-christoph},
  title = {[hid] toolkit: a Unified Framework for Instrument Design},
  pages = {140--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176824},
  url = {http://www.nime.org/proceedings/2005/nime2005_140.pdf},
  keywords = {Instrument design, haptic feedback, gestural control, HID },
  abstract = {The [hid] toolkit is a set of software objects for designingcomputer-based gestural instruments. All too frequently,computer-based performers are tied to the keyboard-mousemonitor model, narrowly constraining the range of possiblegestures. A multitude of gestural input devices are readilyavailable, making it easy to utilize a broader range of gestures. Human Interface Devices (HIDs) such as joysticks,tablets, and gamepads are cheap and can be good musicalcontrollers. Some even provide haptic feedback. The [hid]toolkit provides a unified, consistent framework for gettinggestural data from these devices, controlling the feedback,and mapping this data to the desired output. The [hid]toolkit is built in Pd, which provides an ideal platform forthis work, combining the ability to synthesize and controlaudio and video. The addition of easy access to gesturaldata allows for rapid prototypes. A usable environmentalso makes computer music instrument design accessible tonovices.}
}

@inproceedings{Maki-patola2005b,
  author = {Maki-patola, Teemu},
  title = {User Interface Comparison for Virtual Drums},
  pages = {144--147},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176784},
  url = {http://www.nime.org/proceedings/2005/nime2005_144.pdf},
  keywords = {Virtual drum, user interface, feedback, musical instrument design, virtual reality, sound control, percussion instrument. },
  abstract = {An experimental study comparing different user interfaces for a virtual drum is reported. Virtual here means that the drum is not a physical object. 16 subjects played the drum on five different interfaces and two metronome patterns trying to match their hits to the metronome clicks. Temporal accuracy of the playing was evaluated. The subjects also rated the interfaces subjectively. The results show that hitting the drum alternately from both sides with motion going through the drum plate was less accurate than the traditional one sided hitting. A physical stick was more accurate than a virtual computer graphic stick. Visual feedback of the drum slightly increased accuracy compared to receiving only auditory feedback. Most subjects evaluated the physical stick to offer a better feeling and to be more pleasant than the virtual stick. }
}

@inproceedings{Gutknecht2005,
  author = {Gutknecht, J{\''u}rg and Clay, Art and Frey, Thomas},
  title = {GoingPublik: Using Realtime Global Score Synthesis},
  pages = {148--151},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176754},
  url = {http://www.nime.org/proceedings/2005/nime2005_148.pdf},
  keywords = {Mobile Multimedia, Wearable Computers, Score Synthesis, Sound Art, System Research, HCIs },
  abstract = {This paper takes the reader through various elements of the GoingPublik sound artwork for distributive ensemble and introduces the Realtime Score Synthesis tool (RSS) used as a controller in the work. The collaboration between artists and scientists, details concerning the experimental hardware and software, and new theories of sound art are briefly explained and illustrated. The scope of this project is too broad to be fully covered in this paper, therefore the selection of topics made attempts to draw attention to the work itself and balance theory with practice. }
}

@inproceedings{Pellarin2005,
  author = {Pellarin, Lars and B\"{o}ttcher, Niels and Olsen, Jakob M. and Gregersen, Ole and Serafin, Stefania and Guglielmi, Michel},
  title = {Connecting Strangers at a Train Station},
  pages = {152--155},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176798},
  url = {http://www.nime.org/proceedings/2005/nime2005_152.pdf},
  keywords = {Motion tracking, mapping strategies, public installation, multiple participants music interfaces. },
  abstract = {In this paper we describe a virtual instrument or a performance space, placed at H{\o}je T{\aa}strup train station in Denmark, which is meant to establish communicative connections between strangers, by letting users of the system create soundscapes together across the rails. We discuss mapping strategies and complexity and suggest a possible solution for a final instance of our interactive musical performance system.}
}

@inproceedings{Schiemer2005,
  author = {Schiemer, Greg and Havryliv, Mark},
  title = {Pocket Gamelan: a Pure Data interface for mobile phones},
  pages = {156--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176812},
  url = {http://www.nime.org/proceedings/2005/nime2005_156.pdf},
  keywords = {Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media Performance; Just Intonation. },
  abstract = {This paper describes software tools used to create java applications for performing music using mobile phones. The tools provide a means for composers working in the Pure Data composition environment to design and audition performances using ensembles of mobile phones. These tools were developed as part of a larger project motivated by the desire to allow large groups of non-expert players to perform music based on just intonation using ubiquitous technology. The paper discusses the process that replicates a Pure Data patch so that it will operate within the hardware and software constraints of the Java 2 Micro Edition. It also describes development of objects that will enable mobile phone performances to be simulated accurately in PD and to audition microtonal tuning implemented using MIDI in the j2me environment. These tools eliminate the need for composers to compose for mobile phones by writing java code. In a single desktop application, they offer the composer the flexibility to write music for multiple phones. }
}

@inproceedings{Birchfield2005,
  author = {Birchfield, David and Lorig, David and Phillips, Kelly},
  title = {Sustainable: a dynamic, robotic, sound installation},
  pages = {160--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176705},
  url = {http://www.nime.org/proceedings/2005/nime2005_160.pdf},
  keywords = {computing,dynamic systems,evolutionary,generative arts,installation art,music,robotics,sculpture,sound},
  abstract = {This paper details the motivations, design, and realization of Sustainable, a dynamic, robotic sound installation that employs a generative algorithm for music and sound creation. The piece is comprised of seven autonomous water gong nodes that are networked together by water tubes to distribute water throughout the system. A water resource allocation algorithm guides this distribution process and produces an ever-evolving sonic and visual texture. A simple set of behaviors govern the individual gongs, and the system as a whole exhibits emergent properties that yield local and large scale forms in sound and light. }
}

@inproceedings{Rodrigues2005,
  author = {Rodrigues, Paulo Maria and Gir\~{a}o, Luis Miguel and Gehlhaar, Rolf},
  title = {CyberSong},
  pages = {164--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176808},
  url = {http://www.nime.org/proceedings/2005/nime2005_164.pdf},
  keywords = {Theatrical music, computer interaction, voice, gestural control. },
  abstract = {We present our work in the development of an interface for an actor/singer and its use in performing. Our work combines aspects of theatrical music with technology. Our interface has allowed the development of a new vocabulary for musical and theatrical expression and the possibility for merging classical and experimental music. It gave rise to a strong, strange, unpredictable, yet coherent, "character" and opens up the possibility for a full performance that will explore aspects of voice, theatrical music and, in the future, image projection. }
}

@inproceedings{Allen2005,
  author = {Allen, Jamie},
  title = {boomBox},
  pages = {168--171},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176691},
  url = {http://www.nime.org/proceedings/2005/nime2005_168.pdf},
  keywords = {Visceral control, sample manipulation, Bluetooth®, metaphor, remutualizing instrument, Human Computer Interaction.},
  abstract = {This paper describes the development, function andperformance contexts of a digital musical instrument called "boomBox". The instrument is a wireless, orientation-awarelow-frequency, high-amplitude human motion controller forlive and sampled sound. The instrument has been used inperformance and sound installation contexts. I describe someof what I have learned from the project herein.}
}

@inproceedings{Loscos2005,
  author = {Loscos, Alex and Aussenac, Thomas},
  title = {The wahwactor: a voice controlled wah-wah pedal},
  pages = {172--175},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176776},
  url = {http://www.nime.org/proceedings/2005/nime2005_172.pdf},
  abstract = {Using a wah-wah pedal guitar is something guitar players have to learn. Recently, more intuitive ways to control such effect have been proposed. In this direction, the Wahwactor system controls a wah-wah transformation in real-time using the guitar player’s voice, more precisely, using the performer [wa-wa] utterances. To come up with this system, different vocal features derived from spectral analysis have been studied as candidates for being used as control parameters. This paper details the results of the study and presents the implementation of the whole system.}
}

@inproceedings{Carter2005,
  author = {Carter, William and Liu, Leslie S.},
  title = {Location33: A Mobile Musical},
  pages = {176--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176723},
  url = {http://www.nime.org/proceedings/2005/nime2005_176.pdf},
  keywords = {Mobile Music, Digital Soundscape, Location-Based Entertainment, Mobility, Interactive Music, Augmented Reality },
  abstract = {In this paper, we describe a course of research investigating thepotential for new types of music made possible by locationtracking and wireless technologies. Listeners walk arounddowntown Culver City, California and explore a new type ofmusical album by mixing together songs and stories based ontheir movement. By using mobile devices as an interface, wecan create new types of musical experiences that allowlisteners to take a more interactive approach to an album.}
}

@inproceedings{Bardos2005,
  author = {Bardos, Laszlo and Korinek, Stefan and Lee, Eric and Borchers, Jan},
  title = {Bangarama: Creating Music With Headbanging},
  pages = {180--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176699},
  url = {http://www.nime.org/proceedings/2005/nime2005_180.pdf},
  keywords = {head movements, music controllers, interface design, input devices },
  abstract = {Bangarama is a music controller using headbanging as the primary interaction metaphor. It consists of a head-mounted tilt sensor and aguitar-shaped controller that does not require complex finger positions. We discuss the specific challenges of designing and building this controller to create a simple, yet responsive and playable instrument, and show how ordinary materials such as plywood, tinfoil, and copper wire can be turned into a device that enables a fun, collaborative music-making experience.}
}

@inproceedings{Barbosa2005,
  author = {Barbosa, Alvaro and Cardoso, Jorge and Geiger, G\''{u}nter},
  title = {Network Latency Adaptive Tempo in the Public Sound Objects System},
  pages = {184--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176697},
  url = {http://www.nime.org/proceedings/2005/nime2005_184.pdf},
  keywords = {Network Music Instruments; Latency in Real-Time Performance; Interface-Decoupled Electronic Musical Instruments; Behavioral Driven Interfaces; Collaborative Remote Music Performance; },
  abstract = {In recent years Computer Network-Music has increasingly captured the attention of the Computer Music Community. With the advent of Internet communication, geographical displacement amongst the participants of a computer mediated music performance achieved world wide extension. However, when established over long distance networks, this form of musical communication has a fundamental problem: network latency (or net-delay) is an impediment for real-time collaboration. From a recent study, carried out by the authors, a relation between network latency tolerance and Music Tempo was established. This result emerged from an experiment, in which simulated network latency conditions were applied to the performance of different musicians playing jazz standard tunes. The Public Sound Objects (PSOs) project is web-based shared musical space, which has been an experimental framework to implement and test different approaches for on-line music communication. This paper describe features implemented in the latest version of the PSOs system, including the notion of a network-music instrument incorporating latency as a software function, by dynamically adapting its tempo to the communication delay measured in real-time.}
}

@inproceedings{Villar2005,
  author = {Villar, Nicolas and Lindsay, Adam T. and Gellersen, Hans},
  title = {Pin \& Play \& Perform: A rearrangeable interface for musical composition and performance},
  pages = {188--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176834},
  url = {http://www.nime.org/proceedings/2005/nime2005_188.pdf},
  keywords = {tangible interface, rearrangeable interface, midi controllers },
  abstract = {We present the Pin\&Play\&Perform system: an interface inthe form of a tablet on which a number of physical controlscan be added, removed and arranged on the fly. These controls can easily be mapped to existing music sofware usingthe MIDI protocol. The interface provides a mechanism fordirect manipulation of application parameters and eventsthrough a set of familiar controls, while also encouraging ahigh degree of customisation through the ability to arrange,rearrange and annotate the spatial layout of the interfacecomponents on the surface of the tablet.The paper describes how we have realized this concept using the Pin\&Play technology. As an application example, wedescribe our experiences in using our interface in conjunction with Propellerheads' Reason, a popular piece of musicsynthesis software.}
}

@inproceedings{Birnbaum2005,
  author = {Birnbaum, David and Fiebrink, Rebecca and Malloch, Joseph and Wanderley, Marcelo M.},
  title = {Towards a Dimension Space for Musical Devices},
  pages = {192--195},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176707},
  url = {http://www.nime.org/proceedings/2005/nime2005_192.pdf},
  keywords = {design space analysis,human-computer interaction,interfaces for musical expression,new},
  abstract = {While several researchers have grappled with the problem of comparing musical devices across performance, installation, and related contexts, no methodology yet exists for producing holistic, informative visualizations for these devices. Drawing on existing research in performance interaction, human-computer interaction, and design space analysis, the authors propose a dimension space representation that can be adapted for visually displaying musical devices. This paper illustrates one possible application of the dimension space to existing performance and interaction systems, revealing its usefulness both in exposing patterns across existing musical devices and aiding in the design of new ones.}
}

@inproceedings{Wang2005a,
  author = {Wang, Ge and Cook, Perry R.},
  title = {Yeah, ChucK It! = > Dynamic , Controllable Interface Mapping},
  pages = {196--199},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176838},
  url = {http://www.nime.org/proceedings/2005/nime2005_196.pdf},
  keywords = {Controller mapping, programming language, on-the-fly programming, real-time interaction, concurrency. },
  abstract = {ChucK is a programming language for real-time sound synthesis. It provides generalized audio abstractions and precise control over timing and concurrency --- combining the rapid-prototyping advantages of high-level programming tools, such as Pure Data, with the flexibility and controllability of lower-level, text-based languages like C/C++. In this paper, we present a new time-based paradigm for programming controllers with ChucK. In addition to real-time control over sound synthesis, we show how features such as dynamic patching, on-the-fly controller mapping, multiple control rates, and precisely-timed recording and playback of sensors can be employed under the ChucK programming model. Using this framework, composers, programmers, and performers can quickly write (and read/debug) complex controller/synthesis programs, and experiment with controller mapping on-the-fly. }
}

@inproceedings{Tindale2005,
  author = {Tindale, Adam R. and Kapur, Ajay and Tzanetakis, George and Driessen, Peter and Schloss, Andrew},
  title = {A Comparison of Sensor Strategies for Capturing Percussive Gestures},
  pages = {200--203},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176828},
  url = {http://www.nime.org/proceedings/2005/nime2005_200.pdf},
  keywords = {Percussion Controllers, Timbre-recognition based instruments, Electronic Percussion, Sensors for Interface Design },
  abstract = {Drum controllers designed by researchers and commercialcompanies use a variety of techniques for capturing percussive gestures. It is challenging to obtain both quick responsetimes and low-level data (such as position) that contain expressive information. This research is a comprehensive studyof current methods to evaluate the available strategies andtechnologies. This study aims to demonstrate the benefitsand detriments of the current state of percussion controllersas well as yield tools for those who would wish to conductthis type of study in the future.}
}

@inproceedings{Lee2005,
  author = {Lee, Eric and Borchers, Jan},
  title = {The Role of Time in Engineering Computer Music Systems},
  pages = {204--207},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176766},
  url = {http://www.nime.org/proceedings/2005/nime2005_204.pdf},
  keywords = {time design, conceptual models of time, design spaces, interactive music exhibits, engineering music systems},
  abstract = {Discussion of time in interactive computer music systems engineering has been largely limited to data acquisition rates and latency.Since music is an inherently time-based medium, we believe thattime plays a more important role in both the usability and implementation of these systems. In this paper, we present a time designspace, which we use to expose some of the challenges of developing computer music systems with time-based interaction. Wedescribe and analyze the time-related issues we encountered whilstdesigning and building a series of interactive music exhibits thatfall into this design space. These issues often occur because ofthe varying and sometimes conflicting conceptual models of timein the three domains of user, application (music), and engineering.We present some of our latest work in conducting gesture interpretation and frameworks for digital audio, which attempt to analyzeand address these conflicts in temporal conceptual models.}
}

@inproceedings{Kobayashi2005,
  author = {Kobayashi, Shigeru and Masayuki, Akamasu},
  title = {Spinner: A Simple Approach to Reconfigurable User Interfaces},
  pages = {208--211},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176764},
  url = {http://www.nime.org/proceedings/2005/nime2005_208.pdf},
  keywords = {Reconfigurable, Sensors, Computer Music },
  abstract = {This paper reports our recent development on a reconfigurable user interface. We created a system that consists of a dial type controller ‘Spinner’, and the GUI (Graphical User Interface) objects for the Max/MSP environment[1]. One physical controller corresponds to one GUI controller on a PC’s display device, and a user can freely change the connection on the fly (i.e. associate the physical controller to another GUI controller). Since the user interface on the PC side is running on the Max/MSP environment that has high flexibility, a user can freely reconfigure the layout of GUI controllers. A single ‘Spinner’ control device consists of a rotary encoder with a push button to count rotations and a photo IC to detect specific patterns from the GUI objects to identify. Since ‘Spinner’ features a simple identification method, it is capable of being used with normal display devices like LCD (Liquid Crystal Display) or a CRT (Cathode Ray Tube) and so on. A user can access multiple ‘Spinner’ devices simultaneously. By using this system, a user can build a reconfigurable user interface.}
}

@inproceedings{Magnusson2005,
  author = {Magnusson, Thor},
  title = {ixi software: The Interface as Instrument},
  pages = {212--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176782},
  url = {http://www.nime.org/proceedings/2005/nime2005_212.pdf},
  keywords = {Graphical user interfaces, abstract graphical interfaces, hypercontrol, intelligent instruments, live performance, machine learning, catalyst software, OSC, interfacing code, open source, Pure Data, SuperCollider. },
  abstract = {This paper describes the audio human computer interface experiments of ixi in the past and outlines the current platform for future research. ixi software [5] was founded by Thor Magnusson and Enrike Hurtado Mendieta in year 2000 and since then we've been working on building prototypes in the form of screen-based graphical user interfaces for musical performance, researching human computer interaction in the field of music and creating environments which other people can use to do similar work and for us to use in our workshops. Our initial starting point was that computer music software and the way their interfaces are built need not necessarily be limited to copying the acoustic musical instruments and studio technology that we already have, but additionally we can create unique languages and work processes for the virtual world. The computer is a vast creative space with specific qualities that can and should be explored. }
}

@inproceedings{Miranda2005,
  author = {Miranda, Eduardo and Brouse, Andrew},
  title = {Toward Direct Brain-Computer Musical Interfaces},
  pages = {216--219},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176792},
  url = {http://www.nime.org/proceedings/2005/nime2005_216.pdf},
  keywords = {Brain-Computer Interface, BCI, Electroencephalogram, EEG, brainwaves, music and the brain, interactive music systems.},
  abstract = { Musicians and composers have been using brainwaves as generative sources in music for at least 40 years and the possibility of a brain-computer interface for direct communication and control was first seriously investigated in the early 1970s. Work has been done by many artists and technologists in the intervening years to attempt to control music systems with brainwaves and --- indeed --- many other biological signals. Despite the richness of EEG, fMRI and other data which can be read from the human brain, there has up to now been only limited success in translating the complex encephalographic data into satisfactory musical results. We are currently pursuing research which we believe will lead to the possibility of direct brain-computer interfaces for rich and expressive musical control. This report will outline the directions of our current research and results. }
}

@inproceedings{Taylor2005,
  author = {Taylor, Robyn and Torres, Daniel and Boulanger, Pierre},
  title = {Using Music to Interact with a Virtual Character},
  pages = {220--223},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176826},
  url = {http://www.nime.org/proceedings/2005/nime2005_220.pdf},
  keywords = {Music, synthetic characters, advanced man-machine interfaces, virtual reality, behavioural systems, interaction techniques, visualization, immersive entertainment, artistic in- stallations },
  abstract = {We present a real-time system which allows musicians tointeract with synthetic virtual characters as they perform.Using Max/MSP to parameterize keyboard and vocal input, meaningful features (pitch, amplitude, chord information, and vocal timbre) are extracted from live performancein real-time. These extracted musical features are thenmapped to character behaviour in such a way that the musician's performance elicits a response from the virtual character. The system uses the ANIMUS framework to generatebelievable character expressions. Experimental results arepresented for simple characters.}
}

@inproceedings{Chew2005,
  author = {Chew, Elaine and Francois, Alexander R. and Liu, Jie and Yang, Aaron},
  title = {ESP: A Driving Interface for Expression Synthesis},
  pages = {224--227},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176725},
  url = {http://www.nime.org/proceedings/2005/nime2005_224.pdf},
  keywords = {Music expression synthesis system, driving interface. },
  abstract = {In the Expression Synthesis Project (ESP), we propose adriving interface for expression synthesis. ESP aims toprovide a compelling metaphor for expressive performance soas to make high-level expressive decisions accessible to nonexperts. In ESP, the user drives a car on a virtual road thatrepresents the music with its twists and turns; and makesdecisions on how to traverse each part of the road. The driver'sdecisions affect in real-time the rendering of the piece. Thepedals and wheel provide a tactile interface for controlling thecar dynamics and musical expression, while the displayportrays a first person view of the road and dashboard from thedriver's seat. This game-like interface allows non-experts tocreate expressive renderings of existing music without havingto master an instrument, and allows expert musicians toexperiment with expressive choice without having to firstmaster the notes of the piece. The prototype system has beentested and refined in numerous demonstrations. This paperpresents the concepts underlying the ESP system and thearchitectural design and implementation of a prototype.}
}

@inproceedings{Poepel2005,
  author = {Poepel, Cornelius},
  title = {On Interface Expressivity: A Player-Based Study},
  pages = {228--231},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176802},
  url = {http://www.nime.org/proceedings/2005/nime2005_228.pdf},
  keywords = {Musical Expression, electronic bowed string instrument, evaluation of musical input devices, audio signal driven sound synthesis },
  abstract = {While many new interfaces for musical expression have been presented in the past, methods to evaluate these interfaces are rare.This paper presents a method and a study comparing the potentialfor musical expression of different string-instrument based musicalinterfaces. Cues for musical expression are defined based on results of research in musical expression and on methods for musicaleducation in instrumental pedagogy. Interfaces are evaluated according to how well they are estimated to allow players making useof their existing technique for the creation of expressive music.}
}

@inproceedings{Wingstedt2005,
  author = {Wingstedt, Johnny and Liljedahl, Mats and Lindberg, Stefan and Berg, Jan},
  title = {REMUPP -- An Interactive Tool for Investigating Musical Properties and Relations},
  pages = {232--235},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176842},
  url = {http://www.nime.org/proceedings/2005/nime2005_232.pdf},
  keywords = {Musical experience, non-verbal test techniques, musical parameters.},
  abstract = {A typical experiment design within the field of music psychology is playing music to a test subject who listens and reacts – most often by verbal means. One limitation of this kind of test is the inherent difficulty of measuring an emotional reaction in a laboratory setting. This paper describes the design, functions and possible uses of the software tool REMUPP (Relations between musical parameters and perceived properties), designed for investigating various aspects of musical experience. REMUPP allows for non-verbal examination of selected musical parameters (such as tonality, tempo, timbre, articulation, volume, register etc.) in a musical context. The musical control is put into the hands of the subject, introducing an element of creativity and enhancing the sense of immersion. Information acquired with REMUPP can be output as numerical data for statistical analysis, but the tool is also suited for the use with more qualitatively oriented methods.}
}

@inproceedings{Cook2005,
  author = {Cook, Perry R.},
  title = {Real-Time Performance Controllers for Synthesized Singing},
  pages = {236--237},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176846},
  url = {http://www.nime.org/proceedings/2005/nime2005_236.pdf},
  keywords = {Singing synthesis, real-time singing synthesis control. },
  abstract = {A wide variety of singing synthesis models and methods exist,but there are remarkably few real-time controllers for thesemodels. This paper describes a variety of devices developedover the last few years for controlling singing synthesismodels implemented in the Synthesis Toolkit in C++ (STK),Max/MSP, and ChucK. All of the controllers share somecommon features, such as air-pressure sensing for breathingand/or loudness control, means to control pitch, and methodsfor selecting and blending phonemes, diphones, and words.However, the form factors, sensors, mappings, and algorithmsvary greatly between the different controllers.}
}

@inproceedings{Kim-Boyle2005,
  author = {Kim-Boyle, David},
  title = {Musical Score Generation in Valses and Etudes},
  pages = {238--239},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176762},
  url = {http://www.nime.org/proceedings/2005/nime2005_238.pdf},
  keywords = {Score generation, Jitter. },
  abstract = {The author describes a recent composition for piano and computer in which the score performed by the pianist, read from a computer monitor, is generated in real-time from a vocabulary of predetermined scanned score excerpts. The author outlines the algorithm used to choose and display a particular excerpt and describes some of the musical difficulties faced by the pianist in a performance of the work.}
}

@inproceedings{Baird2005,
  author = {Baird, Kevin C.},
  title = {Real-Time Generation of Music Notation via Audience Interaction Using Python and {GNU} Lilypond},
  pages = {240--241},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176695},
  url = {http://www.nime.org/proceedings/2005/nime2005_240.pdf},
  keywords = {notation, stochastic, interactive, audience, Python, Lilypond },
  abstract = {No Clergy is an interactive music performance/installation inwhich the audience is able to shape the ongoing music. In it,members of a small acoustic ensemble read music notation fromcomputer screens. As each page refreshes, the notation is alteredand shaped by both stochastic transformations of earlier musicwith the same performance and audience feedback, collected viastandard CGI forms. }
}

@inproceedings{Fox2005,
  author = {Fox, Jesse and Carlile, Jennifer},
  title = {SoniMime: Movement Sonification for Real-Time Timbre Shaping},
  pages = {242--243},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176741},
  url = {http://www.nime.org/proceedings/2005/nime2005_242.pdf},
  keywords = {Sonification, Musical Controller, Human Computer Interaction },
  abstract = {This paper describes the design of SoniMime, a system forthe sonification of hand movement for real-time timbre shaping. We explore the application of the tristimulus timbremodel for the sonification of gestural data, working towardthe goals of musical expressivity and physical responsiveness. SoniMime uses two 3-D accelerometers connected toan Atmel microprocessor which outputs OSC control messages. Data filtering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.}
}

@inproceedings{Huott2005,
  author = {Huott, Robert},
  title = {Precise Control on Compound Curves},
  pages = {244--245},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176848},
  url = {http://www.nime.org/proceedings/2005/nime2005_244.pdf},
  keywords = {Musical controller, sensate surface, mapping system },
  abstract = {This paper presents the ‘Bean’, a novel controller employing a multi-touch sensate surface in a compound curve shape. The design goals, construction, and mapping system are discussed, along with a retrospective from a previous, similar design.}
}

@inproceedings{Lugo2005,
  author = {Lugo, Robert and Damondrick, Jack},
  title = {Beat Boxing : Expressive Control for Electronic Music Performance and Musical Applications},
  pages = {246--247},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176778},
  url = {http://www.nime.org/proceedings/2005/nime2005_246.pdf},
  keywords = {Performance, Gestural Mapping, Music Controller, Human-Computer Interaction, PureData (Pd), OSC },
  abstract = {This paper describes the design and implementation of BeatBoxing, a percussive gestural interface for the liveperformance of electronic music and control of computerbased games and musical activities.}
}

@inproceedings{Franco2005,
  author = {Franco, Ivan},
  title = {The Airstick: A Free-Gesture Controller Using Infrared Sensing},
  pages = {248--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176747},
  url = {http://www.nime.org/proceedings/2005/nime2005_248.pdf},
  keywords = {Music Controller, Infrared Sensing, Computer Music. },
  abstract = {This paper describes the development of AirStick, an interface for musical expression. AirStick is played {in the air}, in a Theremin style. It is composed of an array of infrared proximity sensors, which allow the mapping of the position of any interfering obstacle inside a bi-dimensional zone. This controller sends both x and y control data to various real-time synthesis algorithms. }
}

@inproceedings{Carlile2005,
  author = {Carlile, Jennifer and Hartmann, Bj{\''{o}}rn},
  title = {{OR}OBORO: A Collaborative Controller with Interpersonal Haptic Feedback},
  pages = {250--251},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176721},
  url = {http://www.nime.org/proceedings/2005/nime2005_250.pdf},
  keywords = {Musical Controller, Collaborative Control, Haptic Interfaces },
  abstract = {OROBORO is a novel collaborative controller which focuses on musical performance as social experience by exploring synchronized actions of two musicians operating a single instrument. Each performer uses two paddle mechanisms – one for hand orientation sensing and one for servo-motor actuated feedback. We introduce a haptic mirror in which the movement of one performer’s sensed hand is used to induce movement of the partner’s actuated hand and vice versa. We describe theoretical motivation, and hardware/software implementation.}
}

@inproceedings{Rodriguez2005,
  author = {Rodr\'{\i}guez, David and Rodr\'{\i}guez, Iv\'{a}n},
  title = {VIFE \_alpha v.01 Real-time Visual Sound Installation performed by Glove-Gesture},
  pages = {252--253},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176806},
  url = {http://www.nime.org/proceedings/2005/nime2005_252.pdf},
  keywords = {Synaesthesia, 3D render, new reality, virtual interface, creative interaction, sensors. },
  abstract = {We present a Virtual Interface to Feel Emotions called VIFE {\_}alpha v.01 (Virtual Interface to Feel Emotions). The work investigates the idea of Synaesthesia and her enormous possibilities creating new realities, sensations and zones where the user can find new points of interaction. This interface allows the user to create sonorous and visual compositions in real time. 6 three-dimensional sonorous forms are modified according to the movements of the user. These forms represent sonorous objects that respond to this by means of sensorial stimuli. Multiple combinations of colors and sound effects superpose to an a the others to give rise to a unique experience.}
}

@inproceedings{Hindman2005,
  author = {Hindman, David and Kiser, Spencer},
  title = {Sonictroller},
  pages = {254--255},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176756},
  url = {http://www.nime.org/proceedings/2005/nime2005_254.pdf},
  keywords = {video game, Nintendo, music, sound, controller, Mortal Kombat, trumpet, guitar, voice },
  abstract = {The Sonictroller was originally conceived as a means ofintroducing competition into an improvisatory musicalperformance. By reverse-engineering a popular video gameconsole, we were able to map sound information (volume,pitch, and pitch sequences) to any continuous or momentaryaction of a video game sprite.}
}

@inproceedings{Verplank2005,
  author = {Verplank, William},
  title = {Haptic Music Exercises},
  pages = {256--257},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176832},
  url = {http://www.nime.org/proceedings/2005/nime2005_256.pdf},
  keywords = {Music control, haptic feedback, physical interaction design, Input/output devices, interactive systems, haptic I/O},
  abstract = {Pluck, ring, rub, bang, strike, and squeeze are all simple gestures used in controlling music. A single motor/encoder plus a force-sensor has proved to be a useful platform for experimenting with haptic feedback in controlling computer music. The surprise is that the “best” haptics (precise, stable) may not be the most “musical”.}
}

@inproceedings{Eaton2005,
  author = {Eaton, John and Moog, Robert},
  title = {Multiple-Touch-Sensitive Keyboard},
  pages = {258--259},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176735},
  url = {http://www.nime.org/proceedings/2005/nime2005_258.pdf},
  keywords = {Multiple touch sensitive, MTS, keyboard, key sensor design, upgrading to present-day computers },
  abstract = {In this presentation, we discuss and demonstrate a multiple touch sensitive (MTS) keyboard developed by Robert Moog for John Eaton. Each key of the keyboard is equipped with sensors that detect the three-dimensional position of the performer's finger. The presentation includes some of Eaton's performances for certain earlier prototypes as well as this keyboard. }
}

@inproceedings{Fraietta2005,
  author = {Fraietta, Angelo},
  title = {Smart Controller / Bell Garden Demo},
  pages = {260--261},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176743},
  url = {http://www.nime.org/proceedings/2005/nime2005_260.pdf},
  keywords = {Control Voltage, Open Sound Control, Algorithmic Composition, MIDI, Sound Installations, Programmable Logic Control, Synthesizers. },
  abstract = {This paper will demonstrate the use of the Smart Controller workbench in the Interactive Bell Garden. }
}

@inproceedings{Melo2005,
  author = {Melo, Mauricio and Fan, Doria},
  title = {Swayway --- Midi Chimes},
  pages = {262--263},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176790},
  url = {http://www.nime.org/proceedings/2005/nime2005_262.pdf},
  keywords = {Interactive sound sculpture, flex sensors, midi chimes, LEDs, sound installation. },
  abstract = {The Swayway is an audio/MIDI device inspired by the simpleconcept of the wind chime.This interactive sculpture translates its swaying motion,triggered by the user, into sound and light. Additionally, themotion of the reeds contributes to the visual aspect of thepiece, converting the whole into a sensory and engagingexperience.}
}

@inproceedings{Wang2005,
  author = {Wang, Derek},
  title = {Bubbaboard and Mommaspeaker: Creating Digital Tonal Sounds from an Acoustic Percussive Instrument},
  pages = {264--265},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176836},
  url = {http://www.nime.org/proceedings/2005/nime2005_264.pdf},
  keywords = {Gesture based controllers, Musical Performance, MIDI, Accelerometer, Microcontroller, Contact Microphone },
  abstract = {This paper describes the transformation of an everyday object into a digital musical instrument. By tracking hand movements and tilt on one of two axes, the Bubbaboard, a transformed handheld washboard, allows a user to play scales at different octaves while simultaneously offering the ability to use its inherent acoustic percussive qualities. Processed sound is fed to the Mommaspeaker, which creates physically generated vibrato at a speed determined by tilting the Bubbaboard on its second axis. }
}

@inproceedings{Flety2005,
  author = {Fl\'{e}ty, Emmanuel},
  title = {The WiSe Box: a Multi-performer Wireless Sensor Interface using {WiFi} and OSC},
  pages = {266--267},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176739},
  url = {http://www.nime.org/proceedings/2005/nime2005_266.pdf},
  keywords = {Gesture, Sensors, WiFi, 802.11, OpenSoundControl. },
  abstract = {The Wise Box is a new wireless digitizing interface for sensors and controllers. An increasing demand for this kind of hardware, especially in the field of dance and computer performance lead us to design a wireless digitizer that allows for multiple users, with high bandwidth and accuracy. The interface design was initiated in early 2004 and shortly described in reference [1]. Our recent effort was directed to make this device available for the community on the form of a manufactured product, similarly to our previous interfaces such as AtoMIC Pro, Eobody or Ethersense [1][2][3]. We describe here the principles we used for the design of the device as well as its technical specifications. The demo will show several devices running at once and used in real-time with a various set of sensors. }
}

@inproceedings{Bowen2005,
  author = {Bowen, Adam},
  title = {Soundstone: A {3-D} Wireless Music Controller},
  pages = {268--269},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176711},
  url = {http://www.nime.org/proceedings/2005/nime2005_268.pdf},
  keywords = {Gesture recognition, haptics, human factors, force, acceleration, tactile feedback, general purpose controller, wireless. },
  abstract = {Soundstone is a small wireless music controller that tracks movement and gestures, and maps these signals to characteristics of various synthesized and sampled sounds. It is intended to become a general-purpose platform for exploring the sonification of movement, with an emphasis on tactile (haptic) feedback. }
}

@inproceedings{Guisan2005,
  author = {Guisan, Alain C.},
  title = {Interactive Sound Installation: INTRIUM},
  pages = {270--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176752},
  url = {http://www.nime.org/proceedings/2005/nime2005_270.pdf},
  keywords = {Interactive sound installation, collaborative work, sound processing, acoustic source localization.},
  abstract = {INTRIUM is an interactive sound installation exploring the inside vibration of the atrium. A certain number of architectural elements are fitted with acoustic sensors in order to capture the vibration they produce when they are manipulated or touched by hands. This raw sound is further processed in real-time, allowing the participants to create a sonic landscape in the atrium, as the result of a collaborative and collective work between them.}
}

@inproceedings{Socolofsky2005,
  author = {Socolofsky, Eric},
  title = {Contemplace},
  pages = {271--271},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176822},
  url = {http://www.nime.org/proceedings/2005/nime2005_271.pdf},
  keywords = {Interactive space, spatial installation, graphic and aural display, motion tracking, Processing, Flosc },
  abstract = {Contemplace is a spatial personality that redesigns itselfdynamically according to its conversations with its visitors.Sometimes welcoming, sometimes shy, and sometimeshostile, Contemplace's mood is apparent through a display ofprojected graphics, spatial sound, and physical motion.Contemplace is an environment in which inhabitationbecomes a two-way dialogue.}
}

@inproceedings{Marinelli2005,
  author = {Marinelli, Maia and Lamenzo, Jared and Borissov, Liubo},
  title = {Mocean},
  pages = {272--272},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176786},
  url = {http://www.nime.org/proceedings/2005/nime2005_272.pdf},
  keywords = {New interface, water, pipe organ, natural media, PIC microcontroller, wind instrument, human computer interface. },
  abstract = {Mocean is an immersive environment that creates sensoryrelationships between natural media, particularly exploringthe potential of water as an emotive interface.}
}

@inproceedings{Matsumura2005,
  author = {Matsumura, Seiichiro and Arakawa, Chuichi},
  title = {Hop Step Junk: Sonic Visualization using Footsteps},
  pages = {273--273},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176788},
  url = {http://www.nime.org/proceedings/2005/nime2005_273.pdf},
  keywords = {Footsteps, body action, interactive, visualization, simple and reliable interface, contact microphone, sound playground},
  abstract = {'Hop Step Junk' is an interactive sound installation that creates audio and visual representations of the audience's footsteps. The sound of a footstep is very expressive. Depending on one's weight, clothing and gate, a footstep can sound quite different. The period between steps defines one's personal rhythm. The sound output of 'Hop Step Junk' is wholly derived from the audience's footsteps. 'Hop Step Junk' creates a multi-generational playground, an instrument that an audience can easily play.}
}

@inproceedings{Deutscher2005,
  author = {Deutscher, Meghan and Fels, Sidney S. and Hoskinson, Reynald and Takahashi, Sachiyo},
  title = {Echology},
  pages = {274--274},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2005},
  address = {Vancouver, BC, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176733},
  url = {http://www.nime.org/proceedings/2005/nime2005_274.pdf},
  keywords = {Mediascape, sound spatialization, interactive art, Beluga whale}
}

