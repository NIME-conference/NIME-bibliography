@inproceedings{NIME22_1,
 PDF = {101.pdf},
 abstract = {This paper discusses a quantitative method to evaluate whether an expert player is able to execute skilled actions on an unfamiliar interface while keeping the focus of their performance on the musical outcome rather than on the technology itself. In our study, twelve professional electric guitar players used an augmented plectrum to replicate prerecorded timbre variations in a set of musical excerpts. The task was undertaken in two experimental conditions: a reference condition, and a subtle gradual change in the sensitivity of the augmented plectrum which is designed to affect the guitarist’s performance without making them consciously aware of its effect. We propose that players’ subconscious response to the disruption of changing the sensitivity, as well as their overall ability to replicate the stimuli, may indicate the strength of the relationship they developed with the new interface. The case study presented in this paper highlights the strengths and limitations of this method.},
 address = {The University of Auckland, New Zealand},
 articleno = {1},
 author = {Guidi, Andrea and McPherson, Andrew},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.79d0b38f},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/J4981qsq_7c},
 title = {Quantitative evaluation of aspects of embodiment in new digital musical instruments},
 url = {https://doi.org/10.21428%2F92fbeb44.79d0b38f},
 year = {2022}
}

@inproceedings{NIME22_2,
 PDF = {102.pdf},
 abstract = {This article explores two in-depth interviews with distinguished Chinese NIMEers, across generations, from the late 1970s to the present. Tian Jinqin and Meng Qi represent role models in the Chinese NIME community. From the innovative NIME designers’ historical technological innovation of the 1970s’ analog ribbon control string synthesizer Xian Kong Qin to the 2020’s Wing Pinger evolving harmony synthesizer, the author shines a light from different angles on the Chinese NIME community.},
 address = {The University of Auckland, New Zealand},
 articleno = {2},
 author = {Wu, Jiayue Cecilia},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.57e41c54},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=4PMmDnUNgRk},
 title = {Today and Yesterday: Two Case Studies of China{\textquotesingle}s {NIME} Community},
 url = {https://doi.org/10.21428%2F92fbeb44.57e41c54},
 year = {2022}
}


@inproceedings{NIME22_3,
 PDF = {107.pdf},
 abstract = {One of the consequences of the pandemic has been the potential to embrace hybrid support for different human group activities, including music performance, resulting in accommodating a wider range of situations. We believe that we are barely at the tip of the iceberg and that we can explore further the possibilities of the medium by promoting a more active role of the audience during telematic performance. In this paper, we present personic, a mobile web app designed for distributed audiences to constitute a digital musical instrument. This has the twofold purpose of letting the audience contribute to the performance with a non-intrusive and easy-to-use approach, as well as providing audiovisual feedback that is helpful for both the performers and the audience alike. The challenges and possibilities of this approach are discussed from pilot testing the app using a practice-based approach. We conclude by pointing to new directions of telematic performance, which is a promising direction for network music and digital performance.},
 address = {The University of Auckland, New Zealand},
 articleno = {3},
 author = {Xamb{\'{o}}, Anna and Goudarzi, Visda},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.706b549e},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/xu5ySfbqYs8},
 title = {The Mobile Audience as a Digital Musical Persona in Telematic Performance},
 url = {https://doi.org/10.21428%2F92fbeb44.706b549e},
 year = {2022}
}

@inproceedings{NIME22_4,
 PDF = {109.pdf},
 abstract = {Physical modelling sound synthesis methods generate vast and intricate sound spaces that are navigated using meaningful parameters. Numerical based physical modelling synthesis methods provide authentic representations of the physics they model. Unfortunately, the application of these physical models are often limited because of their considerable computational requirements. In previous studies, the CPU has been shown to reliably support two-dimensional linear finite-difference models in real-time with resolutions up to 64x64. However, the near-ubiquitous parallel processing units known as GPUs have previously been used to process considerably larger resolutions, as high as 512x512 in real-time. GPU programming requires a low-level understanding of the architecture, which often imposes a barrier for entry for inexperienced practitioners. Therefore, this paper proposes HyperModels, a framework for automating the mapping of linear finite-difference based physical modelling synthesis into an optimised parallel form suitable for the GPU. An implementation of the design is then used to evaluate the objective performance of the framework by comparing the automated solution to manually developed equivalents. For the majority of the extensive performance profiling tests, the auto-generated programs were observed to perform only 60% slower but in the worst-case scenario it was 50% slower. The initial results suggests that, in most circumstances, the automation provided by the framework avoids the lowlevel expertise required to manually optimise the GPU, with only a small reduction in performance. However, there is still scope to improve the auto-generated optimisations. When comparing the performance of CPU to GPU equivalents, the parallel CPU version supports resolutions of up to 128x128 whilst the GPU continues to support higher resolutions up to 512x512. To conclude the paper, two instruments are developed using HyperModels based on established physical model designs.},
 address = {The University of Auckland, New Zealand},
 articleno = {4},
 author = {Renney, Harri and Willemsen, Silvin and Gaster, Benedict and Mitchell, Tom},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.98a4210a},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/Pb4pAr2v4yU},
 title = {{HyperModels} - A Framework for {GPU} Accelerated Physical Modelling Sound Synthesis},
 url = {https://doi.org/10.21428%2F92fbeb44.98a4210a},
 year = {2022}
}

@inproceedings{NIME22_5,
 PDF = {11.pdf},
 abstract = {In this paper, we propose a set of reflections to actively incorporate environmental sustainability instances in the practice of circuit bending. This proposal combines circuit bending-related concepts with literature from the domain of sustainable Human-Computer Interaction (HCI). We commence by presenting an overview of the critical discourse within the New Interfaces for Musical Expression (NIME) community, and of circuit bending itself—exposing the linkages this practice has with themes directly related to this research, such as environmental sustainability and philosophy. Afterwards, we look at how the topic of environmental sustainability has been discussed, concerning circuit bending, within the NIME literature. We conclude by developing a list of recommendations for a sustainable circuit bending practice.},
 address = {The University of Auckland, New Zealand},
 articleno = {5},
 author = {Dorigatti, Enrico and Masu, Raul},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.18502d1d},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/n3GcaaHkats},
 title = {Circuit Bending and Environmental Sustainability: Current Situation and Steps Forward},
 url = {https://doi.org/10.21428%2F92fbeb44.18502d1d},
 year = {2022}
}

@inproceedings{NIME22_6,
 PDF = {110.pdf},
 abstract = {This paper explores a minimalist approach to live coding using a single input parameter to manipulate the graph structure of a finite state machine through a stream of bits. This constitutes an example of bottom-up live coding, which operates on a low level language to generate a high level structure output. Here we examine systematically how to apply mappings of continuous gestural interactions to develop a bottom-up system for predicting programming behaviours. We conducted a statistical analysis based on a controlled data generation procedure. The findings concur with the subjective experience of the behavior of the system when the user modulates the sampling frequency of a variable clock using a knob as an input device. This suggests that a sequential predictive model may be applied towards the development of a tactically predictive system according to Tanimoto’s hierarchy of liveness. The code is provided in a git repository.},
 address = {The University of Auckland, New Zealand},
 articleno = {6},
 author = {Diapoulis, Georgios and Zannos, Iannis and Tatar, Kivan{\c{c}} and Dahlstedt, Palle},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.51fecaab},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/L_v5P7jGK8Y},
 title = {Bottom-up live coding: Analysis of continuous interactions towards predicting programming behaviours},
 url = {https://doi.org/10.21428%2F92fbeb44.51fecaab},
 year = {2022}
}

@inproceedings{NIME22_7,
 PDF = {111.pdf},
 abstract = {Deformable interfaces are an emerging area of Human- Computer Interaction (HCI) research that offers nuanced and responsive physical interaction with digital technologies. They are well suited to creative and expressive forms of HCI such as Digital Musical Interfaces (DMIs). However, research on the design of deformable DMIs is limited. This paper explores the role that deformable interfaces might play in DMI design. We conducted an online study with 23 DMI designers in which they were invited to create non-functional deformable DMIs together. Our results suggest forms of gestural input and sound mappings that deformable interfaces intuitively lend themselves to for DMI design. From our results, we highlight four styles of DMI that deformable interfaces might be most suited to, and suggest the kinds of experience that deformable DMIs might be most compelling for musicians and audiences. We discuss how DMI designers explore deformable materials and gestures input and the role of unexpected affordances in the design process.},
 address = {The University of Auckland, New Zealand},
 articleno = {7},
 author = {Zheng, Jianing and Bryan-Kinns, Nick},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.41da9da5},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/KHqfxL4F7Bg},
 title = {Squeeze, Twist, Stretch: Exploring Deformable Digital Musical Interfaces Design Through Non-Functional Prototypes},
 url = {https://doi.org/10.21428%2F92fbeb44.41da9da5},
 year = {2022}
}

@inproceedings{NIME22_8,
 PDF = {112.pdf},
 abstract = {We present a systematic review of voice-centered NIME publications from the past two decades. Musical expression has been a key driver of innovation in voicebased technologies, from traditional architectures that amplify singing to cutting-edge research in vocal synthesis. NIME conference has emerged as a prime venue for innovative vocal interfaces. However, there hasn’t been a systematic analysis of all voice-related work or an effort to characterize their features. Analyzing trends in Vocal NIMEs can help the community better understand common interests, identify uncharted territories, and explore directions for future research. We identified a corpus of 98 papers about Vocal NIMEs from 2001 to 2021, which we analyzed in 3 ways. First, we automatically extracted latent themes and possible categories using natural language processing. Taking inspiration from concepts surfaced through this process, we then defined several core dimensions with associated descriptors of Vocal NIMEs and assigned each paper relevant descriptors under each dimension. Finally, we defined a classification system, which we then used to uniquely and more precisely situate each paper on a map, taking into account the overall goals of each work. Based on our analyses, we present trends and challenges, including questions of gender and diversity in our community, and reflect on opportunities for future work.},
 address = {The University of Auckland, New Zealand},
 articleno = {8},
 author = {Kleinberger, R{\'{e}}becca and Singh, Nikhil and Xiao, Xiao and Troyer, Akito van},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.4308fb94},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/PUlGjAblfPM},
 title = {Voice at {NIME}: a Taxonomy of New Interfaces for Vocal Musical Expression},
 url = {https://doi.org/10.21428%2F92fbeb44.4308fb94},
 year = {2022}
}

@inproceedings{NIME22_9,
 PDF = {114.pdf},
 abstract = {Digital musical instruments (DMIs) built to be used in performance settings need to go beyond the prototypical stage of design to become robust, reliable, and responsive devices for extensive usage. This paper presents the Tapbox and the Slapbox, two generations of a standalone DMI built for percussion practice. After summarizing the requirements for performance DMIs from previous surveys, we introduce the Tapbox and comment on its strong and weak points. We then focus on the design process of the Slapbox, an improved version that captures a broader range of percussive gestures. Design tasks are reflected upon, including enclosure design, sensor evaluations, gesture extraction algorithms, and sound synthesis methods and mappings. Practical exploration of the Slapbox by two professional percussionists is performed and their insights summarized, providing directions for future work.},
 address = {The University of Auckland, New Zealand},
 articleno = {9},
 author = {Boettcher, Brady and Sullivan, John and Wanderley, Marcelo M.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.78fd89cc},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/NkYGAp4rmj8},
 title = {Slapbox: Redesign of a Digital Musical Instrument Towards Reliable Long-Term Practice},
 url = {https://doi.org/10.21428%2F92fbeb44.78fd89cc},
 year = {2022}
}

@inproceedings{NIME22_10,
 PDF = {115.pdf},
 abstract = {This paper presents Mapper4Live, a software plugin made for the popular digital audio workstation software Ableton Live. Mapper4Live exposes Ableton’s synthesis and effect parameters on the distributed libmapper signal mapping network, providing new opportunities for interaction between software and hardware synths, audio effects, and controllers. The plugin’s uses and relevance in research, music production and musical performance settings are explored, detailing the development journey and ideas for future work on the project.},
 address = {The University of Auckland, New Zealand},
 articleno = {10},
 author = {Boettcher, Brady and Malloch, Joseph and Wang, Johnty and Wanderley, Marcelo M.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.625fbdbf},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/Sv3v3Jmemp0},
 title = {Mapper4Live: Using Control Structures to Embed Complex Mapping Tools into Ableton Live},
 url = {https://doi.org/10.21428%2F92fbeb44.625fbdbf},
 year = {2022}
}

@inproceedings{NIME22_11,
 PDF = {120.pdf},
 abstract = {Music technology ensembles—often consisting of multiple laptops as the performers’ primary instrument— provide collaborative artistic experiences for electronic musicians. In an effort to remove the significant technical and financial barriers that laptops can present to performers looking to start their own group, this paper proposes a solution in the form of the Norns Shield, a computer music instrument (CMI) that requires minimal set-up and promotes immediate music-making to performers of all skill levels. Prior research centered on using alternative CMIs to supplant laptops in ensemble settings is discussed, and the benefits of adopting the Norns Shield in service of democratizing and diversifying the music technology ensemble are demonstrated in a discussion centered on the University of Texas Rio Grande Valley New Music Ensemble’s adoption of the instrument. A description of two software packages developed by the author showcases an extension of the instrument’s abilities to share collaborative control data between internet-enabled CMIs and to remotely manage script launching and parameter configuration across a group of Norns Shields, providing resources for ensembles interested in incorporating the device into their ranks.},
 address = {The University of Auckland, New Zealand},
 articleno = {11},
 author = {Marasco, Anthony T.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.89003700},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=2XixSYrgRuQ},
 title = {Approaching the Norns Shield as a Laptop Alternative for Democratizing Music Technology Ensembles},
 url = {https://doi.org/10.21428%2F92fbeb44.89003700},
 year = {2022}
}

@inproceedings{NIME22_12,
 PDF = {123.pdf},
 abstract = {In this article we present Bandoneon 2.0, an interdisciplinary project whose main objective is to produce electronic bandoneons in Argentina. The current prices of bandoneons and the scarcity of manufacturers are endangering the possibility of access for the new generations to one of the most emblematic instruments of the culture of this country. Therefore, we aim to create an expressive and accessible electronic bandoneon that can be used in recreational, academic and professional contexts, providing an inclusive response to the current sociocultural demand. The project also involves research on instrument acoustics and the development of specialized software and hardware tools.},
 address = {The University of Auckland, New Zealand},
 articleno = {12},
 author = {Ramos, Juan and Calcagno, Esteban and Vergara, Ramiro Oscar and Riera, Pablo and Rizza, Joaqu{\'{\i}}n},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.c38bfb86},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=5y4BbQWVNGQ},
 title = {Bandoneon 2.0: an interdisciplinary project for research and development of electronic bandoneons in Argentina},
 url = {https://doi.org/10.21428%2F92fbeb44.c38bfb86},
 year = {2022}
}

@inproceedings{NIME22_13,
 PDF = {125.pdf},
 abstract = {The On Board Call is a bespoke musical interface designed to engage the general public’s interest in wildlife sounds—such as bird, frog or animal calls—through imitation and interaction. The device is a handheld, battery-operated, microprocessor-based machine that synthesizes sounds using frequency modulation synthesis methods. It includes a small amplifier and loudspeaker for playback and employs an accelerometer and force sensor that register gestural motions that control sound parameters in real time. The device is handmade from off-the-shelf components onto a specially designed PCB and laser cut wooden boards. Development versions of the device have been tested in wildlife listening contexts and in location-based ensemble performance. The device is simple to use, compact and inexpensive to facilitate use in community-based active listening workshops intended to enhance user’s appreciation of the eco acoustic richness of natural environments. Unlike most of the previous work in wildlife call imitation, the Call does not simply play back recorded wildlife sounds, it is designed for performative interaction by a user to bring synthesized sounds to life and imbue them with expression.},
 address = {The University of Auckland, New Zealand},
 articleno = {13},
 author = {Brown, Andrew R.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.71a5a0ba},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=iBTBPpaSGi8},
 title = {On Board Call: A Gestural Wildlife Imitation Machine},
 url = {https://doi.org/10.21428%2F92fbeb44.71a5a0ba},
 year = {2022}
}

@inproceedings{NIME22_14,
 PDF = {126.pdf},
 abstract = {In a search for a symbiotic relationship between the digital and physical worlds, I am developing a hybrid, digital-acoustic wind instrument - the Post-Digital Sax. As the name implies, the instrument combines the advantages and flexibility of digital control with a hands-on physical interface and a non-orthodox means of sound production, in which the airflow, supplied by the player’s lungs, is the actual sound source. The pitch, however, is controlled digitally, allowing a wide range of musical material manipulation, bringing the possibilities of a digitally augmented performance into the realm of acoustic sound.},
 address = {The University of Auckland, New Zealand},
 articleno = {14},
 author = {Cybulski, Krzysztof},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.756616d4},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/RnuEvjMdEj4},
 title = {Post-digital sax - a digitally controlled acoustic single-reed woodwind instrument},
 url = {https://doi.org/10.21428%2F92fbeb44.756616d4},
 year = {2022}
}

@inproceedings{NIME22_15,
 PDF = {128.pdf},
 abstract = {An emerging approach to building new musical instruments is based on training neural networks to generate audio conditioned upon parametric input. We use the term "generative models" rather than "musical instruments" for the trained networks because it reflects the statistical way the instruments are trained to "model" the association between parameters and the distribution of audio data, and because "musical" carries historical baggage as a reference to a restricted domain of sound. Generative models are musical instruments in that they produce a prescribed range of sound playable through the expressive manipulation of an interface. To learn the mapping from interface to audio, generative models require large amounts of parametrically labeled audio data. This paper introduces the Synthetic Audio Textures (Syn- Tex1) collection of data set generators. SynTex is a database of parameterized audio textures and a suite of tools for creating and labeling datasets designed for training and testing generative neural networks for parametrically conditioned sound synthesis. While there are many existing labeled speech and traditional musical instrument databases available for training generative models, most datasets of general (e.g. environmental) audio are oriented and labeled for the purpose of classification rather than expressive musical generation. SynTex is designed to provide an open shareable reference set of audio for creating generative sound models including their interfaces. SynTex sound sets are synthetically generated. This facilitates dense and accurate labeling necessary for conditionally training generative networks conditionally dependent on input parameter values. SynTex has several characteristics designed to support a data-centric approach to developing, exploring, training, and testing generative models.},
 address = {The University of Auckland, New Zealand},
 articleno = {15},
 author = {Wyse, Lonce and Ravikumar, Prashanth Thattai},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.0fe70450},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/KZHXck9c75s},
 title = {Syntex: parametric audio texture datasets for conditional training of instrumental interfaces.},
 url = {https://doi.org/10.21428%2F92fbeb44.0fe70450},
 year = {2022}
}

@inproceedings{NIME22_16,
 PDF = {13.pdf},
 abstract = {This paper describes a toolkit for analyzing the NIME proceedings archive, which facilitates the bibliometric study of the conference papers and the identification of trends and patterns. The toolkit is implemented as a collection of Python methods that aggregate, scrape and retrieve various meta-data from published papers. Extracted data is stored in a large numeric table as well as plain text files. Analytical functions within the toolkit can be easily extended or modified. The text mining script that can be highly customized without the need for programming. The toolkit uses only publicly available information organized in standard formats, and is available as open-source software to promote continuous development in step with the NIME archive.},
 address = {The University of Auckland, New Zealand},
 articleno = {16},
 author = {Goode, Jackson and Fasciani, Stefano},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.58efca21},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/Awp5-oxL-NM},
 title = {A Toolkit for the Analysis of the {NIME} Proceedings Archive},
 url = {https://doi.org/10.21428%2F92fbeb44.58efca21},
 year = {2022}
}

@inproceedings{NIME22_17,
 PDF = {131.pdf},
 abstract = {This paper focuses on the redundancy and physicality of magnetic recording media as a defining factor in the design of a lo-fi audio device, the Concentric Sampler. A modified floppy disk drive (FDD) and additional circuitry enables the FDD to record to and playback audio from a 3.5” floppy disk. The Concentric Sampler is designed as an instrument for live performance and a tool for sonic manipulation, resulting in primitive looping and time-based granular synthesis. This paper explains the motivation and background of the Concentric Sampler, related applications and approaches, its technical realisation, and its musical possibilities. To conclude, the Concentric Sampler’s potential as an instrument and compositional tool is discussed alongside the future possibilities for development.},
 address = {The University of Auckland, New Zealand},
 articleno = {17},
 author = {Tate, Timothy},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.324729a3},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/7Myu1W7tbts},
 title = {The Concentric Sampler: A musical instrument from a repurposed floppy disk drive},
 url = {https://doi.org/10.21428%2F92fbeb44.324729a3},
 year = {2022}
}

@inproceedings{NIME22_18,
 PDF = {136.pdf},
 abstract = {Ghost Play is a violin-playing robot that aims to realize bowing and fingering similar to human players. Existing violin-playing machines have faced various problems concerning performance techniques owing to constraints imposed by their design. Bowing and fingering that require accurate and high-acceleration movement (e.g., a spiccato, tremolo, and glissando) are essential but challenging. To overcome this problem, Ghost Play is equipped with seven electromagnetic linear actuators, three for controlling the bow (i.e., the right hand), and the other four for controlling the pitch on each string (i.e., the left hand). The violin-playing robot is mounted with an unmodified violin bow. A sensor is attached to the bow to measure bow pressure. The control software receives a time series of performance data and manipulates the actuators accordingly. The performance data consists of the bow direction, bow speed, bow pressure, pitch, vibrato interval, vibrato width, and string to be drawn. We also developed an authoring tool for the performance data using a graphic user interface. Finally, we demonstrated Ghost Play performing bowing and fingering techniques such as a spiccato, tremolo, and glissando, as well as a piece of classical music.},
 address = {The University of Auckland, New Zealand},
 articleno = {18},
 author = {Kamatani, Takahiro and Sato, Yoshinao and Fujino, Masato},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.754a50b5},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/FOivgYXk1_g},
 title = {Ghost Play - A Violin-Playing Robot using Electromagnetic Linear Actuators},
 url = {https://doi.org/10.21428%2F92fbeb44.754a50b5},
 year = {2022}
}

@inproceedings{NIME22_19,
 PDF = {151.pdf},
 abstract = {Feedback is a technique that has been used in musical performance since the advent of electricity. From the early cybernetic explorations of Bebe and Louis Barron, through the screaming sound of Hendrix’s guitar, to the systems design of David Tudor or Nic Collins, we find the origins of feedback in music being technologically and aesthetically diverse. Through interviews with participants in a recent Feedback Musicianship Network symposium, this paper seeks to investigate the contemporary use of this technique and explore how key protagonists discuss the nature of their practice. We see common concepts emerging in these conversations: agency, complexity, coupling, play, design and posthumanism. The paper presents a terminological and ideological framework as manifested at this point in time, and makes a theoretical contribution to the understanding of the rationale and potential of this technological and compositional approach.},
 address = {The University of Auckland, New Zealand},
 articleno = {19},
 author = {Magnusson, Thor and Kiefer, Chris and Ulfarsson, Halldor},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.aa7de712},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=ouwIA_aVmEM},
 title = {Reflexions upon Feedback},
 url = {https://doi.org/10.21428%2F92fbeb44.aa7de712},
 year = {2022}
}

@inproceedings{NIME22_20,
 PDF = {16.pdf},
 abstract = {Beginning, amateur, and professional violinists alike make use of a shoulder rest with a typical form factor for ergonomic support. Numerous commercial devices are available. We saturate these inert devices with electronics and actuators to open a new design space for “active shoulder rests” (ASRs), a pathway for violinists to adopt inexpensive and transparent electroacoustic interfaces. We present a dual-mode ASR that features a built-in microphone pickup and parametric control of mixing between sound diffusion and actuation modes for experiments with active acoustics and feedback. We document a modular approach to signal processing allowing quick adaptation and differentiation of control signals, and demonstrate rich sound processing techniques that create lively improvisation environments. By fostering participation and convergence among digital media practices and diverse musical cultures, we envision ASRs broadly rekindling creative practice for the violin, long a tool of improvisation before the triumph of classical works. ASRs decolonize the violin by activating new flows and connectivities, freeing up habitual relations, and refreshing the musical affordances of this otherwise quintessentially western and canonical instrument.},
 address = {The University of Auckland, New Zealand},
 articleno = {20},
 author = {Thorn, Seth and Lahey, Byron},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.91f87875},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/7qNTa4QplC4},
 title = {Decolonizing the Violin with Active Shoulder Rests ({ASRs})},
 url = {https://doi.org/10.21428%2F92fbeb44.91f87875},
 year = {2022}
}

@inproceedings{NIME22_21,
 PDF = {162.pdf},
 abstract = {Augmented reality (AR) is increasingly being envisaged as a process of perceptual mediation or modulation, not only as a system that combines aligned and interactive virtual objects with a real environment. Within artistic practice, this reconceptualisation has led to a medium that emphasises this multisensory integration of virtual processes, leading to expressive, narrative-driven, and thought-provoking AR experiences. This paper outlines the development and evaluation of the polaris~ experience. polaris~ is built using a set of open-source hardware and software components that can be used to create privacy-respecting and cost-effective audiovisual AR experiences. Its wearable component is comprised of the open-source Project North Star AR headset and a pair of bone conduction headphones, providing simultaneous real and virtual visual and auditory elements. These elements are spatially aligned using Unity and PureData to the real space that they appear in and can be gesturally interacted with in a way that fosters artistic and musical expression. In order to evaluate the polaris~, 10 participants were recruited, who spent approximately 30 minutes each in the AR scene and were interviewed about their experience. Using grounded theory, the author extracted coded remarks from the transcriptions of these studies, that were then sorted into the categories of Sentiment, Learning, Adoption, Expression, and Immersion. In evaluating polaris~ it was found that the experience engaged participants fruitfully, with many noting their ability to express themselves audiovisually in creative ways. The experience and the framework the author used to create it is available in a Github respository.},
 address = {The University of Auckland, New Zealand},
 articleno = {21},
 author = {Bilbow, Sam},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.8abb9ce6},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=eCdQku5hFOE},
 title = {Evaluating polaris{\textasciitilde} - An Audiovisual Augmented Reality Experience Built on Open-Source Hardware and Software},
 url = {https://doi.org/10.21428%2F92fbeb44.8abb9ce6},
 year = {2022}
}

@inproceedings{NIME22_22,
 PDF = {165.pdf},
 abstract = {This paper presents the MappEMG pipeline. The goal of this pipeline is to augment the traditional classical concert experience by giving listeners access, through the sense of touch, to an intimate and non-visible dimension of the musicians’ bodily experience while performing. The live-stream pipeline produces vibrations based on muscle activity captured through surface electromyography (EMG). Therefore, MappEMG allows the audience to experience the performer’s muscle effort, an essential component of music performance which is typically unavailable to direct visual observation. The paper is divided in four sections. First, we overview related works on EMG, music performance, and vibrotactile feedback. We then present conceptual and methodological issues of capturing musicians’ muscle effort related to their expressive intentions. We further explain the different components of the live-stream data pipeline: a python software named Biosiglive for data acquisition and processing, a Max/MSP patch for data post-processing and mapping, and a mobile application named hAPPtiks for real-time control of smartphones’ vibration. Finally, we address the application of the pipeline in an actual music performance. Thanks to their modular structure, the tools presented could be used in different creative and biomedical contexts involving gestural control of haptic stimuli.},
 address = {The University of Auckland, New Zealand},
 articleno = {22},
 author = {Verdugo, Felipe and Ceglia, Amedeo and Frisson, Christian and Burton, Alexandre and Begon, Mickael and Gibet, Sylvie and Wanderley, Marcelo M.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.3ce22588},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/gKM0lGs9rxw},
 title = {Feeling the Effort of Classical Musicians - A Pipeline from Electromyography to Smartphone Vibration for Live Music Performance},
 url = {https://doi.org/10.21428%2F92fbeb44.3ce22588},
 year = {2022}
}

@inproceedings{NIME22_23,
 PDF = {172.pdf},
 abstract = {ForceHost is an opensource toolchain for generating firmware that hosts authoring and rendering of forcefeedback and audio signals and that communicates through I2C with guest motor and sensor boards. With ForceHost, the stability of audio and haptic loops is no longer delegated to and dependent on operating systems and drivers, and devices remain discoverable beyond planned obsolescence. We modified Faust, a highlevel language and compiler for real-time audio digital signal processing, to support haptics. Our toolchain compiles audio-haptic firmware applications with Faust and embeds web-based UIs exposing their parameters. We validate our toolchain by example applications and modifications of integrated development environments: script-based programming examples of haptic firmware applications with our haptic1D Faust library, visual programming by mapping input and output signals between audio and haptic devices in Webmapper, visual programming with physically-inspired mass-interaction models in Synth-a-Modeler Designer. We distribute the documentation and source code of ForceHost and all of its components and forks.},
 address = {The University of Auckland, New Zealand},
 articleno = {23},
 author = {Frisson, Christian and Kirkegaard, Mathias and Pietrzak, Thomas and Wanderley, Marcelo M.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.76cfc96e},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/smFpkdw-J2w},
 title = {{ForceHost}: an open-source toolchain for generating firmware embedding the authoring and rendering of audio and force-feedback haptics},
 url = {https://doi.org/10.21428%2F92fbeb44.76cfc96e},
 year = {2022}
}

@inproceedings{NIME22_24,
 PDF = {173.pdf},
 abstract = {Physical metaphor provides a visceral and universal logical framework for composing musical gestures. Physical simulations can aid composers in creating musical gestures based in complex physical metaphors. CHON (Coupled Harmonic Oscillator Network) is a new crossplatform application for composing musical gestures based in Newtonian physics. It simulates a network of particles connected by springs and sonifies the motion of individual particles. CHON is an interactive instrument that can provide complex yet tangible and physically grounded control data for synthesis, sound processing, and musical score generation. Composers often deploy dozens of independent LFOs to control various parameters in a DAW or synthesizer. By coupling numerous control signals together using physical principles, CHON represents an innovation on the traditional LFO model of musical control. Unlike independent LFOs, CHON’s signals push and pull on each other, creating a tangible causality in the resulting gestures. In this paper, I briefly describe the design of CHON and discuss its use in composition through examples in my own works.},
 address = {The University of Auckland, New Zealand},
 articleno = {24},
 author = {DuPlessis, Rodney},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.18aeca0e},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/yXr1m6dW5jo},
 title = {A virtual instrument for physics-based musical gesture: {CHON}},
 url = {https://doi.org/10.21428%2F92fbeb44.18aeca0e},
 year = {2022}
}

@inproceedings{NIME22_25,
 PDF = {176.pdf},
 abstract = {This paper describes the development of CAVI, a coadaptive audiovisual instrument for collaborative humanmachine improvisation. We created this agent-based live processing system to explore how a machine can interact musically based on a human performer’s bodily actions. CAVI utilized a generative deep learning model that monitored muscle and motion data streamed from a Myo armband worn on the performer’s forearm. The generated control signals automated layered time-based effects modules and animated a virtual body representing the artificial agent. In the final performance, two expert musicians (a guitarist and a drummer) performed with CAVI. We discuss the outcome of our artistic exploration, present the scientific methods it was based on, and reflect on developing an interactive system that is as much an audiovisual composition as an interactive musical instrument.},
 address = {The University of Auckland, New Zealand},
 articleno = {25},
 author = {Erdem, Cagri and Wallace, Benedikte and Refsum Jensenius, Alexander},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.803c24dd},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/WO766vmghcQ},
 title = {{CAVI}: A Coadaptive Audiovisual Instrument{\textendash}Composition},
 url = {https://doi.org/10.21428%2F92fbeb44.803c24dd},
 year = {2022}
}

@inproceedings{NIME22_26,
 PDF = {179.pdf},
 abstract = {The authors introduce and document how to build the t-Tree, a digital musical instrument (DMI), interactive music system (IMS), hub, and docking station that embeds several t-Sticks. The t-Tree’s potential for collaborative performance as well as an installation is discussed. Specific design choices and inspiration for the t-Tree are explored. Finally, a prototype is developed and showcased that attempts to meet the authors’ goals of creating a novel musical experience for musicians and non-musicians alike, expanding on the premise of the original t-Stick, and mitigating technical obsolescence of DMIs.},
 address = {The University of Auckland, New Zealand},
 articleno = {26},
 author = {Kirby, Linnea and Buser, Paul and Wanderley, Marcelo M.},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.2d00f04f},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/gS87Tpg3h_I},
 title = {Introducing the t-Tree: Using Multiple t-Sticks for Performance and Installation},
 url = {https://doi.org/10.21428%2F92fbeb44.2d00f04f},
 year = {2022}
}

@inproceedings{NIME22_27,
 PDF = {183.pdf},
 abstract = {We present an empirical study of designing a NIME for the head-mounted augmented reality (HMAR) environment. In the NIME community, various sonic applications have incorporated augmented reality (AR) for sonic experience and audio production. With this novel digital form, new opportunities for musical expression and interface are presented. Yet few works consider whether and how the design of the NIME will be affected given the technology’s affordance. In this paper, we take an autobiographical design approach to design a NIME in HMAR, exploring what is a genuine application of AR in a NIMEs and how AR mediates between the performer and sound as a creative expression. Three interface prototypes are created for a frequency modulation synthesis system. We report on their design process and our learning and experiences through self-usage and improvisation. Our designs explore free-hand and embodied interaction in our interfaces, and we reflect on how these unique qualities of HMAR contribute to an expressive medium for sonic creation.},
 address = {The University of Auckland, New Zealand},
 articleno = {27},
 author = {Wang, Yichen and Martin, Charles},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.b540aa59},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/iOuZqwIwinU},
 title = {Cubing Sound: Designing a {NIME} for Head-mounted Augmented Reality},
 url = {https://doi.org/10.21428%2F92fbeb44.b540aa59},
 year = {2022}
}

@inproceedings{NIME22_28,
 PDF = {191.pdf},
 abstract = {We describe a new set of affordances for networked live coding performances in the browser-based environment Gibber, and discuss their implications in the context of three different performances by three different ensembles at three universities. Each ensemble possessed differing levels of programming and musical expertise, leading to different challenges and subsequent extensions to Gibber to address them. We describe these and additional extensions that came about after shared reflection on our experiences. While our chosen design contains computational inefficiencies that pose challenges for larger ensembles, our experiences suggest that this is a reasonable tradeoff for the low barrier-to-entry that browser-based environments provide, and that the design in general supports a variety of educational goals and compositional strategies.},
 address = {The University of Auckland, New Zealand},
 articleno = {28},
 author = {Roberts, Charlie and Hattwick, Ian and Sheffield, Eric and Smith, Gillian},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.38cb7745},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/BKlHkEAqUOo},
 title = {Rethinking networked collaboration in the live coding environment Gibber},
 url = {https://doi.org/10.21428%2F92fbeb44.38cb7745},
 year = {2022}
}

@inproceedings{NIME22_29,
 PDF = {193.pdf},
 abstract = {In the context of immersive sonic interaction, Virtual Reality Musical Instruments have had the relative majority of attention thus far, fueled by the increasing availability of affordable technology. Recent advances in Mixed Reality (MR) experiences have provided the means for a new wave of research that goes beyond Virtual Reality. In this paper, we explore the taxonomy of Extended Reality systems, establishing our own notion of MR. From this, we propose a new classification of Virtual Musical Instrument, known as a Mixed Reality Musical Instrument (MRMI). We define this system as an embodied interface for expressive musical performance, characterized by the relationships between the performer, the virtual, and the physical environment. After a review of existing literature concerning the evaluation of immersive musical instruments and the affordances of MR systems, we offer a new framework based on three dimensions to support the design and analysis of MRMIs. We illustrate its use with application to existing works.},
 address = {The University of Auckland, New Zealand},
 articleno = {29},
 author = {Zellerbach, Karitta Christina and Roberts, Charlie},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.b2a44bc9},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/Pb4pAr2v4yU},
 title = {A Framework for the Design and Analysis of Mixed Reality Musical Instruments},
 url = {https://doi.org/10.21428%2F92fbeb44.b2a44bc9},
 year = {2022}
}

@inproceedings{NIME22_30,
 PDF = {201.pdf},
 abstract = {NIME has recently seen critique emerging around colonisation of music technology, and the need to decolonise digital audio workstations and music software. While commercial DAWs tend to sideline musical styles outside of western norms (and even many inside too), viewing this problem through an historical lens of imperialist legacies misses the influence of a more recent - and often invisible - hegemony that bears significant direct responsibility: The culture of technological development. In this paper we focus on the commercial technological development culture that produces these softwares, to better understand the more latent reasons why music production software ends up supporting some music practices while failing others. By using this lens we can more meaningfully separate the influence of historic cultural colonisation and music tech development culture, in order to better advocate for and implement meaningful change. We will discuss why the meaning of the term “decolonisation” should be carefully examined when addressing the limitations of DAWs, because while larger imperialist legacies continue to have significant impact on our understanding of culture, this can direct attention away from the techno-cultural subset of this hegemony that is actively engaged in making the decisions that shape the software we use. We discuss how the conventions of this techno-cultural hegemony shape the affordances of major DAWs (and thereby musical creativity). We also examine specific factors that impact decision making in developing and evolving typical music software alongside latent social structures, such as competing commercial demands, how standards are shaped, and the impact of those standards. Lastly, we suggest that, while we must continue to discuss the impact of imperialist legacies on the way we make music, understanding the techno-cultural subset of the colonial hegemony and its motives can create a space to advocate for conventions in music software that are more widely inclusive.},
 address = {The University of Auckland, New Zealand},
 articleno = {30},
 author = {Pardue, Laurel and Bin, S. M. Astrid},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.0cc78aeb},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=a53vwOUDh0M},
 title = {The Other Hegemony: Effects of software development culture on music software, and what we can do about it},
 url = {https://doi.org/10.21428%2F92fbeb44.0cc78aeb},
 year = {2022}
}

@inproceedings{NIME22_31,
 PDF = {21.pdf},
 abstract = {Latin American (LATAM) contributions to Music Technology date back to the early 1940’s. However, as evidenced in historical analyses of NIME, the input from LATAM institutions to its proceedings is considerably low, even when the conference was recently held in Porto Alegre, Brazil. Reflecting on this visible disparity and joining efforts as a group of LATAM researchers, we conducted a workshop and distributed a survey with members of the LATAM community with the aim of
sounding out their perspectives on NIME-related practices and the prospect of establishing a LATAM NIME Network. Based on our findings we provide a contemporary contextual overview of the activities happening in
LATAM and the particular challenges that practitioners face emerging from their socio-political reality. We also offer LATAM perspectives on critical epistemological issues that affect the NIME community as a whole, contributing to a pluriversal view on these matters, and to the embracement of multiple realities and ways of doing things.},
 address = {The University of Auckland, New Zealand},
 articleno = {31},
 author = {Martinez Avila, Juan Pablo and Tragtenberg, Jo{\=a}o and Calegario, Filipe and Alarcon, Ximena and Cadavid Hinojosa, Laddy Patricia and Corintha, Isabela and Dannemann, Teodoro and Jaimovich, Javier and Marquez-Borbon, Adnan and Lerner, Martin Matus and Ortiz, Miguel and Ramos, Juan and Sol{\'{i}}s Garc{\'{i}}a, Hugo},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.b7a7ba4f},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/dCxkrqrbM-M},
 title = {Being (A)part of NIME: Embracing Latin American Perspectives},
 url = {https://doi.org/10.21428/92fbeb44.b7a7ba4f},
 year = {2022}
}

@inproceedings{NIME22_32,
 PDF = {222.pdf},
 abstract = {This article provides a lens for viewing technology as land, transformed through resource extraction, manufacturing, distribution, disassembly and waste. This lens is applied to processes of artistic creation with technology, exploring ways of fostering personal and informed relationships with that technology. The goal of these explorations will be to inspire a greater awareness of the colonial and capitalist processes that shape the technology we use and the land and people it is in relationship with. Beyond simply identifying the influence of these colonial and capitalist processes, the article will also provide creative responses (alterations to a creative process with technology) which seek to address these colonial processes in a sensitive and critical way. This will be done not to answer the broad question of ‘how do we decolonise art making with technology?’, but to break that question apart into prompts or potential pathways for decolonising.},
 address = {The University of Auckland, New Zealand},
 articleno = {32},
 author = {Argabrite, Zak and Murphy, Jim and Norman, Sally Jane and Carnegie, Dale},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.68f7c268},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/JZTmiIByYN4},
 title = {Technology is Land: Strategies towards decolonisation of technology in artmaking},
 url = {https://doi.org/10.21428%2F92fbeb44.68f7c268},
 year = {2022}
}

@inproceedings{NIME22_33,
 PDF = {26.pdf},
 abstract = {The following paper presents L2Ork Tweeter, a new control-data-driven free and open source crowdsourced telematic musicking platform and a new interface for musical expression that deterministically addresses three of the greatest challenges associated with the telematic music medium, that of latency, sync, and bandwidth. Motivated by the COVID-19 pandemic, Tweeter’s introduction in April 2020 has ensured uninterrupted operation of Virginia Tech’s Linux Laptop Orchestra (L2Ork), resulting in 6 international performances over the past 18 months. In addition to enabling tightly-timed sync between clients, it also uniquely supports all stages of NIME-centric telematic musicking, from collaborative instrument design and instruction, to improvisation, composition, rehearsal, and performance, including audience participation. Tweeter is also envisioned as a prototype for the crowdsourced approach to telematic musicking. Below, the paper delves deeper into motivation, constraints, design and implementation, and the observed impact as an applied instance of a proposed paradigmshift in telematic musicking and its newfound identity fueled by the live crowdsourced telematic music genre.},
 address = {The University of Auckland, New Zealand},
 articleno = {33},
 author = {Bukvic, Ivica},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.a0a8d914},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/5pawphncSmg},
 title = {Latency-, Sync-, and Bandwidth-Agnostic Tightly-Timed Telematic and Crowdsourced Musicking Made Possible Using L2Ork Tweeter},
 url = {https://doi.org/10.21428%2F92fbeb44.a0a8d914},
 year = {2022}
}

@inproceedings{NIME22_34,
 PDF = {30.pdf},
 abstract = {In this paper we propose a Spatial Augmented Reality interface for actuated acoustic instruments with active vibration control. We adopt a performance-led research approach to design augmentations throughout multiple residences. The resulting system enables two musicians to improvise with four augmented instruments through virtual shapes distributed in their peripheral space: two 12-string guitars and 1 drum kit actuated with surface speakers and a trumpet attached to an air compressor. Using ethnographic methods, we document the evolution of the augmentations and conduct a thematic analysis to shine a light on the collaborative and iterative design process. In particular, we provide insights on the opportunities brought by Spatial AR and on the role of improvisation.},
 address = {The University of Auckland, New Zealand},
 articleno = {34},
 author = {Arslan, Cagan and Berthaut, Florent and Beuchey, Anthony and Cambourian, Paul and Pat{\'{e}}, Arthur},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.c28dd323},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/oxMrv3R6jK0},
 title = {Vibrating shapes : Design and evolution of a spatial augmented reality interface for actuated instruments},
 url = {https://doi.org/10.21428%2F92fbeb44.c28dd323},
 year = {2022}
}

@inproceedings{NIME22_35,
 PDF = {31.pdf},
 abstract = {Digital Musical Instruments (DMIs) offer new opportunities for collaboration, such as exchanging sounds or sharing controls between musicians. However, in the context of spontaneous and heterogeneous orchestras, such as jam sessions, collective music-making may become challenging due to the diversity and complexity of the DMIs and the musicians’ unfamiliarity with the others’ instruments. In particular, the potential lack of visibility into each musician’s respective contribution to the sound they hear, i.e. who is playing what, might impede their capacity to play together. In this paper, we propose to augment each instrument in a digital orchestra with visual feedback extracted in real-time from the instrument’s activity, in order to increase this awareness. We present the results of a user study in which we investigate the influence of visualisation level and situational visibility during short improvisations by groups of three musicians. Our results suggest that internal visualisations of all instruments displayed close to each musician’s instrument provide the best awareness.},
 address = {The University of Auckland, New Zealand},
 articleno = {35},
 author = {Berthaut, Florent and Dahl, Luke},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.9d974714},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=903cs_oFfwo},
 title = {The Effect of Visualisation Level and Situational Visibility in Co-located Digital Musical Ensembles},
 url = {https://doi.org/10.21428%2F92fbeb44.9d974714},
 year = {2022}
}

@inproceedings{NIME22_36,
 PDF = {32.pdf},
 abstract = {The management of the musical structures and the awareness of the performer’s processes during a performance are two important aspects of live coding improvisations. To support these aspects, we developed and evaluated two systems, Time_X and Time_Z, for visualizing the musical form during live coding. Time_X allows visualizing an entire performance, while Time_Z provides a detailed overview of the last improvised musical events. Following an autobiographical approach, the two systems have been used in five sessions by the first author of this paper, who created a diary about the experience. These diaries have been analyzed to understand the two systems individually and compare them. We finally discuss the main benefits related to the practical use of these systems, and possible use scenarios.},
 address = {The University of Auckland, New Zealand},
 articleno = {36},
 author = {R{\`i}, Francesco Ardan Dal and Masu, Raul},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.828b6114},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=r-cxEXjnDzg},
 title = {Exploring Musical Form: Digital Scores to Support Live Coding Practice},
 url = {https://doi.org/10.21428%2F92fbeb44.828b6114},
 year = {2022}
}

@inproceedings{NIME22_37,
 PDF = {33.pdf},
 abstract = {In this paper, we discuss our ongoing work to leverage virtual reality and digital fabrication to investigate sensory mappings across the visual, auditory, and haptic modalities in VR, and how such mappings can affect musical expression in this medium. Specifically, we introduce a custom adapter for the Oculus Touch controller that allows it to be augmented with physical parts that can be tracked, visualized, and sonified in VR. This way, a VR instrument can be made to have a physical manifestation that facilitates additional forms of tactile feedback besides those offered by the Touch controller, enabling new forms of musical interaction. We then discuss a case study, where we use the adapter to implement a new VR instrument that integrates the repelling force between neodymium magnets into the controllers. This allows us to imbue the virtual instrument, which is inherently devoid of tactility, with haptic feedback—-an essential affordance of many musical instruments.},
 address = {The University of Auckland, New Zealand},
 articleno = {37},
 author = {{\c{C}}amci, Anil and Granzow, John},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.a26a4014},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/fnoQOO4rz4M},
 title = {Augmented Touch: A Mounting Adapter for Oculus Touch Controllers that Enables New Hyperreal Instruments},
 url = {https://doi.org/10.21428%2F92fbeb44.a26a4014},
 year = {2022}
}

@inproceedings{NIME22_38,
 PDF = {34.pdf},
 abstract = {Automated processes in musical instruments can serve to free a performer from the physical and mental constraints of music performance, allowing them to expressively control more aspects of music simultaneously. Modular synthesis has been a prominent platform for exploring automation through the use of sequencers and has therefore fostered a tradition of user interface design utilizing increasingly complex abstraction methods. We investigate the history of sequencer design from this perspective and introduce machine learning as a potential source for a new type of intelligent abstraction. We then offer a case study based on this approach and present Latent Drummer, which is a prototype system dedicated to integrating machine learning-based interface abstractions into the tradition of sequencers for modular synthesis.},
 address = {The University of Auckland, New Zealand},
 articleno = {38},
 author = {Warren, Nick and {\c{C}}amci, Anil},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.ed873363},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=Hr6B5dIhMVo},
 title = {Latent Drummer: A New Abstraction for Modular Sequencers},
 url = {https://doi.org/10.21428%2F92fbeb44.ed873363},
 year = {2022}
}

@inproceedings{NIME22_39,
 PDF = {39.pdf},
 abstract = {We present an AI-empowered music tutor with a systematic curriculum design. The tutoring system fully utilizes the interactivity space in the auditory, visual, and haptic modalities, supporting seven haptic feedback modes and four visual feedback modes. The combinations of those modes form different cross-modal tasks of varying difficulties, allowing the curriculum to apply the “scaffolding then fading” educational technique to foster active learning and amortize cognitive load. We study the effect of multimodal instructions, guidance, and feedback using a qualitative pilot study with two subjects over ~11 hours of training with our tutoring system. The study reveals valuable insights about the music learning process and points towards new features and learning modes for the next prototype.},
 address = {The University of Auckland, New Zealand},
 articleno = {39},
 author = {Chin, Daniel and Xia, Gus},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.c6910363},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/DifOKvH1ErQ},
 title = {A Computer-aided Multimodal Music Learning System with Curriculum: A Pilot Study},
 url = {https://doi.org/10.21428%2F92fbeb44.c6910363},
 year = {2022}
}

@inproceedings{NIME22_40,
 PDF = {42.pdf},
 abstract = {Movement-sound interactive systems are at the interface of different artistic and educational practices. Within this multiplicity of uses, we examine common denominators in terms of learning, appropriation and relationship to technological systems. While these topics have been previously reported at NIME, we wanted to investigate how practitioners, coming from different perspectives, relate to these questions. We conducted interviews with 6 artists who are engaged in movement-sound interactions: 1 performer, 1 performer/composer, 1 composer, 1 teacher/composer, 1 dancer/teacher, 1 dancer. Through a thematic analysis of the transcripts we identified three main themes related to (1) the mediating role of technological tools (2) usability and normativity, and (3) learning and practice. These results provide ground for discussion about the design and study of movementsound interactive systems.},
 address = {The University of Auckland, New Zealand},
 articleno = {40},
 author = {Paredes, Victor and Fran{\c{c}}oise, Jules and Bevilacqua, Frederic},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.5b9ac5ba},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/n6DZE7TdEeI},
 title = {Entangling Practice with Artistic and Educational Aims: Interviews on Technology-based Movement-Sound Interactions},
 url = {https://doi.org/10.21428%2F92fbeb44.5b9ac5ba},
 year = {2022}
}

@inproceedings{NIME22_41,
 PDF = {43.pdf},
 abstract = {The Web MIDI API allows the Web browser to interact with hardware and software MIDI devices detected at the operating system level. This ability for the browser to interface with most electronic instruments made in the past 30 years offers significant opportunities to preserve, enhance or re-discover a rich musical and technical heritage. By including MIDI in the broaderWeb ecosystem, this API also opens endless possibilities to create music in a networked and socially engaging way. However, the Web MIDI API specification only offers low-level access to MIDI devices and messages. For instance, it does not provide semantics on top of the raw numerical messages exchanged between devices. This is likely to deter novice programmers and significantly slow down experienced programmers. After reviewing the usability of the bare Web MIDI API, the WEBMIDI. js JavaScript library was created to alleviate this situation. By decoding raw MIDI messages, encapsulating complicated processes and providing semantically significant objects, properties, methods and events, the library makes it easier to interface with MIDI devices from compatible browsers. This paper first looks at the context in which the specification was created and then discusses the usability improvements layered on top of the API by the opensource WEBMIDI.js library.},
 address = {The University of Auckland, New Zealand},
 articleno = {41},
 author = {C{\^o}t{\'e}, Jean-Philippe},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.388e4764},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/jMzjpUJO860},
 title = {User-Friendly {MIDI} in the Web Browser},
 url = {https://doi.org/10.21428%2F92fbeb44.388e4764},
 year = {2022}
}

@inproceedings{NIME22_42,
 PDF = {53.pdf},
 abstract = {In the search for better designs, one tool is to specify the design problem such that globally optimal solutions can be found. I present a design process using this approach, its strengths and limitations, and its results in the form of four pitch fingering systems that are ergonomic, simple, and symmetric. In hindsight, I emphasize the subjectivity of the design process, despite its reliance on objective quantitative assessment.},
 address = {The University of Auckland, New Zealand},
 articleno = {42},
 author = {West, Travis},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.d6c9dcae},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/4QB3sNRmK1E},
 title = {Pitch Fingering Systems and the Search for Perfection},
 url = {https://doi.org/10.21428%2F92fbeb44.d6c9dcae},
 year = {2022}
}

@inproceedings{NIME22_43,
 PDF = {54.pdf},
 abstract = {The mubone (lowercase “m”) is a family of instruments descended from the trombone family, a conceptual design space for trombone augmentations, and a growing musical practice rooted in this design space and the artistic affordances that emerge from it. We present the design of the mubone and discuss our initial implementations. We then reflect on the beginnings of an artistic practice: playing mubone, as well as exploring how the instrument adapts to diverse creative contexts. We discuss mappings, musical exercises, and the development of Garcia, a sound-and-movement composition for mubone.},
 address = {The University of Auckland, New Zealand},
 articleno = {43},
 author = {West, Travis and Leung, Kalun},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.e56a93c9},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/B51eofO4f4Y},
 title = {early prototypes and artistic practice with the mubone},
 url = {https://doi.org/10.21428%2F92fbeb44.e56a93c9},
 year = {2022}
}

@inproceedings{NIME22_44,
 PDF = {59.pdf},
 abstract = {The study of extended reality musical instruments is a burgeoning topic in the field of new interfaces for musical expression. We developed a mixed reality musical interface (MRMI) as a technology probe to inspire design for experienced musicians. We namely explore (i) the ergonomics of the interface in relation to musical expression and (ii) user-adaptive hand pose recognition as gestural control. The MRMI probe was experienced by 10 musician participants (mean age: 25.6 years [SD=3.0], 6 females, 4 males). We conducted a user evaluation comprising three stages. After an experimentation period, participants were asked to accompany a pre-recorded piece of music. In a post-task stage, participants took part in semi-structured interviews, which were subjected to thematic analysis. Prevalent themes included reducing the size of the interface, issues with the field of view of the device and physical strain from playing. Participants were largely in favour of hand poses as expressive control, although this depended on customisation and temporal dynamics; the use of interactive machine learning (IML) for user-adaptive hand pose recognition was well received by participants.},
 address = {The University of Auckland, New Zealand},
 articleno = {44},
 author = {Graf, Max and Barthet, Mathieu},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.56ba9b93},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/qhE5X3rAWgg},
 title = {Mixed Reality Musical Interface: Exploring Ergonomics and Adaptive Hand Pose Recognition for Gestural Control},
 url = {https://doi.org/10.21428%2F92fbeb44.56ba9b93},
 year = {2022}
}

@inproceedings{NIME22_45,
 PDF = {64.pdf},
 abstract = {Active participation of Deaf individuals in the design and performance of artistic practice benefits increasing collaboration potentials between Deaf and hearing individuals. In this research, we present co-design sessions with a Deaf dancer and a hearing musician to explore how they can influence each other’s expressive explorations. We also study vibrotactile wearable interface designs to better support the Deaf dancer’s perception of sound and music. We report our findings and observations on the co-design process over four workshops and one performance and public demonstration session. We detail the design and implementation of the wearable vibrotactile listening garment and participants’ selfreported experiences. This interface provides participants with more embodied listening opportunities and felt experiences of sound and music. All participants reported that the listening experience highlighted their first-person experience, focusing on their bodies, "regardless of an observer". These findings show how we can improve both such an internal experience of the listener and the collaboration potential between performers for increased inclusion. Overall, this paper addresses two different modalities of haptic feedback, the participation of Deaf users in wearable haptics design as well as music-movement performance practice, and artistic co-creation beyond technology development.},
 address = {The University of Auckland, New Zealand},
 articleno = {45},
 author = {Cavdir, Doga},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.b24043e8},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/tuSo2Sq7jy4},
 title = {Touch, Listen, (Re)Act: Co-designing Vibrotactile Wearable Instruments for Deaf and Hard of Hearing},
 url = {https://doi.org/10.21428%2F92fbeb44.b24043e8},
 year = {2022}
}

@inproceedings{NIME22_46,
 PDF = {65.pdf},
 abstract = {While the value of new digital musical instruments lies to a large extent in their music-making capacity, analyses of new instruments in the research literature often focus on analyses of gesture or performer experience rather than the content of the music made with the instrument. In this paper we present a motivic analysis of music made with new instruments. In the context of music, a motive is a small, analysable musical fragment or phrase that is important in or characteristic of a composition. We outline our method for identifying and analysing motives in music made with new instruments, and display its use in a case study in which 10 musicians created performances with a new large-scale digital musical instrument that we designed. This research illustrates the value of a musicological approach to NIME research, suggesting the need for a broader conversation about a musicology of NIME performances, as distinct from its instruments.},
 address = {The University of Auckland, New Zealand},
 articleno = {46},
 author = {Mice, Lia and McPherson, Andrew},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.8c1c9817},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/nXrRJGt11J4},
 title = {The M in {NIME}: Motivic analysis and the case for a musicology of {NIME} performances},
 url = {https://doi.org/10.21428%2F92fbeb44.8c1c9817},
 year = {2022}
}

@inproceedings{NIME22_47,
 PDF = {66.pdf},
 abstract = {While it is accepted that accessible digital musical instruments (ADMIs) should be created with the involvement of targeted communities, participatory design (PD) is an unsettled practice that gets defined variously, loosely or not at all. In this paper, we explore the concept of dialogic design and provide a case study of how it can be used in the design of an ADMI. While a future publication will give detail of the design of this instrument and provide an analysis of the data from this study, in this paper we set out how the conversations between researcher and participant have prepared us to build an instrument that responds to the lived experience of the participant.},
 address = {The University of Auckland, New Zealand},
 articleno = {47},
 author = {Zayas-Garin, Eevee and McPherson, Andrew},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.2b8ce9a4},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=8l1N3G0BdKw},
 title = {Dialogic Design of Accessible Digital Musical Instruments: Investigating Performer Experience},
 url = {https://doi.org/10.21428%2F92fbeb44.2b8ce9a4},
 year = {2022}
}

@inproceedings{NIME22_48,
 PDF = {68.pdf},
 abstract = {To the naked ear, the installation Being With The Waves appears silent, but a hidden composition of voices, instrumental tones, and maritime sounds is revealed through wearing modified headphones. The installation consists of an array of tweeters emitting a multi-channel ultrasonic composition that sounds physically in the space. Ultrasonic phenomena present at the listener’s ears are captured by microphones embedded on the outside of headphone earcups, shifted into audibility, and output to the headphones. The amplitude demodulation of ultrasonic material results in exaggerated Doppler effects and listeners hear the music bend and shift precisely with their movement. There are no movement sensors, mappings, or feedback loops, yet the installation is perceived as interactive due to the close entanglement of the listener with sound phenomena. The dynamic quality of interaction emerges solely through the listening faculties of the visitor, as an embodied sensory experience determined by their orientation to sounds, physical movement, and perceptual behaviour. This paper describes key influences on the installation, its ultrasonic technology, the design of modified headphones, and the compositional approach.},
 address = {The University of Auckland, New Zealand},
 articleno = {48},
 author = {Robson, Nicole and McPherson, Andrew and Bryan-Kinns, Nick},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.376bc758},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=3D5S5moUvUA},
 title = {Being With The Waves: An Ultrasonic Art Installation Enabling Rich Interaction Without Sensors},
 url = {https://doi.org/10.21428%2F92fbeb44.376bc758},
 year = {2022}
}

@inproceedings{NIME22_49,
 PDF = {69.pdf},
 abstract = {This paper introduces micro-phenomenology, a research discipline for exploring and uncovering the structures of lived experience, as a beneficial methodology for studying and evaluating interactions with digital musical instruments. Compared to other subjective methods, micro-phenomenology evokes and returns one to the moment of experience, allowing access to dimensions and observations which may not be recalled in reflection alone. We present a case study of five microphenomenological interviews conducted with musicians about their experiences with existing digital musical instruments. The interviews reveal deep, clear descriptions of different modalities of synchronic moments in interaction, especially in tactile connections and bodily sensations. We highlight the elements of interaction captured in these interviews which would not have been revealed otherwise and the importance of these elements in researching perception, understanding, interaction, and performance with digital musical instruments.},
 address = {The University of Auckland, New Zealand},
 articleno = {49},
 author = {Reed, Courtney N. and Nordmoen, Charlotte and Martelloni, Andrea and Lepri, Giacomo and Robson, Nicole and Zayas-Garin, Eevee and Cotton, Kelsey and Mice, Lia and McPherson, Andrew},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.b304e4b1},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/-Ket6l90S8I},
 title = {Exploring Experiences with New Musical Instruments through Micro-phenomenology},
 url = {https://doi.org/10.21428%2F92fbeb44.b304e4b1},
 year = {2022}
}

@inproceedings{NIME22_50,
 PDF = {70.pdf},
 abstract = {This paper describes the 10,000 Instruments workshop, a collaborative online event conceived to generate interface ideas and speculate on music technology through open-ended artefacts and playful design explorations. We first present the activity, setting its research and artistic scope. We then report on a selection of outcomes created by workshop attendees, and examine the critical design statements they convey. The paper concludes with reflections on the make-believe, whimsical and troublemaking approach to instrument design adopted in the workshop. In particular, we consider the ways this activity can support individuals’ creativity, unlock shared musical visions and reveal unconventional perspectives on music technology development.},
 address = {The University of Auckland, New Zealand},
 articleno = {50},
 author = {Lepri, Giacomo and Bowers, John and Topley, Samantha and Stapleton, Paul and Bennett, Peter and Andersen, Kristina and McPherson, Andrew},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.9e7c9ba3},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/dif8K23TR1Y},
 title = {The 10,000 Instruments Workshop - (Im)practical Research for Critical Speculation},
 url = {https://doi.org/10.21428%2F92fbeb44.9e7c9ba3},
 year = {2022}
}

@inproceedings{NIME22_51,
 PDF = {77.pdf},
 abstract = {In this paper we present the development of a new gestural musical instrument, the AirSticks 2.0. The AirSticks 2.0 combines the latest advances in sensor fusion of Inertial Measurement Units (IMU) and low latency wireless data transmission over Bluetooth Low Energy (BLE), to give an expressive wireless instrument capable of triggering and manipulating discrete and continuous sound events in real-time. We outline the design criteria for this new instrument that has evolved from previous prototypes, give a technical overview of the custom hardware and software developed, and present short videos of three distinct mappings that intuitively translate movement into musical sounds.},
 address = {The University of Auckland, New Zealand},
 articleno = {51},
 author = {Trolland, Sam and Ilsar, Alon and Frame, Ciaran and McCormack, Jon and Wilson, Elliott},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.c400bdc2},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/TnEzwGshr48},
 title = {{AirSticks} 2.0: Instrument Design for Expressive Gestural Interaction},
 url = {https://doi.org/10.21428%2F92fbeb44.c400bdc2},
 year = {2022}
}

@inproceedings{NIME22_52,
 PDF = {86.pdf},
 abstract = {Musical grid interfaces are becoming an industry standard for interfaces that allow interaction with music software, electronics, or instruments. However, there are no clearly defined design standards or guidelines, resulting in grid interfaces being a multitude of interfaces with competing design approaches, making these already abstract UIs even more challenging. In this paper, we compare the co-existing design approaches of UIs for grid interfaces used by commercial and non-commercial developers and designers, and present the results of three experiments that tested the benefits of co-existing design approaches to mitigate some of the inherent design challenges.},
 address = {The University of Auckland, New Zealand},
 articleno = {52},
 author = {Rossmy, Beat and Rauh, Maximilian and Wiethoff, Alexander},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.db84ecd0},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=JF514EWYiQ8},
 title = {Towards User Interface Guidelines for Musical Grid Interfaces},
 url = {https://doi.org/10.21428%2F92fbeb44.db84ecd0},
 year = {2022}
}

@inproceedings{NIME22_53,
 PDF = {87.pdf},
 abstract = {Applications for musical grid interfaces are designed without any established guidelines or defined design rules. However, within applications of different manufacturers, musicians, and designers, common patterns and conventions can be observed which might be developing towards unofficial standards. In this survey we analyzed 40 applications, instruments, or controllers and collected 18 types of recurring UI elements, which are clustered, described, and interactively presented in this survey. We further postulate 3 theses which standard UI elements should meet and propose novel UI elements deduced from WIMP standards.},
 address = {The University of Auckland, New Zealand},
 articleno = {53},
 author = {Rossmy, Beat},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.563bfea9},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://www.youtube.com/watch?v=CPHY4_G_LR0},
 title = {Buttons, Sliders, and Keys {\textendash} A Survey on Musical Grid Interface Standards},
 url = {https://doi.org/10.21428%2F92fbeb44.563bfea9},
 year = {2022}
}

@inproceedings{NIME22_54,
 PDF = {88.pdf},
 abstract = {Historically marginalised instruments witness and bear vital stories that can deeply affect identity and galvanise communities when revitalised. We present the protolangspil as a contemporary interpretation of the langspil, an Icelandic monochord-like folk instrument, and describe its agential and performative contributions to the first Icelandic NIME research lab. This paper describes how the proto-langspil has served as an instrument in establishing the research methodology of our new lab and concretised the research agenda via a series of encounters with music performers and composers, luthiers, anthropologists, musicologists, designers and philosophers. These encounters have informed and challenged our research practices, mapped our surroundings, and embedded us in the local social fabric. We share our proto-langspil for replication, and reflect on encounters as a methodology framing mechanism that eschews the more traditional empirical approaches in HCI. We conclude with a final provocation for NIME researchers to embrace AI research with an open mind.},
 address = {The University of Auckland, New Zealand},
 articleno = {54},
 author = {Armitage, Jack and Magnusson, Thor and Shepardson, Victor and Ulfarsson, Halldor},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.6178f575},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/8tRTF1lB6Hg},
 title = {The Proto-Langspil: Launching an Icelandic {NIME} Research Lab with the Help of a Marginalised Instrument},
 url = {https://doi.org/10.21428%2F92fbeb44.6178f575},
 year = {2022}
}

@inproceedings{NIME22_55,
 PDF = {99.pdf},
 abstract = {The lived body, or soma, is the designation for the phenomenological experience of being a body, rather than simply a corporeal entity. Bodily knowledge, which evolves through bodily awareness, carries the lived body’s reflectivity. In this paper, such considerations are put in the context of previous work at NIME, specifically that revolving around with the vocal tract or the voice, due to its singular relation with embodiment. We understand that focusing on somaesthetics allows for novel ways of engaging with technology as well as highlighting biases that might go unnoticed otherwise. We present an inexpensive application of a respiration sensor that emerges from the aforementioned conceptualisations. Lastly, we reflect on how to better frame the role of bodily awareness in NIME.},
 address = {The University of Auckland, New Zealand},
 articleno = {55},
 author = {Tapparo, Carla Sophie and Zappi, Victor},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.21428/92fbeb44.7e04cfc8},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/GEndgifZmkI},
 title = {Bodily Awareness Through {NIMEs}:  Deautomatising Music Making Processes},
 url = {https://doi.org/10.21428%2F92fbeb44.7e04cfc8},
 year = {2022}
}

@inproceedings{NIME22_56,
 PDF = {49.pdf},
 abstract = {The Kanchay_Yupana// is an open-source NIME for the generation of rhythms, inspired by the Andean yupana: a tangible board similar to an abacus of different sizes and materials with a system of carved geometric boxes into which seeds or pebbles were disposed to perform arithmetic calculations, used since pre-colonial times. As in the traditional artifact, the interaction of this new electronic yupana is based on the arrangement of seeds on a specially designed board with boxes, holes, and photoresistors. The shadow detected by the seeds’ positioning sends real-time motion data in MIDI messages to Pure Data in a drum machine patch. As a result, percussion samples of Andean instruments fill pulses in a four-quarter beat, generating patterns that can be transformed live into different rhythms. This interface complements the Electronic_Khipu_ (a previous NIME based on an Andean khipu) by producing the rhythmic component. This experience unites ancestral and contemporary technologies in experimental sound performance following the theoretical-practical research on the vindication of the memory in ancestral Andean technological interfaces made invisible by colonization, reusing them from a decolonial perspective in NIMEs.},
 address = {The University of Auckland, New Zealand},
 articleno = {56},
 author = {Cadavid Hinojosa, Laddy Patricia},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 copyright = {Creative Commons Attribution 4.0 International},
 doi = {10.21428/92fbeb44.61d01269},
 issn = {2220-4806},
 month = {jun},
 presentation-video = {https://youtu.be/MpMFL6R14kQ},
 title = {Kanchay_Yupana{\slash \slash}: Tangible rhythm sequencer inspired by ancestral Andean technologies},
 url = {https://doi.org/10.21428/92fbeb44.61d01269},
 year = {2022}
}