@inproceedings{NIME20_0,
  author = {Weng, Ruolun},
  title = {Interactive Mobile Musical Application using faust2smartphone},
  pages = {1--4},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813164},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper0.pdf},
  abstract = {We introduce faust2smartphone, a tool to generate an edit-ready project for musical mobile application, which connects Faust programming language and mobile application’s development. It is an extended implementation of faust2api. Faust DSP objects can be easily embedded as a high level API so that the developers can access various functions and elements across different mobile platforms. This paper provides several modes and technical details on the structures and implementation of this system as well as some applications and future directions for this tool.}
}

@inproceedings{NIME20_1,
  author = {Sullivan, John and Vanasse, Julian and Guastavino, Catherine and Wanderley, Marcelo},
  title = {Reinventing the Noisebox: Designing Embedded Instruments for Active Musicians},
  pages = {5--10},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813166},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper1.pdf},
  presentation-video = {https://youtu.be/DUMXJw-CTVo},
  abstract = {This paper reports on the user-driven redesign of an embedded digital musical instrument that has yielded a trio of new instruments, informed by early user feedback and co-design workshops organized with active musicians. Collectively, they share a stand-alone design, digitally fabricated enclosures, and a common sensor acquisition and sound synthesis architecture, yet each is unique in its playing technique and sonic output. We focus on the technical design of the instruments and provide examples of key design specifications that were derived from user input, while reflecting on the challenges to, and opportunities for, creating instruments that support active practices of performing musicians.}
}

@inproceedings{NIME20_10,
  author = {Gibson, Darrell J and Polfreman, Richard},
  title = {Star Interpolator – A Novel Visualization Paradigm for Graphical Interpolators},
  pages = {49--54},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813168},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper10.pdf},
  presentation-video = {https://youtu.be/3ImRZdSsP-M},
  abstract = {This paper presents a new visualization paradigm for graphical interpolation systems, known as Star Interpolation, that has been specifically created for sound design applications. Through the presented investigation of previous visualizations, it becomes apparent that the existing visuals in this class of system, generally relate to the interpolation model that determines the weightings of the presets and not the sonic output. The Star Interpolator looks to resolve this deficiency by providing visual cues that relate to the parameter space. Through comparative exploration it has been found this visualization provides a number of benefits over the previous systems. It is also shown that hybrid visualization can be generated that combined benefits of the new visualization with the existing interpolation models. These can then be accessed by using an Interactive Visualization (IV) approach. The results from our exploration of these visualizations are encouraging and they appear to be advantageous when using the interpolators for sound designs tasks. Therefore, it is proposed that formal usability testing is undertaken to measure the potential value of this form of visualization.}
}

@inproceedings{NIME20_100,
  author = {Pardue, Laurel S and Ortiz, Miguel and van Walstijn, Maarten and Stapleton, Paul and Rodger, Matthew},
  title = {Vodhrán: collaborative design for evolving a physical model and interface into a proto-instrument},
  pages = {523--524},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813170},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper100.pdf},
  abstract = {This paper reports on the process of development of a virtual-acoustic proto-instrument, Vodhrán, based on a physical model of a plate, within a musical performance-driven ecosystemic environment. Performers explore the plate model via tactile interaction through a Sensel Morph interface, chosen to allow damping and localised striking consistent with playing hand percussion.  Through an iteration of prototypes, we have designed an embedded proto-instrument that allows a bodily interaction between the performer and the virtual-acoustic plate in a way that redirects from the perception of the Sensel as a touchpad and reframes it as a percussive surface. Due to the computational effort required to run such a rich physical model and the necessity to provide a natural interaction, the audio processing is implemented on a high powered single board computer. We describe the design challenges and report on the technological solutions we have found in the implementation of Vodhrán which we believe are valuable to the wider NIME community.}
}

@inproceedings{NIME20_101,
  author = {Venkatesh, Satvik and Braund, Edward and Miranda, Eduardo},
  title = {Designing Brain-computer Interfaces for Sonic Expression},
  pages = {525--530},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813172},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper101.pdf},
  abstract = {Brain-computer interfaces (BCIs) are beneficial for patients who are suffering from motor disabilities because it offers them a way of creative expression, which improves mental well-being. BCIs aim to establish a direct communication medium between the brain and the computer. Therefore, unlike conventional musical interfaces, it does not require muscular power. This paper explores the potential of building sound synthesisers with BCIs that are based on steady-state visually evoked potential (SSVEP). It investigates novel ways to enable patients with motor disabilities to express themselves. It presents a new concept called sonic expression, that is to express oneself purely by the synthesis of sound. It introduces new layouts and designs for BCI-based sound synthesisers and the limitations of these interfaces are discussed. An evaluation of different sound synthesis techniques is conducted to find an appropriate one for such systems. Synthesis techniques are evaluated and compared based on a framework governed by sonic expression.}
}

@inproceedings{NIME20_102,
  author = {Williams, Duncan A.H. and Fazenda, Bruno and Williamson, Victoria J. and Fazekas, Gyorgy},
  title = {Biophysiologically synchronous computer generated music improves performance and reduces perceived effort in trail runners},
  pages = {531--536},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813174},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper102.pdf},
  abstract = {Music has previously been shown to be beneficial in improving runners performance in treadmill based experiments. This paper evaluates a generative music system, HEARTBEATS, designed to create biosignal synchronous music in real-time according to an individual athlete’s heart-rate or cadence (steps per minute). The tempo, melody, and timbral features of the generated music are modulated according to biosensor input from each runner using a wearable Bluetooth sensor. We compare the relative performance of athletes listening to heart-rate and cadence synchronous music, across a randomized trial (N=57) on a trail course with 76ft of elevation. Participants were instructed to continue until perceived effort went beyond an 18 using the Borg rating of perceived exertion scale. We found that cadence-synchronous music improved performance and decreased perceived effort in male runners, and improved performance but not perceived effort in female runners, in comparison to heart-rate synchronous music. This work has implications for the future design and implementation of novel portable music systems and in music-assisted coaching.}
}

@inproceedings{NIME20_103,
  author = {Bernardes, Gilberto and Bernardes, Gilberto},
  title = {Interfacing Sounds: Hierarchical Audio-Content Morphologies for Creative Re-purposing in earGram 2.0},
  pages = {537--542},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813176},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper103.pdf},
  presentation-video = {https://youtu.be/zEg9Cpir8zA},
  abstract = {Audio content-based processing has become a pervasive methodology for techno-fluent musicians. System architectures typically create thumbnail audio descriptions, based on signal processing methods, to visualize, retrieve and transform musical audio efficiently. Towards enhanced usability of these descriptor-based frameworks for the music community, the paper advances a minimal content-based audio description scheme, rooted on primary musical notation attributes at the threefold sound object, meso and macro hierarchies. Multiple perceptually-guided viewpoints from rhythmic, harmonic, timbral and dynamic attributes define a discrete and finite alphabet with minimal formal and subjective assumptions using unsupervised and user-guided methods. The Factor Oracle automaton is then adopted to model and visualize temporal morphology. The generative musical applications enabled by the descriptor-based framework at multiple structural hierarchies are discussed.}
}

@inproceedings{NIME20_104,
  author = {Han, Joung Min and Kakehi, Yasuaki},
  title = {ParaSampling: A Musical Instrument with Handheld Tapehead Interfaces for Impromptu Recording and Playing on a Magnetic Tape},
  pages = {543--544},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813178},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper104.pdf},
  abstract = {For a long time, magnetic tape has been commonly utilized as one of physical media for recording and playing music. In this research, we propose a novel interactive musical instrument called ParaSampling that utilizes the technology of magnetic sound recording, and a improvisational sound playing method based on the instrument. While a conventional cassette tape  player has a single tapehead, which rigidly placed, our instrument utilizes multiple handheld tapehead modules as an interface. Players can hold the interfaces and press them against the rotating magnetic tape at an any point to record or reproduce sounds The player can also easily erase and rewrite the sound recorded on the tape. With this instrument, they can achieve improvised and unique musical expressions through tangible and spatial interactions. In this paper, we describe the system design of ParaSampling, the implementation of the prototype system, and discuss music expressions enabled by the system.}
}

@inproceedings{NIME20_105,
  author = {Filandrianos, Giorgos and Kotsani, Natalia and Dervakos, Edmund G and Stamou, Giorgos and Amprazis, Vaios and Kiourtzoglou, Panagiotis},
  title = {Brainwaves-driven Effects Automation in Musical Performance},
  pages = {545--546},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813180},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper105.pdf},
  abstract = {A variety of controllers with multifarious sensors and functions have maximized the real time performers control capabilities. The idea behind this project was to create an interface which enables the interaction between the performers and the effect processor measuring their brain waves amplitudes, e.g., alpha, beta, theta, delta and gamma, not necessarily with the user’s awareness. We achieved this by using an electroencephalography (EEG) sensor for detecting performer’s different emotional states and, based on these, sending midi messages for digital processing units automation. The aim is to create a new generation of digital processor units that could be automatically configured in real-time given the emotions or thoughts of the performer or the audience. By introducing emotional state information in the real time control of several aspects of artistic expression, we highlight the impact of surprise and uniqueness in the artistic performance.}
}

@inproceedings{NIME20_106,
  author = {Wakefield, Graham and Palumbo, Michael and Zonta, Alexander},
  title = {Affordances and Constraints of Modular Synthesis in Virtual Reality},
  pages = {547--550},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813182},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper106.pdf},
  abstract = {This article focuses on the rich potential of hybrid domain translation of modular synthesis (MS) into virtual reality (VR). It asks: to what extent can what is valued in studio-based MS practice find a natural home or rich new interpretations in the immersive capacities of VR? The article attends particularly to the relative affordances and constraints of each as they inform the design and development of a new system ("Mischmasch") supporting collaborative and performative patching of Max gen~ patches and operators within a shared room-scale VR space.}
}

@inproceedings{NIME20_107,
  author = {moraitis, emmanouil},
  title = {Symbiosis: a biological taxonomy for modes of interaction in dance-music collaborations},
  pages = {551--556},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813184},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper107.pdf},
  presentation-video = {https://youtu.be/5X6F_nL8SOg},
  abstract = {Focusing on interactive performance works borne out of dancer-musician collaborations, this paper investigates the relationship between the mediums of sound and movement through a conceptual interpretation of the biological phenomenon of symbiosis. Describing the close and persistent interactions between organisms of different species, symbioses manifest across a spectrum of relationship types, each identified according to the health effect experienced by the engaged organisms. This biological taxonomy is appropriated within a framework which identifies specific modes of interaction between sound and movement according to the collaborating practitioners’ intended outcome, and required provisions, cognition of affect, and system operation. Using the symbiotic framework as an analytical tool, six dancer-musician collaborations from the field of NIME are examined in respect to the employed modes of interaction within each of the four examined areas. The findings reveal the emergence of multiple modes in each work, as well as examples of mutation between different modes over the course of a performance. Furthermore, the symbiotic concept provides a novel understanding of the ways gesture recognition technologies (GRTs) have redefined the relationship dynamics between dancers and musicians, and suggests a more efficient and inclusive approach in communicating the potential and limitations presented by Human-Computer Interaction tools.}
}

@inproceedings{NIME20_108,
  author = {Nonnis, Antonella and Bryan-Kinns, Nick},
  title = {Όλοι: music making to scaffold social playful activities and self-regulation},
  pages = {557--558},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813186},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper108.pdf},
  abstract = {We present Olly, a musical textile tangible user interface (TUI) designed around the observations of a group of five children with autism who like music. The intention is to support scaffolding social interactions and sensory regulation during a semi-structured and open-ended playful activity. Olly was tested in the dance studio of a special education needs (SEN) school in North-East London, UK, for a period of 5 weeks, every Thursday afternoon for 30 minutes. Olly uses one Bare touch board in midi mode and four stretch analog sensors embedded inside four elastic ribbons. These ribbons top the main body of the installation which is made by using an inflatable gym ball wrapped in felt. Each of the ribbons plays a different instrument and triggers different harmonic chords. Olly allows to play pleasant melodies if interacting with it in solo mode and more complex harmonies when playing together with others. Results show great potentials for carefully designed musical TUI implementation aimed at scaffolding social play while affording self-regulation in SEN contexts. We present a brief introduction on the background and motivations, design considerations and results.}
}

@inproceedings{NIME20_109,
  author = {Sithi-Amnuai, Sara},
  title = {Exploring Identity Through Design: A Focus on the Cultural Body Via Nami},
  pages = {559--563},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813188},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper109.pdf},
  presentation-video = {https://youtu.be/QCUGtE_z1LE},
  abstract = {Identity is inextricably linked to culture and sustained through creation and performance of music and dance, yet discussion of agency and cultural tools informing design and performance application of gestural controllers is not widely discussed. The purpose of this paper is to discuss the cultural body, its consideration in existing gestural controller design, and how cultural design methods have the potential to extend musical/social identities and/or traditions within a technological context. In an effort to connect and reconnect with the author’s personal Nikkei heritage, this paper will discuss the design of Nami – a custom built gestural controller and its applicability to extend the author’s cultural body through a community-centric case study performance.}
}

@inproceedings{NIME20_11,
  author = {Xambó, Anna and Roma, Gerard},
  title = {Performing Audiences: Composition Strategies for Network Music using Mobile Phones},
  pages = {55--60},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813192},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper11.pdf},
  abstract = {With the development of web audio standards, it has quickly become technically easy to develop and deploy software for inviting audiences to participate in musical performances using their mobile phones. Thus, a new audience-centric musical genre has emerged, which aligns with artistic manifestations where there is an explicit inclusion of the public (e.g. participatory art, cinema or theatre). Previous research has focused on analysing this new genre from historical, social organisation and technical perspectives. This follow-up paper contributes with reflections on technical and aesthetic aspects of composing within this audience-centric approach. We propose a set of 13 composition dimensions that deal with the role of the performer, the role of the audience, the location of sound and the type of feedback, among others. From a reflective approach, four participatory pieces developed by the authors are analysed using the proposed dimensions. Finally, we discuss a set of recommendations and challenges for the composers-developers of this new and promising musical genre. This paper concludes discussing the implications of this research for the NIME community.}
}

@inproceedings{NIME20_110,
  author = {Wright, Joe},
  title = {The Appropriation and Utility of Constrained ADMIs},
  pages = {564--569},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813194},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper110.pdf},
  presentation-video = {https://youtu.be/RhaIzCXQ3uo},
  abstract = {This paper reflects on players' first responses to a constrained Accessible Digital Musical Instrument (ADMI) in open, child-led sessions with seven children at a special school. Each player's gestures with the instrument were sketched, categorised and compared with those of others among the group. Additionally, sensor data from the instruments was recorded and analysed to give a secondary indication of playing style, based on note and silence durations. In accord with previous studies, the high degree of constraints led to a diverse range of playing styles, allowing each player to appropriate and explore the instruments within a short inaugural session. The open, undirected sessions also provided insights which could potentially direct future work based on each person's responses to the instruments. The paper closes with a short discussion of these diverse styles, and the potential role constrained ADMIs could serve as 'ice-breakers' in musical projects that seek to co-produce or co-design with neurodiverse children and young people.}
}

@inproceedings{NIME20_111,
  author = {Mice, Lia and McPherson, Andrew},
  title = {From miming to NIMEing: the development of idiomatic gestural language on large scale DMIs},
  pages = {570--575},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813200},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper111.pdf},
  presentation-video = {https://youtu.be/mnJN8ELneUU},
  abstract = {When performing with new instruments, musicians often develop new performative gestures and playing techniques. Music performance studies on new instruments often consider interfaces that feature a spectrum of gestures similar to already existing sound production techniques. This paper considers the choices performers make when creating an idiomatic gestural language for an entirely unfamiliar instrument. We designed a musical interface with a unique large-scale layout to encourage new performers to create fully original instrument-body interactions. We conducted a study where trained musicians were invited to perform one of two versions of the same instrument, each physically identical but with a different tone mapping. The study results reveal insights into how musicians develop novel performance gestures when encountering a new instrument characterised by an unfamiliar shape and size. Our discussion highlights the impact of an instrument’s scale and layout on the emergence of new gestural vocabularies and on the qualities of the music performed.}
}

@inproceedings{NIME20_112,
  author = {Payne, William C and Paradiso, Ann and Kane, Shaun},
  title = {Cyclops: Designing an eye-controlled instrument for accessibility and flexible use},
  pages = {576--580},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813204},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper112.pdf},
  presentation-video = {https://youtu.be/G6dxngoCx60},
  abstract = {The Cyclops is an eye-gaze controlled instrument designed for live performance and improvisation. It is primarily mo- tivated by a need for expressive musical instruments that are more easily accessible to people who rely on eye track- ers for computer access, such as those with amyotrophic lateral sclerosis (ALS). At its current implementation, the Cyclops contains a synthesizer and sequencer, and provides the ability to easily create and automate musical parameters and effects through recording eye-gaze gestures on a two- dimensional canvas. In this paper, we frame our prototype in the context of previous eye-controlled instruments, and we discuss we designed the Cyclops to make gaze-controlled music making as fun, accessible, and seamless as possible despite notable interaction challenges like latency, inaccu- racy, and “Midas Touch.”}
}

@inproceedings{NIME20_113,
  author = {Marquez-Borbon, Adnan},
  title = {Collaborative Learning with Interactive Music Systems},
  pages = {581--586},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813206},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper113.pdf},
  presentation-video = {https://youtu.be/1G0bOVlWwyI},
  abstract = {This paper presents the results of an observational study focusing on the collaborative learning processes of a group of performers with an interactive musical system. The main goal of this study was to implement methods for learning and developing practice with these technological objects in order to generate future pedagogical methods. During the research period of six months, four participants regularly engaged in workshop-type scenarios where learning objectives were proposed and guided by themselves.The principal researcher, working as participant-observer, did not impose or prescribed learning objectives to the other members of the group. Rather, all participants had equal say in what was to be done and how it was to be accomplished. Results show that the group learning environment is rich in opportunities for learning, mutual teaching, and for establishing a comunal practice for a given interactive musical system.Key findings suggest that learning by demonstration, observation and modelling are significant for learning in this context. Additionally, it was observed that a dialogue and a continuous flow of information between the members of the community is needed in order to motivate and further their learning.}
}

@inproceedings{NIME20_114,
  author = {Vetter, Jens},
  title = {WELLE - a web-based music environment for the blind},
  pages = {587--590},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813208},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper114.pdf},
  abstract = {This paper presents WELLE, a web-based music environment for blind people, and describes its development, design, notation syntax and first experiences. WELLE is intended to serve as a collaborative, performative and educational tool to quickly create and record musical ideas. It is pattern-oriented, based on textual notation and focuses on accessibility, playful interaction and ease of use. WELLE was developed as part of the research project Tangible Signals and will also serve as a platform for the integration of upcoming new interfaces.}
}

@inproceedings{NIME20_115,
  author = {Pessoa, Margarida and Parauta, Cláudio and Luís, Pedro and Corintha, Isabela and Bernardes, Gilberto},
  title = {Examining Temporal Trends and Design Goals of Digital Music Instruments for Education in NIME: A Proposed Taxonomy},
  pages = {591--595},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813210},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper115.pdf},
  abstract = {This paper presents an overview of the design principles behind Digital Music Instruments (DMIs) for education across all editions of the International Conference on New Interfaces for Music Expression (NIME). We compiled a comprehensive catalogue of over hundred DMIs with varying degrees of applicability in the educational practice. Each catalogue entry is annotated according to a proposed taxonomy for DMIs for education, rooted in the mechanics of control, mapping and feedback of an interactive music system, along with the required expertise of target user groups and the instrument learning curve. Global statistics unpack underlying trends and design goals across the chronological period of the NIME conference. In recent years, we note a growing number of DMIs targeting non-experts and with reduced requirements in terms of expertise. Stemming from the identified trends, we discuss future challenges in the design of DMIs for education towards enhanced degrees of variation and unpredictability.}
}

@inproceedings{NIME20_116,
  author = {Pardue, Laurel S and Bhamra, Kuljit and England, Graham and Eddershaw, Phil and Menzies, Duncan },
  title = {Demystifying tabla through the development of an electronic drum},
  pages = {596--599},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813212},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper116.pdf},
  presentation-video = {https://youtu.be/PPaHq8fQjB0},
  abstract = {The tabla is a traditional pitched two-piece Indian drum set, popular not only within South East Asian music, but whose sounds also regularly feature in western music. Yet tabla remains an aural tradition, taught largely through a guru system heavy in custom and mystique. Tablas can also pose problems for school and professional performance environments as they are physically bulky, fragile, and reactive to environmental factors such as damp and heat. As part of a broader project to demystify tabla, we present an electronic tabla that plays nearly identically to an acoustic tabla and was created in order to make the tabla acces- sible and practical for a wider audience of students, pro- fessional musicians and composers. Along with develop- ment of standardised tabla notation and instructional educational aides, the electronic tabla is designed to be compact, robust, easily tuned, and the electronic nature allows for scoring tabla through playing. Further, used as an interface, it allows the use of learned tabla technique to control other percussive sounds. We also discuss the technological approaches used to accurately capture the localized multi-touch rapid-fire strikes and damping that combine to make tabla such a captivating and virtuosic instrument.}
}

@inproceedings{NIME20_117,
  author = {Sierra, Juan D},
  title = {SpeakerDrum},
  pages = {600--604},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813216},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper117.pdf},
  abstract = {SpeakerDrum is an instrument composed of multiple Dual Voice Coil speakers (DVC) where two coils are used to drive the same membrane. However, in this case, one of them is used as a microphone which is then used by the performer as an input interface of percussive gestures. Of course, this leads to poten- tial feedback, but with enough control, a compelling exploration of resonance haptic feedback and sound embodiment is possible.}
}

@inproceedings{NIME20_118,
  author = {Caren, Matthew and Michon, Romain and Wright, Matthew},
  title = {The KeyWI: An Expressive and Accessible Electronic Wind Instrument},
  pages = {605--608},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813218},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper118.pdf},
  abstract = {This paper presents the KeyWI, an electronic wind instrument design based on the melodica that both improves upon limitations in current systems and is general and powerful enough to support a variety of applications. Four opportunities for growth are identified in current electronic wind instrument systems, which then are used as focuses in the development and evaluation of the instrument. The instrument features a breath pressure sensor with a large dynamic range, a keyboard that allows for polyphonic pitch selection, and a completely integrated construction. Sound synthesis is performed with Faust code compiled to the Bela Mini, which offers low-latency audio and a simple yet powerful development workflow. In order to be as accessible and versatile as possible, the hardware and software is entirely open-source, and fabrication requires only common maker tools.}
}

@inproceedings{NIME20_119,
  author = {Christensen, Pelle Juul and Overholt, Dan and Serafin, Stefania},
  title = {The Da ̈ıs: A Haptically Enabled New Interface for Musical Expression for Controlling Physical Models for Sound Synthesis},
  pages = {609--612},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813220},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper119.pdf},
  presentation-video = {https://youtu.be/XOvnc_AKKX8},
  abstract = {In this paper we provide a detailed description of the development of a new interface for musical expression, the da ̈ıs, with focus on an iterative development process, control of physical models for sounds synthesis, and haptic feedback. The development process, consisting of three iterations, is covered along with a discussion of the tools and methods used. The sound synthesis algorithm for the da ̈ıs, a physical model of a bowed string, is covered and the mapping from the interface parameters to those of the synthesis algorithms is described in detail. Using a qualitative test the affordances, advantages, and disadvantages of the chosen design, synthesis algorithm, and parameter mapping is highlighted. Lastly, the possibilities for future work is discussed with special focus on alternate sounds and mappings.}
}

@inproceedings{NIME20_12,
  author = {Hunt, Samuel J and Mitchell, Tom and Nash, Chris},
  title = {Composing computer generated music, an observational study using IGME: the Interactive Generative Music Environment},
  pages = {61--66},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813222},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper12.pdf},
  abstract = {Computer composed music remains a novel and challenging problem to solve. Despite an abundance of techniques and systems little research has explored how these might be useful for end-users looking to compose with generative and algorithmic music techniques. User interfaces for generative music systems are often inaccessible to non-programmers and neglect established composition workflow and design paradigms that are familiar to computer-based music composers. We have developed a system called the Interactive Generative Music Environment (IGME) that attempts to bridge the gap between generative music and music sequencing software, through an easy to use score editing interface. This paper discusses a series of user studies in which users explore generative music composition with IGME. A questionnaire evaluates the user’s perception of interacting with generative music and from this provide recommendations for future generative music systems and interfaces.}
}

@inproceedings{NIME20_120,
  author = {Wilbert, Joao and Haddad, Don D and Ishii, Hiroshi and Paradiso, Joseph},
  title = {Patch-corde: an expressive patch-cable for the modular synthesizer.},
  pages = {613--616},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813224},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper120.pdf},
  presentation-video = {https://youtu.be/7gklx8ek8U8},
  abstract = {Many opportunities and challenges in both the control and performative aspects of today’s modular synthesizers exist. The user interface prevailing in the world of synthesizers and music controllers has always been revolving around knobs, faders, switches, dials, buttons, or capacitive touchpads, to name a few. This paper presents a novel way of interaction with a modular synthesizer by exploring the affordances of cord-base UIs. A special patch cable was developed us- ing commercially available piezo-resistive rubber cords, and was adapted to fit to the 3.5 mm mono audio jack, making it compatible with the Eurorack modular-synth standard. Moreover, a module was developed to condition this stretch- able sensor/cable, to allow multiple Patch-cordes to be used in a given patch simultaneously. This paper also presents a vocabulary of interactions, labeled through various physical actions, turning the patch cable into an expressive controller that complements traditional patching techniques.}
}

@inproceedings{NIME20_121,
  author = {Suchánek, Jiří},
  title = {SOIL CHOIR v.1.3 - soil moisture sonification installation},
  pages = {617--618},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813226},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper121.pdf},
  abstract = {The artistic sonification offers a creative method for putting direct semantic layers to the abstract sounds. This paper is dedicated to the sound installation “Soil choir v.1.3” that sonifies soil moisture in different depths and transforms this non-musical phenomenon into organized sound structures. The sonification of natural soil moisture processes tests the limits of our attention, patience and willingness to still perceive ultra-slow reactions and examines the mechanisms of our sense adaptation. Although the musical time of the installation is set to almost non-human – environmental time scale (changes happen within hours, days, weeks or even months…) this system can be explored and even played also as an instrument by putting sensors to different soil areas or pouring liquid into the soil and waiting for changes... The crucial aspect of the work was to design the sonification architecture that deals with extreme slow changes of input data – measured values from moisture sensors. The result is the sound installation consisting of three objects – each with different types of soil. Every object is compact, independent unit consisting of three low-cost capacitive soil moisture sensors, 1m long perspex tube filled with soil, full range loudspeaker and Bela platform with custom Supercollider code. I developed this installation during the year 2019 and this paper gives insight into the aspects and issues connected with creating this installation.}
}

@inproceedings{NIME20_122,
  author = {Koutsomichalis, Marinos},
  title = {Rough-hewn Hertzian Multimedia Instruments},
  pages = {619--624},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813228},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper122.pdf},
  presentation-video = {https://youtu.be/DWecR7exl8k},
  abstract = {Three DIY electronic instruments that the author has used in real-life multimedia performance contexts are scrutinised herein. The instruments are made intentionally rough-hewn, non-optimal and user-unfriendly in several respects, and are shown to draw upon experimental traits in electronics de- sign and interfaces for music expression. The various different ways in which such design traits affects their performance are outlined, as are their overall consequence to the artistic outcome and to individual experiences of it. It is shown that, to a varying extent, they all embody, mediate, and aid actualise the specifics their parent projects revolve around. It is eventually suggested that in the context of an exploratory and hybrid artistic practice, bespoke instruments of sorts, their improvised performance, the material traits or processes they implement or pivot on, and the ideas/narratives that perturb thereof, may all intertwine and fuse into one another so that a clear distinction between one another is not always possible, or meaningful. In such a vein, this paper aims at being an account of such a practice upon which prospective researchers/artists may further build upon.}
}

@inproceedings{NIME20_123,
  author = {Olsen, Taylor J},
  title = {Animation, Sonification, and Fluid-Time: A Visual-Audioizer Prototype},
  pages = {625--630},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813230},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper123.pdf},
  abstract = {The visual-audioizer is a patch created in Max in which the concept of fluid-time animation techniques, in tandem with basic computer vision tracking methods, can be used as a tool to allow the visual time-based media artist to create music. Visual aspects relating to the animator’s knowledge of motion, animated loops, and auditory synchronization derived from computer vision tracking methods, allow an immediate connection between the generated audio derived from visuals—becoming a new way to experience and create audio-visual media. A conceptual overview, comparisons of past/current audio-visual contributors, and a summary of the Max patch will be discussed. The novelty of practice-based animation methods in the field of musical expression, considerations of utilizing the visual-audioizer, and the future of fluid-time animation techniques as a tool of musical creativity will also be addressed. }
}

@inproceedings{NIME20_124,
  author = {de las Pozas, Virginia},
  title = {Semi-Automated Mappings for Object-Manipulating Gestural Control of Electronic Music},
  pages = {631--634},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813232},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper124.pdf},
  abstract = {This paper describes a system for automating the generation of mapping schemes between human interaction with extramusical objects and electronic dance music. These mappings are determined through the comparison of sensor input to a synthesized matrix of sequenced audio. The goal of the system is to facilitate live performances that feature quotidian objects in the place of traditional musical instruments. The practical and artistic applications of musical control with quotidian objects is discussed. The associated object-manipulating gesture vocabularies are mapped to musical output so that the objects themselves may be perceived as DMIs. This strategy is used in a performance to explore the liveness qualities of the system.}
}

@inproceedings{NIME20_125,
  author = {Benetatos, Christodoulos and VanderStel, Joseph and Duan, Zhiyao},
  title = {BachDuet: A Deep Learning System for Human-Machine Counterpoint Improvisation},
  pages = {635--640},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813234},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper125.pdf},
  presentation-video = {https://youtu.be/wFGW0QzuPPk},
  abstract = {During theBaroque period, improvisation was a key element of music performance and education. Great musicians, such as J.S. Bach, were better known as improvisers than composers. Today,  however,  there  is  a  lack  of  improvisation culture in classical music performance and education; classical musicians either are not trained to improvise, or cannot find other people to improvise with.  Motivated by this observation,  we  develop BachDuet,  a  system  that  enables real-time counterpoint improvisation between a human anda machine.  This system uses a recurrent neural network toprocess the human musician’s monophonic performance ona MIDI keyboard and generates the machine’s monophonic performance in real time. We  develop a GUI to visualize the generated music content and to facilitate this interaction. We  conduct  user  studies  with  13  musically  trained users  and  show  the  feasibility  of  two-party  duet  counterpoint improvisation and the effectiveness of BachDuet for this purpose.  We also conduct listening tests with 48 participants and show that they cannot tell the difference between duets generated by human-machine improvisation using BachDuet and those generated by human-human improvisation.  Objective evaluation is also conducted to assess the degree to which these improvisations adhere to common rules of counterpoint, showing promising results.}
}

@inproceedings{NIME20_13,
  author = {Capra, Olivier and Berthaut, Florent and Grisoni, Laurent},
  title = {All You Need Is LOD : Levels of Detail in Visual Augmentations for the Audience},
  pages = {67--72},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813236},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper13.pdf},
  presentation-video = {https://youtu.be/3hIGu9QDn4o},
  abstract = {Because they break the physical link between gestures and sound, Digital Musical Instruments offer countless opportunities for musical expression. For the same reason however, they may hinder the audience experience, making the musician contribution and expressiveness difficult to perceive. In order to cope with this issue without altering the instruments, researchers and artists alike have designed techniques to augment their performances with additional information, through audio, haptic or visual modalities. These techniques have however only been designed to offer a fixed level of information, without taking into account the variety of spectators expertise and preferences. In this paper, we investigate the design, implementation and effect on audience experience of visual augmentations with controllable level of detail (LOD). We conduct a controlled experiment with 18 participants, including novices and experts. Our results show contrasts in the impact of LOD on experience and comprehension for experts and novices, and highlight the diversity of usage of visual augmentations by spectators.}
}

@inproceedings{NIME20_14,
  author = {Wang, Johnty and Meneses, Eduardo and Wanderley, Marcelo},
  title = {The Scalability of WiFi for Mobile Embedded Sensor Interfaces},
  pages = {73--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813239},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper14.pdf},
  abstract = {In this work we test the performance of multiple ESP32microcontrollers used as WiFi sensor interfaces in the context of real-time interactive systems. The number of devices from 1 to 13, and individual sending rates from 50 to 2300 messages per second are tested to provide examples of various network load situations that may resemble a performance configuration.  The overall end-to-end latency and bandwidth are measured as the basic performance metrics of interest. The results show that a maximum message rate of 2300 Hz is possible on a 2.4 GHz network for a single embedded device and decreases as the number of devices are added. During testing it was possible to have up to 7 devices transmitting at 100 Hz while attaining less than 10 ms latency, but performance degrades with increasing sending rates and number of devices. Performance can also vary significantly from day to day depending on network usage in a crowded environment.}
}

@inproceedings{NIME20_15,
  author = {Berthaut, Florent and Dahl, Luke},
  title = {Adapting & Openness: Dynamics of Collaboration Interfaces for Heterogeneous Digital Orchestras},
  pages = {77--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813241},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper15.pdf},
  presentation-video = {https://youtu.be/jGpKkbWq_TY},
  abstract = {Advanced musical cooperation, such as concurrent control of musical parameters or sharing data between instruments,has  previously  been  investigated  using  multi-user  instruments  or  orchestras  of  identical  instruments.   In  the  case of heterogeneous digital orchestras, where the instruments, interfaces, and control gestures can be very different, a number of issues may impede such collaboration opportunities. These include the  lack  of  a  standard  method  for  sharing data or control, the incompatibility of parameter types, and limited  awareness  of  other  musicians’  activity  and  instrument  structure.   As  a  result,  most  collaborations  remain limited to synchronising tempo or applying effects to audio outputs. In this paper we present two interfaces for real-time group collaboration amongst musicians with heterogeneous instruments.   We  conducted  a  qualitative  study  to  investigate how these interfaces impact musicians’ experience and their musical output, we performed a thematic analysis of inter-views,  and  we  analysed  logs  of  interactions.   From  these results  we  derive  principles  and  guidelines  for  the  design of advanced collaboration systems for heterogeneous digital orchestras, namely Adapting  (to)  the  System, Support  Development, Default  to  Openness, and Minimise  Friction to Support Expressivity.}
}

@inproceedings{NIME20_16,
  author = {Förster, Andreas and Komesker, Christina and Schnell, Norbert},
  title = {SnoeSky and SonicDive - Design and Evaluation of Two Accessible Digital Musical Instruments for a SEN School},
  pages = {83--88},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813243},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper16.pdf},
  abstract = {Music technology can provide persons who experience physical and/or intellectual barriers using traditional musical instruments with a unique access to active music making. This applies particularly but not exclusively to the so-called group of people with physical and/or mental disabilities. This paper presents two Accessible Digital Musical Instruments (ADMIs) that were specifically designed for the students of a Special Educational Needs (SEN) school with a focus on intellectual disabilities. With SnoeSky, we present an ADMI in the form of an interactive starry sky that integrates into the Snoezel-Room. Here, users can 'play' with 'melodic constellations' using a flashlight. SonicDive is an interactive installation that enables users to explore a complex water soundscape through their movement inside a ball pool. The underlying goal of both ADMIs was the promotion of self-efficacy experiences while stimulating the users' relaxation and activation. This paper reports on the design process involving the users and their environment. In addition, it describes some details of the technical implementaion of the ADMIs as well as first indices for their effectiveness.}
}

@inproceedings{NIME20_17,
  author = {Pritchard, Robert and Lavery, Ian},
  title = {Inexpensive Colour Tracking to Overcome Performer ID Loss },
  pages = {89--92},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813245},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper17.pdf},
  abstract = {The NuiTrack IDE supports writing code for an active infrared camera to track up to six bodies, with up to 25 target points on each person. The system automatically assigns IDs to performers/users as they enter the tracking area, but when occlusion of a performer occurs, or when a user exits and then re-enters the tracking area, upon rediscovery of the user the system generates a new tracking ID. Because of this any assigned and registered target tracking points for specific users are lost, as are the linked abilities of that performer to control media based on their movements. We describe a single camera system for overcoming this problem by assigning IDs based on the colours worn by the performers, and then using the colour tracking for updating and confirming identification when the performer reappears after occlusion or upon re-entry. A video link is supplied showing the system used for an interactive dance work with four dancers controlling individual audio tracks. }
}

@inproceedings{NIME20_18,
  author = {Nishida, Kiyu and jo, kazuhiro},
  title = {Modules for analog synthesizers using Aloe vera biomemristor},
  pages = {93--96},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813249},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper18.pdf},
  presentation-video = {https://youtu.be/bZaCd6igKEA},
  abstract = {In this study, an analog synthesizer module using Aloe vera was proposed as a biomemristor. The recent revival of analog modular synthesizers explores novel possibilities of sounds based on unconventional technologies such as integrating biological forms and structures into traditional circuits. A biosignal has been used in experimental music as the material for composition. However, the recent development of a biocomputor using a slime mold biomemristor expands the use of biomemristors in music. Based on prior research, characteristics of Aloe vera as a biomemristor were electrically measured, and two types of analog synthesizer modules were developed, current to voltage converter and current spike to voltage converter. For this application, a live performance was conducted with the CVC module and the possibilities as a new interface for musical expression were examined.}
}

@inproceedings{NIME20_19,
  author = {Moro, Giulio and McPherson, Andrew},
  title = {A platform for low-latency continuous keyboard sensing and sound generation},
  pages = {97--102},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813253},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper19.pdf},
  presentation-video = {https://youtu.be/Y137M9UoKKg},
  abstract = {On several acoustic and electromechanical keyboard instruments, the produced sound is not always strictly dependent exclusively on a discrete key velocity parameter, and minute gesture details can affect the final sonic result. By contrast, subtle variations in articulation have a relatively limited effect on the sound generation when the keyboard controller uses the MIDI standard, used in the vast majority of digital keyboards. In this paper we present an embedded platform that can generate sound in response to a controller capable of sensing the continuous position of keys on a keyboard. This platform enables the creation of keyboard-based DMIs which allow for a richer set of interaction gestures than would be possible through a MIDI keyboard, which we demonstrate through two example instruments. First, in a Hammond organ emulator, the sensing device allows to recreate the nuances of the interaction with the original instrument in a way a velocity-based MIDI controller could not. Second, a nonlinear waveguide flute synthesizer is shown as an example of the expressive capabilities that a continuous-keyboard controller opens up in the creation of new keyboard-based DMIs.}
}

@inproceedings{NIME20_2,
  author = {Sarkar, Advait and Mattinson, Henry},
  title = {Excello: exploring spreadsheets for music composition},
  pages = {11--16},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813256},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper2.pdf},
  abstract = {Excello is a spreadsheet-based music composition and programming environment. We co-developed Excello with feedback from 21 musicians at varying levels of musical and computing experience. We asked: can the spreadsheet interface be used for programmatic music creation?  Our design process encountered questions such as how time should be represented, whether amplitude and octave should be encoded as properties of individual notes or entire phrases, and how best to leverage standard spreadsheet features, such as formulae and copy-paste. We present the user-centric rationale for our current design, and report a user study suggesting that Excello's notation retains similar cognitive dimensions to conventional music composition tools, while allowing the user to write substantially complex programmatic music.}
}

@inproceedings{NIME20_20,
  author = {Guidi, Andrea and Morreale, Fabio and McPherson, Andrew},
  title = {Design for auditory imagery: altering instruments to explore performer fluency},
  pages = {103--108},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813260},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper20.pdf},
  presentation-video = {https://youtu.be/yK7Tg1kW2No},
  abstract = {In NIME design, thorough attention has been devoted to feedback modalities, including auditory, visual and haptic feedback. How the performer executes the gestures to achieve a sound on an instrument, by contrast, appears to be less examined. Previous research showed that auditory imagery, or the ability to hear or recreate sounds in the mind even when no audible sound is present, is essential to the sensorimotor control involved in playing an instrument. In this paper, we enquire whether auditory imagery can also help to support skill transfer between musical instruments resulting in possible implications for new instrument design. To answer this question, we performed two experimental studies on pitch accuracy and fluency where professional violinists were asked to play a modified violin. Results showed altered or even possibly irrelevant auditory feedback on a modified violin does not appear to be a significant impediment to performance. However, performers need to have coherent imagery of what they want to do, and the sonic outcome needs to be coupled to the motor program to achieve it. This finding shows that the design lens should be shifted from a direct feedback model of instrumental playing toward a model where imagery guides the playing process. This result is in agreement with recent research on skilled sensorimotor control that highlights the value of feedforward anticipation in embodied musical performance. It is also of primary importance for the design of new instruments: new sounds that cannot easily be imagined and that are not coupled to a motor program are not likely to be easily performed on the instrument.}
}

@inproceedings{NIME20_21,
  author = {Masu, Raul and Bala, Paulo and Ahmad, Muhammad and Correia, Nuno N. and Nisi, Valentina and Nunes, Nuno and Romão, Teresa},
  title = {VR Open Scores: Scores as Inspiration for VR Scenarios},
  pages = {109--114},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813262},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper21.pdf},
  presentation-video = {https://youtu.be/JSM6Rydz7iE},
  abstract = {In this paper, we introduce the concept of VR Open Scores: aleatoric score-based virtual scenarios where an aleatoric score is embedded in a virtual environment. This idea builds upon the notion of graphic scores and composed instrument, and apply them in a new context. Our proposal also explores possible parallels between open meaning in interaction design, and aleatoric score, conceptualized as Open Work by the Italian philosopher Umberto Eco. Our approach has two aims. The first aim is to create an environment where users can immerse themselves in the visual elements of a score while listening to the corresponding music. The second aim is to facilitate users to develop a personal relationship with both the system and the score. To achieve those aims, as a practical implementation of our proposed concept, we developed two immersive scenarios: a 360º video and an interactive space. We conclude presenting how our design aims were accomplished in the two scenarios, and describing positive and negative elements of our implementations.}
}

@inproceedings{NIME20_22,
  author = {Skuse, Amble H C  and Knotts, Shelly},
  title = {Creating an Online Ensemble for Home Based Disabled Musicians: Disabled Access and Universal Design - why disabled people must be at the heart of developing technology.},
  pages = {115--120},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813266},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper22.pdf},
  presentation-video = {https://youtu.be/m4D4FBuHpnE},
  abstract = {The project takes a Universal Design approach to exploring the possibility of creating a software platform to facilitate a Networked Ensemble for Disabled musicians. In accordance with the Nothing About Us Without Us (Charlton, 1998) principle I worked with a group of 15 professional musicians who are also disabled. The group gave interviews as to their perspectives and needs around networked music practices and this data was then analysed to look at how software design could be developed to make it more accessible. We also identified key messages for the wider design of digital musical instrument makers, performers and event organisers to improve practice around working with and for disabled musicians. }
}

@inproceedings{NIME20_23,
  author = {Çamcı, Anıl and Vilaplana, Matias and Wang, Ruth},
  title = {Exploring the Affordances of VR for Musical Interaction Design with VIMEs},
  pages = {121--126},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813268},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper23.pdf},
  abstract = {As virtual reality (VR) continues to gain prominence as a medium for artistic expression, a growing number of projects explore the use of VR for musical interaction design. In this paper, we discuss the concept of VIMEs (Virtual Interfaces for Musical Expression) through four case studies that explore different aspects of musical interactions in virtual environments. We then describe a user study designed to evaluate these VIMEs in terms of various usability considerations, such as immersion, perception of control, learnability and physical effort. We offer the results of the study, articulating the relationship between the design of a VIME and the various performance behaviors observed among its users. Finally, we discuss how these results, combined with recent developments in VR technology, can inform the design of new VIMEs.}
}

@inproceedings{NIME20_24,
  author = {Çamcı, Anıl and Willette, Aaron and Gargi, Nachiketa and Kim, Eugene and Xu, Julia and Lai, Tanya},
  title = {Cross-platform and Cross-reality Design of Immersive Sonic Environments},
  pages = {127--130},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813270},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper24.pdf},
  abstract = {The continued growth of modern VR (virtual reality) platforms into mass adoption is fundamentally driven by the work of content creators who offer engaging experiences. It is therefore essential to design accessible creativity support tools that can facilitate the work of a broad range of practitioners in this domain. In this paper, we focus on one facet of VR content creation, namely immersive audio design. We discuss a suite of design tools that enable both novice and expert users to rapidly prototype immersive sonic environments across desktop, virtual reality and augmented reality platforms. We discuss the design considerations adopted for each implementation, and how the individual systems informed one another in terms of interaction design. We then offer a preliminary evaluation of these systems with reports from first-time users. Finally, we discuss our road-map for improving individual and collaborative creative experiences across platforms and realities in the context of immersive audio.}
}

@inproceedings{NIME20_25,
  author = {Schebella, Marius and Fischbacher, Gertrud and Mosher, Matthew},
  title = {Silver: A Textile Wireframe Interface for the Interactive Sound Installation Idiosynkrasia},
  pages = {131--132},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813272},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper25.pdf},
  abstract = {Silver is an artwork that deals with the emotional feeling of contact by exaggerating it acoustically. It originates from an interactive room installation, where several textile sculptures merge with sounds. Silver is made from a wire mesh and its surface is reactive to closeness and touch. This material property forms a hybrid of artwork and parametric controller for the real-time sound generation. The textile quality of the fine steel wire-mesh evokes a haptic familiarity inherent to textile materials.  This makes it easy for the audience to overcome the initial threshold barrier to get in touch with the artwork in an exhibition situation. Additionally, the interaction is not dependent on visuals. The characteristics of the surface sensor allows a user to play the instrument without actually touching it.}
}

@inproceedings{NIME20_26,
  author = {Yang, Ning and Savery, Richard and Sankaranarayanan, Raghavasimhan and Zahray, Lisa and Weinberg, Gil},
  title = {Mechatronics-Driven Musical Expressivity for Robotic Percussionists},
  pages = {133--138},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813274},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper26.pdf},
  presentation-video = {https://youtu.be/KsQNlArUv2k},
  abstract = {Musical expressivity is an important aspect of musical performance for humans as well as robotic musicians. We present a novel mechatronics-driven implementation of Brushless Direct Current (BLDC) motors in a robotic marimba player, named ANON, designed to improve speed, dynamic range (loudness), and ultimately perceived musical expressivity in comparison to state-of-the-art robotic percussionist actuators. In an objective test of dynamic range, we find that our implementation provides wider and more consistent dynamic range response in comparison with solenoid-based robotic percussionists. Our implementation also outperforms both solenoid and human marimba players in striking speed. In a subjective listening test measuring musical expressivity, our system performs significantly better than a solenoid-based system and is statistically indistinguishable from human performers.}
}

@inproceedings{NIME20_27,
  author = {Dunham, Paul},
  title = {Click::RAND. A Minimalist Sound Sculpture.},
  pages = {139--142},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813276},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper27.pdf},
  presentation-video = {https://youtu.be/vWKw8H0F9cI},
  abstract = {Discovering outmoded or obsolete technologies and appropriating them in creative practice can uncover new relationships between those technologies. Using a media archaeological research approach, this paper presents the electromechanical relay and a book of random numbers as related forms of obsolete media. Situated within the context of electromechanical sound art, the work uses a non-deterministic approach to explore the non-linear and unpredictable agency and materiality of the objects in the work. Developed by the first author, Click::RAND is an object-based sound installation. The work has been developed as an audio-visual representation of a genealogy of connections between these two forms of media in the history of computing.}
}

@inproceedings{NIME20_28,
  author = {Tomás, Enrique},
  title = {A Playful Approach to Teaching NIME: Pedagogical Methods from a Practice-Based Perspective},
  pages = {143--148},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813280},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper28.pdf},
  presentation-video = {https://youtu.be/94o3J3ozhMs},
  abstract = {This paper reports on the experience gained after five years of teaching a NIME master course designed specifically for artists. A playful pedagogical approach based on practice-based methods is presented and evaluated. My goal was introducing the art of NIME design and performance giving less emphasis to technology. Instead of letting technology determine how we teach and think during the class, I propose fostering at first the student's active construction and understanding of the field experimenting with physical materials,sound production and bodily movements. For this intention I developed a few classroom exercises which my students had to study and practice. During this period of five years, 95 students attended the course. At the end of the semester course, each student designed, built and performed a new interface for musical expression in front of an audience. Thus, in this paper I describe and discuss the benefits of applying playfulness and practice-based methods for teaching NIME in art universities. I introduce the methods and classroom exercises developed and finally I present some lessons learned from this pedagogical experience.}
}

@inproceedings{NIME20_29,
  author = {Jarvis Holland, Quinn D and Quartez, Crystal and Botello, Francisco and Gammill, Nathan},
  title = {EXPANDING ACCESS TO MUSIC TECHNOLOGY-  Rapid Prototyping Accessible Instrument Solutions For Musicians With Intellectual Disabilities},
  pages = {149--153},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813286},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper29.pdf},
  abstract = {Using open-source and creative coding frameworks, a team of artist-engineers from Portland Community College working with artists that experience Intellectual/Developmental disabilities prototyped an ensemble of adapted instruments and synthesizers that facilitate real-time in-key collaboration. The instruments employ a variety of sensors, sending the resulting musical controls to software sound generators via MIDI. Careful consideration was given to the balance between freedom of expression, and curating the possible sonic outcomes as adaptation. Evaluation of adapted instrument design may differ greatly from frameworks for evaluating traditional instruments or products intended for mass-market, though the results of such focused and individualised design have a variety of possible applications.}
}

@inproceedings{NIME20_3,
  author = {Boem, Alberto and Troiano, Giovanni M and and Lepri, Giacomo and Zappi, Victor},
  title = {Non-Rigid Musical Interfaces: Exploring Practices, Takes, and Future Perspective},
  pages = {17--22},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813288},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper3.pdf},
  presentation-video = {https://youtu.be/o4CuAglHvf4},
  abstract = {Non-rigid interfaces allow for exploring new interactive paradigms that rely on deformable input and shape change, and whose possible applications span several branches of human-computer interaction (HCI). While extensively explored as deformable game controllers, bendable smartphones, and shape-changing displays, non-rigid interfaces are rarely framed in a musical context, and their use for composition and performance is rather sparse and unsystematic.  With this work, we start a systematic exploration of this relatively uncharted research area, by means of (1) briefly reviewing existing musical interfaces that capitalize on deformable input,and (2) surveying 11 among experts and pioneers in the field about their experience with and vision on non-rigid musical interfaces.Based on experts’ input, we suggest possible next steps of musical appropriation with deformable and shape-changing technologies.We conclude by discussing how cross-overs between NIME and HCI research will benefit non-rigid interfaces.}
}

@inproceedings{NIME20_30,
  author = {Atherton, Jack and Wang, Ge},
  title = {Curating Perspectives: Incorporating Virtual Reality into Laptop Orchestra Performance},
  pages = {154--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813290},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper30.pdf},
  presentation-video = {https://youtu.be/tmeDO5hg56Y},
  abstract = {Despite a history spanning nearly 30 years, best practices for the use of virtual reality (VR) in computer music performance remain exploratory. Here, we present a case study of a laptop orchestra performance entitled Resilience, involving one VR performer and an ensemble of instrumental performers, in order to explore values and design principles for incorporating this emerging technology into computer music performance. We present a brief history at the intersection of VR and the laptop orchestra. We then present the design of the piece and distill it into a set of design principles. Broadly, these design principles address the interplay between the different conflicting perspectives at play: those of the VR performer, the ensemble, and the audience. For example, one principle suggests that the perceptual link between the physical and virtual world maybe enhanced for the audience by improving the performers' sense of embodiment. We argue that these design principles are a form of generalized knowledge about how we might design laptop orchestra pieces involving virtual reality.}
}

@inproceedings{NIME20_31,
  author = {Morreale, Fabio and Bin, S. M. Astrid and McPherson, Andrew and Stapleton, Paul and Wanderley, Marcelo},
  title = {A NIME Of The Times: Developing an Outward-Looking Political Agenda For This Community},
  pages = {160--165},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813294},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper31.pdf},
  presentation-video = {https://youtu.be/y2iDN24ZLTg},
  abstract = {So far, NIME research has been mostly inward-looking, dedicated to divulging and studying our own work and having limited engagement with trends outside our community. Though musical instruments as cultural artefacts are inherently political, we have so far not sufficiently engaged with confronting these themes in our own research. In this paper we argue that we should consider how our work is also political, and begin to develop a clear political agenda that includes social, ethical, and cultural considerations through which to consider not only our own musical instruments, but also those not created by us. Failing to do so would result in an unintentional but tacit acceptance and support of such ideologies. We explore one item to be included in this political agenda: the recent trend in music technology of ``democratising music'', which carries implicit political ideologies grounded in techno-solutionism. We conclude with a number of recommendations for stimulating community-wide discussion on these themes in the hope that this leads to the development of an outward-facing perspective that fully engages with political topics.}
}

@inproceedings{NIME20_32,
  author = {Ko, Chantelle L and Oehlberg, Lora},
  title = {Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard},
  pages = {166--171},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813300},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper32.pdf},
  presentation-video = {https://youtu.be/XIAd_dr9PHE},
  abstract = {We present TRAVIS II, an augmented acoustic violin with touch sensors integrated into its 3D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament which produce an electric signal when fingers press down on each string. While these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four FSRs attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. While the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. In this paper, we describe the design of the instrument, experiments leading to the sensing fingerboard, and performative applications of the instrument.}
}

@inproceedings{NIME20_33,
  author = {Gold, Nicolas E and Wang, Chongyang and Olugbade, Temitayo and Berthouze, Nadia and Williams, Amanda},
  title = {P(l)aying Attention: Multi-modal, multi-temporal music control},
  pages = {172--175},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813303},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper33.pdf},
  abstract = {The expressive control of sound and music through body movements is well-studied.  For some people, body movement is demanding, and although they would prefer to express themselves freely using gestural control, they are unable to use such interfaces without difficulty.  In this paper, we present the P(l)aying Attention framework for manipulating recorded music to support these people, and to help the therapists that work with them. The aim is to facilitate body awareness, exploration, and expressivity by allowing the manipulation of a pre-recorded ‘ensemble’ through an interpretation of body movement, provided by a machine-learning system trained on physiotherapist assessments and movement data from people with chronic pain.  The system considers the nature of a person’s movement (e.g. protective) and offers an interpretation in terms of the joint-groups that are playing a major role in the determination at that point in the movement, and to which attention should perhaps be given (or the opposite at the user’s discretion).  Using music to convey the interpretation offers informational (through movement sonification) and creative (through manipulating the ensemble by movement) possibilities.  The approach offers the opportunity to explore movement and music at multiple timescales and under varying musical aesthetics.}
}

@inproceedings{NIME20_34,
  author = {Cavdir, Doga and Wang, Ge},
  title = {Felt Sound: A Shared Musical Experience for the Deaf and Hard of Hearing},
  pages = {176--181},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813305},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper34.pdf},
  presentation-video = {https://youtu.be/JCvlHu4UaZ0},
  abstract = {We present a musical interface specifically designed for inclusive performance that offers a shared experience for both individuals who are deaf and hard of hearing as well as those who are not. This interface borrows gestures (with or without overt meaning) from American Sign Language (ASL), rendered using low-frequency sounds that can be felt by everyone in the performance. The Deaf and Hard of Hearing cannot experience the sound in the same way. Instead, they are able to physically experience the vibrations, nuances, contours, as well as its correspondence with the hand gestures. Those who are not hard of hearing can experience the sound, but also feel it just the same, with the knowledge that the same physical vibrations are shared by everyone. The employment of sign language adds another aesthetic dimension to the instrument --a nuanced borrowing of a functional communication medium for an artistic end.  }
}

@inproceedings{NIME20_35,
  author = {Leitman, Sasha},
  title = {Sound Based Sensors for NIMEs},
  pages = {182--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813309},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper35.pdf},
  abstract = {This paper examines the use of Sound Sensors and audio as input material for New Interfaces for Musical Expression (NIMEs), exploring the unique affordances and character of the interactions and instruments that leverage it. Examples of previous work in the literature that use audio as sensor input data are examined for insights into how the use of Sound Sensors provides unique opportunities within the NIME context.  We present the results of a user study comparing sound-based sensors to other sensing modalities within the context of controlling parameters.  The study suggests that the use of Sound Sensors can enhance gestural flexibility and nuance  but that they also present challenges in accuracy and repeatability.}
}

@inproceedings{NIME20_36,
  author = {Ikawa, Yuma and Matsuura, Akihiro},
  title = {Playful Audio-Visual Interaction with Spheroids },
  pages = {188--189},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813311},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper36.pdf},
  abstract = {This paper presents a novel interactive system for creating audio-visual expressions on tabletop display by dynamically manipulating solids of revolution called spheroids. The four types of basic spinning and rolling movements of spheroids are recognized from the physical conditions such as the contact area, the location of the centroid, the (angular) velocity, and the curvature of the locus all obtained from sensor data on the display. They are then used for interactively generating audio-visual effects that match each of the movements. We developed a digital content that integrated these functionalities and enabled composition and live performance through manipulation of spheroids.}
}

@inproceedings{NIME20_37,
  author = {Park, Sihwa},
  title = {Collaborative Mobile Instruments in a Shared AR Space: a Case of ARLooper},
  pages = {190--195},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813313},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper37.pdf},
  presentation-video = {https://youtu.be/Trw4epKeUbM},
  abstract = {This paper presents ARLooper, an augmented reality mobile interface that allows multiple users to record sound and perform together in a shared AR space. ARLooper is an attempt to explore the potential of collaborative mobile AR instruments in supporting non-verbal communication for musical performances. With ARLooper, the user can record, manipulate, and play sounds being visualized as 3D waveforms in an AR space. ARLooper provides a shared AR environment wherein multiple users can observe each other's activities in real time, supporting increasing the understanding of collaborative contexts. This paper provides the background of the research and the design and technical implementation of ARLooper, followed by a user study.}
}

@inproceedings{NIME20_38,
  author = {Schwarz, Diemo and Liu, Abby Wanyu and Bevilacqua, Frederic},
  title = {A Survey on the Use of 2D Touch Interfaces for Musical Expression},
  pages = {196--201},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813318},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper38.pdf},
  presentation-video = {https://youtu.be/eE8I3mecaB8},
  abstract = {Expressive 2D multi-touch interfaces have in recent years moved from research prototypes to industrial products, from repurposed generic computer input devices to controllers specially designed for musical expression. A host of practicioners use this type of devices in many different ways, with different gestures and sound synthesis or transformation methods. In order to get an overview of existing and desired usages, we launched an on-line survey that collected 37 answers from practicioners in and outside of academic and design communities. In the survey we inquired about the participants' devices, their strengths and weaknesses, the layout of control dimensions, the used gestures and mappings, the synthesis software or hardware and the use of audio descriptors and machine learning. The results can inform the design of future interfaces, gesture analysis and mapping, and give directions for the need and use of machine learning for user adaptation.}
}

@inproceedings{NIME20_39,
  author = {Renney, Harri L and Mitchell, Tom and Gaster, Benedict},
  title = {There and Back Again: The Practicality of GPU Accelerated Digital Audio},
  pages = {202--207},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813320},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper39.pdf},
  presentation-video = {https://youtu.be/xAVEHJZRIx0},
  abstract = {General-Purpose GPU computing is becoming an increasingly viable option for acceleration, including in the audio domain. Although it can improve performance, the intrinsic nature of a device like the GPU involves data transfers and execution commands which requires time to complete. Therefore, there is an understandable caution concerning the overhead involved with using the GPU for audio computation. This paper aims to clarify the limitations by presenting a performance benchmarking suite. The benchmarks utilize OpenCL and CUDA across various tests to highlight the considerations and limitations of processing audio in the GPU environment. The benchmarking suite has been used to gather a collection of results across various hardware. Salient results have been reviewed in order to highlight the benefits and limitations of the GPU for digital audio. The results in this work show that the minimal GPU overhead fits into the real-time audio requirements provided the buffer size is selected carefully. The baseline overhead is shown to be roughly 0.1ms, depending on the GPU. This means buffer sizes 8 and above are completed within the allocated time frame. Results from more demanding tests, involving physical modelling synthesis, demonstrated a balance was needed between meeting the sample rate and keeping within limits for latency and jitter. Buffer sizes from 1 to 16 failed to sustain the sample rate whilst buffer sizes 512 to 32768 exceeded either latency or jitter limits. Buffer sizes in between these ranges, such as 256, satisfied the sample rate, latency and jitter requirements chosen for this paper.}
}

@inproceedings{NIME20_4,
  author = {Shaw, Tim and Bowers, John},
  title = {Ambulation: Exploring Listening Technologies for an Extended Sound Walking Practice},
  pages = {23--28},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813322},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper4.pdf},
  presentation-video = {https://youtu.be/dDXkNnQXdN4},
  abstract = {Ambulation is a sound walk that uses field recording techniques and listening technologies to create a walking performance using environmental sound. Ambulation engages with the act of recording as an improvised performance in response to the soundscapes it is presented within. In this paper we describe the work and place it in relationship to other artists engaged with field recording and extended sound walking practices. We will give technical details of the Ambulation system we developed as part of the creation of the piece, and conclude with a collection of observations that emerged from the project. The research around the development and presentation of Ambulation contributes to the idea of field recording as a live, procedural practice, moving away from the ideas of the movement of documentary material from one place to another. We will show how having an open, improvisational approach to technologically supported sound walking enables rich and unexpected results to occur and how this way of working can contribute to NIME design and thinking.}
}

@inproceedings{NIME20_40,
  author = {Xia, Gus and Chin, Daniel and Zhang, Yian and Zhang, Tianyu and Zhao, Junbo},
  title = {Interactive Rainbow Score:  A Visual-centered Multimodal Flute Tutoring System},
  pages = {208--213},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813324},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper40.pdf},
  abstract = {Learning to play an instrument is intrinsically multimodal, and we have seen a trend of applying visual and haptic feedback in music games and computer-aided music tutoring systems. However, most current systems are still designed to master individual pieces of music; it is unclear how well the learned skills can be generalized to new pieces. We aim to explore this question. In this study, we contribute Interactive Rainbow Score, an interactive visual system to boost the learning of sight-playing, the general musical skill to read music and map the visual representations to performance motions. The key design of Interactive Rainbow Score is to associate pitches (and the corresponding motions) with colored notation and further strengthen such association via real-time interactions. Quantitative results show that the interactive feature on average increases the learning efficiency by 31.1%. Further analysis indicates that it is critical to apply the interaction in the early period of learning.}
}

@inproceedings{NIME20_41,
  author = {Davanzo, Nicola and Avanzini, Federico},
  title = {A Dimension Space for the Evaluation of Accessible Digital Musical Instruments},
  pages = {214--220},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813326},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper41.pdf},
  presentation-video = {https://youtu.be/pJlB5k8TV9M},
  abstract = {Research on Accessible Digital Musical Instruments (ADMIs) has received growing attention over the past decades, carving out an increasingly large space in the literature. Despite the recent publication of state-of-the-art review works, there are still few systematic studies on ADMIs design analysis. In this paper we propose a formal tool to explore the main design aspects of ADMIs based on Dimension Space Analysis, a well established methodology in the NIME literature which allows to generate an effective visual representation of the design space. We therefore propose a set of relevant dimensions, which are based both on categories proposed in recent works in the research context, and on original contributions. We then proceed to demonstrate its applicability by selecting a set of relevant case studies, and analyzing a sample set of ADMIs found in the literature.}
}

@inproceedings{NIME20_42,
  author = {Melbye, Adam Pultz and Ulfarsson, Halldor A},
  title = {Sculpting the behaviour of the Feedback-Actuated Augmented Bass: Design strategies for subtle manipulations of string feedback using simple adaptive algorithms},
  pages = {221--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813328},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper42.pdf},
  presentation-video = {https://youtu.be/jXePge1MS8A},
  abstract = {This paper describes physical and digital design strategies for the Feedback-Actuated Augmented Bass - a self-contained feedback double bass with embedded DSP capabilities. A primary goal of the research project is to create an instrument that responds well to the use of extended playing techniques and can manifest complex harmonic spectra while retaining the feel and sonic 
fingerprint of an acoustic double bass. While the physical con
figuration of the instrument builds on similar feedback string instruments being developed in recent years, this project focuses on modifying the feedback behaviour through low-level audio feature extractions coupled to computationally lightweight 
filtering and amplitude management algorithms. We discuss these adaptive and time-variant processing strategies and how we apply them in sculpting the system's dynamic and complex behaviour to our liking.}
}

@inproceedings{NIME20_43,
  author = {Le Vaillant, Gwendal and Dutoit, Thierry and Giot, Rudi},
  title = {Analytic vs. holistic approaches for the live search of sound presets using graphical interpolation},
  pages = {227--232},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813330},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper43.pdf},
  presentation-video = {https://youtu.be/Korw3J_QvQE},
  abstract = {The comparative study presented in this paper focuses on two approaches for the search of sound presets using a specific geometric touch app. The first approach is based on independent sliders on screen and is called analytic. The second is based on interpolation between presets represented by polygons on screen and is called holistic. Participants had to listen to, memorize, and search for sound presets characterized by four parameters. Ten different configurations of sound synthesis and processing were presented to each participant, once for each approach. The performance scores of 28 participants (not including early testers) were computed using two measured values: the search duration, and the parametric distance between the reference and answered presets. Compared to the analytic sliders-based interface, the holistic interpolation-based interface demonstrated a significant performance improvement for 60% of sound synthesizers. The other 40% led to equivalent results for the analytic and holistic interfaces. Using sliders, expert users performed nearly as well as they did with interpolation. Beginners and intermediate users struggled more with sliders, while the interpolation allowed them to get quite close to experts’ results.}
}

@inproceedings{NIME20_44,
  author = {Mitchusson, Chase},
  title = {Indeterminate Sample Sequencing in Virtual Reality},
  pages = {233--236},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813332},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper44.pdf},
  abstract = {The purpose of this project is to develop an interface for writing and performing music using sequencers in virtual reality (VR). The VR sequencer deals with chance-based operations to select audio clips for playback and spatial orientation-based rhythm and melody generation, while incorporating three-dimensional (3-D) objects as omnidirectional playheads. Spheres which grow from a variable minimum size to a variable maximum size at a variable speed, constantly looping, represent the passage of time in this VR sequencer. The 3-D assets which represent samples are actually sample containers that come in six common dice shapes. As the dice come into contact with a sphere, their samples are triggered to play. This behavior mimics digital audio workstation (DAW) playheads reading MIDI left-to-right in popular professional and consumer software sequencers. To incorporate height into VR music making, the VR sequencer is capable of generating terrain at the press of a button. Each terrain will gradually change, creating the possibility for the dice to roll on their own. Audio effects are built in to each scene and mapped to terrain parameters, creating another opportunity for chance operations in the music making process. The chance-based sample selection, spatial orientation-defined rhythms, and variable terrain mapped to audio effects lead to indeterminacy in performance and replication of a single piece of music. This project aims to give the gaming community access to experimental music making by means of consumer virtual reality hardware.}
}

@inproceedings{NIME20_45,
  author = {Fiebrink, Rebecca and Sonami, Laetitia},
  title = {Reflections on Eight Years of Instrument Creation with Machine Learning},
  pages = {237--242},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813334},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper45.pdf},
  presentation-video = {https://youtu.be/EvXZ9NayZhA},
  abstract = {Machine learning (ML) has been used to create mappings for digital musical instruments for over twenty-five years, and numerous ML toolkits have been developed for the NIME community. However, little published work has studied how ML has been used in sustained instrument building and performance practices. This paper examines the experiences of instrument builder and performer Laetitia Sonami, who has been using ML to build and refine her Spring Spyre instrument since 2012. Using Sonami’s current practice as a case study, this paper explores the utility, opportunities, and challenges involved in using ML in practice over many years. This paper also reports the perspective of Rebecca Fiebrink, the creator of the Wekinator ML tool used by Sonami, revealing how her work with Sonami has led to changes to the software and to her teaching. This paper thus contributes a deeper understanding of the value of ML for NIME practitioners, and it can inform design considerations for future ML toolkits as well as NIME pedagogy. Further, it provides new perspectives on familiar NIME conversations about mapping strategies, expressivity, and control, informed by a dedicated practice over many years.}
}

@inproceedings{NIME20_46,
  author = {Lucas, Alex and Ortiz, Miguel and Schroeder, Franziska},
  title = {The Longevity of Bespoke, Accessible Music Technology: A Case for Community},
  pages = {243--248},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813338},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper46.pdf},
  presentation-video = {https://youtu.be/cLguyuZ9weI},
  abstract = {Based on the experience garnered through a longitudinal ethnographic study, the authors reflect on the practice of designing and fabricating bespoke, accessible music tech- nologies. Of particular focus are the social, technical and environmental factors at play which make the provision of such technology a reality. The authors make suggestions of ways to achieve long-term, sustained use. Seemingly those involved in its design, fabrication and use could benefit from a concerted effort to share resources, knowledge and skill as a mobilised community of practitioners.}
}

@inproceedings{NIME20_47,
  author = {Bukvic, Ivica I and Sardana, Disha and Joo, Woohun},
  title = {New Interfaces for Spatial Musical Expression},
  pages = {249--254},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813342},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper47.pdf},
  presentation-video = {https://youtu.be/GQ0552Lc1rw},
  abstract = {With the proliferation of venues equipped with the high density loudspeaker arrays there is a growing interest in developing new interfaces for spatial musical expression (NISME). Of particular interest are interfaces that focus on the emancipation of the spatial domain as the primary dimension for musical expression. Here we present Monet NISME that leverages multitouch pressure-sensitive surface and the D4 library's spatial mask and thereby allows for a unique approach to interactive spatialization. Further, we present a study with 22 participants designed to assess its usefulness and compare it to the Locus, a NISME introduced in 2019 as part of a localization study which is built on the same design principles of using natural gestural interaction with the spatial content. Lastly, we briefly discuss the utilization of both NISMEs in two artistic performances and propose a set of guidelines for further exploration in the NISME domain.}
}

@inproceedings{NIME20_48,
  author = {Durham, Mark},
  title = {Inhabiting the Instrument},
  pages = {255--258},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813344},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper48.pdf},
  abstract = {This study presents an ecosystemic approach to music interaction, through the practice-based development of a mixed reality installation artwork. It fuses a generative, immersive audio composition with augmented reality visualisation, within an architectural space as part of a blended experience. Participants are encouraged to explore and interact with this combination of elements through physical engagement, to then develop an understanding of how the blending of real and virtual space occurs as the installation unfolds. The sonic layer forms a link between the two, as a three-dimensional sound composition. Connections in the system allow for multiple streams of data to run between the layers, which are used for the real-time modulation of parameters. These feedback mechanisms form a complete loop between the participant in real space, soundscape, and mixed reality visualisation, providing a participant mediated experience that exists somewhere between creator and observer.}
}

@inproceedings{NIME20_49,
  author = {Nash, Chris},
  title = {Crowd-driven Music: Interactive and Generative Approaches using Machine Vision and Manhattan},
  pages = {259--264},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813346},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper49.pdf},
  presentation-video = {https://youtu.be/DHIowP2lOsA},
  abstract = {This paper details technologies and artistic approaches to crowd-driven music, discussed in the context of a live public installation in which activity in a public space (a busy railway platform) is used to drive the automated composition and performance of music. The approach presented uses realtime machine vision applied to a live video feed of a scene, from which detected objects and people are fed into Manhattan (Nash, 2014), a digital music notation that integrates sequencing and programming to support the live creation of complex musical works that combine static, algorithmic, and interactive elements. The paper discusses the technical details of the system and artistic development of specific musical works, introducing novel techniques for mapping chaotic systems to musical expression and exploring issues of agency, aesthetic, accessibility and adaptability relating to composing interactive music for crowds and public spaces. In particular, performances as part of an installation for BBC Music Day 2018 are described. The paper subsequently details a practical workshop, delivered digitally, exploring the development of interactive performances in which the audience or general public actively or passively control live generation of a musical piece. Exercises support discussions on technical, aesthetic, and ontological issues arising from the identification and mapping of structure, order, and meaning in non-musical domains to analogous concepts in musical expression. Materials for the workshop are available freely with the Manhattan software.}
}

@inproceedings{NIME20_5,
  author = {Krzyzaniak, Michael J},
  title = {Words to Music Synthesis},
  pages = {29--34},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813350},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper5.pdf},
  abstract = {This paper discusses the design of a musical synthesizer that takes words as input, and attempts to generate music that somehow underscores those words. This is considered as a tool for sound designers who could, for example, enter dialogue from a film script and generate appropriate back- ground music. The synthesizer uses emotional valence and arousal as a common representation between words and mu- sic. It draws on previous studies that relate words and mu- sical features to valence and arousal. The synthesizer was evaluated with a user study. Participants listened to music generated by the synthesizer, and described the music with words. The arousal of the words they entered was highly correlated with the intended arousal of the music. The same was, surprisingly, not true for valence. The synthesizer is online, at [redacted URL].}
}

@inproceedings{NIME20_50,
  author = {Mclean, Alex},
  title = {Algorithmic Pattern},
  pages = {265--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813352},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper50.pdf},
  presentation-video = {https://youtu.be/X9AkOAEDV08},
  abstract = {This paper brings together two main perspectives on algorithmic pattern. First, the writing of musical patterns in live coding performance, and second, the weaving of patterns in textiles. In both cases, algorithmic pattern is an interface between the human and the outcome, where small changes have far-reaching impact on the results. By bringing contemporary live coding and ancient textile approaches together, we reach a common view of pattern as algorithmic movement (e.g. looping, shifting, reflecting, interfering) in the making of things. This works beyond the usual definition of pattern used in musical interfaces, of mere repeating sequences. We conclude by considering the place of algorithmic pattern in a wider activity of making.}
}

@inproceedings{NIME20_51,
  author = {McCallum, Louis and Grierson, Mick S},
  title = {Supporting Interactive Machine Learning Approaches to Building Musical Instruments in the Browser},
  pages = {271--272},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813357},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper51.pdf},
  abstract = {Interactive machine learning (IML) is an approach to building interactive systems, including DMIs, focusing on iterative end-user data provision and direct evaluation. This paper describes the implementation of a Javascript library, encapsulating many of the boilerplate needs of building IML systems for creative tasks with minimal code inclusion and low barrier to entry. Further, we present a set of complimentary Audio Worklet-backed instruments to allow for in-browser creation of new musical systems able to run concurrently with various computationally expensive feature extractor and lightweight machine learning models without the interference often seen in interactive Web Audio applications.}
}

@inproceedings{NIME20_52,
  author = {Kirkegaard, Mathias S and Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo},
  title = {TorqueTuner: A self contained module for designing rotary haptic force feedback for digital musical instruments},
  pages = {273--278},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813359},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper52.pdf},
  presentation-video = {https://youtu.be/V8WDMbuX9QA},
  abstract = {TorqueTuner is an embedded module that allows Digital Musical Instrument (DMI) designers to map sensors to parameters of haptic effects and dynamically modify rotary force feedback in real-time. We embedded inside TorqueTuner a collection of haptic effects (Wall, Magnet, Detents, Spring, Friction, Spin, Free) and a bi-directional interface through libmapper, a software library for making connections between data signals on a shared network. To increase affordability and portability of force-feedback implementations in DMI design, we designed our platform to be wireless, self-contained and built from commercially available components. To provide examples of modularity and portability, we integrated TorqueTuner into a standalone haptic knob and into an existing DMI, the T-Stick. We implemented 3 musical applications (Pitch wheel, Turntable and Exciter), by mapping sensors to sound synthesis in audio programming environment SuperCollider. While the original goal was to simulate the haptic feedback associated with turning a knob, we found that the platform allows for further expanding interaction possibilities in application scenarios where rotary control is familiar.}
}

@inproceedings{NIME20_53,
  author = {Ford, Corey J and Nash, Chris},
  title = {An Iterative Design ‘by proxy’ Method for Developing Educational Music Interfaces},
  pages = {279--284},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813361},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper53.pdf},
  presentation-video = {https://youtu.be/fPbZMQ5LEmk},
  abstract = {Iterative design methods involving children and educators are difficult to conduct, given both the ethical implications and time commitments understandably required. The qualitative design process presented here recruits introductory teacher training students, towards discovering useful design insights relevant to music education technologies “by proxy”. Therefore, some of the barriers present in child-computer interaction research are avoided. As an example, the method is applied to the creation of a block-based music notation system, named Codetta. Building upon successful educational technologies that intersect both music and computer programming, Codetta seeks to enable child composition, whilst aiding generalist educator’s confidence in teaching music.}
}

@inproceedings{NIME20_54,
  author = {Calegario, Filipe and Wanderley, Marcelo and Tragtenberg, João and Meneses, Eduardo and Wang, Johnty and Sullivan, John and Franco, Ivan and Kirkegaard, Mathias S and Bredholt, Mathias and Rohs, Josh},
  title = {Probatio 1.0: collaborative development of a toolkit for functional DMI prototypes},
  pages = {285--290},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813363},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper54.pdf},
  presentation-video = {https://youtu.be/jkFnZZUA3xs},
  abstract = {Probatio is an open-source toolkit for prototyping new digital musical instruments created in 2016. Based on a morphological chart of postures and controls of musical instruments, it comprises a set of blocks, bases, hubs, and supports that, when combined, allows designers, artists, and musicians to experiment with different input devices for musical interaction in different positions and postures. Several musicians have used the system, and based on these past experiences, we assembled a list of improvements to implement version 1.0 of the toolkit through a unique international partnership between two laboratories in Brazil and Canada. In this paper, we present the original toolkit and its use so far, summarize the main lessons learned from musicians using it, and present the requirements behind, and the final design of, v1.0 of the project. We also detail the work developed in digital fabrication using two different techniques: laser cutting and 3D printing, comparing their pros and cons. We finally discuss the opportunities and challenges of fully sharing the project online and replicating its parts in both countries.}
}

@inproceedings{NIME20_55,
  author = {West, Travis J and Wanderley, Marcelo and Caramiaux, Baptiste},
  title = {Making Mappings: Examining the Design Process},
  pages = {291--296},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813365},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper55.pdf},
  presentation-video = {https://youtu.be/aaoResYjqmE},
  abstract = {We conducted a study which examines mappings from a relatively unexplored perspective: how they are made. Twelve skilled NIME users designed a mapping from a T-Stick to a subtractive synthesizer, and were interviewed about their approach to mapping design. We present a thematic analysis of the interviews, with reference to data recordings captured while the designers worked. Our results suggest that the mapping design process is an iterative process that alternates between two working modes: diffuse exploration and directed experimentation.  }
}

@inproceedings{NIME20_56,
  author = {Sidler, Michael and Bisson, Matthew C and Grotz, Jordan and Barton, Scott},
  title = {Parthenope: A Robotic Musical Siren},
  pages = {297--300},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813367},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper56.pdf},
  presentation-video = {https://youtu.be/HQuR0aBJ70Y},
  abstract = {Parthenope is a robotic musical siren developed to produce unique timbres and sonic gestures. Parthenope uses perforated spinning disks through which air is directed to produce sound. Computer-control of disk speed and air flow in conjunction with a variety of nozzles allow pitches to be precisely produced at different volumes. The instrument is controlled via Open Sound Control (OSC) messages sent over an ethernet connection and can interface with common DAWs and physical controllers. Parthenope is capable of microtonal tuning, portamenti, rapid and precise articulation (and thus complex rhythms) and distinct timbres that result from its aerophonic character. It occupies a unique place among robotic musical instruments.}
}

@inproceedings{NIME20_57,
  author = {Kemper, Steven},
  title = {Tremolo-Harp: A Vibration-Motor Actuated Robotic String Instrument},
  pages = {301--304},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813369},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper57.pdf},
  abstract = {The Tremolo-Harp is a twelve-stringed robotic instrument, where each string is actuated with a DC vibration motor to produce a mechatronic “tremolo” effect. It was inspired by instruments and musical styles that employ tremolo as a primary performance technique, including the hammered dulcimer, pipa, banjo, flamenco guitar, and surf rock guitar. Additionally, the Tremolo-Harp is designed to produce long, sustained textures and continuous dynamic variation. These capabilities represent a different approach from the majority of existing robotic string instruments, which tend to focus on actuation speed and rhythmic precision. The composition Tremolo-Harp Study 1 (2019) presents an initial exploration of the Tremolo-Harp’s unique timbre and capability for continuous dynamic variation.  }
}

@inproceedings{NIME20_58,
  author = {Kobayashi, Atsuya and Anzai, Reo and Tokui, Nao},
  title = {ExSampling: a system for the real-time ensemble performance of field-recorded environmental sounds},
  pages = {305--308},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813371},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper58.pdf},
  abstract = {We propose ExSampling: an integrated system of recording application and Deep Learning environment for a real-time music performance of environmental sounds sampled by field recording. Automated sound mapping to Ableton Live tracks by Deep Learning enables field recording to be applied to real-time performance, and create interactions among sound recorder, composers and performers.}
}

@inproceedings{NIME20_59,
  author = {Yepez Placencia, Juan Pablo and Murphy, Jim and Carnegie, Dale},
  title = {Designing an Expressive Pitch Shifting Mechanism for Mechatronic Chordophones},
  pages = {309--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813375},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper59.pdf},
  presentation-video = {https://youtu.be/rpX8LTZd-Zs},
  abstract = {The exploration of musical robots has been an area of interest due to the timbral and mechanical advantages they offer for music generation and performance. However, one of the greatest challenges in mechatronic music is to enable these robots to deliver a nuanced and expressive performance. This depends on their capability to integrate dynamics, articulation, and a variety of ornamental techniques while playing a given musical passage. In this paper we introduce a robot arm pitch shifter for a mechatronic monochord prototype. This is a fast, precise, and mechanically quiet system that enables sliding techniques during musical performance. We discuss the design and construction process, as well as the system's advantages and restrictions. We also review the quantitative evaluation process used to assess if the instrument meets the design requirements. This process reveals how the pitch shifter outperforms existing configurations, and potential areas of improvement for future work.}
}

@inproceedings{NIME20_6,
  author = {Ehrhardt, Marcel and Neupert, Max and Wegener, Clemens},
  title = {Piezoelectric strings as a musical interface},
  pages = {35--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813377},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper6.pdf},
  abstract = {Flexible strings with piezoelectric properties have been developed but until date not evaluated for the use as part of a musical instrument. This paper is assessing the properties of these new fibers, looking at their possibilities for NIME applications.}
}

@inproceedings{NIME20_60,
  author = {Ilsar, Alon A and Hughes, Matthew and Johnston, Andrew},
  title = {NIME or Mime: A Sound-First Approach to Developing an Audio-Visual Gestural Instrument},
  pages = {315--320},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813383},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper60.pdf},
  presentation-video = {https://youtu.be/ZFQKKI3dFhE},
  abstract = {This paper outlines the development process of an audio-visual gestural instrument—the AirSticks—and elaborates on the role ‘miming’ has played in the formation of new mappings for the instrument. The AirSticks, although fully-functioning, were used as props in live performances in order to evaluate potential mapping strategies that were later implemented for real. This use of mime when designing Digital Musical Instruments (DMIs) can help overcome choice paralysis, break from established habits, and liberate creators to realise more meaningful parameter mappings. Bringing this process into an interactive performance environment acknowledges the audience as stakeholders in the design of these instruments, and also leads us to reflect upon the beliefs and assumptions made by an audience when engaging with the performance of such ‘magical’ devices. This paper establishes two opposing strategies to parameter mapping, ‘movement-first’ mapping, and the less conventional ‘sound-first’ mapping that incorporates mime. We discuss the performance ‘One Five Nine’, its transformation from a partial mime into a fully interactive presentation, and the influence this process has had on the outcome of the performance and the AirSticks as a whole.}
}

@inproceedings{NIME20_61,
  author = {Hughes, Matthew and Johnston, Andrew},
  title = {URack: Audio-visual Composition and Performance using Unity and VCV Rack},
  pages = {321--322},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813389},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper61.pdf},
  abstract = {This demonstration presents URack, a custom-built audio-visual composition and performance environment that combines the Unity video-game engine with the VCV Rack software modular synthesiser. In alternative cross-modal solutions, a compromise is likely made in either the sonic or visual output, or the consistency and intuitiveness of the composition environment. By integrating control mechanisms for graphics inside VCV Rack, the music-making metaphors used to build a patch are extended into the visual domain. Users familiar with modular synthesizers are immediately able to start building high-fidelity graphics using the same control voltages regularly used to compose sound. Without needing to interact with two separate development environments, languages or metaphorical domains, users are encouraged to freely, creatively and enjoyably construct their own highly-integrated audio-visual instruments. This demonstration will showcase the construction of an audio-visual patch using URack, focusing on the integration of flexible GPU particle systems present in Unity with the vast library of creative audio composition modules inside VCV.}
}

@inproceedings{NIME20_62,
  author = {Wicaksono, Irmandy and Paradiso, Joseph},
  title = {KnittedKeyboard: Digital Knitting of Electronic Textile Musical Controllers},
  pages = {323--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813391},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper62.pdf},
  abstract = {In this work, we have developed a textile-based interactive surface fabricated through digital knitting technology. Our prototype explores intarsia, interlock patterning, and a collection of functional and non-functional fibers to create a piano-pattern textile for expressive and virtuosic sonic interaction. We combined conductive, thermochromic, and composite yarns with high-flex polyester yarns to develop KnittedKeyboard with its soft physical properties and responsive sensing and display capabilities. The individual and combination of each key could simultaneously sense discrete touch, as well as continuous proximity and pressure. The KnittedKeyboard enables performers to experience fabric-based multimodal interaction as they explore the seamless texture and materiality of the electronic textile.}
}

@inproceedings{NIME20_63,
  author = {Capra, Olivier and Berthaut, Florent and Grisoni, Laurent},
  title = {A Taxonomy of Spectator Experience Augmentation Techniques},
  pages = {327--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813396},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper63.pdf},
  abstract = {In the context of artistic performances, the complexity and diversity of digital interfaces may impair the spectator experience, in particular hiding the engagement and virtuosity of the performers. Artists and researchers have made attempts at solving this by augmenting performances with additional information provided through visual, haptic or sonic modalities. However, the proposed techniques have not yet been formalized and we believe a clarification of their many aspects is necessary for future research. In this paper, we propose a taxonomy for what we define as Spectator Experience Augmentation Techniques (SEATs). We use it to analyse existing techniques and we demonstrate how it can serve as a basis for the exploration of novel ones.}
}

@inproceedings{NIME20_64,
  author = {Sen, Sourya and Tahiroğlu, Koray and Lohmann, Julia},
  title = {Sounding Brush: A Tablet based Musical Instrument for Drawing and Mark Making},
  pages = {331--336},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813398},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper64.pdf},
  presentation-video = {https://youtu.be/7RkGbyGM-Ho},
  abstract = {Existing applications of mobile music tools are often concerned with the simulation  of acoustic or digital musical instruments, extended with graphical representations of keys, pads, etc. Following an intensive review of existing tools and approaches to mobile music making, we implemented a digital drawing tool, employing a time-based graphical/gestural interface for music composition and performance. In this paper, we introduce our Sounding Brush project, through which we explore music making in various forms with the natural gestures of drawing and mark making on a tablet device. Subsequently, we present the design and development of the Sounding Brush application. Utilising this project idea, we discuss the act of drawing as an activity that is not separated from the act of playing musical instrument. Drawing is essentially the act of playing music by means of a continuous process of observation, individualisation and exploring time and space in a unique way.}
}

@inproceedings{NIME20_65,
  author = {Tahiroğlu, Koray and Kastemaa, Miranda and Koli, Oskar},
  title = {Al-terity: Non-Rigid Musical Instrument with Artificial Intelligence Applied to Real-Time Audio Synthesis},
  pages = {337--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813402},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper65.pdf},
  presentation-video = {https://youtu.be/giYxFovZAvQ},
  abstract = {A deformable musical instrument can take numerous distinct shapes with its non-rigid features. Building audio synthesis module for such an interface behaviour can be challenging. In this paper, we present the Al-terity, a non-rigid musical instrument that comprises a deep learning model with generative adversarial network  architecture and use it for generating audio samples for real-time audio synthesis. The particular deep learning model we use for this instrument was trained with existing data set as input for purposes of further experimentation. The main benefits of the model used are the ability to produce the realistic range of timbre of the trained data set and the ability to generate new audio samples in real-time, in the moment of playing, with the characteristics of sounds that the performer ever heard before.   We argue that these advanced intelligence features on the audio synthesis level could allow us to explore performing music with particular response features that define the instrument's digital idiomaticity and allow us reinvent the instrument in the act of music performance.}
}

@inproceedings{NIME20_66,
  author = {Kiefer, Chris and Overholt, Dan and Eldridge, Alice},
  title = {Shaping the behaviour of feedback instruments with complexity-controlled gain dynamics},
  pages = {343--348},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813406},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper66.pdf},
  presentation-video = {https://youtu.be/sf6FwsUX-84},
  abstract = {Feedback instruments offer radical new ways of engaging with instrument design and musicianship. They are defined by recurrent circulation of signals through the instrument, which give the instrument ‘a life of its own’ and a 'stimulating uncontrollability'.  Arguably, the most interesting musical behaviour in these instruments happens when their dynamic complexity is maximised, without falling into saturating feedback. It is often challenging to keep the instrument in this zone; this research looks at algorithmic ways to manage the behaviour of feedback loops in order to make feedback instruments more playable and musical; to expand and maintain the `sweet spot'. We propose a solution that manages gain dynamics based on measurement of complexity, using a realtime implementation of the Effort to Compress algorithm. The system was evaluated with four musicians, each of whom have different variations of string-based feedback instruments, following an autobiographical design approach.  Qualitative feedback was gathered, showing that the system was successful in modifying the behaviour of these instruments to allow easier access to edge transition zones, sometimes at the expense of losing some of the more compelling dynamics of the instruments. The basic efficacy of the system is evidenced by descriptive audio analysis. This paper is accompanied by a dataset of sounds collected during the study, and the open source software that was written to support the research.}
}

@inproceedings{NIME20_67,
  author = {Williams, Duncan A.H.},
  title = {MINDMIX: Mapping of brain activity to congruent audio mixing features},
  pages = {349--352},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813408},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper67.pdf},
  abstract = {Brain-computer interfacing (BCI) offers novel methods to facilitate participation in audio engineering, providing access for individuals who might otherwise be unable to take part (either due to lack of training, or physical disability).  This paper describes the development of a BCI system for conscious, or ‘active’, control of parameters on an audio mixer by generation of synchronous MIDI Machine Control messages. The mapping between neurophysiological cues and audio parameter must be intuitive for a neophyte audience (i.e., one without prior training or the physical skills developed by professional audio engineers when working with tactile interfaces). The prototype is dubbed MINDMIX (a portmanteau of ‘mind’ and ‘mixer’), combining discrete and many-to-many mappings of audio mixer parameters and BCI control signals measured via Electronecephalograph (EEG). In future, specific evaluation of discrete mappings would be useful for iterative system design.}
}

@inproceedings{NIME20_68,
  author = {DeSmith, Marcel O and Piepenbrink, Andrew and Kapur, Ajay},
  title = {SQUISHBOI: A Multidimensional Controller for Complex Musical Interactions using Machine Learning},
  pages = {353--356},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813412},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper68.pdf},
  abstract = {We present SQUISHBOI, a continuous touch controller for interacting with complex musical systems. An elastic rubber membrane forms the playing surface of the instrument, while machine learning is used for dimensionality reduction and gesture recognition. The membrane is stretched over a hollow shell which permits considerable depth excursion, with an array of distance sensors tracking the surface displacement from underneath. The inherent dynamics of the membrane lead to cross-coupling between nearby sensors, however we do not see this as a flaw or limitation. Instead we find this coupling gives structure to the playing techniques and mapping schemes chosen by the user. The instrument is best utilized as a tool for actively designing abstraction and forming a relative control structure within a given system, one which allows for intuitive gestural control beyond what can be accomplished with conventional musical controllers.}
}

@inproceedings{NIME20_69,
  author = {Bryan-Kinns, Nick and ZIJIN, LI and Sun, Xiaohua},
  title = {On Digital Platforms and AI for Music in the UK and China},
  pages = {357--360},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813414},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper69.pdf},
  presentation-video = {https://youtu.be/c7nkCBBTnDA},
  abstract = {Digital technologies play a fundamental role in New Interfaces for Musical Expression as well as music making and consumption  more  widely. This paper reports on two workshops with music professionals and researchers who undertook an initial exploration of the differences between digital platforms (software and online services) for music in the UK and China. Differences were found in primary target user groups of digital platforms in the UK and China as well as the stages of the culture creation cycle they were developed for. Reasons for the divergence of digital platforms include differences in culture, regulation, and infrastructure, as well as the inherent Western bias of software for music making such as Digital Audio Workstations. Using AI to bridge between Western and Chinese music traditions is suggested as an opportunity to address aspects of the divergent landscape of digital platforms for music inside and outside China.}
}

@inproceedings{NIME20_7,
  author = {Chu, Jean and Choi, Jaewon},
  title = {Reinterpretation of Pottery as a Musical Interface},
  pages = {37--38},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813416},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper7.pdf},
  abstract = {Digitally integrating the materiality, form, and tactility in everyday objects (e.g., pottery) provides inspiration for new ways of musical expression and performance. In this project we reinterpret the creative process and aesthetic philosophy of pottery as algorithmic music to help users rediscover the latent story behind pottery through a synesthetic experience. Projects Mobius I and Mobius II illustrate two potential directions toward a musical interface, one focusing on the circular form, and the other, on graphical ornaments of pottery. Six conductive graphics on the pottery function as capacitive sensors while retaining their resemblance to traditional ornamental patterns in pottery. Offering pottery as a musical interface, we invite users to orchestrate algorithmic music by physically touching the different graphics.}
}

@inproceedings{NIME20_70,
  author = {Eskildsen, Anders and Walther-Hansen, Mads},
  title = {Force dynamics as a design framework for mid-air musical interfaces},
  pages = {361--366},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813418},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper70.pdf},
  presentation-video = {https://youtu.be/REe967aGVN4},
  abstract = {In this paper we adopt the theory of force dynamics in human cognition as a fundamental design principle for the development of mid-air musical interfaces. We argue that this principle can provide more intuitive user experiences when the interface does not provide direct haptic feedback – such as interfaces made with various gesture-tracking technologies. Grounded in five concepts from the theoretical literature on force dynamics in musical cognition, the paper presents a set of principles for interaction design focused on five force schemas: Path restraint, Containment restraint, Counter-force, Attraction, and Compulsion. We describe an initial set of examples that implement these principles using a Leap Motion sensor for gesture tracking and SuperCollider for interactive audio design. Finally, the paper presents a pilot experiment that provides initial ratings of intuitiveness in the user experience.}
}

@inproceedings{NIME20_71,
  author = {Nyström, Erik},
  title = {Intra-Actions: Experiments with Velocity and Position in Continuous Controllers},
  pages = {367--368},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813420},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper71.pdf},
  abstract = {Continuous MIDI controllers commonly output their position only, with no influence of the performative energy with which they were set. In this paper, creative uses of time as a parameter in continuous controller mapping are demonstrated: the speed of movement affects the position mapping and control output. A set of SuperCollider classes are presented, developed in the author’s practice in computer music, where they have been used together with commercial MIDI controllers. The creative applications employ various approaches and metaphors for scaling time, but also machine learning for recognising patterns. In the techniques, performer, controller and synthesis ‘intra-act’, to use Karen Barad’s term: because position and velocity are derived from the same data, sound output cannot be predicted without the temporal context of performance.}
}

@inproceedings{NIME20_72,
  author = {Leonard, James and Giomi, Andrea},
  title = {Towards an Interactive Model-Based Sonification of Hand Gesture for Dance Performance},
  pages = {369--374},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813422},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper72.pdf},
  presentation-video = {https://youtu.be/HQqIjL-Z8dA},
  abstract = {This paper presents an ongoing research on hand gesture interactive sonification in dance performances. For this purpose, a conceptual framework and a multilayered mapping model issued from an experimental case study will be proposed. The goal of this research is twofold. On the one hand, we aim to determine action-based perceptual invariants that allow us to establish pertinent relations between gesture qualities and sound features. On the other hand, we are interested in analysing how an interactive model-based sonification can provide useful and effective feedback for dance practitioners. From this point of view, our research explicitly addresses the convergence between the scientific understandings provided by the field of movement sonification and the traditional know-how developed over the years within the digital instrument and interaction design communities. A key component of our study is the combination between physically-based sound synthesis and motion features analysis. This approach has proven effective in providing interesting insights for devising novel sonification models for artistic and scientific purposes, and for developing a collaborative platform involving the designer, the musician and the performer.}
}

@inproceedings{NIME20_74,
  author = {MacLean, Alex},
  title = {Immersive Dreams: A Shared VR Experience},
  pages = {380--381},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813426},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper74.pdf},
  abstract = {This paper reports on a project that aimed to break apart the isolation of VR and share an experience between both the wearer of a headset and a room of observers. It presented the user with an acoustically playable virtual environment in which their interactions with objects spawned audio events from the room’s 80 loudspeakers and animations on the room’s 3 display walls. This required the use of several Unity engines running on separate machines and SuperCollider running as the audio engine. The perspectives into what the wearer of the headset was doing allowed the audience to connect their movements to the sounds and images being experienced, effectively allowing them all to participate in the installation simultaneously.}
}

@inproceedings{NIME20_75,
  author = {Bryan-Kinns, Nick and ZIJIN, LI},
  title = {ReImagining: Cross-cultural Co-Creation of a Chinese Traditional Musical Instrument with Digital Technologies},
  pages = {382--387},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813428},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper75.pdf},
  presentation-video = {https://youtu.be/NvHcUQea82I},
  abstract = {There are many studies of Digital Musical Instrument (DMI) design, but there is little research on the cross-cultural co-creation of DMIs drawing on traditional musical instruments. We present a study of cross-cultural co-creation inspired by the Duxianqin - a traditional Chinese Jing ethnic minority single stringed musical instrument. We report on how we structured the co-creation with European and Chinese participants ranging from DMI designers to composers and performers. We discuss how we identified the `essence' of the Duxianqin and used this to drive co-creation of three Duxianqin reimagined through digital technologies. Music was specially composed for these reimagined Duxianqin and performed in public as the culmination of the design process. We reflect on our co-creation process and how others could use such an approach to identify the essence of traditional instruments and reimagine them in the digital age.}
}

@inproceedings{NIME20_76,
  author = {Vasilakos, Konstantinos n/a and Wilson, Scott and McCauley, Thomas and Yeung, Tsun Winston and Margetson, Emma and Khosravi Mardakheh, Milad},
  title = {Sonification of High Energy Physics Data Using Live Coding and Web Based Interfaces.},
  pages = {388--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813430},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper76.pdf},
  presentation-video = {https://youtu.be/1vS_tFUyz7g},
  abstract = {This paper presents a discussion of Dark Matter, a sonification project using live coding and just-in-time programming techniques. The project uses data from proton-proton collisions produced by the Large Hadron Collider (LHC) at CERN, Switzerland, and then detected and reconstructed by the Compact Muon Solenoid (CMS) experiment, and was developed with the support of the art@CMS project. Work for the Dark Matter project included the development of a custom-made environment in the SuperCollider (SC) programming language that lets the performers of the group engage in collective improvisations using dynamic interventions and networked music systems. This paper will also provide information about a spin-off project entitled the Interactive Physics Sonification System (IPSOS), an interactive and standalone online application developed in the JavaScript programming language. It provides a web-based interface that allows users to map particle data to sound on commonly used web browsers, mobile devices, such as smartphones, tablets etc. The project was developed as an educational outreach tool to engage young students and the general public with data derived from LHC collisions.}
}

@inproceedings{NIME20_77,
  author = {Takase, Haruya and Shiramatsu, Shun},
  title = {Support System for Improvisational Ensemble Based on Long Short-Term Memory Using Smartphone Sensor},
  pages = {394--398},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813434},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper77.pdf},
  presentation-video = {https://youtu.be/WhrGhas9Cvc},
  abstract = {Our goal is to develop an improvisational ensemble support system for music beginners who do not have knowledge of chord progressions and do not have enough experience of playing an instrument. We hypothesized that a music beginner cannot determine tonal pitches of melody over a particular chord but can use body movements to specify the pitch contour (i.e., melodic outline) and the attack timings (i.e., rhythm). We aim to realize a performance interface for supporting expressing intuitive pitch contour and attack timings using body motion and outputting harmonious pitches over the chord progression of the background music. Since the intended users of this system are not limited to people with music experience, we plan to develop a system that uses Android smartphones, which many people have. Our system consists of three modules: a module for specifying attack timing using smartphone sensors, module for estimating the vertical movement of the smartphone using smartphone sensors, and module for estimating the sound height using smartphone vertical movement and background chord progression. Each estimation module is developed using long short-term memory (LSTM), which is often used to estimate time series data. We conduct evaluation experiments for each module. As a result, the attack timing estimation had zero misjudgments, and the mean error time of the estimated attack timing was smaller than the sensor-acquisition interval. The accuracy of the vertical motion estimation was 64%, and that of the pitch estimation was 7.6%. The results indicate that the attack timing is accurate enough, but the vertical motion estimation and the pitch estimation need to be improved for actual use.}
}

@inproceedings{NIME20_78,
  author = {Tsiros, Augoustinos and Palladini, Alessandro},
  title = {Towards a Human-Centric Design Framework for AI Assisted Music Production},
  pages = {399--404},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813436},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper78.pdf},
  abstract = {In this paper, we contribute to the discussion on how to best design human-centric MIR tools for live audio mixing by bridging the gap between research on complex systems, the psychology of automation and the design of tools that support creativity in music production. We present the design of the Channel-AI, an embedded AI system which performs instrument recognition and generates parameter settings suggestions for gain levels, gating, compression and equalization which are specific to the input signal and the instrument type. We discuss what we believe to be the key design principles and perspectives on the making of intelligent tools for creativity and for experts in the loop. We demonstrate how these principles have been applied to inform the design of the interaction between expert live audio mixing engineers with the Channel-AI (i.e. a corpus of AI features embedded in the Midas HD Console. We report the findings from a preliminary evaluation we conducted with three professional mixing engineers and reflect on mixing engineers’ comments about the Channel-AI on social media.}
}

@inproceedings{NIME20_79,
  author = {Rodger, Matthew and Stapleton, Paul and van Walstijn, Maarten and Ortiz, Miguel and Pardue, Laurel S},
  title = {What Makes a Good Musical Instrument? A Matter of Processes, Ecologies and Specificities },
  pages = {405--410},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813438},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper79.pdf},
  presentation-video = {https://youtu.be/ADLo-QdSwBc},
  abstract = {Understanding the question of what makes a good musical instrument raises several conceptual challenges. Researchers have regularly adopted tools from traditional HCI as a framework to address this issue, in which instrumental musical activities are taken to comprise a device and a user, and should be evaluated as such. We argue that this approach is not equipped to fully address the conceptual issues raised by this question. It is worth reflecting on what exactly an instrument is, and how instruments contribute toward meaningful musical experiences. Based on a theoretical framework that incorporates ideas from ecological psychology, enactivism, and phenomenology, we propose an alternative approach to studying musical instruments. According to this approach, instruments are better understood in terms of processes rather than as devices, while musicians are not users, but rather agents in musical ecologies. A consequence of this reframing is that any evaluations of instruments, if warranted, should align with the specificities of the relevant processes and ecologies concerned. We present an outline of this argument and conclude with a description of a current research project to illustrate how our approach can shape the design and performance of a musical instrument in-progress.}
}

@inproceedings{NIME20_8,
  author = {Martin, Charles Patrick and Liu, Zeruo and Wang, Yichen and He, Wennan and Gardner, Henry},
  title = {Sonic Sculpture: Activating Engagement with Head-Mounted Augmented Reality},
  pages = {39--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813445},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper8.pdf},
  presentation-video = {https://youtu.be/RlTWXnFOLN8},
  abstract = {We describe a sonic artwork, "Listening To Listening", that has been designed to accompany a real-world sculpture with two prototype interaction schemes. Our artwork is created for the HoloLens platform so that users can have an individual experience in a mixed reality context. Personal AR systems have recently become available and practical for integration into public art projects, however research into sonic sculpture works has yet to account for the affordances of current portable and mainstream AR systems. In this work, we take advantage of the HoloLens' spatial awareness to build sonic spaces that have a precise spatial relationship to a given sculpture and where the sculpture itself is modelled in the augmented scene as an "invisible hologram". We describe the artistic rationale for our artwork, the design of the two interaction schemes, and the technical and usability feedback that we have obtained from demonstrations during iterative development. This work appears to be the first time that head-mounted AR has been used to build an interactive sonic landscape to engage with a public sculpture.}
}

@inproceedings{NIME20_80,
  author = {Santini, Giovanni },
  title = {Augmented Piano in Augmented Reality},
  pages = {411--415},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813449},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper80.pdf},
  presentation-video = {https://youtu.be/3HBWvKj2cqc},
  abstract = {Augmented instruments have been a widely explored research topic since the late 80s. The possibility to use sensors for providing an input for sound processing/synthesis units let composers and sound artist open up new ways for experimentation. Augmented Reality, by rendering virtual objects in the real world and by making those objects interactive (via some sensor-generated input), provides a new frame for this research field. In fact, the 3D visual feedback, delivering a precise indication of the spatial configuration/function of each virtual interface, can make the instrumental augmentation process more intuitive for the interpreter and more resourceful for a composer/creator: interfaces can change their behavior over time, can be reshaped, activated or deactivated. Each of these modifications can be made obvious to the performer by using strategies of visual feedback. In addition, it is possible to accurately sample space and to map it with differentiated functions. Augmenting interfaces can also be considered a visual expressive tool for the audience and designed accordingly: the performer’s point of view (or another point of view provided by an external camera) can be mirrored to a projector. This article will show some example of different designs of AR piano augmentation from the composition Studi sulla realtà nuova.}
}

@inproceedings{NIME20_81,
  author = {Davis, Tom and Reid, Laura},
  title = {Taking Back Control: Taming the Feral Cello},
  pages = {416--421},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813453},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper81.pdf},
  presentation-video = {https://youtu.be/9npR0T6YGiA},
  abstract = {Whilst there is a large body of NIME papers that concentrate on the presentation of new technologies there are fewer papers that have focused on a longitudinal understanding of NIMEs in practice. This paper embodies the more recent acknowledgement of the importance of practice-based methods of evaluation [1,2,3,4] concerning the use of NIMEs within performance and the recognition that it is only within the situation of practice that the context is available to actually interpret and evaluate the instrument [2]. Within this context this paper revisits the Feral Cello performance system that was first presented at NIME 2017 [5]. This paper explores what has been learned through the artistic practice of performing and workshopping in this context by drawing heavily on the experiences of the performer/composer who has become an integral part of this project and co-author of this paper. The original philosophical context is also revisited and reflections are made on the tensions between this position and the need to ‘get something to work’. The authors feel the presentation of the semi-structured interview within the paper is the best method of staying truthful to Hayes understanding of musical improvisation as an enactive framework ‘in its ability to demonstrate the importance of participatory, relational, emergent, and embodied musical activities and processes’ [4].}
}

@inproceedings{NIME20_82,
  author = {Jaccard, Thibault and Lieck, Robert and Rohrmeier, Martin},
  title = {AutoScale: Automatic and Dynamic Scale Selection for Live Jazz Improvisation},
  pages = {422--427},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813457},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper82.pdf},
  presentation-video = {https://youtu.be/KqGpTTQ9ZrE},
  abstract = {Becoming a practical musician traditionally requires an extensive amount of preparatory work to master the technical and theoretical challenges of the particular instrument and musical style before being able to devote oneself to musical expression. In particular, in jazz improvisation, one of the major barriers is the mastery and appropriate selection of scales from a wide range, according to harmonic context and style. In this paper, we present AutoScale, an interactive software for making jazz improvisation more accessible by lifting the burden of scale selection from the musician while still allowing full controllability if desired. This is realized by implementing a MIDI effect that dynamically maps the desired scales onto a standardized layout. Scale selection can be pre-programmed, automated based on algorithmic lead sheet analysis, or interactively adapted. We discuss the music-theoretical foundations underlying our approach, the design choices taken for building an intuitive user interface, and provide implementations as VST plugin and web applications for use with a Launchpad or traditional MIDI keyboard.}
}

@inproceedings{NIME20_83,
  author = {Hayes, Lauren and Marquez-Borbon, Adnan},
  title = {Nuanced and Interrelated Mediations and Exigencies (NIME): Addressing the Prevailing Political and Epistemological Crises},
  pages = {428--433},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813459},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper83.pdf},
  presentation-video = {https://youtu.be/4UERHlFUQzo},
  abstract = {Nearly two decades after its inception as a workshop at the ACM Conference on Human Factors in Computing Systems, NIME exists as an established international conference significantly distinct from its precursor. While this origin story is often noted, the implications of NIME's history as emerging from a field predominantly dealing with human-computer interaction have rarely been discussed. In this paper we highlight many of the recent—and some not so recent—challenges that have been brought upon the NIME community as it attempts to maintain and expand its identity as a platform for multidisciplinary research into HCI, interface design, and electronic and computer music. We discuss the relationship between the market demands of the neoliberal university—which have underpinned academia's drive for innovation—and the quantification and economisation of research performance which have facilitated certain disciplinary and social frictions to emerge within NIME-related research and practice. Drawing on work that engages with feminist theory and cultural studies, we suggest that critical reflection and moreover mediation is necessary in order to address burgeoning concerns which have been raised within the NIME discourse in relation to methodological approaches,'diversity and inclusion', 'accessibility', and the fostering of rigorous interdisciplinary research.}
}

@inproceedings{NIME20_84,
  author = {McPherson, Andrew and Lepri, Giacomo},
  title = {Beholden to our tools: negotiating with technology while sketching digital instruments},
  pages = {434--439},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813461},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper84.pdf},
  presentation-video = {https://youtu.be/-nRtaucPKx4},
  abstract = {Digital musical instrument design is often presented as an open-ended creative process in which technology is adopted and adapted to serve the musical will of the designer. The real-time music programming languages powering many new instruments often provide access to audio manipulation at a low level, theoretically allowing the creation of any sonic structure from primitive operations. As a result, designers may assume that these seemingly omnipotent tools are pliable vehicles for the expression of musical ideas. We present the outcomes of a compositional game in which sound designers were invited to create simple instruments using common sensors and the Pure Data programming language. We report on the patterns and structures that often emerged during the exercise, arguing that designers respond strongly to suggestions offered by the tools they use. We discuss the idea that current music programming languages may be as culturally loaded as the communities of practice that produce and use them. Instrument making is then best viewed as a protracted negotiation between designer and tools.}
}

@inproceedings{NIME20_85,
  author = {Martelloni, Andrea and McPherson, Andrew and Barthet, Mathieu},
  title = {Percussive Fingerstyle Guitar through the Lens of NIME: an Interview Study},
  pages = {440--445},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813463},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper85.pdf},
  presentation-video = {https://youtu.be/ON8ckEBcQ98},
  abstract = {Percussive fingerstyle is a playing technique adopted by many contemporary acoustic guitarists, and it has grown substantially in popularity over the last decade. Its foundations lie in the use of the guitar's body for percussive lines, and in the extended range given by the novel use of altered tunings. There are very few formal accounts of percussive fingerstyle, therefore, we devised an interview study to investigate its approach to composition, performance and musical experimentation. Our aim was to gain insight into the technique from a gesture-based point of view, observe whether modern fingerstyle shares similarities to the approaches in NIME practice and investigate possible avenues for guitar augmentations inspired by the percussive technique. We conducted an inductive thematic analysis on the transcribed interviews: our findings highlight the participants' material-based approach to musical interaction and we present a three-zone model of the most common percussive gestures on the guitar's body. Furthermore, we examine current trends in Digital Musical Instruments, especially in guitar augmentation, and we discuss possible future directions in augmented guitars in light of the interviewees' perspectives.}
}

@inproceedings{NIME20_86,
  author = {Jack, Robert and Harrison, Jacob and McPherson, Andrew},
  title = {Digital Musical Instruments as Research Products},
  pages = {446--451},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813465},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper86.pdf},
  presentation-video = {https://youtu.be/luJwlZBeBqY},
  abstract = {In the field of human computer interaction (HCI) the limitations of prototypes as the primary artefact used in research are being realised. Prototypes often remain open in their design, are partially-finished, and have a focus on a specific aspect of interaction. Previous authors have proposed `research products' as a specific category of artefact distinct from both research prototypes and commercial products. The characteristics of research products are their holistic completeness as a design artefact, their situatedness in a specific cultural context, and the fact that they are evaluated for what they are, not what they will become. This paper discusses the ways in which many instruments created within the context of New Interfaces for Musical Expression (NIME), including those that are used in performances, often fall into the category of prototype. We shall discuss why research products might be a useful framing for NIME research. Research products shall be weighed up against some of the main themes of NIME research: technological innovation; musical expression; instrumentality. We conclude this paper with a case study of Strummi, a digital musical instrument which we frame as research product.}
}

@inproceedings{NIME20_87,
  author = {Patel, Amit D and Richards, John },
  title = {Pop-up for Collaborative Music-making},
  pages = {452--457},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813473},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper87.pdf},
  abstract = {This paper presents a micro-residency in a pop-up shop and collaborative making amongst a group of researchers and practitioners. The making extends to sound(-making) objects, instruments, workshop, sound installation, performance and discourse on DIY electronic music. Our research builds on creative workshopping and speculative design and is informed by ideas of collective making. The ad hoc and temporary pop-up space is seen as formative in shaping the outcomes of the work. Through the lens of curated research, working together with a provocative brief, we explored handmade objects, craft, non-craft, human error, and the spirit of DIY, DIYness. We used the Studio Bench - a method that brings making, recording and performance together in one space - and viewed workshopping and performance as a holistic event. A range of methodologies were investigated in relation to NIME. These included the Hardware Mash-up, Speculative Sound Circuits and Reverse Design, from product to prototype, resulting in the instrument the Radical Nails. Finally, our work drew on the notion of design as performance and making in public and further developed our understanding of workshop-installation and performance-installation.}
}

@inproceedings{NIME20_88,
  author = {Reed, Courtney and McPherson, Andrew},
  title = {Surface Electromyography for Direct Vocal Control},
  pages = {458--463},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813475},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper88.pdf},
  presentation-video = {https://youtu.be/1nWLgQGNh0g},
  abstract = {This paper introduces a new method for direct control using the voice via measurement of vocal muscular activation with surface electromyography (sEMG). Digital musical interfaces based on the voice have typically used indirect control, in which features extracted from audio signals control the parameters of sound generation, for example in audio to MIDI controllers. By contrast, focusing on the musculature of the singing voice allows direct muscular control, or alternatively, combined direct and indirect control in an augmented vocal instrument. In this way we aim to both preserve the intimate relationship a vocalist has with their instrument and key timbral and stylistic characteristics of the voice while expanding its sonic capabilities. This paper discusses other digital instruments which effectively utilise a combination of indirect and direct control as well as a history of controllers involving the voice. Subsequently, a new method of direct control from physiological aspects of singing through sEMG and its capabilities are discussed. Future developments of the system are further outlined along with usage in performance studies, interactive live vocal performance, and educational and practice tools.}
}

@inproceedings{NIME20_89,
  author = {von Coler, Henrik and Lepa, Steffen and Weinzierl, Stefan},
  title = {User-Defined Mappings for Spatial Sound Synthesis},
  pages = {464--469},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813477},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper89.pdf},
  abstract = {The presented sound synthesis system allows the individual spatialization of spectral components in real-time, using a sinusoidal modeling approach within 3-dimensional sound reproduction systems. A co-developed, dedicated haptic interface is used to jointly control spectral and spatial attributes of the sound. Within a user study, participants were asked to create an individual mapping between control parameters of the interface and rendering parameters of sound synthesis and spatialization, using a visual programming environment. Resulting mappings of all participants are evaluated, indicating the preference of single control parameters for specific tasks. In comparison with mappings intended by the development team, the results validate certain design decisions and indicate new directions.}
}

@inproceedings{NIME20_9,
  author = {Proctor, Rohan and Martin, Charles Patrick},
  title = {A Laptop Ensemble Performance System using Recurrent Neural Networks},
  pages = {43--48},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813481},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper9.pdf},
  abstract = {The popularity of applying machine learning techniques in musical domains has created an inherent availability of freely accessible pre-trained neural network (NN) models ready for use in creative applications. This work outlines the implementation of one such application in the form of an assistance tool designed for live improvisational performances by laptop ensembles. The primary intention was to leverage off-the-shelf pre-trained NN models as a basis for assisting individual performers either as musical novices looking to engage with more experienced performers or as a tool to expand musical possibilities through new forms of creative expression. The system expands upon a variety of ideas found in different research areas including new interfaces for musical expression, generative music and group performance to produce a networked performance solution served via a web-browser interface. The final implementation of the system offers performers a mixture of high and low-level controls to influence the shape of sequences of notes output by locally run NN models in real time, also allowing performers to define their level of engagement with the assisting generative models. Two test performances were played, with the system shown to feasibly support four performers over a four minute piece while producing musically cohesive and engaging music. Iterations on the design of the system exposed technical constraints on the use of a JavaScript environment for generative models in a live music context, largely derived from inescapable processing overheads.}
}

@inproceedings{NIME20_90,
  author = {Brizolara, Tiago and Gibet, Sylvie and Larboulette, Caroline },
  title = {Elemental: a Gesturally Controlled System to Perform Meteorological Sounds},
  pages = {470--476},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813483},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper90.pdf},
  abstract = {In this paper, we present and evaluate Elemental, a NIME (New Interface for Musical Expression) based on audio synthesis of sounds of meteorological phenomena, namely rain, wind and thunder, intended for application in contemporary music/sound art, performing arts and entertainment. We first describe the system, controlled by the performer’s arms through Inertial Measuring Units and Electromyography sensors. The produced data is analyzed and used through mapping strategies as input of the sound synthesis engine. We conducted user studies to refine the sound synthesis engine, the choice of gestures and the mappings between them, and to finally evaluate this proof of concept. Indeed, the users approached the system with their own awareness ranging from the manipulation of abstract sound to the direct simulation of atmospheric phenomena - in the latter case, it could even be to revive memories or to create novel situations. This suggests that the approach of instrumentalization of sounds of known source may be a fruitful strategy for constructing expressive interactive sonic systems.}
}

@inproceedings{NIME20_91,
  author = {Erdem, Çağrı and Jensenius, Alexander Refsum},
  title = {RAW: Exploring Control Structures for Muscle-based Interaction in Collective Improvisation},
  pages = {477--482},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813485},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper91.pdf},
  presentation-video = {https://youtu.be/gX-X1iw7uWE},
  abstract = {This paper describes the ongoing process of developing RAW, a collaborative body–machine instrument that relies on 'sculpting' the sonification of raw EMG signals. The instrument is built around two Myo armbands located on the forearms of the performer. These are used to investigate muscle contraction, which is again used as the basis for the sonic interaction design. Using a practice-based approach, the aim is to explore the musical aesthetics of naturally occurring bioelectric signals. We are particularly interested in exploring the differences between processing at audio rate versus control rate, and how the level of detail in the signal–and the complexity of the mappings–influence the experience of control in the instrument. This is exemplified through reflections on four concerts in which RAW has been used in different types of collective improvisation.}
}

@inproceedings{NIME20_92,
  author = {MacDonald, Travis C and Hughes, James and MacKenzie, Barry},
  title = {SmartDrone: An Aurally Interactive Harmonic Drone},
  pages = {483--488},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813488},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper92.pdf},
  abstract = {Mobile devices provide musicians with the convenience of musical accompaniment wherever they are, granting them new methods for developing their craft. We developed the application SmartDrone to give users the freedom to practice in different harmonic settings with the assistance of their smartphone. This application further explores the area of dynamic accompaniment by implementing functionality so that chords are generated based on the key in which the user is playing. Since this app was designed to be a tool for scale practice, drone-like accompaniment was chosen so that musicians could experiment with combinations of melody and harmony. The details of the application development process are discussed in this paper, with the main focus on scale analysis and harmonic transposition. By using these two components, the application is able to dynamically alter key to reflect the user's playing. As well as the design and implementation details, this paper reports and examines feedback from a small user study of undergraduate music students who used the app. }
}

@inproceedings{NIME20_93,
  author = {Martinez Avila, Juan  P and Tsaknaki, Vasiliki and Karpashevich, Pavel and Windlin, Charles and Valenti, Niklas and Höök, Kristina and McPherson, Andrew and Benford, Steve},
  title = {Soma Design for NIME},
  pages = {489--494},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813491},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper93.pdf},
  presentation-video = {https://youtu.be/i4UN_23A_SE},
  abstract = {Previous research on musical embodiment has reported that expert performers often regard their instruments as an extension of their body. Not every digital musical instrument seeks to create a close relationship between body and instrument, but even for the many that do, the design process often focuses heavily on technical and sonic factors, with relatively less attention to the bodily experience of the performer. In this paper we propose Somaesthetic design as an alternative to explore this space. The Soma method aims to attune the sensibilities of designers, as well as their experience of their body, and make use of these notions as a resource for creative design. We then report on a series of workshops exploring the relationship between the body and the guitar with a Soma design approach. The workshops resulted in a series of guitar-related artefacts and NIMEs that emerged from the somatic exploration of balance and tension during guitar performance. Lastly we present lessons learned from our research that could inform future Soma-based musical instrument design, and how NIME research may also inform Soma design.}
}

@inproceedings{NIME20_94,
  author = {Cadavid, Laddy P},
  title = {Knotting the memory//Encoding the Khipu_: Reuse of an ancient Andean device as a NIME },
  pages = {495--498},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813495},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper94.pdf},
  presentation-video = {https://youtu.be/nw5rbc15pT8},
  abstract = {The khipu is an information processing and transmission device used mainly by the Inca empire and previous Andean societies. This mnemotechnic interface is one of the first textile computers known, consisting of a central wool or cotton cord to which other strings are attached with knots of different shapes, colors, and sizes encrypting different kinds of values and information. The system was widely used until the Spanish colonization that banned their use and destroyed a large number of these devices. This paper introduces the creation process of a NIME based in a khipu converted into an electronic instrument for the interaction and generation of live experimental sound by weaving knots with conductive rubber cords, and its implementation in the performance Knotting the memory//Encoding the Khipu_  that aim to pay homage to this system, from a decolonial perspective continuing the interrupted legacy of this ancestral practice in a different experience of tangible live coding and computer music, as well as weaving the past with the present of the indigenous and people resistance of the Andean territory with their sounds.}
}

@inproceedings{NIME20_95,
  author = {Knotts, Shelly and Collins, Nick},
  title = {A survey on the uptake of Music AI Software},
  pages = {499--504},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813499},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper95.pdf},
  presentation-video = {https://youtu.be/v6hT3ED3N60},
  abstract = {The recent proliferation of commercial software claiming ground in the field of music AI has provided opportunity to engage with AI in music making without the need to use libraries aimed at those with programming skills. Pre-packaged music AI software has the potential to broaden access to machine learning tools but it is unclear how widely these softwares are used by music technologists or how engagement affects attitudes towards AI in music making. To interrogate these questions we undertook a survey in October 2019, gaining 117 responses. The survey collected statistical information on the use of pre-packaged and self-written music AI software. Respondents reported a range of musical outputs including producing recordings, live performance and generative work across many genres of music making. The survey also gauged general attitudes towards AI in music and provided an open field for general comments. The responses to the survey suggested a forward-looking attitude to music AI with participants often pointing to the future potential of AI tools, rather than present utility. Optimism was partially related to programming skill with those with more experience showing higher skepticism towards the current state and future potential of AI.}
}

@inproceedings{NIME20_96,
  author = {Barton, Scott},
  title = {Circularity in Rhythmic Representation and Composition},
  pages = {505--508},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813501},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper96.pdf},
  presentation-video = {https://youtu.be/0CEKbyJUSw4},
  abstract = {Cycle is a software tool for musical composition and improvisation that represents events along a circular timeline. In doing so, it breaks from the linear representational conventions of European Art music and modern Digital Audio Workstations. A user specifies time points on different layers, each of which corresponds to a particular sound. The layers are superimposed on a single circle, which allows a unique visual perspective on the relationships between musical voices given their geometric positions. Positions in-between quantizations are possible, which encourages experimentation with expressive timing and machine rhythms. User-selected transformations affect groups of notes, layers, and the pattern as a whole. Past and future states are also represented, synthesizing linear and cyclical notions of time. This paper will contemplate philosophical questions raised by circular rhythmic notation and will reflect on the ways in which the representational novelties and editing functions of Cycle have inspired creativity in musical composition.}
}

@inproceedings{NIME20_97,
  author = {Magnusson, Thor},
  title = {Instrumental Investigations at Emute Lab},
  pages = {509--513},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813503},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper97.pdf},
  abstract = {This lab report discusses recent projects and activities of the Experimental Music Technologies Lab at the University of Sussex. The lab was founded in 2014 and has contributed to the development of the field of new musical technologies. The report introduces the lab’s agenda, gives examples of its activities through common themes and gives short description of lab members’ work. The lab environment, funding income and future vision are also presented.}
}

@inproceedings{NIME20_98,
  author = {Venkatesh, Satvik and Braund, Edward and Miranda, Eduardo},
  title = {Composing Popular Music with Physarum polycephalum-based Memristors},
  pages = {514--519},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813507},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper98.pdf},
  presentation-video = {https://youtu.be/NBLa-KoMUh8},
  abstract = {Creative systems such as algorithmic composers often use Artificial Intelligence models like Markov chains, Artificial Neural Networks, and Genetic Algorithms in order to model stochastic processes. Unconventional Computing (UC) technologies explore non-digital ways of data storage, processing, input, and output. UC paradigms such as Quantum Computing and Biocomputing delve into domains beyond the binary bit to handle complex non-linear functions. In this paper, we harness Physarum polycephalum as memristors to process and generate creative data for popular music. While there has been research conducted in this area, the literature lacks examples of popular music and how the organism's non-linear behaviour can be controlled while composing music. This is important because non-linear forms of representation are not as obvious as conventional digital means. This study aims at disseminating this technology to non-experts and musicians so that they can incorporate it in their creative processes. Furthermore, it combines resistors and memristors to have more flexibility while generating music and optimises parameters for faster processing and performance. }
}

@inproceedings{NIME20_99,
  author = {Camara Halac, Fede and Addy, Shadrick},
  title = {PathoSonic: Performing Sound In Virtual Reality Feature Space},
  pages = {520--522},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Romain Michon and Franziska Schroeder},
  year = {2020},
  month = {July},
  publisher = {Birmingham City University},
  address = {Birmingham, UK},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4813510},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper99.pdf},
  abstract = {PathoSonic is a VR experience that enables a participant to visualize and perform a sound file based on timbre feature descriptors displayed in space. The name comes from the different paths the participant can create through their sonic explorations. The goal of this research is to leverage affordances of virtual reality technology to visualize sound through different levels of performance-based interactivity that immerses the participant's body in a spatial virtual environment. Through implementation of a multi-sensory experience, including visual aesthetics, sound, and haptic feedback, we explore inclusive approaches to sound visualization, making it more accessible to a wider audience including those with hearing, and mobility impairments. The online version of the paper can be accessed here: https://fdch.github.io/pathosonic}
}

