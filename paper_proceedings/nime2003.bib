@inproceedings{Cannon2003,
  author = {Cannon, Cormac and Hughes, Stephen and O'Modhrain, Sile},
  title = {EpipE: Exploration of the Uilleann Pipes as a Potential Controller for Computer-based Music},
  pages = {3--8},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176497},
  url = {http://www.nime.org/proceedings/2003/nime2003_003.pdf},
  keywords = {Controllers, continuous woodwind tonehole sensor, uilleann pipes, Irish bagpipe, physical modelling, double reed, conical bore, tonehole. },
  abstract = {In this paper we present a design for the EpipE, a newexpressive electronic music controller based on the IrishUilleann Pipes, a 7-note polyphonic reeded woodwind. Thecore of this proposed controller design is a continuouselectronic tonehole-sensing arrangement, equally applicableto other woodwind interfaces like those of the flute, recorder orJapanese shakuhachi. The controller will initially be used todrive a physically-based synthesis model, with the eventualgoal being the development of a mapping layer allowing theEpipE interface to operate as a MIDI-like controller of arbitrarysynthesis models.}
}

@inproceedings{Young2003,
  author = {Young, Diana and Essl, Georg},
  title = {HyperPuja: A Tibetan Singing Bowl Controller},
  pages = {9--14},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176577},
  url = {http://www.nime.org/proceedings/2003/nime2003_009.pdf},
  abstract = {HyperPuja is a novel controller that closely mimicks the behavior of a Tibetan Singing Bowl rubbed with a "puja" stick. Our design hides the electronics from the performer to maintain the original look and feel of the instrument and the performance. This is achieved by using wireless technology to keep the stick un-tethered as well as burying the electronics inside the the core of the stick. The measured parameters closely resemble the input parameters of a related physical synthesis model allowing for convenient mapping of sensor parameters to synthesis input. The new controller allows for flexible choice of sound synthesis while fully maintaining the characteristics of the physical interaction of the original instrument.}
}

@inproceedings{Scavone2003,
  author = {Scavone, Gary},
  title = {THE PIPE: Explorations with Breath Control},
  pages = {15--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176557},
  url = {http://www.nime.org/proceedings/2003/nime2003_015.pdf},
  keywords = {MIDI Controller, Wind Controller, Breath Control, Human Computer Interaction. },
  abstract = {The Pipe is an experimental, general purpose music input device designed and built in the form of a compact MIDI wind controller. The development of this device was motivated in part by an interest in exploring breath pressure as a control input. The Pipe provides a variety of common sensor types, including force sensing resistors, momentary switches, accelerometers, potentiometers, and an air pressure transducer, which allow maximum flexibility in the design of a sensor mapping scheme. The Pipe uses a programmable BASIC Stamp 2sx microprocessor which outputs control messages via a standard MIDI jack.}
}

@inproceedings{Baalman2003,
  author = {Baalman, Marije A.},
  title = {The {STRIMIDILATOR}: a String Controlled {MIDI}-Instrument},
  pages = {19--23},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176486},
  url = {http://www.nime.org/proceedings/2003/nime2003_019.pdf},
  keywords = {MIDI controllers, tactile force feedback, strings. Figure The STRIMIDILATOR },
  abstract = {The STRIMIDILATOR is an instrument that uses the deviation and the vibration of strings as MIDI-controllers. Thismethod of control gives the user direct tactile force feedbackand allows for subtle control. The development of the instrument and its different functions are described.}
}

@inproceedings{Wilson2003,
  author = {Wilson, Scott and Gurevich, Michael and Verplank, Bill and Stang, Pascal},
  title = {Microcontrollers in Music HCI Instruction: Reflections on our Switch to the Atmel AVR Platform},
  pages = {24--29},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176571},
  url = {http://www.nime.org/proceedings/2003/nime2003_024.pdf},
  keywords = {Microcontrollers, Music Controllers, Pedagogy, Atmel AVR, BASIC Stamp.},
  abstract = {Over the past year the instructors of the Human ComputerInteraction courses at CCRMA have undertaken a technology shift to a much more powerful teaching platform. Wedescribe the technical features of the new Atmel AVR basedplatform, contrasting it with the Parallax BASIC Stampplatform used in the past. The successes and failures ofthe new platform are considered, and some student projectsuccess stories described.}
}

@inproceedings{Andersen2003,
  author = {Andersen, Tue H.},
  title = {Mixxx : Towards Novel DJ Interfaces},
  pages = {30--35},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176484},
  url = {http://www.nime.org/proceedings/2003/nime2003_030.pdf},
  keywords = {DJ, software, interaction, visualization, controllers, augmented reality.},
  abstract = {The Disc Jockey (DJ) software system Mixxx is presented.Mixxx makes it possible to conduct studies of new interaction techniques in connection with the DJ situation, by itsopen design and easy integration of new software modulesand MIDI connection to external controllers. To gain a better understanding of working practices, and to aid the designprocess of new interfaces, interviews with two contemporarymusicians and DJ's are presented. In contact with thesemusicians development of several novel prototypes for DJinteraction have been made. Finally implementation detailsof Mixxx are described.}
}

@inproceedings{Orio2003,
  author = {Orio, Nicola and Lemouton, Serge and Schwarz, Diemo},
  title = {Score Following: State of the Art and New Developments},
  pages = {36--41},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176547},
  url = {http://www.nime.org/proceedings/2003/nime2003_036.pdf},
  keywords = {Score following, score recognition, real time audio alignment, virtual accompaniment.},
  abstract = {Score following is the synchronisation of a computer with a performer playing a known musical score. It now has a history of about twenty years as a research and musical topic, and is an ongoing project at Ircam. We present an overview of existing and historical score following systems, followed by fundamental definitions and terminology, and considerations about score formats, evaluation of score followers, and training. The score follower that we developed at Ircam is based on a Hidden Markov Model and on the modeling of the expected signal received from the performer. The model has been implemented in an audio and a Midi version, and is now being used in production. We report here our first experiences and our first steps towards a complete evaluation of system performances. Finally, we indicate directions how score following can go beyond the artistic applications known today.}
}

@inproceedings{Traube2003,
  author = {Traube, Caroline and Depalle, Philippe and Wanderley, Marcelo M.},
  title = {Indirect Acquisition of Instrumental Gesture Based on Signal , Physical and Perceptual Information},
  pages = {42--47},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176567},
  url = {http://www.nime.org/proceedings/2003/nime2003_042.pdf},
  keywords = {Signal analysis, indirect acquisition of instrumental gesture, guitar},
  abstract = {In this paper, we describe a multi-level approach for the extraction of instrumental gesture parameters taken from the characteristics of the signal captured by a microphone and based on the knowledge of physical mechanisms taking place on the instrument. We also explore the relationships between some features of timbre and gesture parameters, taking as a starting point for the exploration the timbre descriptors commonly used by professional musicians when they verbally describe the sounds they produce with their instrument. Finally, we present how this multi-level approach can be applied to the study of the timbre space of the classical guitar.}
}

@inproceedings{Nagashima2003,
  author = {Nagashima, Yoichi},
  title = {Bio-Sensing Systems and Bio-Feedback Systems for Interactive Media Arts},
  pages = {48--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176539},
  url = {http://www.nime.org/proceedings/2003/nime2003_048.pdf},
  abstract = {This is a report of research and some experimental applications of human-computer interaction in multi-media performing arts. The human performer and the computer systems perform computer graphic and computer music interactively in real-time. In general, many sensors are used for the interactive communication as interfaces, and the performer receives the output of the system via graphics, sounds and physical reactions of interfaces like musical instruments. I have produced many types of interfaces, not only with physical/electrical sensors but also with biological/physiological sensors. This paper is intended as an investigation of some special approaches: (1) 16-channel electromyogram sensor called “MiniBioMuse-III” and its application work called “BioCosmicStorm-II” performed in Paris, Kassel and Hamburg in 2001, (2) sensing/reacting with “breathing” in performing arts, (3) 8-channel electric-feedback system and its experiments of “body-hearing sounds” and “body-listening to music”.}
}

@inproceedings{Momeni2003,
  author = {Momeni, Ali and Wessel, David},
  title = {Characterizing and Controlling Musical Material Intuitively with Geometric Models},
  pages = {54--62},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176535},
  url = {http://www.nime.org/proceedings/2003/nime2003_054.pdf},
  keywords = {Perceptual Spaces, Graphical Models, Real-time Instruments, Dimensionality Reduction, Multidimensional Scaling, Live Performance, Gestural Controllers, Live Interaction, High-level Control.},  
  abstract = {In this paper, we examine the use of spatial layouts of musicalmaterial for live performance control. Emphasis is given tosoftware tools that provide for the simple and intuitivegeometric organization of sound material, sound processingparameters, and higher-level musical structures.}
}

@inproceedings{Burtner2003,
  author = {Burtner, Matthew},
  title = {Composing for the (dis)Embodied Ensemble : Notational Systems in (dis)Appearances},
  pages = {63--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176492},
  url = {http://www.nime.org/proceedings/2003/nime2003_063.pdf},
  keywords = {Composition, notation systems, virtual reality, controllers, physical modeling, string, violin.},
  abstract = {This paper explores compositional and notational approaches for working with controllers. The notational systems devised for the composition (dis)Appearances are discussed in depth in an attempt to formulate a new approach to composition using ensembles that navigates a performative space between reality and virtuality.}
}

@inproceedings{Jorda2003,
  author = {Jord\`{a}, Sergi},
  title = {Sonigraphical Instruments: From {FM}OL to the reacTable*},
  pages = {70--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176519},
  url = {http://www.nime.org/proceedings/2003/nime2003_070.pdf},
  keywords = {Interactive music instruments, audio visualization, visual interfaces, visual feedback, tangible interfaces, computer vision, augmented reality, music instruments for novices, collaborative music.},
  abstract = {This paper first introduces two previous software-based musicinstruments designed by the author, and analyses the crucialimportance of the visual feedback introduced by theirinterfaces. A quick taxonomy and analysis of the visualcomponents in current trends of interactive music software isthen proposed, before introducing the reacTable*, a newproject that is currently under development. The reacTable* isa collaborative music instrument, aimed both at novices andadvanced musicians, which employs computer vision andtangible interfaces technologies, and pushes further the visualfeedback interface ideas and techniques aforementioned.}
}

@inproceedings{Hatanaka2003,
  author = {Hatanaka, Motohide},
  title = {Ergonomic Design of A Portable Musical Instrument},
  pages = {77--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176509},
  url = {http://www.nime.org/proceedings/2003/nime2003_077.pdf},
  keywords = {MIDI controller, electronic musical instrument, musical instrument design, ergonomics, playability, human computer interface. },
  abstract = {A handheld electronic musical instrument, named the BentoBox, was developed. The motivation was to develop aninstrument which one can easily carry around and play inmoments of free time, for example when riding public transportation or during short breaks at work. The device wasdesigned to enable quick learning by having various scalesprogrammed for different styles of music, and also beexpressive by having hand controlled timbral effects whichcan be manipulated while playing. Design analysis anditeration lead to a compact and ergonomic device. This paperfocuses on the ergonomic design process of the hardware.}
}

@inproceedings{Shiraiwa2003,
  author = {Shiraiwa, Hiroko and Segnini, Rodrigo and Woo, Vivian},
  title = {Sound Kitchen: Designing a Chemically Controlled Musical Performance},
  pages = {83--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176561},
  url = {http://www.nime.org/proceedings/2003/nime2003_083.pdf},
  keywords = {Chemical music, Applied chemistry, Battery Controller.},
  abstract = {This paper presents a novel use of a chemical experiments’ framework as a control layer and sound source in a con- cert situation. Signal fluctuations from electrolytic batteries made out of household chemicals, and acoustic samples obtained from an acid/base reaction are used for musical purposes beyond the standard data sonification role. The batteries are controlled in handy ways such as warming, stirring and pouring that are also visually engaging. Audio mappings include synthetic and sampled sounds completing a recipe that concocts a live performance of computer music.}
}

@inproceedings{Ryan2003,
  author = {Ryan, Joel and Salter, Christopher L.},
  title = {TGarden: Wearable Instruments and Augmented Physicality},
  pages = {87--90},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176555},
  url = {http://www.nime.org/proceedings/2003/nime2003_087.pdf},
  keywords = {Gesture, interaction, embodied action, enaction, physical model, responsive environment, interactive musical systems, affordance, interface, phenomenology, energy, kinetics, time constant, induced ballistics, wearable computing, accelerometer, audience participation, dynamical system, dynamic compliance, effort, wearable instrument, augmented physicality. },
  abstract = {This report details work on the interdisciplinary mediaproject TGarden. The authors discuss the challengesencountered while developing a responsive musicalenvironment for the general public involving wearable,sensor-integrated clothing as the central interface and input device. The project's dramaturgical andtechnical/implementation background are detailed toprovide a framework for the creation of a responsive hardwareand software system that reinforces a tangible relationshipbetween the participant's improvised movement and musicalresponse. Finally, the authors take into consideration testingscenarios gathered from public prototypes in two Europeanlocales in 2001 to evaluate user experience of the system.}
}

@inproceedings{Ventura2003,
  author = {Ventura, David and Mase, Kenji},
  title = {Duet Musical Companion: Improvisational Interfaces for Children},
  pages = {91--94},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176569},
  url = {http://www.nime.org/proceedings/2003/nime2003_091.pdf},
  keywords = {Musical improvisation, toy interface agent, sensor doll, context awareness. },
  abstract = {We present a sensor-doll interface as a musical outlet forpersonal expression. A doll serves the dual role of being bothan expressive agent and a playmate by allowing solo andaccompanied performance. An internal computer and sensorsystem allow the doll to receive input from the user and itssurroundings, and then respond accordingly with musicalfeedback. Sets of musical timbres and melodies may bechanged by presenting the doll with a series of themed clothhats, each suggesting a different style of play. The doll mayperform by itself and play a number of melodies, or it maycollaborate with the user when its limbs are squeezed or bent.Shared play is further encouraged by a basic set of aural tonesmimicking conversation.}
}

@inproceedings{Howard2003,
  author = {Howard, David M. and Rimell, Stuart and Hunt, Andy D.},
  title = {Force Feedback Gesture Controlled Physical Modelling Synthesis},
  pages = {95--98},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176515},
  url = {http://www.nime.org/proceedings/2003/nime2003_095.pdf},
  keywords = {Physical modeling, haptic controllers, gesture control, force feedback.},
  abstract = {A physical modelling music synthesis system known as ‘Cymatic’ is described that enables ‘virtual instruments’ to be controlled in real-time via a force-feedback joystick and a force-feedback mouse. These serve to provide the user with gestural controllers whilst in addition giving tactile feedback to the user. Cymatic virtual instruments are set up via a graphical user interface in a manner that is highly intuitive. Users design and play these virtual instruments by interacting directly with their physical shape and structure in terms of the physical properties of basic objects such as strings, membranes and solids which can be interconnected to form complex structures. The virtual instrument can be excited at any point mass by the following: bowing, plucking, striking, sine/square/sawtooth/random waveform, or an external sound source. Virtual microphones can be placed at any point masses to deliver the acoustic output. This paper describes the underlying structure and principles upon which Cymatic is based, and illustrates its acoustic output.}
}

@inproceedings{Hoskinson2003,
  author = {Hoskinson, Reynald and van den Doel, Kees and Fels, Sidney S.},
  title = {Real-time Adaptive Control of Modal Synthesis},
  pages = {99--103},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176513},
  url = {http://www.nime.org/proceedings/2003/nime2003_099.pdf},
  abstract = {We describe the design and implementation of an adaptive system to map control parameters to modal audio synthesis parameters in real-time. The modal parameters describe the linear response of a virtual vibrating solid, which is played as a musical instrument by a separate interface. The system uses a three layer feedforward backpropagation neural network which is trained by a discrete set of input-output examples. After training, the network extends the training set, which functions as the specification by example of the controller, to a continuous mapping allowing the real-time morphing of synthetic sound models.  We have implemented a prototype application using a controller which collects data from a hand-drawn digital picture. The virtual instrument consists of a bank of modal resonators whose frequencies, dampings, and gains are the parameters we control. We train the system by providing pictorial representations of physical objects such as a bell or a lamp, and associate high quality modal models obtained from measurements on real objects with these inputs. After training, the user can draw pictures interactively and “play” modal models which provide interesting (though unrealistic) interpolations of the models from the training set in real-time.}
}

@inproceedings{Young2003a,
  author = {Young, Diana and Serafin, Stefania},
  title = {Playability Evaluation of a Virtual Bowed String Instrument},
  pages = {104--108},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176579},
  url = {http://www.nime.org/proceedings/2003/nime2003_104.pdf},
  abstract = {Driving a bowed string physical model using a bow controller, we explore the potentials of using the real gestures of a violinist to simulate violin sound using a virtual instrument. After a description of the software and hardware developed, preliminary results and future work are discussed.}
}

@inproceedings{Gaye2003,
  author = {Gaye, Lalya and Maz\'{e}, Ramia and Holmquist, Lars E.},
  title = {Sonic City: The Urban Environment as a Musical Interface},
  pages = {109--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176507},
  url = {http://www.nime.org/proceedings/2003/nime2003_109.pdf},
  keywords = {Interactive music, interaction design, urban environment, wearable computing, context-awareness, mobility},
  abstract = {In the project Sonic City, we have developed a system thatenables users to create electronic music in real time by walkingthrough and interacting with the urban environment. Weexplore the use of public space and everyday behaviours forcreative purposes, in particular the city as an interface andmobility as an interaction model for electronic music making.A multi-disciplinary design process resulted in theimplementation of a wearable, context-aware prototype. Thesystem produces music by retrieving information aboutcontext and user action and mapping it to real-time processingof urban sounds. Potentials, constraints, and implications ofthis type of music creation are discussed.}
}

@inproceedings{Lyons2003,
  author = {Lyons, Michael J. and Haehnel, Michael and Tetsutani, Nobuji},
  title = {Designing, Playing, and Performing with a Vision-based Mouth Interface},
  pages = {116--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176529},
  url = {http://www.nime.org/proceedings/2003/nime2003_116.pdf},
  keywords = {Video-based interface; mouth controller; alternative input devices. },
  abstract = {The role of the face and mouth in speech production as well asnon-verbal communication suggests the use of facial action tocontrol musical sound. Here we document work on theMouthesizer, a system which uses a headworn miniaturecamera and computer vision algorithm to extract shapeparameters from the mouth opening and output these as MIDIcontrol changes. We report our experience with variousgesture-to-sound mappings and musical applications, anddescribe a live performance which used the Mouthesizerinterface.}
}

@inproceedings{Hewitt2003,
  author = {Hewitt, Donna and Stevenson, Ian},
  title = {E-mic: Extended Mic-stand Interface Controller},
  pages = {122--128},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176511},
  url = {http://www.nime.org/proceedings/2003/nime2003_122.pdf},
  keywords = {Alternate controller, gesture, microphone technique, vocal performance, performance interface, electronic music. },
  abstract = {This paper describes work in progress for the development of a gestural controller interface for contemporary vocal performance and electronic processing. The paper includes a preliminary investigation of the gestures and movements of vocalists who use microphones and microphone stands. This repertoire of gestures forms the foundation of a well-practiced ‘language’ and social code for communication between performers and audiences and serves as a basis for alternate controller design principles. A prototype design, based on a modified microphone stand, is presented along with a discussion of possible controller mapping strategies and identification of directions for future research.}
}

@inproceedings{Blaine2003,
  author = {Blaine, Tina and Fels, Sidney S.},
  title = {Contexts of Collaborative Musical Experiences},
  pages = {129--134},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176490},
  url = {http://www.nime.org/proceedings/2003/nime2003_129.pdf},
  keywords = {Design, collaborative interface, musical experience, multiplayer, novice, musical control. },
  abstract = {We explore a variety of design criteria applicable to thecreation of collaborative interfaces for musical experience. Themain factor common to the design of most collaborativeinterfaces for novices is that musical control is highlyrestricted, which makes it possible to easily learn andparticipate in the collective experience. Balancing this tradeoff is a key concern for designers, as this happens at theexpense of providing an upward path to virtuosity with theinterface. We attempt to identify design considerationsexemplified by a sampling of recent collaborative devicesprimarily oriented toward novice interplay. It is our intentionto provide a non-technical overview of design issues inherentin configuring multiplayer experiences, particularly for entrylevel players.}
}

@inproceedings{Hunt2003,
  author = {Hunt, Andy D. and Kirk, Ross},
  title = {MidiGrid: Past, Present and Future},
  pages = {135--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176517},
  url = {http://www.nime.org/proceedings/2003/nime2003_135.pdf},
  keywords = {Live performance, Computer-based musical instruments, Human Computer Interaction for Music},
  abstract = {MidiGrid is a computer-based musical instrument, primarilycontrolled with the computer mouse, which allows liveperformance of MIDI-based musical material by mapping 2dimensional position onto musical events. Since itsinvention in 1987, it has gained a small, but enthusiastic,band of users, and has become the primary instrument forseveral people with physical disabilities. This paper reviewsits development, uses and user interface issues, and highlightsthe work currently in progress for its transformation intoMediaGrid.}
}

@inproceedings{Kessous2003,
  author = {Kessous, Lo\''{\i}c and Arfib, Daniel},
  title = {Bimanuality in Alternate Musical Instruments},
  pages = {140--145},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176523},
  url = {http://www.nime.org/proceedings/2003/nime2003_140.pdf},
  keywords = {Gesture control, mapping, alternate controllers, musical instruments. },
  abstract = {This paper presents a study of bimanual control applied tosound synthesis. This study deals with coordination,cooperation, and abilities of our hands in musical context. Wedescribe examples of instruments made using subtractivesynthesis, scanned synthesis in Max/MSP and commercialstand-alone software synthesizers via MIDI communicationprotocol. These instruments have been designed according to amulti-layer-mapping model, which provides modular design.They have been used in concerts and performanceconsiderations are discussed too.}
}

@inproceedings{Modler2003,
  author = {Modler, Paul and Myatt, Tony and Saup, Michael},
  title = {An Experimental Set of Hand Gestures for Expressive Control of Musical Parameters in Realtime},
  pages = {146--150},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176533},
  url = {http://www.nime.org/proceedings/2003/nime2003_146.pdf},
  keywords = {Gesture Recognition, Artificial Neural Network, Expressive Control, Real-time Interaction },
  abstract = {This paper describes the implementation of Time Delay NeuralNetworks (TDNN) to recognize gestures from video images.Video sources are used because they are non-invasive and do notinhibit performer's physical movement or require specialistdevices to be attached to the performer which experience hasshown to be a significant problem that impacts musiciansperformance and can focus musical rehearsals and performancesupon technical rather than musical concerns (Myatt 2003).We describe a set of hand gestures learned by an artificial neuralnetwork to control musical parameters expressively in real time.The set is made up of different types of gestures in order toinvestigate:-aspects of the recognition process-expressive musical control-schemes of parameter mapping-generalization issues for an extended set for musicalcontrolThe learning procedure of the Neural Network is describedwhich is based on variations by affine transformations of imagesequences of the hand gestures.The whole application including the gesture capturing isimplemented in jMax to achieve real time conditions and easyintegration into a musical environment to realize differentmappings and routings of the control stream.The system represents a practice-based research using actualmusic models like compositions and processes of compositionwhich will follow the work described in the paper.}
}

@inproceedings{Nakra2003,
  author = {Nakra, Teresa M.},
  title = {Immersion Music: a Progress Report},
  pages = {151--152},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176541},
  url = {http://www.nime.org/proceedings/2003/nime2003_151.pdf},
  keywords = {Interactive computer music systems, gestural interaction, Conductor's Jacket, Digital Baton },
  abstract = {This paper describes the artistic projects undertaken at ImmersionMusic, Inc. (www.immersionmusic.org) during its three-yearexistence. We detail work in interactive performance systems,computer-based training systems, and concert production.}
}

@inproceedings{Wright2003,
  author = {Wright, Matthew and Freed, Adrian and Momeni, Ali},
  title = {OpenSound Control: State of the Art 2003},
  pages = {153--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176575},
  url = {http://www.nime.org/proceedings/2003/nime2003_153.pdf},
  keywords = {OpenSound Control, Networking, client/server communication},
  abstract = {OpenSound Control (“OSC”) is a protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. OSC has achieved wide use in the field of computer-based new interfaces for musical expression for wide-area and local-area networked distributed music systems, inter-process communication, and even within a single application.}
}

@inproceedings{Dobrian2003,
  author = {Dobrian, Christopher and Bevilacqua, Fr\'{e}d\'{e}ric},
  title = {Gestural Control of Music Using the Vicon 8 Motion Capture System},
  pages = {161--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176503},
  url = {http://www.nime.org/proceedings/2003/nime2003_161.pdf},
  keywords = {Motion capture, gestural control, mapping. },
  abstract = {This article reports on a project that uses unfettered gestural motion for expressive musical purposes. The project involves the development of, and experimentation with, software to receive data from a Vicon motion capture system, and to translate and map that data into data for the control of music and other media such as lighting. In addition to the commercially standard MIDI-which allows direct control of external synthesizers, processors, and other devices-other mappings are used for direct software control of digital audio and video. This report describes the design and implementation of the software, discusses specific experiments performed with it, and evaluates its application in terms of aesthetic pros and cons.}
}

@inproceedings{Nishimoto2003,
  author = {Nishimoto, Kazushi and Oshima, Chika and Miyagawa, Yohei},
  title = {Why Always Versatile? Dynamically Customizable Musical Instruments Facilitate Expressive Performances},
  pages = {164--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176545},
  url = {http://www.nime.org/proceedings/2003/nime2003_164.pdf},
  keywords = {Musical instruments, expression, design principle, degree of freedom, dynamic specialization},
  abstract = {In this paper, we discuss a design principle for the musical instruments that are useful for both novices and professional musicians and that facilitate musically rich expression. We believe that the versatility of conventional musical instruments causes difficulty in performance. By dynamically specializing a musical instrument for performing a specific (genre of) piece, the musical instrument could become more useful for performing the piece and facilitates expressive performance. Based on this idea, we developed two new types of musical instruments, i.e., a "given-melody-based musical instrument" and a "harmonic-function-based musical instrument". From the experimental results using two prototypes, we demonstrate the efficiency of the design principle.}
}

@inproceedings{Newton-Dunn2003,
  author = {Newton-Dunn, Henry and Nakano, Hiroaki and Gibson, James},
  title = {Block Jam: A Tangible Interface for Interactive Music},
  pages = {170--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176543},
  url = {http://www.nime.org/proceedings/2003/nime2003_170.pdf},
  keywords = {Tangible interface, modular system, polyrhythmic sequencer. VISION We believe in a future where music will no longer be considered a linear composition, but a dynamic structure, and musical composition will extend to interaction. We also believe that through the },
  abstract = {In this paper, we introduce Block Jam, a Tangible UserInterface that controls a dynamic polyrhythmic sequencerusing 26 physical artifacts. These physical artifacts, that wecall blocks, are a new type of input device for manipulatingan interactive music system. The blocks' functional andtopological statuses are tightly coupled to an ad hocsequencer, interpreting the user's arrangement of the blocksas meaningful musical phrases and structures.We demonstrate that we have created both a tangible andvisual language that enables both the novice and musicallytrained users by taking advantage of both their explorativeand intuitive abilities. The tangible nature of the blocks andthe intuitive interface promotes face-to-face collaborationand social interaction within a single system. The principleof collaboration is further extended by linking two BlockJam systems together to create a network.We discuss our project vision, design rational, relatedworks, and the implementation of Block Jam prototypes.Figure 1. A cluster of blocks, note the mother block on thebottom right}
}

@inproceedings{Kartadinata2003,
  author = {Kartadinata, Sukandar},
  title = {The Gluiph: a Nucleus for Integrated Instruments},
  pages = {180--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176521},
  url = {http://www.nime.org/proceedings/2003/nime2003_180.pdf},
  keywords = {Musical instrument, integration, single-board computer (SBC), embedded system, stand-alone system, pd, DSP, sensor, latency, flexibility, coherency.},
  abstract = {In this paper I present the gluiph, a single-board computer thatwas conceived as a platform for integrated electronic musicalinstruments. It aims to provide new instruments as well asexisting ones with a stronger identity by untethering themfrom the often lab-like stage setups built around general purpose computers. The key additions to its core are a flexiblesensor subsystem and multi-channel audio I/O. In contrast toother stand-alone approaches it retains a higher degree offlexibility by supporting popular music programming languages, with Miller Puckette's pd [1] being the current focus.}
}

@inproceedings{Couturier2003,
  author = {Couturier, Jean-Michel and Arfib, Daniel},
  title = {Pointing Fingers: Using Multiple Direct Interactions with Visual Objects to Perform Music},
  pages = {184--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176501},
  url = {http://www.nime.org/proceedings/2003/nime2003_184.pdf},
  keywords = {HCI, touch screen, multimodality, mapping, direct interaction, gesture devices, bimanual interaction, two-handed, Max/MSP. },
  abstract = {In this paper, we describe a new interface for musicalperformance, using the interaction with a graphical userinterface in a powerful manner: the user directly touches ascreen where graphical objects are displayed and can useseveral fingers simultaneously to interact with the objects. Theconcept of this interface is based on the superposition of thegesture spatial place and the visual feedback spatial place; i tgives the impression that the graphical objects are real. Thisconcept enables a huge freedom in designing interfaces. Thegesture device we have created gives the position of fourfingertips using 3D sensors and the data is performed in theMax/MSP environment. We have realized two practicalexamples of musical use of such a device, using PhotosonicSynthesis and Scanned Synthesis.}
}

@inproceedings{Singer2003a,
  author = {Singer, Eric and Larke, Kevin and Bianciardi, David},
  title = {{LEMUR} GuitarBot: {MIDI} Robotic String Instrument},
  pages = {188--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176565},
  url = {http://www.nime.org/proceedings/2003/nime2003_188.pdf},
  keywords = {Robotics, interactive, performance, MIDI, string instrument.},
  abstract = {This paper describes the LEMUR GuitarBot, a robotic musical instrument composed of four independent MIDI controllable single-stringed movable bridge units. Design methodology, development and fabrication process, control specification and results are discussed.}
}

@inproceedings{Peiper2003,
  author = {Peiper, Chad and Warden, David and Garnett, Guy},
  title = {An Interface for Real-time Classification of Articulations Produced by Violin Bowing},
  pages = {192--196},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176553},
  url = {http://www.nime.org/proceedings/2003/nime2003_192.pdf},
  abstract = {We introduce a software system for real-time classification of violin bow strokes (articulations). The system uses an electromagnetic motion tracking system to capture raw gesture data. The data is analyzed to extract stroke features. These features are provided to a decision tree for training and classification. Feedback from feature and classification data is presented visually in an immersive graphic environment.}
}

@inproceedings{Settel2003,
  author = {Settel, Zack and Lippe, Cort},
  title = {Convolution Brother's Instrument Design},
  pages = {197--200},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176559},
  url = {http://www.nime.org/proceedings/2003/nime2003_197.pdf},
  abstract = {The subject of instrument design is quite broad. Much work has been done at Ircam, MIT, CNMAT, Stanford and elsewhere in the area. In this paper we will present our own developed approach to designing and using instruments in composition and performance for the authors’ “Convolution Brothers” pieces. The presentation of this paper is accompanied by a live Convolution Brothers demonstration.}
}

@inproceedings{Choi2003,
  author = {Choi, Insook},
  title = {A Component Model of Gestural Primitive Throughput},
  pages = {201--204},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176499},
  url = {http://www.nime.org/proceedings/2003/nime2003_201.pdf},
  keywords = {Performance gestures, musical gestures, instrument design, mapping, tuning, affordances, stability. },
  abstract = {This paper suggests that there is a need for formalizing acomponent model of gestural primitive throughput in musicinstrument design. The purpose of this model is to construct acoherent and meaningful interaction between performer andinstrument. Such a model has been implicit in previous researchfor interactive performance systems. The model presented heredistinguishes gestural primitives from units of measure ofgestures. The throughput model identifies symmetry betweenperformance gestures and musical gestures, and indicates a rolefor gestural primitives when a performer navigates regions ofstable oscillations in a musical instrument. The use of a highdimensional interface tool is proposed for instrument design, forfine-tuning the mapping between movement sensor data andsound synthesis control data.}
}

@inproceedings{PalacioQuintin2003,
  author = {Palacio-Quintin, Cl\'{e}o},
  title = {The Hyper-Flute},
  pages = {206--207},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176549},
  url = {http://www.nime.org/proceedings/2003/nime2003_206.pdf},
  keywords = {Digital sound processing, flute, hyper-instrument, interactive music, live electronics, performance, sensors.},
  abstract = {The Hyper-Flute is a standard Boehm flute (the model used is a Powell 2100, made in Boston) extended via electronic sensors that link it to a computer, enabling control of digital sound processing parameters while performing. The instrument’s electronic extensions are described in some detail, and performance applications are briefly discussed.}
}

@inproceedings{Allison2003,
  author = {Allison, Jesse T. and Place, Timothy},
  title = {SensorBox: Practical Audio Interface for Gestural Performance},
  pages = {208--210},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176482},
  url = {http://www.nime.org/proceedings/2003/nime2003_208.pdf},
  keywords = {Sensors, gestural acquisition, audio interface, interactive music, SensorBox. },
  abstract = {SensorBox is a low cost, low latency, high-resolutioninterface for obtaining gestural data from sensors for use inrealtime with a computer-based interactive system. Wediscuss its implementation, benefits, current limitations, andcompare it with several popular interfaces for gestural dataacquisition.}
}

@inproceedings{Baird2003,
  author = {Baird, Kevin C.},
  title = {Multi-Conductor: An Onscreen Polymetrical Conducting and Notation Display System},
  pages = {211--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176488},
  url = {http://www.nime.org/proceedings/2003/nime2003_211.pdf},
  keywords = {Open form, notation, polymeter, polytempi, Max/MSP. },
  abstract = {This software tool, developed in Max/MSP, presentsperformers with image files consisting of traditional notationas well as conducting in the form of video playback. Theimpetus for this work was the desire to allow the musicalmaterial for each performer of a given piece to differ withregard to content and tempo.}
}

@inproceedings{Kleinsasser2003,
  author = {Kleinsasser, William},
  title = {Dsp.rack: Laptop-based Modular, Programmable Digital Signal Processing and Mixing for Live Performance},
  pages = {213--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176525},
  url = {http://www.nime.org/proceedings/2003/nime2003_213.pdf},
  keywords = {Digital signal processing, Max/MSP, computer music performance, matrix routing, live performance processing. },
  abstract = {This document describes modular software supporting livesignal processing and sound file playback within theMax/MSP environment. Dsp.rack integrates signalprocessing, memory buffer recording, and pre-recordedmulti-channel file playback using an interconnected,programmable signal flow matrix, and an eight-channel i/oformat.}
}

@inproceedings{Laibowitz2003,
  author = {Laibowitz, Mat},
  title = {BASIS: A Genesis in Musical Interfaces},
  pages = {216--217},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176527},
  url = {http://www.nime.org/proceedings/2003/nime2003_216.pdf},
  keywords = {Performance, Design, Experimentation, DNA, Big Five. },
  abstract = {This paper is a demo proposal for a new musical interfacebased on a DNA-like double-helix and concepts in charactergeneration. It contains a description of the interface,motivations behind developing such an interface, variousmappings of the interface to musical applications, and therequirements to demo the interface.}
}

@inproceedings{Merrill2003,
  author = {Merrill, David},
  title = {Head-Tracking for Gestural and Continuous Control of Parameterized Audio Effects},
  pages = {218--219},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176531},
  url = {http://www.nime.org/proceedings/2003/nime2003_218.pdf},
  keywords = {Head-tracking, gestural control, continuous control, parameterized effects processor. },
  abstract = {This paper describes a system which uses the output fromhead-tracking and gesture recognition software to drive aparameterized guitar effects synthesizer in real-time.}
}

@inproceedings{Singer2003,
  author = {Singer, Eric},
  title = {Sonic Banana: A Novel Bend-Sensor-Based {MIDI} Controller},
  pages = {220--221},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176563},
  url = {http://www.nime.org/proceedings/2003/nime2003_220.pdf},
  keywords = {Interactive, controller, bend, sensors, performance, MIDI.},
  abstract = {This paper describes the Sonic Banana, a bend-sensor based alternative MIDI controller.}
}

@inproceedings{Muth2003,
  author = {Muth, David and Burton, Ed},
  title = {Sodaconductor},
  pages = {222--224},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176537},
  url = {http://www.nime.org/proceedings/2003/nime2003_222.pdf},
  keywords = {Sodaconstrucor, Soda, Open Sound Control, Networked Performance, Physical Simulation, Generative Composition, Java Application, Non-Linear Sequencing.},
  abstract = {Sodaconductor is a musical interface for generating OSCcontrol data based on the dynamic physical simulation toolSodaconstructor as it can be seen and heard onhttp://www.sodaplay.com.}
}

@inproceedings{Flety2003,
  author = {Fl\'{e}ty, Emmanuel and Sirguy, Marc},
  title = {EoBody : a Follow-up to AtoMIC Pro's Technology},
  pages = {225--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176505},
  url = {http://www.nime.org/proceedings/2003/nime2003_225.pdf},
  keywords = {Gestural controller, Sensor, MIDI, Computer Music. },
  abstract = {Ircam has been deeply involved into gesture analysis and sensingfor about four years now, as several artistic projects demonstrate.Ircam has often been solicited for sharing software and hardwaretools for gesture sensing, especially devices for the acquisition andconversion of sensor data, such as the AtoMIC Pro [1][2]. Thisdemo-paper describes the recent design of a new sensor to MIDIinterface called EoBody1}
}

@inproceedings{Paradiso2003,
  author = {Paradiso, Joseph A.},
  title = {Dual-Use Technologies for Electronic Music Controllers: A Personal Perspective},
  pages = {228--234},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176551},
  url = {http://www.nime.org/proceedings/2003/nime2003_228.pdf},
  abstract = {Several well-known alternative musical controllers were inspired by sensor systems developed in other fields, often coming to their musical application via surprising routes. Correspondingly, work on electronic music controllers has relevance to other applications and broader research themes. In this article, I give a tour though several controller systems that I have been involved with over the past decade and outline their connections with other areas of inquiry.}
}

@inproceedings{Cadoz2003,
  author = {Cadoz, Claude and Luciani, Annie and Florens, Jean-Loup and Castagn\'{e}, Nicolas},
  title = {{AC}ROE --- {ICA} Artistic Creation and Computer Interactive Multisensory Simulation Force Feedback Gesture Transducers},
  pages = {235--246},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176494},
  url = {http://www.nime.org/proceedings/2003/nime2003_235.pdf}
}

