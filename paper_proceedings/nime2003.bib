@inproceedings{Allison2003,
  author = {Allison, Jesse T. and Place, Timothy},
  title = {SensorBox: Practical Audio Interface for Gestural Performance},
  pages = {208--210},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176482},
  url = {http://www.nime.org/proceedings/2003/nime2003_208.pdf},
  keywords = {Sensors, gestural acquisition, audio interface, interactive music, SensorBox. },
  abstract = {SensorBox is a low cost, low latency, high-resolutioninterface for obtaining gestural data from sensors for use inrealtime with a computer-based interactive system. Wediscuss its implementation, benefits, current limitations, andcompare it with several popular interfaces for gestural dataacquisition.}
}

@inproceedings{Andersen2003,
  author = {Andersen, Tue H.},
  title = {Mixxx : Towards Novel DJ Interfaces},
  pages = {30--35},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176484},
  url = {http://www.nime.org/proceedings/2003/nime2003_030.pdf},
  abstract = {The Disc Jockey (DJ) software system Mixxx is presented.Mixxx makes it possible to conduct studies of new interaction techniques in connection with the DJ situation, by itsopen design and easy integration of new software modulesand MIDI connection to external controllers. To gain a better understanding of working practices, and to aid the designprocess of new interfaces, interviews with two contemporarymusicians and DJ's are presented. In contact with thesemusicians development of several novel prototypes for DJinteraction have been made. Finally implementation detailsof Mixxx are described.}
}

@inproceedings{Baalman2003,
  author = {Baalman, Marije A.},
  title = {The STRIMIDILATOR: a String Controlled MIDI-Instrument},
  pages = {19--23},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176486},
  url = {http://www.nime.org/proceedings/2003/nime2003_019.pdf},
  keywords = {MIDI controllers, tactile force feedback, strings. Figure The STRIMIDILATOR },
  abstract = {The STRIMIDILATOR is an instrument that uses the deviation and the vibration of strings as MIDI-controllers. Thismethod of control gives the user direct tactile force feedbackand allows for subtle control. The development of the instrument and its different functions are described.}
}

@inproceedings{Baird2003,
  author = {Baird, Kevin C.},
  title = {Multi-Conductor: An Onscreen Polymetrical Conducting and Notation Display System},
  pages = {211--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176488},
  url = {http://www.nime.org/proceedings/2003/nime2003_211.pdf},
  keywords = {Open form, notation, polymeter, polytempi, Max/MSP. },
  abstract = {This software tool, developed in Max/MSP, presentsperformers with image files consisting of traditional notationas well as conducting in the form of video playback. Theimpetus for this work was the desire to allow the musicalmaterial for each performer of a given piece to differ withregard to content and tempo.}
}

@inproceedings{Blaine2003,
  author = {Blaine, Tina and Fels, Sidney S.},
  title = {Contexts of Collaborative Musical Experiences},
  pages = {129--134},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176490},
  url = {http://www.nime.org/proceedings/2003/nime2003_129.pdf},
  keywords = {Design, collaborative interface, musical experience, multiplayer, novice, musical control. },
  abstract = {We explore a variety of design criteria applicable to thecreation of collaborative interfaces for musical experience. Themain factor common to the design of most collaborativeinterfaces for novices is that musical control is highlyrestricted, which makes it possible to easily learn andparticipate in the collective experience. Balancing this tradeoff is a key concern for designers, as this happens at theexpense of providing an upward path to virtuosity with theinterface. We attempt to identify design considerationsexemplified by a sampling of recent collaborative devicesprimarily oriented toward novice interplay. It is our intentionto provide a non-technical overview of design issues inherentin configuring multiplayer experiences, particularly for entrylevel players.}
}

@inproceedings{Burtner2003,
  author = {Burtner, Matthew},
  title = {Composing for the (dis)Embodied Ensemble : Notational Systems in (dis)Appearances},
  pages = {63--69},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176492},
  url = {http://www.nime.org/proceedings/2003/nime2003_063.pdf}
}

@inproceedings{Cadoz2003,
  author = {Cadoz, Claude and Luciani, Annie and Florens, Jean-Loup and Castagn\'{e}, Nicolas},
  title = {{AC}ROE --- {ICA} Artistic Creation and Computer Interactive Multisensory Simulation Force Feedback Gesture Transducers},
  pages = {235--246},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176494},
  url = {http://www.nime.org/proceedings/2003/nime2003_235.pdf}
}

@inproceedings{Cannon2003,
  author = {Cannon, Cormac and Hughes, Stephen and O'Modhrain, Sile},
  title = {EpipE: Exploration of the Uilleann Pipes as a Potential Controller for Computer-based Music},
  pages = {3--8},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176497},
  url = {http://www.nime.org/proceedings/2003/nime2003_003.pdf},
  keywords = {Controllers, continuous woodwind tonehole sensor, uilleann pipes, Irish bagpipe, physical modelling, double reed, conical bore, tonehole. },
  abstract = {In this paper we present a design for the EpipE, a newexpressive electronic music controller based on the IrishUilleann Pipes, a 7-note polyphonic reeded woodwind. Thecore of this proposed controller design is a continuouselectronic tonehole-sensing arrangement, equally applicableto other woodwind interfaces like those of the flute, recorder orJapanese shakuhachi. The controller will initially be used todrive a physically-based synthesis model, with the eventualgoal being the development of a mapping layer allowing theEpipE interface to operate as a MIDI-like controller of arbitrarysynthesis models.}
}

@inproceedings{Choi2003,
  author = {Choi, Insook},
  title = {A Component Model of Gestural Primitive Throughput},
  pages = {201--204},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176499},
  url = {http://www.nime.org/proceedings/2003/nime2003_201.pdf},
  keywords = {Performance gestures, musical gestures, instrument design, mapping, tuning, affordances, stability. },
  abstract = {This paper suggests that there is a need for formalizing acomponent model of gestural primitive throughput in musicinstrument design. The purpose of this model is to construct acoherent and meaningful interaction between performer andinstrument. Such a model has been implicit in previous researchfor interactive performance systems. The model presented heredistinguishes gestural primitives from units of measure ofgestures. The throughput model identifies symmetry betweenperformance gestures and musical gestures, and indicates a rolefor gestural primitives when a performer navigates regions ofstable oscillations in a musical instrument. The use of a highdimensional interface tool is proposed for instrument design, forfine-tuning the mapping between movement sensor data andsound synthesis control data.}
}

@inproceedings{Couturier2003,
  author = {Couturier, Jean-Michel and Arfib, Daniel},
  title = {Pointing Fingers: Using Multiple Direct Interactions with Visual Objects to Perform Music},
  pages = {184--187},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176501},
  url = {http://www.nime.org/proceedings/2003/nime2003_184.pdf},
  keywords = {HCI, touch screen, multimodality, mapping, direct interaction, gesture devices, bimanual interaction, two-handed, Max/MSP. },
  abstract = {In this paper, we describe a new interface for musicalperformance, using the interaction with a graphical userinterface in a powerful manner: the user directly touches ascreen where graphical objects are displayed and can useseveral fingers simultaneously to interact with the objects. Theconcept of this interface is based on the superposition of thegesture spatial place and the visual feedback spatial place; i tgives the impression that the graphical objects are real. Thisconcept enables a huge freedom in designing interfaces. Thegesture device we have created gives the position of fourfingertips using 3D sensors and the data is performed in theMax/MSP environment. We have realized two practicalexamples of musical use of such a device, using PhotosonicSynthesis and Scanned Synthesis.}
}

@inproceedings{Dobrian2003,
  author = {Dobrian, Christopher and Bevilacqua, Fr\'{e}d\'{e}ric},
  title = {Gestural Control of Music Using the Vicon 8 Motion Capture System},
  pages = {161--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176503},
  url = {http://www.nime.org/proceedings/2003/nime2003_161.pdf},
  keywords = {Motion capture, gestural control, mapping. }
}

@inproceedings{Flety2003,
  author = {Fl\'{e}ty, Emmanuel and Sirguy, Marc},
  title = {EoBody : a Follow-up to AtoMIC Pro's Technology},
  pages = {225--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176505},
  url = {http://www.nime.org/proceedings/2003/nime2003_225.pdf},
  keywords = {Gestural controller, Sensor, MIDI, Computer Music. },
  abstract = {Ircam has been deeply involved into gesture analysis and sensingfor about four years now, as several artistic projects demonstrate.Ircam has often been solicited for sharing software and hardwaretools for gesture sensing, especially devices for the acquisition andconversion of sensor data, such as the AtoMIC Pro [1][2]. Thisdemo-paper describes the recent design of a new sensor to MIDIinterface called EoBody1}
}

@inproceedings{Gaye2003,
  author = {Gaye, Lalya and Maz\'{e}, Ramia and Holmquist, Lars E.},
  title = {Sonic City: The Urban Environment as a Musical Interface},
  pages = {109--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176507},
  url = {http://www.nime.org/proceedings/2003/nime2003_109.pdf},
  abstract = {In the project Sonic City, we have developed a system thatenables users to create electronic music in real time by walkingthrough and interacting with the urban environment. Weexplore the use of public space and everyday behaviours forcreative purposes, in particular the city as an interface andmobility as an interaction model for electronic music making.A multi-disciplinary design process resulted in theimplementation of a wearable, context-aware prototype. Thesystem produces music by retrieving information aboutcontext and user action and mapping it to real-time processingof urban sounds. Potentials, constraints, and implications ofthis type of music creation are discussed.}
}

@inproceedings{Hatanaka2003,
  author = {Hatanaka, Motohide},
  title = {Ergonomic Design of A Portable Musical Instrument},
  pages = {77--82},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176509},
  url = {http://www.nime.org/proceedings/2003/nime2003_077.pdf},
  keywords = {MIDI controller, electronic musical instrument, musical instrument design, ergonomics, playability, human computer interface. },
  abstract = {A handheld electronic musical instrument, named the BentoBox, was developed. The motivation was to develop aninstrument which one can easily carry around and play inmoments of free time, for example when riding public transportation or during short breaks at work. The device wasdesigned to enable quick learning by having various scalesprogrammed for different styles of music, and also beexpressive by having hand controlled timbral effects whichcan be manipulated while playing. Design analysis anditeration lead to a compact and ergonomic device. This paperfocuses on the ergonomic design process of the hardware.}
}

@inproceedings{Hewitt2003,
  author = {Hewitt, Donna and Stevenson, Ian},
  title = {E-mic: Extended Mic-stand Interface Controller},
  pages = {122--128},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176511},
  url = {http://www.nime.org/proceedings/2003/nime2003_122.pdf},
  keywords = {Alternate controller, gesture, microphone technique, vocal performance, performance interface, electronic music. }
}

@inproceedings{Hoskinson2003,
  author = {Hoskinson, Reynald and van den Doel, Kees and Fels, Sidney S.},
  title = {Real-time Adaptive Control of Modal Synthesis},
  pages = {99--103},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176513},
  url = {http://www.nime.org/proceedings/2003/nime2003_099.pdf}
}

@inproceedings{Howard2003,
  author = {Howard, David M. and Rimell, Stuart and Hunt, Andy D.},
  title = {Force Feedback Gesture Controlled Physical Modelling Synthesis},
  pages = {95--98},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176515},
  url = {http://www.nime.org/proceedings/2003/nime2003_095.pdf}
}

@inproceedings{Hunt2003,
  author = {Hunt, Andy D. and Kirk, Ross},
  title = {MidiGrid: Past, Present and Future},
  pages = {135--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176517},
  url = {http://www.nime.org/proceedings/2003/nime2003_135.pdf},
  abstract = {MidiGrid is a computer-based musical instrument, primarilycontrolled with the computer mouse, which allows liveperformance of MIDI-based musical material by mapping 2dimensional position onto musical events. Since itsinvention in 1987, it has gained a small, but enthusiastic,band of users, and has become the primary instrument forseveral people with physical disabilities. This paper reviewsits development, uses and user interface issues, and highlightsthe work currently in progress for its transformation intoMediaGrid.}
}

@inproceedings{Jorda2003,
  author = {Jord\`{a}, Sergi},
  title = {Sonigraphical Instruments: From {FM}OL to the reacTable*},
  pages = {70--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176519},
  url = {http://www.nime.org/proceedings/2003/nime2003_070.pdf},
  abstract = {This paper first introduces two previous software-based musicinstruments designed by the author, and analyses the crucialimportance of the visual feedback introduced by theirinterfaces. A quick taxonomy and analysis of the visualcomponents in current trends of interactive music software isthen proposed, before introducing the reacTable*, a newproject that is currently under development. The reacTable* isa collaborative music instrument, aimed both at novices andadvanced musicians, which employs computer vision andtangible interfaces technologies, and pushes further the visualfeedback interface ideas and techniques aforementioned.}
}

@inproceedings{Kartadinata2003,
  author = {Kartadinata, Sukandar},
  title = {The Gluiph: a Nucleus for Integrated Instruments},
  pages = {180--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176521},
  url = {http://www.nime.org/proceedings/2003/nime2003_180.pdf},
  abstract = {In this paper I present the gluiph, a single-board computer thatwas conceived as a platform for integrated electronic musicalinstruments. It aims to provide new instruments as well asexisting ones with a stronger identity by untethering themfrom the often lab-like stage setups built around general purpose computers. The key additions to its core are a flexiblesensor subsystem and multi-channel audio I/O. In contrast toother stand-alone approaches it retains a higher degree offlexibility by supporting popular music programming languages, with Miller Puckette's pd [1] being the current focus.}
}

@inproceedings{Kessous2003,
  author = {Kessous, Lo\''{\i}c and Arfib, Daniel},
  title = {Bimanuality in Alternate Musical Instruments},
  pages = {140--145},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176523},
  url = {http://www.nime.org/proceedings/2003/nime2003_140.pdf},
  keywords = {Gesture control, mapping, alternate controllers, musical instruments. },
  abstract = {This paper presents a study of bimanual control applied tosound synthesis. This study deals with coordination,cooperation, and abilities of our hands in musical context. Wedescribe examples of instruments made using subtractivesynthesis, scanned synthesis in Max/MSP and commercialstand-alone software synthesizers via MIDI communicationprotocol. These instruments have been designed according to amulti-layer-mapping model, which provides modular design.They have been used in concerts and performanceconsiderations are discussed too.}
}

@inproceedings{Kleinsasser2003,
  author = {Kleinsasser, William},
  title = {Dsp.rack: Laptop-based Modular, Programmable Digital Signal Processing and Mixing for Live Performance},
  pages = {213--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176525},
  url = {http://www.nime.org/proceedings/2003/nime2003_213.pdf},
  keywords = {Digital signal processing, Max/MSP, computer music performance, matrix routing, live performance processing. },
  abstract = {This document describes modular software supporting livesignal processing and sound file playback within theMax/MSP environment. Dsp.rack integrates signalprocessing, memory buffer recording, and pre-recordedmulti-channel file playback using an interconnected,programmable signal flow matrix, and an eight-channel i/oformat.}
}

@inproceedings{Laibowitz2003,
  author = {Laibowitz, Mat},
  title = {BASIS: A Genesis in Musical Interfaces},
  pages = {216--217},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176527},
  url = {http://www.nime.org/proceedings/2003/nime2003_216.pdf},
  keywords = {Performance, Design, Experimentation, DNA, Big Five. },
  abstract = {This paper is a demo proposal for a new musical interfacebased on a DNA-like double-helix and concepts in charactergeneration. It contains a description of the interface,motivations behind developing such an interface, variousmappings of the interface to musical applications, and therequirements to demo the interface.}
}

@inproceedings{Lyons2003,
  author = {Lyons, Michael J. and Haehnel, Michael and Tetsutani, Nobuji},
  title = {Designing, Playing, and Performing with a Vision-based Mouth Interface},
  pages = {116--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176529},
  url = {http://www.nime.org/proceedings/2003/nime2003_116.pdf},
  keywords = {Video-based interface; mouth controller; alternative input devices. },
  abstract = {The role of the face and mouth in speech production as well asnon-verbal communication suggests the use of facial action tocontrol musical sound. Here we document work on theMouthesizer, a system which uses a headworn miniaturecamera and computer vision algorithm to extract shapeparameters from the mouth opening and output these as MIDIcontrol changes. We report our experience with variousgesture-to-sound mappings and musical applications, anddescribe a live performance which used the Mouthesizerinterface.}
}

@inproceedings{Merrill2003,
  author = {Merrill, David},
  title = {Head-Tracking for Gestural and Continuous Control of Parameterized Audio Effects},
  pages = {218--219},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176531},
  url = {http://www.nime.org/proceedings/2003/nime2003_218.pdf},
  keywords = {Head-tracking, gestural control, continuous control, parameterized effects processor. },
  abstract = {This paper describes a system which uses the output fromhead-tracking and gesture recognition software to drive aparameterized guitar effects synthesizer in real-time.}
}

@inproceedings{Modler2003,
  author = {Modler, Paul and Myatt, Tony and Saup, Michael},
  title = {An Experimental Set of Hand Gestures for Expressive Control of Musical Parameters in Realtime},
  pages = {146--150},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176533},
  url = {http://www.nime.org/proceedings/2003/nime2003_146.pdf},
  keywords = {Gesture Recognition, Artificial Neural Network, Expressive Control, Real-time Interaction },
  abstract = {This paper describes the implementation of Time Delay NeuralNetworks (TDNN) to recognize gestures from video images.Video sources are used because they are non-invasive and do notinhibit performer's physical movement or require specialistdevices to be attached to the performer which experience hasshown to be a significant problem that impacts musiciansperformance and can focus musical rehearsals and performancesupon technical rather than musical concerns (Myatt 2003).We describe a set of hand gestures learned by an artificial neuralnetwork to control musical parameters expressively in real time.The set is made up of different types of gestures in order toinvestigate:-aspects of the recognition process-expressive musical control-schemes of parameter mapping-generalization issues for an extended set for musicalcontrolThe learning procedure of the Neural Network is describedwhich is based on variations by affine transformations of imagesequences of the hand gestures.The whole application including the gesture capturing isimplemented in jMax to achieve real time conditions and easyintegration into a musical environment to realize differentmappings and routings of the control stream.The system represents a practice-based research using actualmusic models like compositions and processes of compositionwhich will follow the work described in the paper.}
}

@inproceedings{Momeni2003,
  author = {Momeni, Ali and Wessel, David},
  title = {Characterizing and Controlling Musical Material Intuitively with Geometric Models},
  pages = {54--62},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176535},
  url = {http://www.nime.org/proceedings/2003/nime2003_054.pdf},
  abstract = {In this paper, we examine the use of spatial layouts of musicalmaterial for live performance control. Emphasis is given tosoftware tools that provide for the simple and intuitivegeometric organization of sound material, sound processingparameters, and higher-level musical structures.}
}

@inproceedings{Muth2003,
  author = {Muth, David and Burton, Ed},
  title = {Sodaconductor},
  pages = {222--224},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176537},
  url = {http://www.nime.org/proceedings/2003/nime2003_222.pdf},
  abstract = {Sodaconductor is a musical interface for generating OSCcontrol data based on the dynamic physical simulation toolSodaconstructor as it can be seen and heard onhttp://www.sodaplay.com.}
}

@inproceedings{Nagashima2003,
  author = {Nagashima, Yoichi},
  title = {Bio-Sensing Systems and Bio-Feedback Systems for Interactive Media Arts},
  pages = {48--53},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176539},
  url = {http://www.nime.org/proceedings/2003/nime2003_048.pdf}
}

@inproceedings{Nakra2003,
  author = {Nakra, Teresa M.},
  title = {Immersion Music: a Progress Report},
  pages = {151--152},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176541},
  url = {http://www.nime.org/proceedings/2003/nime2003_151.pdf},
  keywords = {Interactive computer music systems, gestural interaction, Conductor's Jacket, Digital Baton },
  abstract = {This paper describes the artistic projects undertaken at ImmersionMusic, Inc. (www.immersionmusic.org) during its three-yearexistence. We detail work in interactive performance systems,computer-based training systems, and concert production.}
}

@inproceedings{Newton-Dunn2003,
  author = {Newton-Dunn, Henry and Nakano, Hiroaki and Gibson, James},
  title = {Block Jam: A Tangible Interface for Interactive Music},
  pages = {170--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176543},
  url = {http://www.nime.org/proceedings/2003/nime2003_170.pdf},
  keywords = {Tangible interface, modular system, polyrhythmic sequencer. VISION We believe in a future where music will no longer be considered a linear composition, but a dynamic structure, and musical composition will extend to interaction. We also believe that through the },
  abstract = {In this paper, we introduce Block Jam, a Tangible UserInterface that controls a dynamic polyrhythmic sequencerusing 26 physical artifacts. These physical artifacts, that wecall blocks, are a new type of input device for manipulatingan interactive music system. The blocks' functional andtopological statuses are tightly coupled to an ad hocsequencer, interpreting the user's arrangement of the blocksas meaningful musical phrases and structures.We demonstrate that we have created both a tangible andvisual language that enables both the novice and musicallytrained users by taking advantage of both their explorativeand intuitive abilities. The tangible nature of the blocks andthe intuitive interface promotes face-to-face collaborationand social interaction within a single system. The principleof collaboration is further extended by linking two BlockJam systems together to create a network.We discuss our project vision, design rational, relatedworks, and the implementation of Block Jam prototypes.Figure 1. A cluster of blocks, note the mother block on thebottom right}
}

@inproceedings{Nishimoto2003,
  author = {Nishimoto, Kazushi and Oshima, Chika and Miyagawa, Yohei},
  title = {Why Always Versatile? Dynamically Customizable Musical Instruments Facilitate Expressive Performances},
  pages = {164--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176545},
  url = {http://www.nime.org/proceedings/2003/nime2003_164.pdf},
  abstract = {In this paper, we discuss a design principle for the musical instruments that are useful for both novices and professional musicians and that facilitate musically rich expression. We believe that the versatility of conventional musical instruments causes difficulty in performance. By dynamically specializing a musical instrument for performing a specific (genre of) piece, the musical instrument could become more useful for performing the piece and facilitates expressive performance. Based on this idea, we developed two new types of musical instruments, i.e., a "given-melody-based musical instrument" and a "harmonic-function-based musical instrument". From the experimental results using two prototypes, we demonstrate the efficiency of the design principle.}
}

@inproceedings{Orio2003,
  author = {Orio, Nicola and Lemouton, Serge and Schwarz, Diemo},
  title = {Score Following: State of the Art and New Developments},
  pages = {36--41},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176547},
  url = {http://www.nime.org/proceedings/2003/nime2003_036.pdf}
}

@inproceedings{PalacioQuintin2003,
  author = {Palacio-Quintin, Cl\'{e}o},
  title = {The Hyper-Flute},
  pages = {206--207},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176549},
  url = {http://www.nime.org/proceedings/2003/nime2003_206.pdf}
}

@inproceedings{Paradiso2003,
  author = {Paradiso, Joseph A.},
  title = {Dual-Use Technologies for Electronic Music Controllers: A Personal Perspective},
  pages = {228--234},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176551},
  url = {http://www.nime.org/proceedings/2003/nime2003_228.pdf}
}

@inproceedings{Peiper2003,
  author = {Peiper, Chad and Warden, David and Garnett, Guy},
  title = {An Interface for Real-time Classification of Articulations Produced by Violin Bowing},
  pages = {192--196},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176553},
  url = {http://www.nime.org/proceedings/2003/nime2003_192.pdf}
}

@inproceedings{Ryan2003,
  author = {Ryan, Joel and Salter, Christopher L.},
  title = {TGarden: Wearable Instruments and Augmented Physicality},
  pages = {87--90},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176555},
  url = {http://www.nime.org/proceedings/2003/nime2003_087.pdf},
  keywords = {Gesture, interaction, embodied action, enaction, physical model, responsive environment, interactive musical systems, affordance, interface, phenomenology, energy, kinetics, time constant, induced ballistics, wearable computing, accelerometer, audience participation, dynamical system, dynamic compliance, effort, wearable instrument, augmented physicality. },
  abstract = {This report details work on the interdisciplinary mediaproject TGarden. The authors discuss the challengesencountered while developing a responsive musicalenvironment for the general public involving wearable,sensor-integrated clothing as the central interface and input device. The project's dramaturgical andtechnical/implementation background are detailed toprovide a framework for the creation of a responsive hardwareand software system that reinforces a tangible relationshipbetween the participant's improvised movement and musicalresponse. Finally, the authors take into consideration testingscenarios gathered from public prototypes in two Europeanlocales in 2001 to evaluate user experience of the system.}
}

@inproceedings{Scavone2003,
  author = {Scavone, Gary},
  title = {THE PIPE: Explorations with Breath Control},
  pages = {15--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176557},
  url = {http://www.nime.org/proceedings/2003/nime2003_015.pdf},
  keywords = {MIDI Controller, Wind Controller, Breath Control, Human Computer Interaction. }
}

@inproceedings{Settel2003,
  author = {Settel, Zack and Lippe, Cort},
  title = {Convolution Brother's Instrument Design},
  pages = {197--200},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176559},
  url = {http://www.nime.org/proceedings/2003/nime2003_197.pdf}
}

@inproceedings{Shiraiwa2003,
  author = {Shiraiwa, Hiroko and Segnini, Rodrigo and Woo, Vivian},
  title = {Sound Kitchen: Designing a Chemically Controlled Musical Performance},
  pages = {83--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176561},
  url = {http://www.nime.org/proceedings/2003/nime2003_083.pdf},
  keywords = {Chemical music, Applied chemistry, Battery Controller. }
}

@inproceedings{Singer2003,
  author = {Singer, Eric},
  title = {Sonic Banana: A Novel Bend-Sensor-Based MIDI Controller},
  pages = {220--221},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176563},
  url = {http://www.nime.org/proceedings/2003/nime2003_220.pdf}
}

@inproceedings{Singer2003a,
  author = {Singer, Eric and Larke, Kevin and Bianciardi, David},
  title = {LEMUR GuitarBot: MIDI Robotic String Instrument},
  pages = {188--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176565},
  url = {http://www.nime.org/proceedings/2003/nime2003_188.pdf}
}

@inproceedings{Traube2003,
  author = {Traube, Caroline and Depalle, Philippe and Wanderley, Marcelo M.},
  title = {Indirect Acquisition of Instrumental Gesture Based on Signal , Physical and Perceptual Information},
  pages = {42--47},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176567},
  url = {http://www.nime.org/proceedings/2003/nime2003_042.pdf}
}

@inproceedings{Ventura2003,
  author = {Ventura, David and Mase, Kenji},
  title = {Duet Musical Companion: Improvisational Interfaces for Children},
  pages = {91--94},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176569},
  url = {http://www.nime.org/proceedings/2003/nime2003_091.pdf},
  keywords = {Musical improvisation, toy interface agent, sensor doll, context awareness. },
  abstract = {We present a sensor-doll interface as a musical outlet forpersonal expression. A doll serves the dual role of being bothan expressive agent and a playmate by allowing solo andaccompanied performance. An internal computer and sensorsystem allow the doll to receive input from the user and itssurroundings, and then respond accordingly with musicalfeedback. Sets of musical timbres and melodies may bechanged by presenting the doll with a series of themed clothhats, each suggesting a different style of play. The doll mayperform by itself and play a number of melodies, or it maycollaborate with the user when its limbs are squeezed or bent.Shared play is further encouraged by a basic set of aural tonesmimicking conversation.}
}

@inproceedings{Wilson2003,
  author = {Wilson, Scott and Gurevich, Michael and Verplank, Bill and Stang, Pascal},
  title = {Microcontrollers in Music HCI Instruction: Reflections on our Switch to the Atmel AVR Platform},
  pages = {24--29},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176571},
  url = {http://www.nime.org/proceedings/2003/nime2003_024.pdf},
  abstract = {Over the past year the instructors of the Human ComputerInteraction courses at CCRMA have undertaken a technology shift to a much more powerful teaching platform. Wedescribe the technical features of the new Atmel AVR basedplatform, contrasting it with the Parallax BASIC Stampplatform used in the past. The successes and failures ofthe new platform are considered, and some student projectsuccess stories described.}
}

@inproceedings{Wright2003,
  author = {Wright, Matthew and Freed, Adrian and Momeni, Ali},
  title = {OpenSound Control: State of the Art 2003},
  pages = {153--159},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176575},
  url = {http://www.nime.org/proceedings/2003/nime2003_153.pdf}
}

@inproceedings{Young2003,
  author = {Young, Diana and Essl, Georg},
  title = {HyperPuja: A Tibetan Singing Bowl Controller},
  pages = {9--14},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176577},
  url = {http://www.nime.org/proceedings/2003/nime2003_009.pdf}
}

@inproceedings{Young2003a,
  author = {Young, Diana and Serafin, Stefania},
  title = {Playability Evaluation of a Virtual Bowed String Instrument},
  pages = {104--108},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2003},
  date = {22-24 May, 2003},
  address = {Montreal, Canada},
  doi = {10.5281/zenodo.1176579},
  url = {http://www.nime.org/proceedings/2003/nime2003_104.pdf}
}

