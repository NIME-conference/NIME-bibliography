@inproceedings{Nakanishi2013,
  author = {Yoshihito Nakanishi and Seiichiro Matsumura and Chuichi Arakawa},
  title = {{POWDER} {BOX}: An Interactive Device with Sensor Based Replaceable Interface For Musical Session},
  pages = {373--376},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178620},
  url = {http://www.nime.org/proceedings/2013/nime2013_101.pdf},
  keywords = {Musical instrument, synthesizer, replaceable interface, sensors},
  abstract = {In this paper, the authors introduce an interactive device, ``POWDER BOX''for use by novices in musical sessions. ``POWDER BOX'' is equipped withsensor-based replaceable interfaces, which enable participants to discover andselect their favorite playing styles of musical instruments during a musicalsession. In addition, it has a wireless communication function thatsynchronizes musical scale and BPM between multiple devices. To date, various kinds of ``inventive'' electronic musical instruments havebeen created in the field of Computer Music field. The authors are interestedin formations of musical sessions, aiming for a balance between simpleinteraction and musical expression. This study focuses on the development ofperformance playing styles.Musicians occasionally change their playing styles (e.g., guitar pluckingstyle) during a musical session. Generally, it is difficult for nonmusicians toachieve this kind of smooth changing depends on levels of their skillacquisition. However, it is essentially important for enjoying musical sessionswhether people could acquire these skills. Here, the authors attempted to develop the device that supports nonmusicians toconquer this point using replaceable interfaces. The authors expected thatchanging interfaces would bring similar effect as changing playing style by theskillful player. This research aims to establish an environment in whichnonmusicians and musicians share their individual musical ideas easily. Here,the interaction design and configuration of the ``POWDER BOX'' is presented.}
}

@inproceedings{Fohl2013,
  author = {Wolfgang Fohl and Malte Nogalski},
  title = {A Gesture Control Interface for a Wave Field Synthesis System},
  pages = {341--346},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178522},
  url = {http://www.nime.org/proceedings/2013/nime2013_106.pdf},
  keywords = {Wave field synthesis, gesture control},
  abstract = {This paper presents the design and implementation of agesture control interface for a wave field synthesis system.The user's motion is tracked by a IR-camera-based trackingsystem. The developed connecting software processes thetracker data to modify the positions of the virtual soundsources of the wave field synthesis system. Due to the mod-ular design of the software, the triggered actions of the ges-tures may easily be modified. Three elementary gestureswere designed and implemented: Select / deselect, circularmovement and radial movement. The guidelines for gesturedesign and detection are presented, and the user experiencesare discussed.}
}

@inproceedings{Burlet2013,
  author = {Gregory Burlet and Ichiro Fujinaga},
  title = {Stompboxes: Kicking the Habit},
  pages = {41--44},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178488},
  url = {http://www.nime.org/proceedings/2013/nime2013_109.pdf},
  keywords = {Augmented instrument, gesture recognition, accelerometer, pattern recognition, performance practice},
  abstract = {Sensor-based gesture recognition is investigated as a possible solution to theproblem of managing an overwhelming number of audio effects in live guitarperformances. A realtime gesture recognition system, which automaticallytoggles digital audio effects according to gestural information captured by anaccelerometer attached to the body of a guitar, is presented. To supplement theseveral predefined gestures provided by the recognition system, personalizedgestures may be trained by the user. Upon successful recognition of a gesture,the corresponding audio effects are applied to the guitar signal and visualfeedback is provided to the user. An evaluation of the system yielded 86%accuracy for user-independent recognition and 99% accuracy for user-dependentrecognition, on average.}
}

@inproceedings{Jensenius2013,
  author = {Alexander Refsum Jensenius},
  title = {Kinectofon: Performing with Shapes in Planes},
  pages = {196--197},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178564},
  url = {http://www.nime.org/proceedings/2013/nime2013_110.pdf},
  keywords = {Kinect, motiongram, sonification, video analysis},
  abstract = {The paper presents the Kinectofon, an instrument for creating sounds through free-hand interaction in a 3D space. The instrument is based on the RGB anddepth image streams retrieved from a Microsoft Kinect sensor device. These twoimage streams are used to create different types of motiongrams, which, again, are used as the source material for a sonification process based on inverse FFT. The instrument is intuitive to play, allowing the performer to createsound by "touching" a virtual sound wall.}
}

@inproceedings{Fried2013,
  author = {Ohad Fried and Rebecca Fiebrink},
  title = {Cross-modal Sound Mapping Using Deep Learning},
  pages = {531--534},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178528},
  url = {http://www.nime.org/proceedings/2013/nime2013_111.pdf},
  keywords = {Deep learning, feature learning, mapping, gestural control},
  abstract = {We present a method for automatic feature extraction and cross-modal mappingusing deep learning. Our system uses stacked autoencoders to learn a layeredfeature representation of the data. Feature vectors from two (or more)different domains are mapped to each other, effectively creating a cross-modalmapping. Our system can either run fully unsupervised, or it can use high-levellabeling to fine-tune the mapping according a user's needs. We show severalapplications for our method, mapping sound to or from images or gestures. Weevaluate system performance both in standalone inference tasks and incross-modal mappings.}
}

@inproceedings{Kapur2013,
  author = {Ajay Kapur and Dae Hong Kim and Raakhi Kapur and Kisoon Eom},
  title = {New Interfaces for Traditional Korean Music and Dance},
  pages = {45--48},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178576},
  url = {http://www.nime.org/proceedings/2013/nime2013_113.pdf},
  keywords = {Hyperinstrument, Korean interface design, wearable sensors, dance controllers, bowed controllers, drum controllers},
  abstract = {This paper describes the creation of new interfaces that extend traditionalKorean music and dance. Specifically, this research resulted in the design ofthe eHaegum (Korean bowed instrument), eJanggu (Korean drum), and ZiOm wearableinterfaces. The paper describes the process of making these new interfaces aswell as how they have been used to create new music and forms of digital artmaking that blend traditional practice with modern techniques.}
}

@inproceedings{Zhang2013,
  author = {Edward Zhang},
  title = {KIB: Simplifying Gestural Instrument Creation Using Widgets},
  pages = {519--524},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178698},
  url = {http://www.nime.org/proceedings/2013/nime2013_114.pdf},
  keywords = {Kinect, gesture, widgets, OSC, mapping},
  abstract = {The Microsoft Kinect is a popular and versatile input devicefor musical interfaces. However, using the Kinect for suchinterfaces requires not only signi_x000C_cant programming experience,but also the use of complex geometry or machinelearning techniques to translate joint positions into higherlevel gestures. We created the Kinect Instrument Builder(KIB) to address these di_x000E_culties by structuring gesturalinterfaces as combinations of gestural widgets. KIB allowsthe user to design an instrument by con_x000C_guring gesturalprimitives, each with a set of simple but attractive visualfeedback elements. After designing an instrument on KIB'sweb interface, users can play the instrument on KIB's performanceinterface, which displays visualizations and transmitsOSC messages to other applications for sound synthesisor further remapping.}
}

@inproceedings{Hochenbaum2013,
  author = {Jordan Hochenbaum and Ajay Kapur},
  title = {Toward The Future Practice Room: Empowering Musical Pedagogy through Hyperinstruments},
  pages = {307--312},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178552},
  url = {http://www.nime.org/proceedings/2013/nime2013_116.pdf},
  keywords = {Hyperinstruments, Pedagogy, Metrics, Ezither, Practice Room},
  abstract = {Music education is a rich subject with many approaches and methodologies thathave developed over hundreds of years. More than ever, technology playsimportant roles at many levels of a musician's practice. This paper begins toexplore some of the ways in which technology developed out of the NIMEcommunity (specifically hyperinstruments), can inform a musician's dailypractice, through short and long term metrics tracking and data visualization.}
}

@inproceedings{Michon2013,
  author = {Romain Michon and Myles Borins and David Meisenholder},
  title = {The Black Box},
  pages = {464--465},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178612},
  url = {http://www.nime.org/proceedings/2013/nime2013_117.pdf},
  keywords = {Satellite CCRMA, Beagleboard, PureData, Faust, Embedded-Linux, Open Sound Control},
  abstract = {Black Box is a site based installation that allows users to create uniquesounds through physical interaction. The installation consists of a geodesicdome, surround sound speakers, and a custom instrument suspended from the apexof thedome. Audience members entering the space are able to create sound by strikingor rubbing the cube, and are able to control a delay system by moving the cubewithin the space.}
}

@inproceedings{Choi2013,
  author = {Hongchan Choi and Jonathan Berger},
  title = {WAAX: Web Audio {API} eXtension},
  pages = {499--502},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178494},
  url = {http://www.nime.org/proceedings/2013/nime2013_119.pdf},
  keywords = {Web Audio API, Chrome, JavaScript, web-based music system, collaborative music making, audience participation},
  abstract = {The advent of Web Audio API in 2011 marked a significant advance for web-basedmusic systems by enabling real-time sound synthesis on web browsers simply bywriting JavaScript code. While this powerful functionality has arrived there isa yet unaddressed need for an extension to the API to fully reveal itspotential. To meet this need, a JavaScript library dubbed WAAX was created tofacilitate music and audio programming based on Web Audio API bypassingunderlying tasks and augmenting useful features. In this paper, we describecommon issues in web audio programming, illustrate how WAAX can speed up thedevelopment, and discuss future developments.}
}

@inproceedings{Hamano2013,
  author = {Takayuki Hamano and Tomasz Rutkowski and Hiroko Terasawa and Kazuo Okanoya and Kiyoshi Furukawa},
  title = {Generating an Integrated Musical Expression with a Brain--Computer Interface},
  pages = {49--54},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178542},
  url = {http://www.nime.org/proceedings/2013/nime2013_120.pdf},
  keywords = {Brain-computer interface (BCI), qualitative and quantitative information, classification, sonification},
  abstract = {Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.}
}

@inproceedings{Martin2013,
  author = {Charles Martin},
  title = {Performing with a Mobile Computer System for Vibraphone},
  pages = {377--380},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178602},
  url = {http://www.nime.org/proceedings/2013/nime2013_121.pdf},
  keywords = {percussion, mobile computer music, Apple iOS, collaborative performance practice, ethnography, artistic research},
  abstract = {This paper describes the development of an Apple iPhone based mobile computersystem for vibraphone and its use in a series of the author's performanceprojects in 2011 and 2012.This artistic research was motivated by a desire to develop an alternative tolaptop computers for the author's existing percussion and computer performancepractice. The aims were to develop a light, compact and flexible system usingmobile devices that would allow computer music to infiltrate solo and ensembleperformance situations where it is difficult to use a laptop computer.The project began with a system that brought computer elements to NordligVinter, a suite of percussion duos, using an iPhone, RjDj, Pure Data and ahome-made pickup system. This process was documented with video recordings andanalysed using ethnographic methods.The mobile computer music setup proved to be elegant and convenient inperformance situations with very little time and space to set up, as well as inperformance classes and workshops. The simple mobile system encouragedexperimentation and the platforms used enabled sharing with a wider audience.}
}

@inproceedings{McLean2013,
  author = {Alex McLean and EunJoo Shin and Kia Ng},
  title = {Paralinguistic Microphone},
  pages = {381--384},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178608},
  url = {http://www.nime.org/proceedings/2013/nime2013_122.pdf},
  keywords = {face tracking, computer vision, installation, microphone},
  abstract = {The Human vocal tract is considered for its sonorous qualities incarrying prosodic information, which implicates vision in theperceptual processes of speech. These considerations are put in thecontext of previous work in NIME, forming background for theintroduction of two sound installations; ``Microphone'', which uses acamera and computer vision to translate mouth shapes to sounds, and``Microphone II'', a work-in-progress, which adds physical modellingsynthesis as a sound source, and visualisation of mouth movements.}
}

@inproceedings{Bisig2013,
  author = {Daniel Bisig and S{\'e}bastien Schiesser},
  title = {Coral -- a Physical and Haptic Extension of a Swarm Simulation},
  pages = {385--388},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178482},
  url = {http://www.nime.org/proceedings/2013/nime2013_126.pdf},
  keywords = {haptic interface, swarm simulation, generative art},
  abstract = {This paper presents a proof of concept implementation of an interface entitledCoral. The interface serves as a physical and haptic extension of a simulatedcomplex system, which will be employed as an intermediate mechanism for thecreation of generative music and imagery. The paper discusses the motivationand conceptual context that underly the implementation, describes its technicalrealisation and presents some first interaction experiments. The paper focuseson the following two aspects: the interrelation between the physical andvirtual behaviours and properties of the interface and simulation, and thecapability of the interface to enable an intuitive and tangible exploration ofthis hybrid dynamical system.}
}

@inproceedings{Schacher2013,
  author = {Jan C. Schacher},
  title = {Hybrid Musicianship --- Teaching Gestural Interaction with Traditional and Digital Instruments},
  pages = {55--60},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178656},
  url = {http://www.nime.org/proceedings/2013/nime2013_127.pdf},
  keywords = {gestural interaction, digital musical instruments, pedagogy, mapping, enactive approach},
  abstract = {This article documents a class that teaches gestural interaction and juxtaposestraditional instrumental skills with digital musical instrument concepts. Inorder to show the principles and reflections that informed the choices made indeveloping this syllabus, fundamental elements of an instrument-bodyrelationship and the perceptual import of sensori-motor integration areinvestigated. The methods used to let participants learn in practicalexperimental settings are discussed, showing a way to conceptualise andexperience the entire workflow from instrumental sound to electronictransformations by blending gestural interaction with digital musicalinstrument techniques and traditional instrumental playing skills. Thetechnical interfaces and software that were deployed are explained, focussingof the interactive potential offered by each solution. In an attempt tosummarise and evaluate the impact of this course, a number of insights relatingto this specific pedagogical situation are put forward. Finally, concreteexamples of interactive situations that were developed by the participants areshown in order to demonstrate the validity of this approach.}
}

@inproceedings{Jackie2013,
  author = {Jackie and Yi Tang Chui and Mubarak Marafa and Samson and Ka Fai Young},
  title = {SoloTouch: A Capacitive Touch Controller with Lick-based Note Selector},
  pages = {389--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178560},
  url = {http://www.nime.org/proceedings/2013/nime2013_130.pdf},
  keywords = {Capacitive touch controller, automated note selector, virtual instrument MIDI controller, novice musicians.},
  abstract = {SoloTouch is a guitar inspired pocket sized controller system that consists ofa capacitive touch trigger and a lick-based note selector. The touch triggerallows an intuitive way to play both velocity sensitive notes and vibratoexpressively using only one finger. The lick-based note selector is an originalconcept that provides the player an easy way to play expressive melodic linesby combining pre-programmed ``licks'' without the need to learn the actualnotes. The two-part controller is primarily used as a basic MIDI controller forplaying MIDI controlled virtual instruments, normally played by keyboardcontrollers. The controller is targeted towards novice musicians, playerswithout prior musical training could play musical and expressive solos,suitable for improvised jamming along modern popular music.}
}

@inproceedings{Mital2013,
  author = {Parag Kumar Mital and Mick Grierson},
  title = {Mining Unlabeled Electronic Music Databases through {3D} Interactive Visualization of Latent Component Relationships},
  pages = {227--232},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178614},
  url = {http://www.nime.org/proceedings/2013/nime2013_132.pdf},
  keywords = {mir, plca, mfcc, 3d browser, daphne oram, content-based information retrieval, interactive visualization},
  abstract = {We present an interactive content-based MIR environment specifically designedto aid in the exploration of databases of experimental electronic music,particularly in cases where little or no metadata exist. In recent years,several rare archives of early experimental electronic music have becomeavailable. The Daphne Oram Collection contains one such archive, consisting ofapproximately 120 hours of 1/4 inch tape recordings and representing a perioddating from circa 1957. This collection is recognized as an importantmusicological resource, representing aspects of the evolution of electronicmusic practices, including early tape editing methods, experimental synthesistechniques and composition. However, it is extremely challenging to derivemeaningful information from this dataset, primarily for three reasons. First,the dataset is very large. Second, there is limited metadata --- some titles,track lists, and occasional handwritten notes exist, but where this is true,the reliability of the annotations are unknown. Finally, and mostsignificantly, as this is a collection of early experimental electronic music,the sonic characteristics of the material are often not consistent withtraditional musical information. In other words, there is no score, no knowninstrumentation, and often no recognizable acoustic source. We present amethod for the construction of a frequency component dictionary derived fromthe collection via Probabilistic Latent Component Analysis (PLCA), anddemonstrate how an interactive 3D visualization of the relationships betweenthe PLCA-derived dictionary and the archive is facilitating researcher'sunderstanding of the data.}
}

@inproceedings{Hong2013,
  author = {Dae Ryong Hong and Woon Seung Yeo},
  title = {Laptap: Laptop Computer as a Musical Instrument using Audio Feedback},
  pages = {233--236},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178554},
  url = {http://www.nime.org/proceedings/2013/nime2013_137.pdf},
  keywords = {Laptop music, laptop computer, audio feedback, hand gesture, gestural control, musical mapping, audio visualization, musical notation},
  abstract = {Laptap is a laptop-based, real-time sound synthesis/control system for musicand multimedia performance. The system produces unique sounds by positive audiofeedback between the on-board microphone and the speaker of a laptop com-puter. Users can make a variety of sounds by touching the laptop computer inseveral different ways, and control their timbre with the gestures of the otherhand above the mi,
crophone and the speaker to manipulate the characteristicsof the acoustic feedback path. We introduce the basic con,
cept of this audiofeedback system, describe its features for sound generation and manipulation,and discuss the result of an experimental performance. Finally we suggest somerelevant research topics that might follow in the future.}
}

@inproceedings{Bragg2013,
  author = {Danielle Bragg},
  title = {Synchronous Data Flow Modeling for {DMI}s},
  pages = {237--242},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178486},
  url = {http://www.nime.org/proceedings/2013/nime2013_139.pdf},
  keywords = {DMI design, data flow, mapping function},
  abstract = {This paper presents a graph-theoretic model that supports the design andanalysis of data flow within digital musical instruments (DMIs). The state ofthe art in DMI design fails to provide any standards for the scheduling ofcomputations within a DMI's data flow. It does not provide a theoreticalframework within which we can analyze different scheduling protocols and theirimpact on the DMI's performance. Indeed, the mapping between the DMI's sensoryinputs and sonic outputs is classically treated as a black box. DMI designersand builders are forced to design and schedule the flow of data through thisblack box on their own. Improper design of the data flow can produceundesirable results, ranging from overflowing buffers that cause system crashesto misaligned sensory data that result in strange or disordered sonic events.In this paper, we attempt to remedy this problem by providing a framework forthe design and analysis of the DMI data flow. We also provide a schedulingalgorithm built upon that framework that guarantees desirable properties forthe resulting DMI.}
}

@inproceedings{Feugere2013,
  author = {Lionel Feug{\`e}re and Christophe d'Alessandro},
  title = {Digitartic: bi-manual gestural control of articulation in performative singing synthesis},
  pages = {331--336},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178520},
  url = {http://www.nime.org/proceedings/2013/nime2013_143.pdf},
  keywords = {singing voice synthesis, gestural control, syllabic synthesis, articulation, formants synthesis},
  abstract = {Digitartic, a system for bi-manual gestural control of Vowel-Consonant-Vowelperformative singing synthesis is presented. This system is an extension of areal-time gesture-controlled vowel singing instrument developed in the Max/MSPlanguage. In addition to pitch, vowels and voice strength control, Digitarticis designed for gestural control of articulation parameters for a wide set onconsonant, including various places and manners of articulation. The phases ofarticulation between two phonemes are continuously controlled and can bedriven in real time without noticeable delay, at any stage of the syntheticphoneme production. Thus, as in natural singing, very accurate rhythmicpatterns are produced and adapted while playing with other musicians. Theinstrument features two (augmented) pen tablets for controlling voiceproduction: one is dealing with the glottal source and vowels, the second oneis dealing with consonant/vowel articulation. The results show very naturalconsonant and vowel synthesis. Virtual choral practice confirms theeffectiveness of Digitartic as an expressive musical instrument.}
}

@inproceedings{Schacher2013a,
  author = {Jan C. Schacher},
  title = {The Quarterstaff, a Gestural Sensor Instrument},
  pages = {535--540},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178658},
  url = {http://www.nime.org/proceedings/2013/nime2013_144.pdf},
  keywords = {Gestural sensor interface, instrument design, body-object relation, composition and performance practice, dimension space analysis},
  abstract = {This article describes the motivations and reflections that led to thedevelopment of a gestural sensor instrument called the Quarterstaff. In aniterative design and fabrication process, several versions of this interfacewere build, tested and evaluated in performances. A detailed explanation of thedesign choices concerning the shape but also the sensing capabilities of theinstrument illustrates the emphasis on establishing an `enactive'instrumental relationship. A musical practice for this type of instrument isshown by discussing the methods used in the exploration of the gesturalpotential of the interface and the strategies deployed for the development ofmappings and compositions. Finally, to gain more information about how thisinstrument compares with similar designs, two dimension-space analyses are madethat show a clear positioning in relation to instruments that precede theQuarterstaff.}
}

@inproceedings{Altavilla2013,
  author = {Alessandro Altavilla and Baptiste Caramiaux and Atau Tanaka},
  title = {Towards Gestural Sonic Affordances},
  pages = {61--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178463},
  url = {http://www.nime.org/proceedings/2013/nime2013_145.pdf},
  keywords = {Gestural embodiment of sound, Affordances, Mapping},
  abstract = {We present a study that explores the affordance evoked by sound andsound-gesture mappings. In order to do this, we make use of a sensor systemwith minimal form factor in a user study that minimizes cultural associationThe present study focuses on understanding how participants describe sounds andgestures produced while playing designed sonic interaction mappings. Thisapproach seeks to move from object-centric affordance towards investigatingembodied gestural sonic affordances.}
}

@inproceedings{Cerqueira2013,
  author = {Mark Cerqueira and Spencer Salazar and Ge Wang},
  title = {SoundCraft: Transducing StarCraft 2},
  pages = {243--247},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178492},
  url = {http://www.nime.org/proceedings/2013/nime2013_146.pdf},
  keywords = {interactive sonification, interactive game music, StarCraft 2},
  abstract = {SoundCraft is a framework that enables real-time data gathering from aStarCraft 2 game to external software applications, allowing for musicalinterpretation of the game's internal structure and strategies in novel ways.While players battle each other for victory within the game world, a customStarCraft 2 map collects and writes out data about players' decision-making,performance, and current focus on the map. This data is parsed and transmittedover Open Sound Control (OSC) in real-time, becoming the source for thesoundscape that accompanies the player's game. Using SoundCraft, we havecomposed a musical work for two em StarCraft 2 players, entitled GG Music. Thispaper details the technical and aesthetic development of SoundCraft, includingdata collection and sonic mapping. Please see the attached video file for a performance of GG Music using theSoundCraft framework.}
}

@inproceedings{Fan2013,
  author = {Xin Fan and Georg Essl},
  title = {Air Violin: A Body-centric Style Musical Instrument},
  pages = {122--123},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178512},
  url = {http://www.nime.org/proceedings/2013/nime2013_149.pdf},
  keywords = {NIME, musical instrument, interaction, gesture, Kinect},
  abstract = {We show how body-centric sensing can be integrated in musical interface toenable more flexible gestural control. We present a barehanded body-centricinteraction paradigm where users are able to interact in a spontaneous waythroughperforming gestures. The paradigm employs a wearable camera and see-throughdisplay to enable flexible interaction in the 3D space. We designed andimplemented a prototype called Air Violin, a virtual musical instrument usingdepth camera, to demonstrate the proposed interaction paradigm. We describedthe design and implementation details.}
}

@inproceedings{Wang2013,
  author = {Johnty Wang and Nicolas d'Alessandro and Aura Pon and Sidney Fels},
  title = {PENny: An Extremely Low-Cost Pressure-Sensitive Stylus for Existing Capacitive Touchscreens},
  pages = {input interfaces, touch screens, tablets, pressure-sensitive, low-cost},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178686},
  url = {http://www.nime.org/proceedings/2013/nime2013_150.pdf},
  keywords = {10 in materials, about1/10th the cost of existing solutions. The stylus makes use of the built inaudio interface that is available on most smartphones and tablets on the markettoday. Limitations of the device include the physical constraint of wires, theoccupation of one audio input and output channel, and increased latency equalto the period of at least one audio buffer duration. The stylus has beendemonstrated in two cases thus far: a visual musical score drawing and asinging synthesis application.},
  abstract = {By building a wired passive stylus we have added pressure sensitivity toexisting capacitive touch screen devices for less than }
}

@inproceedings{Johnston2013,
  author = {Andrew Johnston},
  title = {Fluid Simulation as Full Body Audio-Visual Instrument},
  pages = {132--135},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178572},
  url = {http://www.nime.org/proceedings/2013/nime2013_151.pdf},
  keywords = {performance, dance, fluid simulation, composition},
  abstract = {This paper describes an audio-visual performance system based on real-timefluid simulation. The aim is to provide a rich environment for works whichblur the boundaries between dance and instrumental performance -- and sound andvisuals -- while maintaining transparency for audiences and new performers. The system uses infra-red motion tracking to allow performers to manipulate areal-time fluid simulation, which in turn provides control data forcomputer-generated audio and visuals. It also provides a control andconfiguration system which allows the behaviour of the interactive system to bechanged over time, enabling the structure within which interactions take placeto be `composed'.}
}

@inproceedings{Fan2013a,
  author = {Yuan-Yi Fan and Myles Sciotto},
  title = {BioSync: An Informed Participatory Interface for Audience Dynamics and Audiovisual Content Co-creation using Mobile PPG and {EEG}},
  pages = {248--251},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178514},
  url = {http://www.nime.org/proceedings/2013/nime2013_152.pdf},
  keywords = {Mobile, Biometrics, Synchronous Interaction, Social, Audience, Experience},
  abstract = {The BioSync interface presented in this paper merges the heart-rate basedparadigm with the brain-wave based paradigm into one mobile unit which isscalable for large audience real-time applications. The goal of BioSync is toprovide a hybrid interface, which uses audience biometric responses foraudience participation techniques. To provide an affordable and scalablesolution, BioSync collects the user's heart rate via mobile phone pulseoximetry and the EEG data via Bluetooth communication with the off-the-shelfMindWave Mobile hardware. Various interfaces have been designed and implementedin the development of audience participation techniques and systems. In thedesign and concept of BioSync, we first summarize recent interface research foraudience participation within the NIME-related context, followed by the outlineof the BioSync methodology and interface design. We then present a techniquefor dynamic tempo control based on the audience biometric responses and anearly prototype of a mobile dual-channel pulse oximetry and EEG bi-directionalinterface for iOS device (BioSync). Finally, we present discussions and ideasfor future applications, as well as plans for a series of experiments, whichinvestigate if temporal parameters of an audience's physiological metricsencourage crowd synchronization during a live event or performance, acharacteristic, which we see as having great potential in the creation offuture live musical and audiovisual performance applications.}
}

@inproceedings{Yang2013,
  author = {Qi Yang and Georg Essl},
  title = {Visual Associations in Augmented Keyboard Performance},
  pages = {252--255},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178694},
  url = {http://www.nime.org/proceedings/2013/nime2013_156.pdf},
  keywords = {Visual feedback, interaction, NIME, musical instrument, interaction, augmented keyboard, gesture, Kinect},
  abstract = {What is the function of visuals in the design of an augmented keyboardperformance device with projection? We address this question by thinkingthrough the impact of choices made in three examples on notions of locus ofattention, visual anticipation and causal gestalt to articulate a space ofdesign choices. Visuals can emphasize and deemphasize aspects of performanceand help clarify the role input has to the performance. We suggest that thisprocess might help thinking through visual feedback design in NIMEs withrespect to the performer or the audience.}
}

@inproceedings{Thorogood2013,
  author = {Miles Thorogood and Philippe Pasquier},
  title = {Impress: A Machine Learning Approach to Soundscape Affect Classification for a Music Performance Environment},
  pages = {256--260},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178674},
  url = {http://www.nime.org/proceedings/2013/nime2013_157.pdf},
  keywords = {soundscape, performance, machine learning, audio features, affect grid},
  abstract = {Soundscape composition in improvisation and performance contexts involves manyprocesses that can become overwhelming for a performer, impacting on thequality of the composition. One important task is evaluating the mood of acomposition for evoking accurate associations and memories of a soundscape. Anew system that uses supervised machine learning is presented for theacquisition and realtime feedback of soundscape affect. A model of sound-scape mood is created by users entering evaluations of audio environmentsusing a mobile device. The same device then provides feedback to the user ofthe predicted mood of other audio environments. We used a features vector ofTotal Loudness and MFCC extracted from an audio signal to build a multipleregression models. The evaluation of the system shows the tool is effective inpredicting soundscape affect.}
}

@inproceedings{Park2013a,
  author = {Gibeom Park and Kyogu Lee},
  title = {Sound Spray --- can-shaped sound effect device},
  pages = {65--68},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178634},
  url = {http://www.nime.org/proceedings/2013/nime2013_158.pdf},
  keywords = {Sound effect device, Spray paint art, Arduino, Pure Data},
  abstract = {In this paper, we designed a sound effect device, which was applicable forspray paint art process. For the applicability research of the device, wedesigned a prototype which had a form not far off the traditional spray cans,using Arduino and various sensors. Through the test process of the prototype,we verified the elements that would be necessary to apply our newly designeddevice to real spray paint art activities. Thus we checked the possibility ofvarious musical expressions by expanding the functions of the designed device.}
}

@inproceedings{Tobise2013,
  author = {Hayami Tobise and Yoshinari Takegawa and Tsutomu Terada and Masahiko Tsukamoto},
  title = {Construction of a System for Recognizing Touch of Strings for Guitar},
  pages = {261--266},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178676},
  url = {http://www.nime.org/proceedings/2013/nime2013_159.pdf},
  keywords = {Guitar, Touched strings, Fingering recognition},
  abstract = {In guitar performance, fingering is an important factor, and complicated. In particular, the fingering of the left hand comprises various relationshipsbetween the finger and the string, such as a finger touching the strings, afinger pressing the strings, and a finger releasing the strings. The recognition of the precise fingering of the left hand is applied to aself-learning support system, which is able to detect strings being muted by afinger, and which transcribes music automatically, including the details offingering techniques. Therefore, the goal of our study is the construction of a system forrecognizing the touch of strings for the guitar. We propose a method for recognizing the touch of strings based on theconductive characteristics of strings and frets. We develop a prototype system, and evaluate its effectiveness.Furthermore, we propose an application which utilizes our system.}
}

@inproceedings{Tokunaga2013,
  author = {Tomohiro Tokunaga and Michael J. Lyons},
  title = {Enactive Mandala: Audio-visualizing Brain Waves},
  pages = {118--119},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178678},
  url = {http://www.nime.org/proceedings/2013/nime2013_16.pdf},
  keywords = {Brain-computer Interfaces, BCI, EEG, Sonification, Visualization, Artificial Expressions, NIME, Visual Music},
  abstract = {We are exploring the design and implementation of artificial expressions,kinetic audio-visual representations of real-time physiological data whichreflect emotional and cognitive state. In this work we demonstrate a prototype,the Enactive Mandala, which maps real-time EEG signals to modulate ambientmusic and animated visual music. The design draws inspiration from the visualmusic of the Whitney brothers as well as traditional meditative practices.Transparent real-time audio-visual feedback ofbrainwave qualities supports intuitive insight into the connection betweenthoughts and physiological states. Our method is constructive: by linkingphysiology with an dynamic a/v display, and embedding the human-machine systemin the social contexts that arise in real-time play, we hope to seed new, andas yet unknown forms, of non-verbal communication, or ``artificialexpressions''.}
}

@inproceedings{Dobda2013,
  author = {Russell Eric Dobda},
  title = {Applied and Proposed Installations with Silent Disco Headphones for Multi-Elemental Creative Expression},
  pages = {69--72},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178502},
  url = {http://www.nime.org/proceedings/2013/nime2013_161.pdf},
  keywords = {wireless headphones, music production, silent disco, headphone concert, binaural beats, multi-track audio, active music listening, sound healing, mobile clubbing, smart-phone apps},
  abstract = {Breaking musical and creative expression into elements, layers, and formulas, we explore how live listeners create unique sonic experiences from a palette of these elements and their interactions. Bringing us to present-day creative applications, a social and historical overview of silent disco is presented. The advantages of this active listening interface are outlined by the author's expressions requiring discrete elements, such as binaural beats, 3D audio effects, and multiple live music acts in the same space. Events and prototypes as well as hardware and software proposals for live multi-listener manipulation of multielemental sound and music are presented. Examples in audio production, sound healing, music composition, tempo phasing, and spatial audio illustrate the applications.}
}

@inproceedings{Christopher2013,
  author = {Kameron Christopher and Jingyin He and Raakhi Kapur and Ajay Kapur},
  title = {Kontrol: Hand Gesture Recognition for Music and Dance Interaction},
  pages = {267--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178496},
  url = {http://www.nime.org/proceedings/2013/nime2013_164.pdf},
  keywords = {Hand controller, computational ethnomusicology, dance interface, conducting interface, Wekinator, wearable sensors},
  abstract = {This paper describes Kontrol, a new hand interface that extends the intuitivecontrol of electronic music to traditional instrumentalist and dancers. Thegoal of the authors has been to provide users with a device that is capable ofdetecting the highly intricate and expressive gestures of the master performer,in order for that information to be interpreted and used for control ofelectronic music. This paper discusses related devices, the architecture ofKontrol, it's potential as a gesture recognition device, and severalperformance applications.}
}

@inproceedings{Han2013a,
  author = {Yoon Chung Han and Byeong-jun Han and Matthew Wright},
  title = {Digiti Sonus: Advanced Interactive Fingerprint Sonification Using Visual Feature Analysis},
  pages = {136--141},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178548},
  url = {http://www.nime.org/proceedings/2013/nime2013_170.pdf},
  keywords = {Fingerprint, Fingerprint sonification, interactive sonification, sound synthesis, biometric data},
  abstract = {This paper presents a framework that transforms fingerprint patterns intoaudio. We describe Digiti Sonus, an interactive installation performingfingerprint sonification and visualization, including novel techniques forrepresenting user-intended fingerprint expression as audio parameters. In orderto enable personalized sonification and broaden timbre of sound, theinstallation employs sound synthesis based on various visual feature analysissuch as minutiae extraction, area, angle, and push pressure of fingerprints.The sonification results are discussed and the diverse timbres of soundretrieved from different fingerprints are compared.}
}

@inproceedings{Perrotin2013,
  author = {Olivier Perrotin and Christophe d'Alessandro},
  title = {Adaptive mapping for improved pitch accuracy on touch user interfaces},
  pages = {186--189},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178640},
  url = {http://www.nime.org/proceedings/2013/nime2013_178.pdf},
  keywords = {Sound synthesis control, touch user interfaces, pen tablet, automatic correction, accuracy, precision},
  abstract = {Touch user interfaces such as touchpad or pen tablet are often used forcontinuous pitch control in synthesis devices. Usually, pitch is set at thecontact point on the interface, thus introducing possible pitch inaccuracies atthe note onset. This paper proposes a new algorithm, based on an adaptiveattraction mapping, for improving initial pitch accuracy with touch userinterfaces with continuous control. At each new contact on the interface, thealgorithm adjusts the mapping to produce the most likely targeted note of thescale in the vicinity of the contact point. Then, pitch remains continuouslyadjustable as long as the contact is maintained, allowing for vibrato,portamento and other subtle melodic control. The results of experimentscomparing the users' pitch accuracy with and without the help of the algorithmshow that such a correction enables to play sharply in tune at the contact withthe interface, regardless the musical background of the player. Therefore, thedynamic mapping algorithm allows for a clean and accurate attack when playing touch user interfaces for controlling continuous pitch instruments like voicesynthesizers.}
}

@inproceedings{Kikukawa2013,
  author = {Fumitaka Kikukawa and Sojiro Ishihara and Masato Soga and Hirokazu Taki},
  title = {Development of A Learning Environment for Playing Erhu by Diagnosis and Advice regarding Finger Position on Strings},
  pages = {271--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178580},
  url = {http://www.nime.org/proceedings/2013/nime2013_181.pdf},
  keywords = {Magnetic Position Sensors, String Instruments, Skill, Learning Environment, Finger Position},
  abstract = {So far, there are few studies of string instruments with bows because there aremany parameters to acquire skills and it is difficult to measure theseparameters. Therefore, the aim of this paper is to propose a design of alearning environment for a novice learner to acquire an accurate fingerposition skill. For achieving the aim, we developed a learning environmentwhich can diagnose learner's finger position and give the learner advice byusing magnetic position sensors. The system shows three windows; a fingerposition window for visualization of finger position, a score window fordiagnosing finger position along the score and command prompt window forshowing states of system and advices. Finally, we evaluated the system by anexperiment. The experimental group improved accuracy values about fingerpositions and also improved accuracy of pitches of sounds compared withcontrol group. These results shows significant differences.}
}

@inproceedings{Bortz2013,
  author = {Brennon Bortz and Aki Ishida and Ivica Ico Bukvic and R. Benjamin Knapp},
  title = {Lantern Field: Exploring Participatory Design of a Communal, Spatially Responsive Installation},
  pages = {73--78},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178484},
  url = {http://www.nime.org/proceedings/2013/nime2013_192.pdf},
  keywords = {Participatory creation, communal interaction, fields, interactive installation, Japanese lanterns},
  abstract = {Mountains and Valleys (an anonymous name for confidentiality) is a communal,site-specific installation that takes shape as a spatially-responsiveaudio-visual field. The public participates in the creation of theinstallation, resulting in shared ownership of the work between both theartists and participants. Furthermore, the installation takes new shape in eachrealization, both to incorporate the constraints and affordances of eachspecific site, as well as to address the lessons learned from the previousiteration. This paper describes the development and execution of Mountains andValleys over its most recent version, with an eye toward the next iteration ata prestigious art museum during a national festival in Washington, D.C.}
}

@inproceedings{Soria2013,
  author = {Edmar Soria and Roberto Morales-Manzanares},
  title = {Multidimensional sound spatialization by means of chaotic dynamical systems},
  pages = {79--83},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178664},
  url = {http://www.nime.org/proceedings/2013/nime2013_195.pdf},
  keywords = {NIME, spatialization, dynamical systems, chaos},
  abstract = {This work presents a general framework method for cre-ating spatialization systems focused on electroacoustic andacousmatic music performance and creation. Although weused the logistic equation as orbit generator, any dynami-cal system could be suitable. The main idea lies on generating vectors of Rn with entriesfrom data series of di_x000B_erent orbits from an speci_x000C_c dynami-cal system. Such vectors will be called system vectors. Ourproposal is to create ordered paths between those pointsor system vectors using the Splines Quark library by Felix,1which allow us to generate smooth curves joining the points.Finally, interpolating that result with a _x000C_xed sample value,we are able to obtain speci_x000C_c and independent multidimen-sional panning trajectories for each speaker array and forany number of sound sources.Our contribution is intended to be at the very root of the compositionalprocess giving to the creator a method for exploring new ways for spatialsound placement over time for a wide range of speakers ar-rangements. The advantage of using controlled chaotic dy-namical systems like the logistic equation, lies on the factthat the composer can freely and consciously choose be-tween stable or irregular behaviour for the orbits that willgenerate his/her panning trajectories. Besides, with the useof isometries, it is possible to generate di_x000B_erent related or-bits with one single evaluation of the system. The use ofthe spline method in SuperCollider allows the possibilityof joining and relating those values from orbits into a wellde_x000C_ned and coherent general system. Further research willinclude controlling synthesis parameters in the same waywe created panning trajectories.}
}

@inproceedings{Rosselet2013,
  author = {Ulysse Rosselet and Alain Renaud},
  title = {Jam On: a new interface for web-based collective music performance},
  pages = {394--399},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178650},
  url = {http://www.nime.org/proceedings/2013/nime2013_196.pdf},
  keywords = {Networked performance, interface design, mapping, web-based music application},
  abstract = {This paper presents the musical interactions aspects of the design anddevelopment of a web-based interactive music collaboration system called JamOn. Following a design science approach, this system is being built accordingto principles taken from usability engineering and human computer interaction(HCI). The goal of the system is to allow people with no to little musicalbackground to play a song collaboratively. The musicians control the musicalcontent and structure of the song thanks to an interface relying on the freeinking metaphor. One contribution of this interface is that it displays musicalpatterns of different lengths in the same space. The design of Jam On is basedon a list of performance criteria aimed at ensuring the musicality of theperformance and the interactivity of the technical system. The paper comparestwo alternative interfaces used for the system and explores the various stagesof the design process aimed at making the system as musical and interactive aspossible.}
}

@inproceedings{Lai2013,
  author = {Chi-Hsia Lai and Till Bovermann},
  title = {Audience Experience in Sound Performance},
  pages = {170--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178590},
  url = {http://www.nime.org/proceedings/2013/nime2013_197.pdf},
  keywords = {Audience Experience Study, Live Performance, Evaluation, Research Methods},
  abstract = {This paper presents observations from investigating audience experience of apractice-based research in live sound performance with electronics. In seekingto understand the communication flow and the engagement between performer andaudience in this particular performance context, we designed an experiment thatinvolved the following steps: (a) performing WOSAWIP at a new media festival,(b) conducting a qualitative research study with audience members and (c)analyzing the data for new insights.}
}

@inproceedings{Everett2013,
  author = {Steve Everett},
  title = {Sonifying Chemical Evolution},
  pages = {277--278},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178508},
  url = {http://www.nime.org/proceedings/2013/nime2013_198.pdf},
  keywords = {Data-driven composition, sonification, live electronics-video},
  abstract = {This presentation-demonstration discusses the creation of FIRST LIFE, a75-minute mixed media performance for string quartet, live audio processing,live motion capture video, and audience participation utilizing stochasticmodels of chemical data provided by Martha Grover's Research Group at theSchool of Chemical and Biomolecular Engineering at Georgia Institute ofTechnology. Each section of this work is constructed from contingent outcomesdrawn from biochemical research exploring possible early Earth formations oforganic compounds. Audio-video excerpts of the composition will be played during the presentation.Max patches for sonification and for generating stochastic processes will bedemonstrated as well.}
}

@inproceedings{McKinney2013,
  author = {Chad McKinney and Nick Collins},
  title = {An Interactive {3D} Network Music Space},
  pages = {400--405},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178606},
  url = {http://www.nime.org/proceedings/2013/nime2013_199.pdf},
  keywords = {3D, Generative, Network, Environment},
  abstract = {In this paper we present Shoggoth, a 3D graphics based program for performingnetwork music. In Shoggoth, users utilize video game style controls to navigateand manipulate a grid of malleable height maps. Sequences can be created bydefining paths through the maps which trigger and modulate audio playback. Withrespect to a context of computer music performance, and specific problems innetwork music, design goals and technical challenges are outlined. The systemis evaluated through established taxonomies for describing interfaces, followedby an enumeration of the merits of 3D graphics in networked performance. Indiscussing proposed improvements to Shoggoth, design suggestions for otherdevelopers and network musicians are drawn out.}
}

@inproceedings{Ferguson2013,
  author = {Sam Ferguson and Aengus Martin and Andrew Johnston},
  title = {A corpus-based method for controlling guitar feedback},
  pages = {541--546},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178518},
  url = {http://www.nime.org/proceedings/2013/nime2013_200.pdf},
  abstract = {Feedback created by guitars and amplifiers is difficult to use in musicalsettings -- parameters such as pitch and loudness are hard to specify preciselyby fretting a string or by holding the guitar near an amplifier. This researchinvestigates methods for controlling the level and pitch of the feedbackproduced by a guitar and amplifier, which are based on incorporatingcorpus-based control into the system. Two parameters are used to define thecontrol parameter space -- a simple automatic gain control system to controlthe output level, and a band-pass filter frequency for controlling the pitch ofthe feedback. This control parameter space is mapped to a corpus of soundscreated by these parameters and recorded, and these sounds are analysed usingsoftware created for concatenative synthesis. Following this process, thedescriptors taken from the analysis can be used to select control parametersfrom the feedback system.}
}

@inproceedings{KITA2013,
  author = {Toshihiro KITA and Naotoshi Osaka},
  title = {Providing a feeling of other remote learners' presence in an online learning environment via realtime sonification of Moodle access log},
  pages = {198--199},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178584},
  url = {http://www.nime.org/proceedings/2013/nime2013_203.pdf},
  keywords = {e-learning, online learners, Moodle, Csound, realtime sonification, OSC (Open Sound Control)},
  abstract = {When people learn using Web-based educational resources such as an LMS(Learning Management System) or other e-learning related systems, they aresitting in front of their own computer at home and are often physicallyisolated from other online learners. In some courses they are typically gettingin touch online with each others for doing some particular group workassignments, but most of the time they must do their own learning tasks alone.In other courses simply the individual assignments and quizzes are provided, sothe learners are alone all the time from the beginning until the end of thecourse.In order to keep the learners' motivation, it helps to feel other learnersdoing the same learning activities and belonging to the same course.Communicating formally or informally with other learners via Social NetworkingServices or something is one way for learners to get such a feeling, though ina way it might sometimes disturb their learning. Sonification of the access logof the e-learning system could be another indirect way to provide such afeeling.}
}

@inproceedings{Gelineck2013,
  author = {Steven Gelineck and Dan Overholt and Morten B{\''u}chert and Jesper Andersen},
  title = {Towards an Interface for Music Mixing based on Smart Tangibles and Multitouch},
  pages = {180--185},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178532},
  url = {http://www.nime.org/proceedings/2013/nime2013_206.pdf},
  keywords = {music mixing, tangibles, smart objects, multi-touch, control surface, graspables, physical-digital interface, tangible user interface, wireless sensing, sketching in hardware},
  abstract = {This paper presents the continuous work towards the development of an interface for music mixing targeted towards expert sound technicians and producers. The mixing interface uses a stage metaphor mapping scheme where audio channels arerepresented as digital widgets on a 2D surface. These can be controlled bymulti touch or by smart tangibles, which are tangible blocks with embedded sensors. The smart tangibles developed for this interface are able to sense howthey are grasped by the user. The paper presents the design of the mixing interface including the smart tangible as well as a preliminary user study involving a hands-on focus group session where 5 different control technologiesare contrasted and discussed. Preliminary findings suggest that smart tangibles were preferred, but that an optimal interface would include a combination of touch, smart tangibles and an extra function control tangible for extending the functionality of the smart tangibles. Finally, the interface should incorporate both an edit and mix mode---the latter displaying very limited visual feedback in order to force users to focus their attention to listening instead of the interface.}
}

@inproceedings{Tang2013,
  author = {Will W. W. Tang and Stephen Chan and Grace Ngai and Hong-va Leong},
  title = {Computer Assisted Melo-rhythmic Generation of Traditional Chinese Music from Ink Brush Calligraphy},
  pages = {84--89},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178668},
  url = {http://www.nime.org/proceedings/2013/nime2013_208.pdf},
  keywords = {Chinese Calligraphy, Chinese Music, Assisted Music Generation},
  abstract = {CalliMusic, is a system developed for users to generate traditional Chinesemusic by writing Chinese ink brush calligraphy, turning the long-believedstrong linkage between the two art forms with rich histories into reality. Inaddition to traditional calligraphy writing instruments (brush, ink and paper),a camera is the only addition needed to convert the motion of the ink brushinto musical notes through a variety of mappings such as human-inspired,statistical and a hybrid. The design of the system, including details of eachmapping and research issues encountered are discussed. A user study of systemperformance suggests that the result is quite encouraging. The technique is,obviously, applicable to other related art forms with a wide range ofapplications.}
}

@inproceedings{Kaneko2013,
  author = {Shoken Kaneko},
  title = {A Function-Oriented Interface for Music Education and Musical Expressions: ``the Sound Wheel''},
  pages = {202--205},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178574},
  url = {http://www.nime.org/proceedings/2013/nime2013_21.pdf},
  keywords = {Music education, Interactive tonal music generation},
  abstract = {In this paper, a function-oriented musical interface, named the sound wheel_x0011_,is presented. This interface is designed to manipulate musical functions likepitch class sets, tonal centers and scale degrees, rather than the _x0010_musicalsurface_x0011_, i.e. the individual notes with concrete note heights. The sound wheelhas an interface summarizing harmony theory, and the playing actions haveexplicit correspondencewith musical functions. Easy usability is realized by semi-automatizing theconversion process from musical functions into the musical surface. Thus, theplayer can use this interface with concentration on the harmonic structure,without having his attention caught by manipulating the musical surface.Subjective evaluation indicated the e_x001B_ffectiveness of this interface as a toolhelpful for understanding the music theory. Because of such features, thisinterface can be used for education and interactive training of tonal musictheory.}
}

@inproceedings{Andersson2013,
  author = {Anders-Petter Andersson and Birgitta Cappelen},
  title = {Designing Empowering Vocal and Tangible Interaction},
  pages = {406--412},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178465},
  url = {http://www.nime.org/proceedings/2013/nime2013_210.pdf},
  keywords = {Vocal Interaction, Tangible Interaction, Music & Health, Voice, Empowerment, Music Therapy, Resource-Oriented},
  abstract = {Our voice and body are important parts of our self-experience, andcommunication and relational possibilities. They gradually become moreimportant for Interaction Design, due to increased development of tangibleinteraction and mobile communication. In this paper we present and discuss ourwork with voice and tangible interaction in our ongoing research project XXXXX.The goal is to improve health for families, adults and children withdisabilities through use of collaborative, musical, tangible media. We build onuse of voice in Music Therapy and on a humanistic health approach. Ourchallenge is to design vocal and tangible interactive media that through usereduce isolation and passivity and increase empowerment for the users. We usesound recognition, generative sound synthesis, vibrations and cross-mediatechniques, to create rhythms, melodies and harmonic chords to stimulatebody-voice connections, positive emotions and structures for actions.}
}

@inproceedings{Astrinaki2013,
  author = {Maria Astrinaki and Nicolas d'Alessandro and Lo{\"i}c Reboursi{\`e}re and Alexis Moinet and Thierry Dutoit},
  title = {MAGE 2.0: New Features and its Application in the Development of a Talking Guitar},
  pages = {547--550},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178467},
  url = {http://www.nime.org/proceedings/2013/nime2013_214.pdf},
  keywords = {speech synthesis, augmented guitar, hexaphonic guitar},
  abstract = {This paper describes the recent progress in our approach to generateperformative and controllable speech. The goal of the performative HMM-basedspeech and singing synthesis library, called Mage, is to have the ability togenerate natural sounding speech with arbitrary speaker's voicecharacteristics, speaking styles and expressions and at the same time to haveaccurate reactive user control over all the available production levels. Mageallows to arbitrarily change between voices, control speaking style or vocalidentity, manipulate voice characteristics or alter the targeted contexton-the-fly and also maintain the naturalness and intelligibility of the output.To achieve these controls, it was essential to redesign and improve the initiallibrary. This paper focuses on the improvements of the architectural design,the additional user controls and provides an overview of a prototype, where aguitar is used to reactively control the generation of a synthetic voice invarious levels.}
}

@inproceedings{Lee2013a,
  author = {Sang Won Lee and Georg Essl},
  title = {Live Coding The Mobile Music Instrument},
  pages = {493--498},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178592},
  url = {http://www.nime.org/proceedings/2013/nime2013_216.pdf},
  keywords = {live coding, network music, on-the-fly instrument, mobile music},
  abstract = {We introduce a form of networked music performance where a performer plays amobile music instrument while it is being implemented on the fly by a livecoder. This setup poses a set of challenges in performing a music instrumentwhich changes over time and we suggest design guidelines such as making asmooth transition, varying adoption of change, and sharing information betweenthe pair of two performers. A proof-of-concept instrument is implemented on amobile device using UrMus, applying the suggested guidelines. We wish that thismodel would expand the scope of live coding to the distributed interactivesystem, drawing existing performance ideas of NIMEs.}
}

@inproceedings{You2013,
  author = {Jaeseong You and Red Wierenga},
  title = {Remix_Dance 3: Improvisatory Sound Displacing on Touch Screen-Based Interface},
  pages = {124--127},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178696},
  url = {http://www.nime.org/proceedings/2013/nime2013_219.pdf},
  keywords = {Novel controllers, interface for musical expression, musical mapping strategy, music cognition, music perception, MPEG-7},
  abstract = {Remix_Dance Music 3 is a four-channel quasi-fixed media piece that can beimprovised by a single player operating the Max/MSP-based controller on atablet such as iPad. Within the fixed time limit of six minutes, the performercan freely (de)activate and displace the eighty seven precomposed audio filesthat are simultaneously running, generating a sonic structure to one's likingout of the given network of musical possibilities. The interface is designed toinvite an integral musical structuring particularly in the dimensions ofperformatively underexplored (but still sonically viable) parameters that arelargely based on MPEG-7 audio descriptors.}
}

@inproceedings{Barbosa2013,
  author = {Jer{\^{o}}nimo Barbosa, Filipe Calegario, Veronica Teichrieb, Geber Ramalho and Giordano Cabral},
  title = {A Drawing-Based Digital Music Instrument},
  pages = {499--502},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178566},
  url = {http://www.nime.org/proceedings/2013/nime2013_220.pdf},
  keywords = {Digital musical instruments, augmented multi-touch, hierarchical live looping, interaction techniques, evaluation methodology},
  abstract = {This paper presents an innovative digital musical instrument, the Illusio, based on an augmented multi-touch interface that combines a traditional multi-touch surface and a device similar to a guitar pedal. Illusio allows users to perform by drawing and by associating the sketches with live loops. These loops are manipulated based on a concept called hierarchical live looping, which extends traditional live looping through the use of a musical tree, in which any music operation applied to a given node affects all its children nodes. Finally, we evaluate the instrument considering the performer and the audience, which are two of the most important stakeholders involved in the use, conception, and perception of a musical device. The results achieved are encouraging and led to useful insights about how to improve instrument features, performance and usability.}
}

@inproceedings{Sarwate2013,
  author = {Avneesh Sarwate and Rebecca Fiebrink},
  title = {Variator: A Creativity Support Tool for Music Composition},
  pages = {279--282},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178654},
  url = {http://www.nime.org/proceedings/2013/nime2013_224.pdf},
  keywords = {Composition assistance tool, computer-aided composition, social composition},
  abstract = {The Variator is a compositional assistance tool that allows users to quicklyproduce and experiment with variations on musical objects, such as chords,melodies, and chord progressions. The transformations performed by the Variatorcan range from standard counterpoint transformations (inversion, retrograde,transposition) to more complicated custom transformations, and the system isbuilt to encourage the writing of custom transformations.This paper explores the design decisions involved in creating a compositionalassistance tool, describes the Variator interface and a preliminary set ofimplemented transformation functions, analyzes the results of the evaluationsof a prototype system, and lays out future plans for expanding upon thatsystem, both as a stand-alone application and as the basis for an opensource/collaborative community where users can implement and share their owntransformation functions.}
}

@inproceedings{Grierson2013,
  author = {Mick Grierson and Chris Kiefer},
  title = {NoiseBear: A Malleable Wireless Controller Designed In Participation with Disabled Children},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178536},
  url = {http://www.nime.org/proceedings/2013/nime2013_227.pdf},
  keywords = {malleable controllers, assistive technology, multiparametric mapping},
  abstract = {NoiseBear is a wireless malleable controller designed for, and in participationwith, physically and cognitively disabled children. The aim of the project wasto produce a musical controller that was robust, and flexible enough to be usedin a wide range of interactive scenarios in participatory design workshops. NoiseBear demonstrates an open ended system for designing wireless malleablecontrollers in different shapes. It uses pressure sensitive material made fromconductive thread and polyester cushion stuffing, to give the feel of a softtoy. The sensor networks with other devices using the Bluetooth Low Energyprotocol, running on a BlueGiga BLE112 chip. This contains an embedded 8051processor which manages the sensor. NoiseBear has undergone an initialformative evaluation in a workshop session with four autistic children, andcontinues to evolve in series of participatory design workshops. The evaluationshowed that controller could be engaging for the children to use, andhighlighted some technical limitations of the design. Solutions to theselimitations are discussed, along with plans for future design iterations.}
}

@inproceedings{jo2013,
  author = {kazuhiro jo},
  title = {cutting record --- a record without (or with) prior acoustic information},
  pages = {283--286},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178578},
  url = {http://www.nime.org/proceedings/2013/nime2013_228.pdf},
  keywords = {Analog Record, Personal Fabrication, Media Archaeology},
  abstract = {In this paper, we present a method to produce analog records with standardvector graphics software (i.e. Adobe Illustrator) and two different types ofcutting machines: laser cutter, and paper cutter. The method enables us toengrave wave forms on a surface of diverse materials such as paper, wood,acrylic, and leather without or with prior acoustic information (i.e. digitalaudio data). The results could be played with standard record players. Wepresent the method with its technical specification and explain our initialtrials with two performances and a workshop. The work examines the role ofmusical reproduction in the age of personal fabrication. ---p.s. If it's possible, we also would like to submit the work for performanceand workshop.A video of performance < it contains information on the authorshttp://www.youtube.com/watch?v=vbCLe06P7j0}
}

@inproceedings{Klugel2013,
  author = {Niklas Kl{\"u}gel and Georg Groh},
  title = {Towards Mapping Timbre to Emotional Affect},
  pages = {525--530},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178586},
  url = {http://www.nime.org/proceedings/2013/nime2013_23.pdf},
  keywords = {Emotional affect,Timbre, Machine Learning, Deep Belief Networks, Analysis by Synthesis},
  abstract = {Controlling the timbre generated by an audio synthesizerin a goal-oriented way requires a profound understandingof the synthesizer's manifold structural parameters. Especially shapingtimbre expressively to communicate emotional affect requires expertise.Therefore, novices in particular may not be able to adequately control timbrein viewof articulating the wealth of affects musically. In this context, the focus ofthis paper is the development of a model that can represent a relationshipbetween timbre and an expected emotional affect . The results of the evaluationof the presented model are encouraging which supports its use in steering oraugmenting the control of the audio synthesis. We explicitly envision thispaper as a contribution to the field of Synthesis by Analysis in the broadersense, albeit being potentially suitable to other related domains.}
}

@inproceedings{Greenlee2013,
  author = {Shawn Greenlee},
  title = {Graphic Waveshaping},
  pages = {287--290},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178534},
  url = {http://www.nime.org/proceedings/2013/nime2013_232.pdf},
  keywords = {Graphic waveshaping, graphic synthesis, waveshaping synthesis, graphic sound, drawn sound},
  abstract = {In the design of recent systems, I have advanced techniques that positiongraphic synthesis methods in the context of solo, improvisational performance.Here, the primary interfaces for musical action are prepared works on paper,scanned by digital video cameras which in turn pass image data on to softwarefor analysis and interpretation as sound synthesis and signal processingprocedures. The focus of this paper is on one of these techniques, a process Idescribe as graphic waveshaping. A discussion of graphic waveshaping in basicform and as utilized in my performance work, (title omitted), is offered. Inthe latter case, the performer's objective is to guide the interpretation ofimages as sound, constantly tuning and retuning the conversion while selectingand scanning images from a large catalog. Due to the erratic nature of thesystem and the precondition that image to sound relationships are unfixed, theperformance situation is replete with the discovery of new sounds and thecircumstances that bring them into play. Graphic waveshaping may be understood as non-linear distortion synthesis withtime-varying transfer functions stemming from visual scan lines. As a form ofgraphic synthesis, visual images function as motivations for sound generation.There is a strategy applied for creating one out of the other. However, counterto compositionally oriented forms of graphic synthesis where one may assignimage characteristics to musical parameters such as pitches, durations,dynamics, etc., graphic waveshaping is foremost a processing technique, as itdistorts incoming signals according to graphically derived transfer functions.As such, it may also be understood as an audio effect; one that in myimplementations is particularly feedback dependent, oriented towards shapingthe erratic behavior of synthesis patches written in Max/MSP/Jitter. Used inthis manner, graphic waveshaping elicits an emergent system behaviorconditioned by visual features.}
}

@inproceedings{Park2013c,
  author = {Tae Hong Park and Oriol Nieto},
  title = {Fortissimo: Force-Feedback for Mobile Devices},
  pages = {291--294},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178638},
  url = {http://www.nime.org/proceedings/2013/nime2013_233.pdf},
  keywords = {force-feedback, expression, mobile computing devices, mobile music},
  abstract = {In this paper we present a highly expressive, robust, and easy-to-build systemthat provides force-feedback interaction for mobile computing devices (MCD).Our system, which we call Fortissimo (ff), utilizes standard built-inaccelerometer measurements in conjunction with generic foam padding that can beeasily placed under a device to render an expressive force-feedback performancesetup. Fortissimo allows for musically expressive user-interaction with addedforce-feedback which is integral for any musical controller --a feature that isabsent for touchscreen-centric MCDs. This paper details ff core concepts,hardware and software designs, and expressivity of musical features.}
}

@inproceedings{Scott2013,
  author = {Jeffrey Scott and Mickey Moorhead and Justin Chapman and Ryan Schwabe and Youngmoo E. Kim},
  title = {Personalized Song Interaction Using a Multi Touch Interface},
  pages = {417--420},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178660},
  url = {http://www.nime.org/proceedings/2013/nime2013_234.pdf},
  keywords = {Multi-track, Multi-touch, Mobile devices, Interactive media},
  abstract = {Digital music technology has transformed the listener experience and creatednew avenues for creative interaction and expression within the musical domain.The barrier to music creation, distribution and collaboration has been reduced,leading to entirely new ecosystems of musical experience. Software editingtools such as digital audio workstations (DAW) allow nearly limitlessmanipulation of source audio into new sonic elements and textures and havepromoted a culture of recycling and repurposing of content via mashups andremixes. We present a multi-touch application that allows a user to customizetheir listening experience by blending various versions of a song in real time.}
}

@inproceedings{Batula2013,
  author = {Alyssa Batula and Manu Colacot and David Grunberg and Youngmoo Kim},
  title = {Using Audio and Haptic Feedback to Improve Pitched Percussive Instrument Performance in Humanoids},
  pages = {295--300},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178472},
  url = {http://www.nime.org/proceedings/2013/nime2013_235.pdf},
  keywords = {Musical robots, humanoids, auditory feedback, haptic feedback},
  abstract = {We present a system which allows an adult-sized humanoid to determine whetheror not it is correctly playing a pitched percussive instrument to produce adesired sound. As hu,
man musicians utilize sensory feedback to determine ifthey are successfully using their instruments to generate certain pitches,robot performers should be capable of the same feat. We present a noteclassification algorithm that uses auditory and haptic feedback to decide if anote was well- or poorly-struck. This system is demonstrated using Hubo, anadult-sized humanoid, which has been enabled to actu,
ate pitched pipes usingmallets. We show that, with this system, Hubo is able to determine whether ornot a note was played correctly.}
}

@inproceedings{Torresen2013,
  author = {Jim Torresen and Yngve Hafting and Kristian Nymoen},
  title = {A New Wi-Fi based Platform for Wireless Sensor Data Collection},
  pages = {337--340},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178680},
  url = {http://www.nime.org/proceedings/2013/nime2013_236.pdf},
  keywords = {wireless communication, sensor data collection, WLAN, Arduino},
  abstract = {A custom designed WLAN (Wireless Local Area Network) based sensor interface ispresented in this paper. It is aimed at wirelessly interfacing a large varietyof sensors to supplement built-in sensors in smart phones and media players.The target application area is collection of human related motions andcondition to be applied in musical applications. The interface is based oncommercially available units and allows for up to nine sensors. The benefit ofusing WLAN based communication is high data rate with low latency. Ourexperiments show that the average transmission time is less than 2ms for asingle sensor. Further, it is operational for a whole day without batteryrecharging.}
}

@inproceedings{Skogstad2013,
  author = {St{\aa}le A. Skogstad},
  title = {Filtering Motion Capture Data for Real-Time Applications},
  pages = {142--147},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178662},
  url = {http://www.nime.org/proceedings/2013/nime2013_238.pdf},
  abstract = {In this paper we present some custom designed filters for real-time motioncapture applications. Our target application is so-called motion controllers,i.e. systems that interpret hand motion for musical interaction. In earlierresearch we found effective methods to design nearly optimal filters forreal-time applications. However, to be able to design suitable filters for ourtarget application, it is necessary to establish the typical frequency contentof the motion capture data we want to filter. This will again allow us todetermine a reasonable cutoff frequency for the filters. We have thereforeconducted an experiment in which we recorded the hand motion of 20 subjects.The frequency spectra of these data together with a method similar to theresidual analysis method were then used to determine reasonable cutofffrequencies. Based on this experiment, we propose three cutoff frequencies fordifferent scenarios and filtering needs: 5, 10 and 15 Hz, which corresponds toheavy, medium and light filtering respectively. Finally, we propose a range ofreal-time filters applicable to motion controllers. In particular, low-passfilters and low-pass differentiators of degrees one and two, which in ourexperience are the most useful filters for our target application.}
}

@inproceedings{Kleinberger2013,
  author = {Rebecca Kleinberger},
  title = {{PAM}DI Music Box: Primarily Analogico-Mechanical, Digitally Iterated Music Box},
  pages = {19--20},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178588},
  url = {http://www.nime.org/proceedings/2013/nime2013_24.pdf},
  keywords = {Tangible interface, musical controller, music box, mechanical and electronic coupling, mapping.},
  abstract = {PAMDI is an electromechanical music controller based on an expansion of the common metal music boxes. Our system enables an augmentation of the musical properties by adding different musical channels triggered and parameterized by natural gestures during the ``performance''. All the channels are generated form the original melody recorded once at the start. To capture and treat the different expressive parameters both natural and intentional, our platform is composed of a metallic structure supporting sensors. The measured values are processed by an arduino system that finallysends the results by serial communication to a Max/MSP patch for signaltreatment and modification. We will explain how our embedded instrument aims to bring a certain awareness to the player of the mapping and the potential musical freedom of the very specific -- and not that much automatic --- instrument that is a music box. We will also address how our design tackles the different questions of mapping, ergonomics and expressiveness while choosing the controller modalities and the parameters to be sensed.}
}

@inproceedings{McPherson2013,
  author = {Andrew McPherson},
  title = {Portable Measurement and Mapping of Continuous Piano Gesture},
  pages = {152--157},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178610},
  url = {http://www.nime.org/proceedings/2013/nime2013_240.pdf},
  keywords = {Piano, keyboard, optical sensing, gesture sensing, visual feedback, mapping, magnetic resonator piano},
  abstract = {This paper presents a portable optical measurement system for capturingcontinuous key motion on any piano. Very few concert venues have MIDI-enabledpianos, and many performers depend on the versatile but discontinued MoogPianoBar to provide MIDI from a conventional acoustic instrument. The scannerhardware presented in this paper addresses the growing need for alternativesolutions while surpassing existing systems in the level of detail measured.Continuous key position on both black and white keys is gathered at 1kHz samplerate. Software extracts traditional and novel features of keyboard touch fromeach note, which can be flexibly mapped to sound using MIDI or Open SoundControl. RGB LEDs provide rich visual feedback to assist the performer ininteracting with more complex sound mapping arrangements. An application ispresented to the magnetic resonator piano, an electromagnetically-augmentedacoustic grand piano which is performed using continuous key positionmeasurements.}
}

@inproceedings{Tarakajian2013,
  author = {Sam Tarakajian and David Zicarelli and Joshua Clayton},
  title = {Mira: Liveness in iPad Controllers for Max/MSP},
  pages = {421--426},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178670},
  url = {http://www.nime.org/proceedings/2013/nime2013_241.pdf},
  keywords = {NIME, Max/MSP/Jitter, Mira, ipad, osc, bonjour, zeroconf},
  abstract = {Mira is an iPad app for controlling Max patchers in real time with minimalconfiguration. This submission includes a paper describing Mira's design andimplementation, as well as a demo showing how Mira works with Max.The Mira iPad app discovers open Max patchers automatically using the Bonjourprotocol, connects to them over WiFi and negotiates a description of the Maxpatcher. As objects change position and appearance, Mira makes sure that theinterface on the iPad is kept up to date. Mira eliminates the need for anexplicit mapping step between the interface and the system being controlled.The user is never asked to input an IP address, nor to configure the mappingbetween interface objects on the iPad and those in the Max patcher. So theprototyping composer is free to rapidly configure and reconfigure theinterface.}
}

@inproceedings{Kim2013,
  author = {Taehun Kim and Stefan Weinzierl},
  title = {Modelling Gestures in Music Performance with Statistical Latent-State Models},
  pages = {427--430},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178582},
  url = {http://www.nime.org/proceedings/2013/nime2013_244.pdf},
  keywords = {Musical gestures, performance analysis, unsupervised machine learning},
  abstract = {We discuss how to model "gestures" in music performance with statistical latent-states models. A music performance can be described with compositional and expressive properties varying over time. In those property changes we often observe particular patterns, and such a pattern can be understood as a "gesture", since it serves as a medium transferring specific emotions. Assuming a finite number of latent states on each property value changes, we can describe those gestures with statistical latent-states models, and train them by unsupervised learning algorithms. In addition, model entropy provides us a measure for different effects of each properties on the gesture implementation. Test result on some of real performances indicates that the trained models could capture the structure of gestures observed in the given performances, and detect their boundaries. The entropy-based measure was informative to understand the effectiveness of each property on the gesture implementation. Test result on large corpora indicates that our model has potentials for afurther model improvement.}
}

@inproceedings{Pardue2013,
  author = {Laurel Pardue and William Sebastian},
  title = {Hand-Controller for Combined Tactile Control and Motion Tracking},
  pages = {90--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178630},
  url = {http://www.nime.org/proceedings/2013/nime2013_245.pdf},
  keywords = {hand, interface, free gesture, force sensing resistor, new musical instrument, tactile feedback, position tracking},
  abstract = {The Hand Controller is a new interface designed to enable a performer toachieve detailed control of audio and visual parameters through a tangibleinterface combined with motion tracking of the hands to capture large scalephysical movement. Such movement empowers an expressive dynamic for bothperformer and audience. However tracking movements in free space isnotoriously difficult for virtuosic performance. The lack of tactile feedbackleads to difficulty learning the repeated muscle movements required for precisecontrol. In comparison, the hands have shown an impressive ability to mastercomplex motor tasks through feel. The hand controller uses both modes ofinteraction. Electro-magnetic field tracking enables 6D hand motion trackingwhile two options provide tactile interaction- a set of tracks that providelinear positioning and applied finger pressure, or a set of trumpet like sliderkeys that provide continuous data describing key depth. Thumbs actuateadditional pressure sensitive buttons. The two haptic interfaces are mountedto a comfortable hand grip that allows a significant range of reach, andpressure to be applied without restricting hand movement highly desirable inexpressive motion.}
}

@inproceedings{Wiriadjaja2013,
  author = {Antonius Wiriadjaja},
  title = {Gamelan Sampul: Laptop Sleeve Gamelan},
  pages = {469--470},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178688},
  url = {http://www.nime.org/proceedings/2013/nime2013_246.pdf},
  keywords = {Physical computing, product design, traditional folk arts, gamelan},
  abstract = {The Gamelan Sampul is a laptop sleeve with embedded circuitry that allows usersto practice playing Javanese gamelan instruments without a full set ofinstruments. It is part of a larger project that aims to develop a set ofportable and mobile tools for learning, recording and performing classicalJavanese gamelan music.The accessibility of a portable Javanese gamelan set introduces the musicalgenre to audiences who have never experienced this traditional music before,passing down long established customs to future generations. But it also raisesthe question of what is and what isn't appropriate to the musical tradition.The Gamelan Sampul attempts to introduce new technology to traditional folkmusic while staying sensitive to cultural needs.}
}

@inproceedings{Pardue2013a,
  author = {Laurel Pardue and Andrew McPherson},
  title = {Near-Field Optical Reflective Sensing for Bow Tracking},
  pages = {363--368},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178628},
  url = {http://www.nime.org/proceedings/2013/nime2013_247.pdf},
  keywords = {optical sensor, reflectance, LED, photodiode, phototransistor, violin, bow tracking, gesture, near-field sensing},
  abstract = {This paper explores the potential of near-field optical reflective sensing formusical instrument gesture capture. Near-field optical sensors are inexpensive,portable and non-intrusive, and their high spatial and temporal resolutionmakes them ideal for tracking the finer motions of instrumental performance.The paper discusses general optical sensor performance with detailedinvestigations of three sensor models. An application is presented to violinbow position tracking using reflective sensors mounted on the stick. Bowtracking remains a difficult task, and many existing solutions are expensive,bulky, or offer limited temporal resolution. Initial results indicate that bowposition and pressure can be derived from optical measurements of thehair-string distance, and that similar techniques may be used to measure bowtilt.}
}

@inproceedings{Liu2013,
  author = {Qian Liu and Yoon Chung Han and JoAnn Kuchera-Morin and Matthew Wright},
  title = {Cloud Bridge: a Data-driven Immersive Audio-Visual Software Interface},
  pages = {431--436},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178596},
  url = {http://www.nime.org/proceedings/2013/nime2013_250.pdf},
  keywords = {Data Sonification, Data Visualization, Sonification, User Interface, Sonic Interaction Design, Open Sound Control},
  abstract = {Cloud Bridge is an immersive interactive audiovisual software interface forboth data exploration and artistic creation. It explores how information can besonified and visualized to facilitate findings, and eventually becomeinteractive musical compositions. Cloud Bridge functions as a multi-user,multimodal instrument. The data represents the history of items checked out bypatrons of the Seattle Public Library. A single user or agroup of users functioning as a performance ensemble participate in the pieceby interactively querying the database using iOS devices. Each device isassociated with aunique timbre and color for contributing to the piece, whichappears on large shared screens and a surround-sound system for allparticipants and observers. Cloud Bridge leads to a new media interactiveinterface utilizing audio synthesis, visualization and real-time interaction.}
}

@inproceedings{Everman2013,
  author = {Michael Everman and Colby Leider},
  title = {Toward {DMI} Evaluation Using Crowd-Sourced Tagging Techniques},
  pages = {437--440},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178510},
  url = {http://www.nime.org/proceedings/2013/nime2013_251.pdf},
  keywords = {Evaluation, tagging, digital musical instrument},
  abstract = {Few formal methods exist for evaluating digital musical instruments (DMIs) .This paper proposes a novel method of DMI evaluation using crowd-sourcedtagging. One of the challenges in devising such methods is that the evaluationof a musical instrument is an inherently qualitative task. While previouslyproposed methods have focused on quantitative methods and largely ignored thequalitative aspects of the task, tagging is well-suited to this and is alreadyused to classify things such as websites and musical genres. These, like DMIs,do not lend themselves to simple categorization or parameterization. Using the social tagging method, participating individuals assign descriptivelabels, or tags, to a DMI. A DMI can then be evaluated by analyzing the tagsassociated with it. Metrics can be generated from the tags assigned to theinstrument, and comparisons made to other instruments. This can give thedesigner valuable insight into the where the strengths of the design lie andwhere improvements may be needed. A prototype system for testing the method is proposed in the paper and iscurrently being implemented as part of an ongoing DMI evaluation project. It isexpected that results from the prototype will be available to report by thetime of the conference in May.}
}

@inproceedings{Nam2013,
  author = {Sangbong Nam},
  title = {Musical Poi (mPoi)},
  pages = {148--151},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178622},
  url = {http://www.nime.org/proceedings/2013/nime2013_254.pdf},
  keywords = {mPoi, Musical Poi, Jwibulnori, Poi, sensor-based musical instrument},
  abstract = {This paper describes the Musical Poi (mPoi), a unique sensor-based musicalinstrument rooted in the ancient art of poi spinning. The trajectory ofcircular motion drawn by the performance and the momentum of the mPoiinstrument are converted to the energetic and vibrant sound, which makesspiritual and meditative soundscape that opens everyone up the aura and clearsthe thought forms away. The mPoi project and its concepts will be introducedfirst and then its interaction with a performer will be discussed.The mPoi project seeks to develop a prototype for a set of mobile musicalinstrument based on electronic motion sensors and circuit boards. Thistechnology is installed in egg-shaped structure and allows communicationbetween a performer and the mPoi instrument. The principal motivation for themPoi project has been a desire to develop an extensible interface that willsupport the Poi performance, which is a style of performance art originatedwith the Maori people of New Zealand involving swinging tethered weightsthrough a variety of rhythmical and geometric patterns. As an extension of the body and the expansion of the movement, the mPoiutilizes the creative performance of Poi to make spatial and spiritual soundand music. The aims of the mPoi project are:to create a prototype of mPoi instrument that includes circuit board thatconnects the instrument to a sensor.to develop a software, which includes programming of the circuit board and forthe sound generation.to make a new artistic expression to refine the captured sound into artisticmusical notes. The creative part of the project is to design a unique method to translate theperformer's gesture into sound. A unique algorithm was developed to extractfeatures of the swing motion and translate them into various patterns of sound.}
}

@inproceedings{Oda2013,
  author = {Reid Oda and Adam Finkelstein and Rebecca Fiebrink},
  title = {Towards Note-Level Prediction for Networked Music Performance},
  pages = {94--97},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178624},
  url = {http://www.nime.org/proceedings/2013/nime2013_258.pdf},
  keywords = {Networked performance, prediction, computer vision},
  abstract = {The Internet allows musicians and other artists to collaborate remotely.However, network latency presents a fundamental challenge for remotecollaborators who need to coordinate and respond to each other's performancein real time. In this paper, we investigate the viability of predictingpercussion hits before they have occurred, so that information about thepredicted drum hit can be sent over a network, and the sound can be synthesizedat a receiver's location at approximately the same moment the hit occurs atthe sender's location. Such a system would allow two percussionists to playin perfect synchrony despite the delays caused by computer networks. Toinvestigate the feasibility of such an approach, we record vibraphone malletstrikes with a high-speed camera and track the mallet head position. We showthat 30 ms before the strike occurs, it is possible to predict strike time andvelocity with acceptable accuracy. Our method fits a second-order polynomial tothe data to produce a strike time prediction that is within the bounds ofperceptual synchrony, and a velocity estimate that will enable the soundpressure level of the synthesized strike to be accurate within 3 dB.}
}

@inproceedings{Jenkins2013,
  author = {Leonardo Jenkins and Shawn Trail and George Tzanetakis and Peter Driessen and Wyatt Page},
  title = {An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet},
  pages = {352--357},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178562},
  url = {http://www.nime.org/proceedings/2013/nime2013_261.pdf},
  keywords = {hyperinstrument, trumpet, minimally-invasive, gesture sensing, wireless, I2C},
  abstract = {This paper presents a minimally-invasive, wireless optical sensorsystem for use with any conventional piston valve acoustic trumpet. Itis designed to be easy to install and remove by any trumpeter. Ourgoal is to offer the extended control afforded by hyperinstrumentswithout the hard to reverse or irreversible invasive modificationsthat are typically used for adding digital sensing capabilities. Weutilize optical sensors to track the continuous position displacementvalues of the three trumpet valves. These values are trasmittedwirelessly and can be used by an external controller. The hardware hasbeen designed to be reconfigurable by having the housing 3D printed sothat the dimensions can be adjusted for any particular trumpetmodel. The result is a low cost, low power, easily replicable sensorsolution that offers any trumpeter the ability to augment their ownexisting trumpet without compromising the instrument's structure orplaying technique. The extended digital control afforded by our systemis interweaved with the natural playing gestures of an acoustictrumpet. We believe that this seemless integration is critical forenabling effective and musical human computer interaction.Keywords: hyperinstrument, trumpet, minimally-invasive, gesture sensing,wireless, I2C}
}

@inproceedings{Freed2013a,
  author = {Adrian Freed and John MacCallum and Sam Mansfield},
  title = {``Old'' is the New ``New'': a Fingerboard Case Study in Recrudescence as a NIME Development Strategy},
  pages = {441--445},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178524},
  url = {http://www.nime.org/proceedings/2013/nime2013_265.pdf},
  keywords = {Fingerboard controller, Best practices, Recrudescence, Organology, Unobtainium},
  abstract = {This paper positively addresses the problem that most NIME devices are ephemeralasting long enough to signal academic and technical prowess but rarely longerthan a few musical performances. We offer a case study that shows thatlongevity of use depends on stabilizing the interface and innovating theimplementation to maintain the required performance of the controller for theplayer.}
}

@inproceedings{Freed2013,
  author = {Adrian Freed and John MacCallum and David Wessel},
  title = {Agile Interface Development using OSC Expressions and Process Migration},
  pages = {347--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178526},
  url = {http://www.nime.org/proceedings/2013/nime2013_266.pdf},
  keywords = {Gesture Signal Processing, Open Sound Control, Functional Programming, Homoiconicity, Process Migration.},
  abstract = {We describe ``o.expr'' an expression language for dynamic, object- and agent-oriented computation of gesture signal processing workflows using OSC bundles. We illustrate the use of o.expr for a range of gesture processingtasks showing how stateless programming and homoiconicity simplify applications development and provide support for heterogeneous computational networks.}
}

@inproceedings{Hamilton2013,
  author = {Rob Hamilton},
  title = {Sonifying Game-Space Choreographies With UDKOSC},
  pages = {446--449},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178544},
  url = {http://www.nime.org/proceedings/2013/nime2013_268.pdf},
  keywords = {procedural music, procedural audio, interactive sonification, game music, Open Sound Control},
  abstract = {With a nod towards digital puppetry and game-based film genres such asmachinima, recent additions to UDKOSC of- fer an Open Sound Control (OSC)control layer for external control over both third-person ''pawn'' entitiesand camera controllers in fully rendered game-space. Real-time OSC input,driven by algorithmic process or parsed from a human-readable timed scriptingsyntax allows users to shape choreographies of gesture, in this case actormotion and action, as well as an audiences view into the game-spaceenvironment. As UDKOSC outputs real-time coordinate and action data generatedby UDK pawns and players with OSC, individual as well as aggregate virtualactor gesture and motion can be leveraged as a driver for both creative andprocedural/adaptive gaming music and audio concerns.}
}

@inproceedings{John2013,
  author = {David John},
  title = {Updating the Classifications of Mobile Music Projects},
  pages = {301--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178568},
  url = {http://www.nime.org/proceedings/2013/nime2013_273.pdf},
  keywords = {Mobile Music, interactive music, proximity sensing, wearable devices, mobile phone performance, interaction design},
  abstract = {This paper reviews the mobile music projects that have been presented at NIMEin the past ten years in order to assess whether the changes in technology haveaffected the activities of mobile music research. An overview of mobile musicprojects is presented using the categories that describe the main activities:projects that explore the influence and make use of location; applications thatshare audio or promote collaborative composition; interaction using wearabledevices; the use of mobile phones as performance devices; projects that exploreHCI design issues. The relative activity between different types of activity isassessed in order to identify trends. The classification according totechnological, social or geographic showed an overwhelming bias to thetechnological, followed by social investigations. An alternative classificationof survey product, or artifact reveals an increase in the number of productsdescribed with a corresponding decline in the number of surveys and artisticprojects. The increase in technical papers appears to be due to an enthusiasmto make use of increased capability of mobile phones, although there are signsthat the initial interest has already peaked, and researchers are againinterested to explore technologies and artistic expression beyond existingmobile phones.}
}

@inproceedings{Walther2013,
  author = {Thomas Walther and Damir Ismailovi{\'c} and Bernd Br{\''u}gge},
  title = {Rocking the Keys with a Multi-Touch Interface},
  pages = {98--101},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178684},
  url = {http://www.nime.org/proceedings/2013/nime2013_275.pdf},
  keywords = {multi-touch, mobile, keyboard, interface},
  abstract = {Although multi-touch user interfaces have become a widespread form of humancomputer interaction in many technical areas, they haven't found their way intolive performances of musicians and keyboarders yet. In this paper, we present anovel multi-touch interface method aimed at professional keyboard players. Themethod, which is inspired by computer trackpads, allows controlling up to tencontinuous parameters of a keyboard with one hand, without requiring the userto look at the touch area --- a significant improvement over traditional keyboardinput controls. We discuss optimizations needed to make our interface reliable,and show in an evaluation with four keyboarders of different skill level thatthis method is both intuitive and powerful, and allows users to more quicklyalter the sound of their keyboard than they could with current input solutions.}
}

@inproceedings{Berdahl2013,
  author = {Edgar Berdahl and Spencer Salazar and Myles Borins},
  title = {Embedded Networking and Hardware-Accelerated Graphics with Satellite CCRMA},
  pages = {325--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178476},
  url = {http://www.nime.org/proceedings/2013/nime2013_277.pdf},
  keywords = {Satellite CCRMA, embedded musical instruments, embedded installations, Node.js, Interface.js, hardware-accelerated graphics, OpenGLES, SimpleGraphicsOSC, union file system, write endurance},
  abstract = {Satellite CCRMA is a platform for making embedded musical instruments andembedded installations. The project aims to help prototypes live longer byproviding a complete prototyping platform in a single, small, and stand-aloneembedded form factor. A set of scripts makes it easier for artists andbeginning technical students to access powerful features, while advanced usersenjoy the flexibility of the open-source software and open-source hardwareplatform.This paper focuses primarily on networking capabilities of Satellite CCRMA andnew software for enabling interactive control of the hardware-acceleratedgraphical output. In addition, some results are presented from robustness testsalongside specific advice and software support for increasing the lifespan ofthe flash memory.}
}

@inproceedings{Xiao2013,
  author = {Xiao Xiao and Anna Pereira and Hiroshi Ishii},
  title = {Conjuring the Recorded Pianist: A New Medium to Experience Musical Performance},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178692},
  url = {http://www.nime.org/proceedings/2013/nime2013_28.pdf},
  keywords = {piano performance, musical expressivity, body language, recorded music, player piano, augmented reality, embodiment},
  abstract = {The body channels rich layers of information when playing music, from intricatemanipulations of the instrument to vivid personifications of expression. Butwhen music is captured and replayed across distance and time, the performer'sbody is too often trapped behind a small screen or absent entirely.This paper introduces an interface to conjure the recorded performer bycombining the moving keys of a player piano with life-sized projection of thepianist's hands and upper body. Inspired by reflections on a lacquered grandpiano, our interface evokes the sense that the virtual pianist is playing thephysically moving keys.Through our interface, we explore the question of how to viscerally simulate aperformer's presence to create immersive experiences. We discuss designchoices, outline a space of usage scenarios and report reactions from users.}
}

@inproceedings{Taylor2013,
  author = {Ben Taylor and Jesse Allison},
  title = {Plum St: Live Digital Storytelling with Remote Browsers},
  pages = {477--478},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178672},
  url = {http://www.nime.org/proceedings/2013/nime2013_281.pdf},
  keywords = {Remote Performance, Network Music, Internet Art, Storytelling},
  abstract = {What is the place for Internet Art within the paradigm of remote musicperformance? In this paper, we discuss techniques for live audiovisualstorytelling through the Web browsers of remote viewers. We focus on theincorporation of socket technology to create a real-time link between performerand audience, enabling manipulation of Web media directly within the eachaudience member's browser. Finally, we describe Plum Street, an onlinemultimedia performance, and suggest that by involving remote performance,appropriating Web media such as Google Maps, social media, and Web Audio intothe work, we can tell stories in a way that more accurately addresses modernlife and holistically fulfills the Web browser's capabilities as a contemporaryperformance instrument.}
}

@inproceedings{Roberts2013a,
  author = {Charles Roberts and Graham Wakefield and Matthew Wright},
  title = {The Web Browser As Synthesizer And Interface},
  pages = {313--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178648},
  url = {http://www.nime.org/proceedings/2013/nime2013_282.pdf},
  keywords = {mobile devices, javascript, browser-based NIMEs, web audio, websockets},
  abstract = {Web technologies provide an incredible opportunity to present new musicalinterfaces to new audiences. Applications written in JavaScript and designed torun in the browser offer remarkable performance, mobile/desktop portability andlongevity due to standardization. Our research examines the use and potentialof native web technologies for musical expression. We introduce two librariestowards this end: Gibberish.js, a heavily optimized audio DSP library, andInterface.js, a GUI toolkit that works with mouse, touch and motion events.Together these libraries provide a complete system for defining musicalinstruments that can be used in both desktop and mobile browsers. Interface.jsalso enables control of remote synthesis applications by including anapplication that translates the socket protocol used by browsers into both MIDIand OSC messages.}
}

@inproceedings{Grosshauser2013,
  author = {Tobias Grosshauser and Gerhard Tr{\''o}ster},
  title = {Finger Position and Pressure Sensing Techniques for String and Keyboard Instruments},
  pages = {479--484},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178538},
  url = {http://www.nime.org/proceedings/2013/nime2013_286.pdf},
  keywords = {Sensor, Piano, Violin, Guitar, Position, Pressure, Keyboard},
  abstract = {Several new technologies to capture motion, gesture and forces for musical instrument players' analyses have been developed in the last years. In research and for augmented instruments one parameter is underrepresented so far. It is finger position and pressure measurement, applied by the musician while playing the musical instrument. In this paper we show a flexible linear-potentiometer and forcesensitive-resistor (FSR) based solution for position, pressure and force sensing between the contact point of the fingers and the musical instrument. A flexible matrix printed circuit board (PCB) is fixed on a piano key. We further introduce linear potentiometer based left hand finger position sensing for string instruments, integrated into a violin and a guitar finger board. Several calibration and measurement scenarios are shown. The violin sensor was evaluated with 13 music students regarding playability and robustness of the system. Main focus was a the integration of the sensors into these two traditional musical instruments as unobtrusively as possible to keep natural haptic playing sensation. The musicians playing the violin in different performance situations stated good playability and no differences in the haptic sensation while playing. The piano sensor is rated, due to interviews after testing it in a conventional keyboard quite unobtrusive, too, but still evokes a different haptic sensation.}
}

@inproceedings{Allison2013,
  author = {Jesse Allison and Yemin Oh and Benjamin Taylor},
  title = {NEXUS: Collaborative Performance for the Masses, Handling Instrument Interface Distribution through the Web},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178461},
  url = {http://www.nime.org/proceedings/2013/nime2013_287.pdf},
  keywords = {NIME, distributed performance systems, Ruby on Rails, collaborative performance, distributed instruments, distributed interface, HTML5, browser based interface},
  abstract = {Distributed performance systems present many challenges to the artist inmanaging performance information, distribution and coordination of interface tomany users, and cross platform support to provide a reasonable level ofinteraction to the widest possible user base.Now that many features of HTML 5 are implemented, powerful browser basedinterfaces can be utilized for distribution across a variety of static andmobile devices. The author proposes leveraging the power of a web applicationto handle distribution of user interfaces and passing interactions via OSC toand from realtime audio/video processing software. Interfaces developed in thisfashion can reach potential performers by distributing a unique user interfaceto any device with a browser anywhere in the world.}
}

@inproceedings{Baldan2013,
  author = {Stefano Baldan and Amalia De G{\''o}tzen and Stefania Serafin},
  title = {Sonic Tennis: a rhythmic interaction game for mobile devices},
  pages = {200--201},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178470},
  url = {http://www.nime.org/proceedings/2013/nime2013_288.pdf},
  keywords = {Audio game, mobile devices, sonic interaction design, rhythmic interaction, motion-based},
  abstract = {This paper presents an audio-based tennis simulation game for mobile devices, which uses motion input and non-verbal audio feedback as exclusive means of interaction. Players have to listen carefully to the provided auditory clues, like racquet hits and ball bounces, rhythmically synchronizing their movements in order to keep the ball into play. The device can be swung freely and act as a full-fledged motionbased controller, as the game does not rely at all on visual feedback and the device display can thus be ignored. The game aims to be entertaining but also effective for educational purposes, such as ear training or improvement of the sense of timing, and enjoyable both by visually-impaired and sighted users.}
}

@inproceedings{Lee2013,
  author = {Sang Won Lee and Jason Freeman},
  title = {echobo : Audience Participation Using The Mobile Music Instrument},
  pages = {450--455},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178594},
  url = {http://www.nime.org/proceedings/2013/nime2013_291.pdf},
  keywords = {mobile music, audience participation, networked instrument},
  abstract = {This work aims at a music piece for large-scale audience participation usingmobile phones as musical instruments at a music performance. Utilizing theubiquity of smart phones, we attempted to accomplish audience engagement bycrafting an accessible musical instrument with which audience can be a part ofthe performance. Drawing lessons learnt from the creative works of mobilemusic, audience participation, and the networked instrument a mobile musicalinstrument application is developed so that audience can download the app atthe concert, play the instrument instantly, interact with other audiencemembers, and contribute to the music by sound generated from their mobilephones. The post-survey results indicate that the instrument was easy to use,and the audience felt connected to the music and other musicians.}
}

@inproceedings{Trento2013,
  author = {Stefano Trento and Stefania Serafin},
  title = {Flag beat: a novel interface for rhythmic musical expression for kids},
  pages = {456--459},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178682},
  url = {http://www.nime.org/proceedings/2013/nime2013_295.pdf},
  keywords = {Sonic toy, children, auditory feedback.},
  abstract = {This paper describes the development of a prototype of a sonic toy forpre-scholar kids. The device, which is a modified version of a footballratchet, is based on the spinning gesture and it allows to experience fourdifferent types of auditory feedback. These algorithms let a kid play withmusic rhythm, generate a continuous sound feedback and control the pitch of apiece of music. An evaluation test of the device has been performed withfourteen kids in a kindergarten. Results and observations showed that kidspreferred the algorithms based on the exploration of the music rhythm and onpitch shifting.}
}

@inproceedings{Place2013,
  author = {Adam Place and Liam Lacey and Thomas Mitchell},
  title = {AlphaSphere},
  pages = {491--492},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178642},
  url = {http://www.nime.org/proceedings/2013/nime2013_300.pdf},
  keywords = {AlphaSphere, MIDI, HID, polyphonic aftertouch, open source},
  abstract = {The AlphaSphere is an electronic musical instrument featuring a series oftactile, pressure sensitive touch pads arranged in a spherical form. It isdesigned to offer a new playing style, while allowing for the expressivereal-time modulation of sound available in electronic-based music. It is alsodesigned to be programmable, enabling the flexibility to map a series ofdifferent notational arrangements to the pad-based interface. The AlphaSphere functions as an HID, MIDI and OSC device, which connects to acomputer and/or independent MIDI device, and its control messages can be mappedthrough the AlphaLive software. Our primary motivations for creating theAlphaSphere are to design an expressive music interface which can exploit thesound palate of synthesizers in a design which allows for the mapping ofnotational arrangements.}
}

@inproceedings{Roberts2013,
  author = {Charles Roberts and Angus Forbes and Tobias H{\''o}llerer},
  title = {Enabling Multimodal Mobile Interfaces for Musical Performance},
  pages = {102--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178646},
  url = {http://www.nime.org/proceedings/2013/nime2013_303.pdf},
  keywords = {Music, mobile, multimodal, interaction},
  abstract = {We present research that extends the scope of the mobile application Control, aprototyping environment for defining multimodal interfaces that controlreal-time artistic and musical performances. Control allows users to rapidlycreate interfaces employing a variety of modalities, including: speechrecognition, computer vision, musical feature extraction, touchscreen widgets,and inertial sensor data. Information from these modalities can be transmittedwirelessly to remote applications. Interfaces are declared using JSON and canbe extended with JavaScript to add complex behaviors, including the concurrentfusion of multimodal signals. By simplifying the creation of interfaces viathese simple markup files, Control allows musicians and artists to make novelapplications that use and combine both discrete and continuous data from thewide range of sensors available on commodity mobile devices.}
}

@inproceedings{Hadjakos2013,
  author = {Aristotelis Hadjakos and Tobias Grosshauser},
  title = {Motion and Synchronization Analysis of Musical Ensembles with the Kinect},
  pages = {106--110},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178540},
  url = {http://www.nime.org/proceedings/2013/nime2013_304.pdf},
  keywords = {Kinect, Ensemble, Synchronization, Strings, Functional Data Analysis, Cross-Correlogram},
  abstract = {Music ensembles have to synchronize themselves with a very high precision inorder to achieve the desired musical results. For that purpose the musicians donot only rely on their auditory perception but also perceive and interpret themovements and gestures of their ensemble colleges. In this paper we present aKinect-based method to analyze ensemble play based on head tracking. We discussfirst experimental results with a violin duo performance.}
}

@inproceedings{Park2013b,
  author = {Saebyul Park and Seonghoon Ban and Dae Ryong Hong and Woon Seung Yeo},
  title = {Sound Surfing Network (SSN): Mobile Phone-based Sound Spatialization with Audience Collaboration},
  pages = {111--114},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178636},
  url = {http://www.nime.org/proceedings/2013/nime2013_305.pdf},
  keywords = {Mobile music, smartphone, audience participation, spatial sound control, digital performance},
  abstract = {SSN (Sound Surfing Network) is a performance system that provides a new musicalexperience by incorporating mobile phone-based spatial sound control tocollaborative music performance. SSN enables both the performer and theaudience to manipulate the spatial distribution of sound using the smartphonesof the audience as distributed speaker system. Proposing a new perspective tothe social aspect music appreciation, SSN will provide a new possibility tomobile music performances in the context of interactive audience collaborationas well as sound spatialization.}
}

@inproceedings{McGee2013,
  author = {Ryan McGee},
  title = {VOSIS: a Multi-touch Image Sonification Interface},
  pages = {460--463},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178604},
  url = {http://www.nime.org/proceedings/2013/nime2013_310.pdf},
  keywords = {image sonification, multi-touch, visual music},
  abstract = {VOSIS is an interactive image sonification interface that creates complexwavetables by raster scanning greyscale image pixel data. Using a multi-touchscreen to play image regions of unique frequency content rather than a linearscale of frequencies, it becomes a unique performance tool for experimental andvisual music. A number of image filters controlled by multi-touch gestures addvariation to the sound palette. On a mobile device, parameters controlled bythe accelerometer add another layer expressivity to the resulting audio-visualmontages.}
}

@inproceedings{Hoste2013,
  author = {Lode Hoste and Beat Signer},
  title = {Expressive Control of Indirect Augmented Reality During Live Music Performances},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178558},
  url = {http://www.nime.org/proceedings/2013/nime2013_32.pdf},
  keywords = {Expressive control, augmented reality, live music performance, 3D gesture recognition, Kinect, declarative language},
  abstract = {Nowadays many music artists rely on visualisations and light shows to enhanceand augment their live performances. However, the visualisation and triggeringof lights is normally scripted in advance and synchronised with the concert,severely limiting the artist's freedom for improvisation, expression and ad-hocadaptation of their show. These scripts result in performances where thetechnology enforces the artist and their music to stay in synchronisation withthe pre-programmed environment. We argue that these limitations can be overcomebased on emerging non-invasive tracking technologies in combination with anadvanced gesture recognition engine.We present a solution that uses explicit gestures and implicit dance moves tocontrol the visual augmentation of a live music performance. We furtherillustrate how our framework overcomes existing limitations of gestureclassification systems by delivering a precise recognition solution based on asingle gesture sample in combination with expert knowledge. The presentedsolution enables a more dynamic and spontaneous performance and, when combinedwith indirect augmented reality, results in a more intense interaction betweenthe artist and their audience.}
}

@inproceedings{Murphy2013,
  author = {Jim Murphy and James McVay and Ajay Kapur and Dale Carnegie},
  title = {Designing and Building Expressive Robotic Guitars},
  pages = {557--562},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178618},
  url = {http://www.nime.org/proceedings/2013/nime2013_36.pdf},
  keywords = {musical robotics, kinetic sculpture, mechatronics},
  abstract = {This paper provides a history of robotic guitars and bass guitars as well as adiscussion of the design, construction, and evaluation of two new roboticinstruments. Throughout the paper, a focus is made on different techniques toextend the expressivity of robotic guitars. Swivel and MechBass, two newrobots, are built and discussed. Construction techniques of likely interest toother musical roboticists are included. These robots use a variety oftechniques, both new and inspired by prior work, to afford composers andperformers with the ability to precisely control pitch and plucking parameters.Both new robots are evaluated to test their precision, repeatability, andspeed. The paper closes with a discussion of the compositional and performativeimplications of such levels of control, and how it might affect humans who wishto interface with the systems.}
}

@inproceedings{Dezfouli2013,
  author = {Erfan Abdi Dezfouli and Edwin van der Heide},
  title = {Notesaaz: a new controller and performance idiom},
  pages = {115--117},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178498},
  url = {http://www.nime.org/proceedings/2013/nime2013_4.pdf},
  keywords = {musical instrument, custom controller, gestural input, dynamic score},
  abstract = {Notesaaz is both a new physical interface meant for musical performance and aproposal for a three-stage process where the controller is used to navigatewithin a graphical score that on its turn controls the sound generation. It canbe seen as a dynamic and understandable way of using dynamic mapping betweenthe sensor input and the sound generation. Furthermore by presenting thegraphical score to both the performer and the audience a new engagement of theaudience can be established.}
}

@inproceedings{Fuhrmann2013,
  author = {Anton Fuhrmann and Johannes Kretz and Peter Burwik},
  title = {Multi Sensor Tracking for Live Sound Transformation},
  pages = {358--362},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178530},
  url = {http://www.nime.org/proceedings/2013/nime2013_44.pdf},
  keywords = {kinect, multi sensor, sensor fusion, open sound control, motion tracking, parameter mapping, live electronics},
  abstract = {This paper demonstrates how to use multiple Kinect(TM) sensors to map aperformers motion to music. We merge skeleton data streams from multiplesensors to compensate for occlusions of the performer. The skeleton jointpositions drive the performance via open sound control data. We discuss how toregister the different sensors to each other and how to smoothly merge theresulting data streams and how to map position data in a general framework tothe live electronics applied to a chamber music ensemble.}
}

@inproceedings{Mudd2013,
  author = {Tom Mudd},
  title = {Feeling for Sound: Mapping Sonic Data to Haptic Perceptions},
  pages = {369--372},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1293003},
  url = {http://www.nime.org/proceedings/2013/nime2013_46.pdf},
  keywords = {Haptics, force feedback, mapping, human-computer interaction},
  abstract = {This paper presents a system for exploring different dimensions of a soundthrough the use of haptic feedback. The Novint Falcon force feedback interfaceis used to scan through soundfiles as a subject moves their hand horizontallyfrom left to right, and to relay information about volume, frequency content,noisiness, or potentially any analysable parameter back to the subject throughforces acting on their hand. General practicalities of mapping sonic elements to physical forces areconsidered, such as the problem of representing detailed data through vaguephysical sensation, approaches to applying forces to the hand that do notinterfering with the smooth operation of the device, and the relative merits ofdiscreet and continuous mappings. Three approaches to generating the forcevector are discussed: 1) the use of simulated detents to identify areas of anaudio parameter over a certain threshold, 2) applying friction proportional tothe level of the audio parameter along the axis of movement, and 3) creatingforces perpendicular to the subject's hand movements.Presentation of audio information in this manner could be beneficial for`pre-feeling' as a method for selecting material to play during a liveperformance, assisting visually impaired audio engineers, and as a generalaugmentation of standard audio editing environments.}
}

@inproceedings{BenAsher2013,
  author = {Matan Ben-Asher and Colby Leider},
  title = {Toward an Emotionally Intelligent Piano: Real-Time Emotion Detection and Performer Feedback via Kinesthetic Sensing in Piano Performance},
  pages = {21--24},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178474},
  url = {http://www.nime.org/proceedings/2013/nime2013_48.pdf},
  keywords = {Motion Sensors, IMUs, Expressive Piano Performance, Machine Learning, Computer Music, Music and Emotion},
  abstract = {A system is presented for detecting common gestures, musical intentions andemotions of pianists in real-time using only kinesthetic data retrieved bywireless motion sensors. The algorithm can detect common Western musicalstructures such as chords, arpeggios, scales, and trills as well as musicallyintended emotions: cheerful, mournful, vigorous, dreamy, lyrical, and humorouscompletely and solely based on low-sample-rate motion sensor data. Thealgorithm can be trained per performer in real-time or can work based onprevious training sets. The system maps the emotions to a color set andpresents them as a flowing emotional spectrum on the background of a pianoroll. This acts as a feedback mechanism for emotional expression as well as aninteractive display of the music. The system was trained and tested on a numberof pianists and it classified structures and emotions with promising results ofup to 92\% accuracy.}
}

@inproceedings{Diakopoulos2013,
  author = {Dimitri Diakopoulos and Ajay Kapur},
  title = {Netpixl: Towards a New Paradigm for Networked Application Development},
  pages = {206--209},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178500},
  url = {http://www.nime.org/proceedings/2013/nime2013_49.pdf},
  keywords = {networking, ubiquitious computing, toolkits, html5},
  abstract = {Netpixl is a new micro-toolkit built to network devices within interactiveinstallations and environments. Using a familiar client-server model, Netpixlcentrally wraps an important aspect of ubiquitous computing: real-timemessaging. In the context of sound and music computing, the role of Netpixl isto fluidly integrate endpoints like OSC and MIDI within a larger multi-usersystem. This paper considers useful design principles that may be applied totoolkits like Netpixl while also emphasizing recent approaches to applicationdevelopment via HTML5 and Javascript, highlighting an evolution in networkedcreative computing.}
}

@inproceedings{Fasciani2013,
  author = {Stefano Fasciani and Lonce Wyse},
  title = {A Self-Organizing Gesture Map for a Voice-Controlled Instrument Interface},
  pages = {507--512},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.4582292},
  url = {http://www.nime.org/proceedings/2013/nime2013_50.pdf},
  keywords = {Self-Organizing Maps, Gestural Controller, Multi Dimensional Control, Unsupervised Gesture Mapping, Voice Control},
  abstract = {Mapping gestures to digital musical instrument parameters is not trivial when the dimensionality of the sensor-captured data is high and the model relating the gesture to sensor data is unknown. In these cases, a front-end processing system for extracting gestural information embedded in the sensor data is essential. In this paper we propose an unsupervised offline method that learns how to reduce and map the gestural data to a generic instrument parameter control space. We make an unconventional use of the Self-Organizing Maps to obtain only a geometrical transformation of the gestural data, while dimensionality reduction is handled separately. We introduce a novel training procedure to overcome two main Self-Organizing Maps limitations which otherwise corrupt the interface usability. As evaluation, we apply this method to our existing Voice-Controlled Interface for musical instruments, obtaining sensible usability improvements.}
}

@inproceedings{Berthaut2013,
  author = {Florent Berthaut and Mark T. Marshall and Sriram Subramanian and Martin Hachet},
  title = {Rouages: Revealing the Mechanisms of Digital Musical Instruments to the Audience},
  pages = {164--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178478},
  url = {http://www.nime.org/proceedings/2013/nime2013_51.pdf},
  keywords = {rouages, digital musical instruments, mappings, 3D interface, mixed-reality,},
  abstract = {Digital musical instruments bring new possibilities for musical performance.They are also more complex for the audience to understand, due to the diversityof their components and the magical aspect of the musicians' actions whencompared to acoustic instruments. This complexity results in a loss of livenessand possibly a poor experience for the audience. Our approach, called Rouages,is based on a mixed-reality display system and a 3D visualization application.It reveals the mechanisms of digital musical instruments by amplifyingmusicians' gestures with virtual extensions of the sensors, by representingthe sound components with 3D shapes and specific behaviors and by showing theimpact ofmusicians gestures on these components. We believe that Rouages opens up newperspectives to help instrument makers and musicians improve audienceexperience with their digital musical instruments.}
}

@inproceedings{Resch2013,
  author = {Thomas Resch},
  title = {note~ for Max --- An extension for Max/MSP for Media Arts \& music},
  pages = {210--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178644},
  url = {http://www.nime.org/proceedings/2013/nime2013_57.pdf},
  keywords = {Max/MSP, composing, timeline, GUI, sequencing, score, notation.},
  abstract = {note~ for Max consists of four objects for the Software Max/MSP which allow sequencing in floating point resolution and provide a Graphical User Interface and a Scripting Interface for generating events within a timeline. Due to the complete integration into Max/MSP it is possible to control almost every type of client like another software, audio and video or extern hardware by note~ or control note~ itself by other software and hardware.}
}

@inproceedings{Han2013,
  author = {Yoonchang Han and Sejun Kwon and Kibeom Lee and Kyogu Lee},
  title = {A Musical Performance Evaluation System for Beginner Musician based on Real-time Score Following},
  pages = {120--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178546},
  url = {http://www.nime.org/proceedings/2013/nime2013_60.pdf},
  keywords = {Music performance analysis, Music education, Real-time score following},
  abstract = {This paper proposes a musical performance feedback system based on real-time audio-score alignment for musical instrument education of beginner musicians. In the proposed system, we do not make use of symbolic data such as MIDI, but acquire a real-time audio input from on-board microphone of smartphone. Then, the system finds onset and pitch of the note from the signal, to align this information with the ground truth musical score. Real-time alignment allows the system to evaluate whether the user played the correct note or not, regardless of its timing, which enables user to play at their own speed, as playing same tempo with original musical score is problematic for beginners. As an output of evaluation, the system notifies the user about which part they are currently performing, and which note were played incorrectly.}
}

@inproceedings{Hindle2013,
  author = {Abram Hindle},
  title = {{SW}ARMED: Captive Portals, Mobile Devices, and Audience Participation in Multi-User Music Performance},
  pages = {174--179},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178550},
  url = {http://www.nime.org/proceedings/2013/nime2013_62.pdf},
  keywords = {Wifi, Smartphone, Audience Interaction, Adoption, Captive Portal, Multi-User, Hotspot},
  abstract = {Audience participation in computer music has long been limited byresources such as sensor technology or the material goods necessary toshare such an instrument. A recent paradigm is to take advantageof the incredible popularity of the smart-phone, a pocket sizedcomputer, and other mobile devices, to provide the audience aninterface into a computer music instrument. In this paper we discuss amethod of sharing a computer music instrument's interface with anaudience to allow them to interact via their smartphone. We propose amethod that is relatively cross-platform and device-agnostic, yetstill allows for a rich user-interactive experience. By emulating acaptive-portal or hotspot we reduce the adoptability issues and configurationproblems facing performers and their audience. We share ourexperiences with this system, as well as an implementation of thesystem itself.}
}

@inproceedings{Park2013,
  author = {Brett Park and David Gerhard},
  title = {Rainboard and Musix: Building dynamic isomorphic interfaces},
  pages = {319--324},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178632},
  url = {http://www.nime.org/proceedings/2013/nime2013_65.pdf},
  keywords = {isomorphic, mobile application, hexagon, keyboard},
  abstract = {Since Euler's development of the Tonnetz in 1739, musicians, composers and instrument designers have been fascinated with the concept of musicalisomorphism, the idea that by arranging tones by their harmonic relationships rather than by their physical properties, the common shapes of musical constructs will appear, facilitating learning and new ways of exploring harmonic spaces. The construction of isomorphic instruments, beyond limited square isomorphisms present in many stringed instruments, has been a challenge in the past for two reasons: The first problem, that of re-arranging note actuators from their sounding elements, has been solved by digital instrument design. The second, more conceptual problem, is that only a single isomorphism can be designed for any one instrument, requiring the instrument designer (as well as composer and performer) to "lock in" to a single isomorphism, or to have a different instrument for each isomorphism in order to experiment. Musix (an iOS application) and Rainboard (a physical device) are two new musical instruments built to overcome this and other limitations of existing isomorphic instruments. Musix was developed to allow experimentation with a wide variety of different isomorphic layouts, to assess the advantages and disadvantages of each. The Rainboard consists of a hexagonal array of arcade buttons embedded with RGB-LEDs, which are used to indicate characteristics of the isomorphism currently in use on the Rainboard. The creation of these two instruments/experimentation platforms allows for isomorphic layouts to be explored in waysthat are not possible with existing instruments.}
}

@inproceedings{ElShimy2013,
  author = {Dalia El-Shimy and Jeremy R. Cooperstock},
  title = {Reactive Environment for Network Music Performance},
  pages = {158--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178506},
  url = {http://www.nime.org/proceedings/2013/nime2013_66.pdf},
  abstract = {For a number of years, musicians in different locations have been able toperform with one another over a network as though present on the same stage.However, rather than attempt to re-create an environment for Network MusicPerformance (NMP) that mimics co-present performance as closely as possible, wepropose focusing on providing musicians with additional controls that can helpincrease the level of interaction between them. To this end, we have developeda reactive environment for distributed performance that provides participantsdynamic, real-time control over several aspects of their performance, enablingthem to change volume levels and experience exaggerated stereo panning. Inaddition, our reactive environment reinforces a feeling of a ``shared space'' between musicians. It differs most notably from standard ventures into thedesign of novel musical interfaces and installations in its reliance onuser-centric methodologies borrowed from the field of Human-ComputerInteraction (HCI). Not only does this research enable us to closely examine thecommunicative aspects of performance, it also allows us to explore newinterpretations of the network as a performance space. This paper describes themotivation and background behind our project, the work that has been undertakentowards its realization and the future steps that have yet to be explored.}
}

@inproceedings{Johnson2013,
  author = {Bridget Johnson and Ajay Kapur},
  title = {MULTI-TOUCH INTERFACES FOR PHANTOM SOURCE POSITIONING IN LIVE SOUND DIFFUSION},
  pages = {213--216},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178570},
  url = {http://www.nime.org/proceedings/2013/nime2013_75.pdf},
  keywords = {Multi touch, diffusion, VBAP, tabletop surface},
  abstract = {This paper presents a new technique for interface-driven diffusion performance. Details outlining the development of a new tabletop surface-based performance interface, named tactile.space, are discussed. User interface and amplitude panning processes employed in the creation of tactile.space are focused upon,and are followed by a user study-based evaluation of the interface. It is hoped that the techniques described in this paper afford performers and composers an enhanced level of creative expression in the diffusion performance practice. This paper introduces and evaluates tactile.space, a multi-touch performance interface for diffusion built on the BrickTable. It describes how tactile.space implements Vector Base Amplitude Panning to achieve real-time source positioning. The final section of this paper presents the findings of a userstudy that was conducted by those who performed with the interface, evaluating the interface as a performance tool with a focus on the increased creative expression the interface affords, and directly comparing it to the traditional diffusion user interface.}
}

@inproceedings{Lui2013,
  author = {Simon Lui},
  title = {A Compact Spectrum-Assisted Human Beatboxing Reinforcement Learning Tool On Smartphone},
  pages = {25--28},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178600},
  url = {http://www.nime.org/proceedings/2013/nime2013_79.pdf},
  keywords = {Audio analysis, music learning tool, reinforcement learning, smartphone app, audio information retrieval.},
  abstract = {Music is expressive and hard to be described by words. Learning music istherefore not a straightforward task especially for vocal music such as humanbeatboxing. People usually learn beatboxing in the traditional way of imitatingaudio sample without steps and instructions. Spectrogram contains a lot ofinformation about audio, but it is too complicated to be understood inreal-time. Reinforcement learning is a psychological method, which makes use ofreward and/or punishment as stimulus to train the decision-making process ofhuman. We propose a novel music learning approach based on the reinforcementlearning method, which makes use of compact and easy-to-read spectruminformation as visual clue to assist human beatboxing learning on smartphone.Experimental result shows that the visual information is easy to understand inreal-time, which improves the effectiveness of beatboxing self-learning.}
}

@inproceedings{Lo2013,
  author = {Kenneth W.K. Lo and Chi Kin Lau and Michael Xuelin Huang and Wai Wa Tang and Grace Ngai and Stephen C.F. Chan},
  title = {Mobile DJ: a Tangible, Mobile Platform for Active and Collaborative Music Listening},
  pages = {217--222},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178598},
  url = {http://www.nime.org/proceedings/2013/nime2013_81.pdf},
  keywords = {Mobile, music, interaction design, tangible user interface},
  abstract = {Mobile DJ is a music-listening system that allows multiple users to interactand collaboratively contribute to a single song over a social network. Activelistening through a tangible interface facilitates users to manipulate musicaleffects, such as incorporating chords or ``scratching'' the record. Acommunication and interaction server further enables multiple users to connectover the Internet and collaborate and interact through their music. User testsindicate that the device is successful at facilitating user immersion into theactive listening experience, and that users enjoy the added sensory input aswell as the novel way of interacting with the music and each other.}
}

@inproceedings{Caramiaux2013,
  author = {Baptiste Caramiaux and Atau Tanaka},
  title = {Machine Learning of Musical Gestures},
  pages = {513--518},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178490},
  url = {http://www.nime.org/proceedings/2013/nime2013_84.pdf},
  keywords = {Machine Learning, Data mining, Musical Expression, Musical Gestures, Analysis, Control, Gesture, Sound},
  abstract = {We present an overview of machine learning (ML) techniques and theirapplication in interactive music and new digital instruments design. We firstgive to the non-specialist reader an introduction to two ML tasks,classification and regression, that are particularly relevant for gesturalinteraction. We then present a review of the literature in current NIMEresearch that uses ML in musical gesture analysis and gestural sound control.We describe the ways in which machine learning is useful for creatingexpressive musical interaction, and in turn why live music performance presentsa pertinent and challenging use case for machine learning.}
}

@inproceedings{Oh2013,
  author = {Jieun Oh and Ge Wang},
  title = {LOLOL: Laugh Out Loud On Laptop},
  pages = {190--195},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178626},
  url = {http://www.nime.org/proceedings/2013/nime2013_86.pdf},
  keywords = {laughter, vocalization, synthesis model, real-time controller, interface for musical expression},
  abstract = {Significant progress in the domains of speech- and singing-synthesis has enhanced communicative potential of machines. To make computers more vocallyexpressive, however, we need a deeper understanding of how nonlinguistic social signals are patterned and perceived. In this paper, we focus on laughter expressions: how a phrase of vocalized notes that we call ''laughter'' may bemodeled and performed to implicate nuanced meaning imbued in the acousticsignal. In designing our model, we emphasize (1) using high-level descriptors as control parameters, (2) enabling real-time performable laughter, and (3) prioritizing expressiveness over realism. We present an interactive systemimplemented in ChucK that allows users to systematically play with the musicalingredients of laughter. A crowd sourced study on the perception of synthesized laughter showed that our model is capable of generating a range of laughter types, suggesting an exciting potential for expressive laughter synthesis.}
}

@inproceedings{Donnarumma2013,
  author = {Marco Donnarumma and Baptiste Caramiaux and Atau Tanaka},
  title = {Muscular Interactions. Combining {EMG} and MMG sensing for musical practice},
  pages = {128--131},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178504},
  url = {http://www.nime.org/proceedings/2013/nime2013_90.pdf},
  keywords = {NIME, sensorimotor system, EMG, MMG, biosignal, multimodal, mapping},
  abstract = {We present the first combined use of the electromyogram (EMG) andmechanomyogram (MMG), two biosignals that result from muscular activity, forinteractive music applications. We exploit differences between these twosignals, as reported in the biomedical literature, to create bi-modalsonification and sound synthesis mappings that allow performers to distinguishthe two components in a single complex arm gesture. We study non-expertplayers' ability to articulate the different modalities. Results show thatpurposely designed gestures and mapping techniques enable novices to rapidlylearn to independently control the two biosignals.}
}

@inproceedings{Honigman2013,
  author = {Colin Honigman and Andrew Walton and Ajay Kapur},
  title = {The Third Room: A {3D} Virtual Music Framework},
  pages = {29--34},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178556},
  url = {http://www.nime.org/proceedings/2013/nime2013_92.pdf},
  keywords = {Kinect Camera, Third Space, Interface, Virtual Reality, Natural Interaction, Robotics, Arduino},
  abstract = {This paper describes a new framework for music creation using 3D audio andvisual techniques. It describes the Third Room, which uses a Kinect to placeusers in a virtual environment to interact with new instruments for musicalexpression. Users can also interact with smart objects, including the Ember(modified mbira digital interface) and the Fluid (a wireless six degrees offreedom and touch controller). This project also includes new techniques for 3Daudio connected to a 3D virtual space using multi-channel speakers anddistributed robotic instruments.}
}

@inproceedings{Wolf2013,
  author = {KatieAnna E Wolf and Rebecca Fiebrink},
  title = {SonNet: A Code Interface for Sonifying Computer Network Data},
  pages = {503--506},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178690},
  url = {http://www.nime.org/proceedings/2013/nime2013_94.pdf},
  keywords = {Sonification, network data, compositional tools},
  abstract = {As any computer user employs the Internet to accomplish everyday activities, a flow of data packets moves across the network, forming their own patterns in response to his or her actions. Artists and sound designers who are interested in accessing that data to make music must currently possess low-level knowledge of Internet protocols and spend signifi-cant effort working with low-level networking code. We have created SonNet, a new software tool that lowers these practical barriers to experimenting and composing with network data. SonNet executes packet-sniffng and network connection state analysis automatically, and it includes an easy-touse ChucK object that can be instantiated, customized, and queried from a user's own code. In this paper, we present the design and implementation of the SonNet system, and we discuss a pilot evaluation of the system with computer music composers. We also discuss compositional applications of SonNet and illustrate the use of the system in an example composition.}
}

@inproceedings{Tahiroglu2013,
  author = {Koray Tahiro{\u g}lu and Nuno N. Correia and Miguel Espada},
  title = {PESI Extended System: In Space, On Body, with 3 Musicians},
  pages = {35--40},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178666},
  url = {http://www.nime.org/proceedings/2013/nime2013_97.pdf},
  keywords = {Affordances, collaboration, social interaction, mobile music, extended system, NIME},
  abstract = {This paper introduces a novel collaborative environment (PESI) in whichperformers are not only free to move and interact with each other but wheretheir social interactions contribute to the sonic outcome. PESI system isdesigned for co-located collaboration and provides embodied and spatialopportunities for musical exploration. To evaluate PESI with skilled musicians,a user-test jam session was conducted. Musicians' comments indicate that thesystem facilitates group interaction finely to bring up further intentions tomusical ideas. Results from our user-test jam session indicate that, through some modificationof the 'in-space' response to the improvisation, and through more intuitiveinteractions with the 'on-body' mobile instruments, we could make thecollaborative music activity a more engaging and active experience. Despitebeing only user-tested once with musicians, the group interview has raisedfruitful discussions on the precise details of the system components.Furthermore, the paradigms of musical interaction and social actions in groupactivities need to be questioned when we seek design requirements for such acollaborative environment. We introduced a system that we believe can open upnew ways of musical exploration in group music activity with a number ofmusicians. The system brings up the affordances of accessible technologieswhile creating opportunities for novel design applications to be explored. Ourresearch proposes further development of the system, focusing on movementbehavior in long-term interaction between performers. We plan to implement thisversion and evaluate design and implementation with distinct skilled musicians.}
}

@inproceedings{Sanganeria2013,
  author = {Mayank Sanganeria and Kurt Werner},
  title = {GrainProc: a real-time granular synthesis interface for live performance},
  pages = {223--226},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2013},
  month = {May},
  publisher = {Graduate School of Culture Technology, KAIST},
  address = {Daejeon, Republic of Korea},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178652},
  url = {http://www.nime.org/proceedings/2013/nime2013_99.pdf},
  keywords = {Granular synthesis, touch screen interface, toe control, real-time, CCRMA},
  abstract = {GrainProc is a touchscreen interface for real-time granular synthesis designedfor live performance. The user provides a real-time audio input (electricguitar, for example) as a granularization source and controls various synthesisparameters with their fingers or toes. The control parameters are designed togive the user access to intuitive and expressive live granular manipulations.}
}

