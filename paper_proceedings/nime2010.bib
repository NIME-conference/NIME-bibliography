@inproceedings{Vallis2010,
  author = {Vallis, Owen and Hochenbaum, Jordan and Kapur, Ajay},
  title = {A Shift Towards Iterative and Open-Source Design for Musical Interfaces},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177919},
  url = {http://www.nime.org/proceedings/2010/nime2010_001.pdf},
  keywords = {Iterative Design, Monome, Arduinome, Arduino.},
  abstract = {The aim of this paper is to define the process of iterative interface design as it pertains to musical performance. Embodying this design approach, the Monome OSC/MIDI USB controller represents a minimalist, open-source hardware device. The open-source nature of the device has allowed for a small group of Monome users to modify the hardware, firmware, and software associated with the interface. These user driven modifications have allowed the re-imagining of the interface for new and novel purposes, beyond even that of the device's original intentions. With development being driven by a community of users, a device can become several related but unique generations of musical controllers, each one focused on a specific set of needs. }
}

@inproceedings{Maruyama2010,
  author = {Maruyama, Yutaro and Takegawa, Yoshinari and Terada, Tsutomu and Tsukamoto, Masahiko},
  title = {UnitInstrument : Easy Configurable Musical Instruments},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177845},
  url = {http://www.nime.org/proceedings/2010/nime2010_007.pdf},
  keywords = {Musical instruments, Script language},
  abstract = {Musical instruments have a long history, and many types of musical instruments have been created to attain ideal sound production. At the same time, various types of electronic musical instruments have been developed. Since the main purpose of conventional electronic instruments is to duplicate the shape of acoustic instruments with no change in their hardware configuration, the diapason and the performance style of each instrument is inflexible. Therefore, the goal of our study is to construct the UnitInstrument that consists of various types of musical units. A unit is constructed by simulating functional elements of conventional musical instruments, such as output timing of sound and pitch decision. Each unit has connectors for connecting other units to create various types of musical instruments. Additionally, we propose a language for easily and flexibly describing the settings of units. We evaluated the effectiveness of our proposed system by using it in actual performances.}
}

@inproceedings{Mulder2010,
  author = {Mulder, Jos},
  title = {The Loudspeaker as Musical Instrument},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177861},
  url = {http://www.nime.org/proceedings/2010/nime2010_013.pdf},
  keywords = {Sound technology (amplification), musical instruments, multi modal perception, performance practice.},
  abstract = {With the author’s own experiences in mind, this paper argues that, when used to amplify musical instruments or to play back other sonic material to an audience, loudspeakers and the technology that drives them, can be considered as a musical instrument. Particularly in situations with acoustic instruments this perspective can provide insight into the often cumbersome relation between the –technology orientated– sound engineer and the –music orientated– performer. Playing a musical instrument (whether acoustic, electric or electronic) involves navigating often complicated but very precise interfaces. The interface for sound amplification technology in a certain environment is not limited to the control surface of a mixing desk but includes the interaction with other stakeholder, i.e. the performers and the choice of loudspeakers and microphones and their positions. As such this interface can be as accurate and intimate but also as complicated as the interfaces of 'normal' musical instruments. By zooming in on differences between acoustic and electronic sources a step is taken towards inclusion in this discussion of the perception of amplified music and the possible influence of that amplification on performance practise.}
}

@inproceedings{Ciglar2010,
  author = {Ciglar, Miha},
  title = {An Ultrasound Based Instrument Generating Audible and Tactile Sound},
  pages = {19--22},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177745},
  url = {http://www.nime.org/proceedings/2010/nime2010_019.pdf},
  keywords = {haptics, vibro-tactility, feedback, ultrasound, hands-free interface, nonlinear acoustics, parametric array.},
  abstract = {This paper, describes the second phase of an ongoing research project dealing with the implementation of an interactive interface. It is a "hands free" instrument, utilizing a non-contact tactile feedback method based on airborne ultrasound. The three main elements/components of the interface that will be discussed in this paper are: 1. Generation of audible sound by self-demodulation of an ultrasound signal during its propagation through air; 2. The condensation of the ultrasound energy in one spatial point generating a precise tactile reproduction of the audible sound; and 3. The feed-forward method enabling a real-time intervention of the musician, by shaping the tactile (ultra)sound directly with his hands.}
}

@inproceedings{Hayes2010,
  author = {Hayes, Ted},
  title = {Neurohedron : A Nonlinear Sequencer Interface},
  pages = {23--25},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177799},
  url = {http://www.nime.org/proceedings/2010/nime2010_023.pdf},
  keywords = {controller, human computer interaction, interface, live performance, neural network, sequencer},
  abstract = {The Neurohedron is a multi-modal interface for a nonlinear sequencer software model, embodied physically in a dodecahedron. The faces of the dodecahedron are both inputs and outputs, allowing the device to visualize the activity of the software model as well as convey input to it. The software model maps MIDI notes to the faces of the device, and defines and controls the behavior of the sequencer's progression around its surface, resulting in a unique instrument for computer-based performance and composition. }
}

@inproceedings{Umetani2010,
  author = {Umetani, Nobuyuki and Mitani, Jun and Igarashi, Takeo},
  title = {Designing Custom-made Metallophone with Concurrent Eigenanalysis},
  pages = {26--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177917},
  url = {http://www.nime.org/proceedings/2010/nime2010_026.pdf},
  keywords = {Modeling Interfaces, Geometric Modeling, CAD, Education, Real-time FEM},
  abstract = {We introduce an interactive interface for the custom designof metallophones. The shape of each plate must be determined in the design process so that the metallophone willproduce the proper tone when struck with a mallet. Unfortunately, the relationship between plate shape and tone iscomplex, which makes it difficult to design plates with arbitrary shapes. Our system addresses this problem by runninga concurrent numerical eigenanalysis during interactive geometry editing. It continuously presents a predicted tone tothe user with both visual and audio feedback, thus makingit possible to design a plate with any desired shape and tone.We developed this system to demonstrate the effectivenessof integrating real-time finite element method analysis intogeometric editing to facilitate the design of custom-mademusical instruments. An informal study demonstrated theability of technically unsophisticated user to apply the system to complex metallophone design.}
}

@inproceedings{Chun2010,
  author = {Chun, Sungkuk and Hawryshkewich, Andrew and Jung, Keechul and Pasquier, Philippe},
  title = {Freepad : A Custom Paper-based MIDI Interface},
  pages = {31--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177743},
  url = {http://www.nime.org/proceedings/2010/nime2010_031.pdf},
  keywords = {Computer vision, form recognition, collision detection, mixed- reality, custom interface, MIDI},
  abstract = {The field of mixed-reality interface design is relatively young and in regards to music, has not been explored in great depth. Using computer vision and collision detection techniques, Freepad further explores the development of mixed-reality interfaces for music. The result is an accessible user-definable MIDI interface for anyone with a webcam, pen and paper, which outputs MIDI notes with velocity values based on the speed of the strikes on drawn pads. }
}

@inproceedings{Mills2010,
  author = {Mills, John A. and Di Fede, Damien and Brix, Nicolas},
  title = {Music Programming in Minim},
  pages = {37--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177855},
  url = {http://www.nime.org/proceedings/2010/nime2010_037.pdf},
  keywords = {Minim, music programming, audio library, Processing, mu- sic software},
  abstract = {Our team realized that a need existed for a music programming interface in the Minim audio library of the Processingprogramming environment. The audience for this new interface would be the novice programmer interested in usingmusic as part of the learning experience, though the interface should also be complex enough to benefit experiencedartist-programmers. We collected many ideas from currently available music programming languages and librariesto design and create the new capabilities in Minim. Thebasic mechanisms include chained unit generators, instruments, and notes. In general, one "patches" unit generators(for example, oscillators, delays, and envelopes) together inorder to create synthesis algorithms. These algorithms canthen either create continuous sound, or be used in instruments to play notes with specific start time and duration.We have written a base set of unit generators to enablea wide variety of synthesis options, and the capabilities ofthe unit generators, instruments, and Processing allow fora wide range of composition techniques.}
}

@inproceedings{Magnusson2010,
  author = {Magnusson, Thor},
  title = {An Epistemic Dimension Space for Musical Devices},
  pages = {43--46},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177837},
  url = {http://www.nime.org/proceedings/2010/nime2010_043.pdf},
  keywords = {Epistemic tools, music theory, dimension space, analysis.},
  abstract = {The analysis of digital music systems has traditionally been characterized by an approach that can be defined as phenomenological. The focus has been on the body and its relationship to the machine, often neglecting the system's conceptual design. This paper brings into focus the epistemic features of digital systems, which implies emphasizing the cognitive, conceptual and music theoretical side of our musical instruments. An epistemic dimension space for the analysis of musical devices is proposed. }
}

@inproceedings{Kocaballi2010,
  author = {Kocaballi, A. Baki and Gemeinboeck, Petra and Saunders, Rob},
  title = {Investigating the Potential for Shared Agency using Enactive Interfaces},
  pages = {47--50},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177829},
  url = {http://www.nime.org/proceedings/2010/nime2010_047.pdf},
  keywords = {Human agency, sensory supplementation, distal perception, sonic feedback, tactile feedback, enactive interfaces},
  abstract = {Human agency, our capacity for action, has been at the hub of discussions centring upon philosophical enquiry for a long period of time. Sensory supplementation devices can provide us with unique opportunities to investigate the different aspects of our agency by enabling new modes of perception and facilitating the emergence of novel interactions, all of which is impossible without the aforesaid devices. Our preliminary study investigates the non-verbal strategies employed for negotiation of our capacity for action with other bodies and the surrounding space through body-to-body and body-to-space couplings enabled by sensory supplementation devices. We employed a lowfi rapid prototyping approach to build this device, enabling distal perception by sonic and haptic feedback. Further, we conducted a workshop in which participants equipped with this device engaged in game-like activities. }
}

@inproceedings{Liebman2010,
  author = {Liebman, Noah and Nagara, Michael and Spiewla, Jacek and Zolkosky, Erin},
  title = {Cuebert : A New Mixing Board Concept for Musical Theatre},
  pages = {51--56},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177833},
  url = {http://www.nime.org/proceedings/2010/nime2010_051.pdf},
  keywords = {audio, control surfaces, mixing board, multitouch, sound, theatre, touch-screen, user-centered design},
  abstract = {We present Cuebert, a mixing board concept for musical theatre. Using a user-centered design process, our goal was to reconceptualize the mixer using modern technology and interaction techniques, questioning over fifty years of interface design in audio technology. Our research resulted in a design that retains the physical controls — faders and knobs — demanded by sound engineers while taking advantage of multitouch display technology to allow for flexible display of dynamic and context-sensitive content.}
}

@inproceedings{Roberts2010,
  author = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn and Putnam, Lance},
  title = {Dynamic Interactivity Inside the AlloSphere},
  pages = {57--62},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177883},
  url = {http://www.nime.org/proceedings/2010/nime2010_057.pdf},
  keywords = {AlloSphere, mapping, performance, HCI, interactivity, Virtual Reality, OSC, multi-user, network},
  abstract = {We present the Device Server, a framework and application driving interaction in the AlloSphere virtual reality environment. The motivation and development of the Device Server stems from the practical concerns of managing multi-user interactivity with a variety of physical devices for disparate performance and virtual reality environments housed in the same physical location. The interface of the Device Server allows users to see how devices are assigned to application functionalities, alter these assignments and save them into configuration files for later use. Configurations defining how applications use devices can be changed on the fly without recompiling or relaunching applications. Multiple applications can be connected to the Device Server concurrently. The Device Server provides several conveniences for performance environments. It can process control data efficiently using Just-In-Time compiled Lua expressions; in doing so it frees processing cycles on audio and video rendering computers. All control signals entering the Device Server can be recorded, saved, and played back allowing performances based on control data to be recreated in their entirety. The Device Server attempts to homogenize the appearance of different control signals to applications so that users can assign any interface element they choose to application functionalities and easily experiment with different control configurations.}
}

@inproceedings{Alt2010,
  author = {Alt, Florian and Shirazi, Alireza S. and Legien, Stefan and Schmidt, Albrecht and Mennen\''{o}h, Julian},
  title = {Creating Meaningful Melodies from Text Messages},
  pages = {63--68},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177713},
  url = {http://www.nime.org/proceedings/2010/nime2010_063.pdf},
  keywords = {instant messaging, sms, sonority, text sonification},
  abstract = {Writing text messages (e.g. email, SMS, instant messaging) is a popular form of synchronous and asynchronous communication. However, when it comes to notifying users about new messages, current audio-based approaches, such as notification tones, are very limited in conveying information. In this paper we show how entire text messages can be encoded into a meaningful and euphonic melody in such a way that users can guess a message’s intention without actually seeing the content. First, as a proof of concept, we report on the findings of an initial on-line survey among 37 musicians and 32 non-musicians evaluating the feasibility and validity of our approach. We show that our representation is understandable and that there are no significant differences between musicians and non-musicians. Second, we evaluated the approach in a real world scenario based on a Skype plug-in. In a field study with 14 participants we showed that sonified text messages strongly impact on the users’ message checking behavior by significantly reducing the time between receiving and reading an incoming message.}
}

@inproceedings{Humphrey2010,
  author = {Humphrey, Tim and Flynn, Madeleine and Stevens, Jesse},
  title = {Epi-thet : A Musical Performance Installation and a Choreography of Stillness},
  pages = {69--71},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177811},
  url = {http://www.nime.org/proceedings/2010/nime2010_069.pdf},
  keywords = {Sonification installation spectator-choreography micro-array ready-mades morphology stillness},
  abstract = {This paper articulates an interest in a kind of interactive musical instrument and artwork that defines the mechanisms for instrumental interactivity from the iconic morphologies of {ready-mades}, casting historical utilitarian objects as the basis for performed musical experiences by spectators. The interactive repertoires are therefore partially pre-determined through enculturated behaviors that are associated with particular objects, but more importantly, inextricably linked to the thematic and meaningful assemblage of the work itself. Our new work epi-thet gathers data from individual interactions with common microscopes placed on platforms within a large space. This data is correlated with public domain genetic datasets obtained from micro-array analysis. A sonification algorithm generates unique compositions associated with the spectator "as measured" through their individual specification in performing an iconic measurement action. The apparatus is a receptacle for unique compositions in sound, and invites a participatory choreography of stillness that is available for reception as a live musical performance. }
}

@inproceedings{Hahnel2010,
  author = {H\''{a}hnel, Tilo},
  title = {From Mozart to {MIDI} : A Rule System for Expressive Articulation},
  pages = {72--75},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177787},
  url = {http://www.nime.org/proceedings/2010/nime2010_072.pdf},
  keywords = {Articulation, Historically Informed Performance, Expres- sive Performance, Synthetic Performance},
  abstract = {The propriety of articulation, especially of notes that lackannotations, is influenced by the origin of the particularmusic. This paper presents a rule system for articulationderived from late Baroque and early Classic treatises on performance. Expressive articulation, in this respect, is understood as a combination of alterable tone features like duration, loudness, and timbre. The model differentiates globalcharacteristics and local particularities, provides a generalframework for human-like music performances, and, therefore, serves as a basis for further and more complex rulesystems.}
}

@inproceedings{Essl2010,
  author = {Essl, Georg and M\''{u}ller, Alexander},
  title = {Designing Mobile Musical Instruments and Environments with urMus},
  pages = {76--81},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177759},
  url = {http://www.nime.org/proceedings/2010/nime2010_076.pdf},
  keywords = {Mobile music making, meta-environment, design, mapping, user interface},
  abstract = {We discuss how the environment urMus was designed to allow creation of mobile musical instruments on multi-touch smartphones. The design of a mobile musical instrument consists of connecting sensory capabilities to output modalities through various means of processing. We describe how the default mapping interface was designed which allows to set up such a pipeline and how visual and interactive multi-touch UIs for musical instruments can be designed within the system. }
}

@inproceedings{Oh2010,
  author = {Oh, Jieun and Herrera, Jorge and Bryan, Nicholas J. and Dahl, Luke and Wang, Ge},
  title = {Evolving The Mobile Phone Orchestra},
  pages = {82--87},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177871},
  url = {http://www.nime.org/proceedings/2010/nime2010_082.pdf},
  keywords = {mobile phone orchestra, live performance, iPhone, mobile music},
  abstract = {In this paper, we describe the development of the Stanford Mobile Phone Orchestra (MoPhO) since its inceptionin 2007. As a newly structured ensemble of musicians withiPhones and wearable speakers, MoPhO takes advantageof the ubiquity and mobility of smartphones as well asthe unique interaction techniques offered by such devices.MoPhO offers a new platform for research, instrument design, composition, and performance that can be juxtaposedto that of a laptop orchestra. We trace the origins of MoPhO,describe the motivations behind the current hardware andsoftware design in relation to the backdrop of current trendsin mobile music making, detail key interaction conceptsaround new repertoire, and conclude with an analysis onthe development of MoPhO thus far.}
}

@inproceedings{Tanaka2010,
  author = {Tanaka, Atau},
  title = {Mapping Out Instruments, Affordances, and Mobiles},
  pages = {88--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177903},
  url = {http://www.nime.org/proceedings/2010/nime2010_088.pdf},
  keywords = {Musical affordance, NIME, mapping, instrument definition, mobile, multimodal interaction.},
  abstract = {This paper reviews and extends questions of the scope of an interactive musical instrument and mapping strategies for expressive performance. We apply notions of embodiment and affordance to characterize gestural instruments. We note that the democratization of sensor technology in consumer devices has extended the cultural contexts for interaction. We revisit questions of mapping drawing upon the theory of affordances to consider mapping and instrument together. This is applied to recent work by the author and his collaborators in the development of instruments based on mobile devices designed for specific performance situations.}
}

@inproceedings{Havryliv2010,
  author = {Havryliv, Mark},
  title = {Composing For Improvisation with Chaotic Oscillators},
  pages = {94--99},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177795},
  url = {http://www.nime.org/proceedings/2010/nime2010_094.pdf},
  keywords = {chaos and music, chaotic dynamics and oscillators, differential equations and music, mathematica, audio descriptors and mpeg-7},
  abstract = {This paper describes a novel method for composing andimprovisation with real-time chaotic oscillators. Recentlydiscovered algebraically simple nonlinear third-order differential equations are solved and acoustical descriptors relating to their frequency spectrums are determined accordingto the MPEG-7 specification. A second nonlinearity is thenadded to these equations: a real-time audio signal. Descriptive properties of the complex behaviour of these equationsare then determined as a function of difference tones derived from a Just Intonation scale and the amplitude ofthe audio signal. By using only the real-time audio signalfrom live performer/s as an input the causal relationshipbetween acoustic performance gestures and computer output, including any visual or performer-instruction output,is deterministic even if the chaotic behaviours are not.}
}

@inproceedings{Hawryshkewich2010,
  author = {Hawryshkewich, Andrew and Pasquier, Philippe and Eigenfeldt, Arne},
  title = {Beatback : A Real-time Interactive Percussion System for Rhythmic Practise and Exploration},
  pages = {100--105},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177797},
  url = {http://www.nime.org/proceedings/2010/nime2010_100.pdf},
  keywords = {Interactive music interface, real-time, percussion, machine learning, Markov models, MIDI.},
  abstract = {Traditional drum machines and digital drum-kits offer users the ability to practice or perform with a supporting ensemble – such as a bass, guitar and piano – but rarely provide support in the form of an accompanying percussion part. Beatback is a system which develops upon this missing interaction through offering a MIDI enabled drum system which learns and plays in the user's style. In the contexts of rhythmic practise and exploration, Beatback looks at call-response and accompaniment models of interaction to enable new possibilities for rhythmic creativity.}
}

@inproceedings{Gurevich2010,
  author = {Gurevich, Michael and Stapleton, Paul and Marquez-Borbon, Adnan},
  title = {Style and Constraint in Electronic Musical Instruments},
  pages = {106--111},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177785},
  url = {http://www.nime.org/proceedings/2010/nime2010_106.pdf},
  keywords = {design, interaction, performance, persuasive technology},
  abstract = {A qualitative study to investigate the development of stylein performance with a highly constrained musical instrument is described. A new one-button instrument was designed, with which several musicians were each asked topractice and develop a solo performance. Observations oftrends in attributes of these performances are detailed in relation to participants' statements in structured interviews.Participants were observed to develop stylistic variationsboth within the domain of activities suggested by the constraint, and by discovering non-obvious techniques througha variety of strategies. Data suggest that stylistic variationsoccurred in spite of perceived constraint, but also becauseof perceived constraint. Furthermore, participants tendedto draw on unique experiences, approaches and perspectivesthat shaped individual performances.}
}

@inproceedings{Choi2010,
  author = {Choi, Hongchan and Wang, Ge},
  title = {LUSH : An Organic Eco + Music System},
  pages = {112--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177741},
  url = {http://www.nime.org/proceedings/2010/nime2010_112.pdf},
  keywords = {algorithmic composition,audiovisual,automata,behavior simulation,music,music sequencer,musical interface,nime10,visualization},
  abstract = {We propose an environment that allows users to create music by leveraging playful visualization and organic interaction. Our attempt to improve ideas drawn from traditional sequencer paradigm has been made in terms of extemporizing music and associating with visualization in real-time. In order to offer different user experience and musical possibility, this system incorporates many techniques, including; flocking simulation, nondeterministic finite automata (NFA), score file analysis, vector calculation, OpenGL animation, and networking. We transform a sequencer into an audiovisual platform for composition and performance, which is furnished with artistry and ease of use. Thus we believe that it is suitable for not only artists such as algorithmic composers or audiovisual performers, but also anyone who wants to play music and imagery in a different way. }
}

@inproceedings{Yamaguchi2010,
  author = {Yamaguchi, Tomoyuki and Kobayashi, Tsukasa and Ariga, Anna and Hashimoto, Shuji},
  title = {TwinkleBall : A Wireless Musical Interface for Embodied Sound Media},
  pages = {116--119},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177927},
  url = {http://www.nime.org/proceedings/2010/nime2010_116.pdf},
  keywords = {Musical Interface, Embodied Sound Media, Dance Performance.},
  abstract = {In this paper, we introduce a wireless musical interface driven by grasping forces and human motion. The sounds generated by the traditional digital musical instruments are dependent on the physical shape of the musical instruments. The freedom of the musical performance is restricted by its structure. Therefore, the sounds cannot be generated with the body expression like the dance. We developed a ball-shaped interface, TwinkleBall, to achieve the free-style performance. A photo sensor is embedded in the translucent rubber ball to detect the grasping force of the performer. The grasping force is translated into the luminance intensity for processing. Moreover, an accelerometer is also embedded in the interface for motion sensing. By using these sensors, a performer can control the note and volume by varying grasping force and motion respectively. The features of the proposed interface are ball-shaped, wireless, and handheld size. As a result, the proposed interface is able to generate the sound from the body expression such as dance. }
}

@inproceedings{Cannon2010,
  author = {Cannon, Joanne and Favilla, Stuart},
  title = {Expression and Spatial Motion : Playable Ambisonics},
  pages = {120--124},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177735},
  url = {http://www.nime.org/proceedings/2010/nime2010_120.pdf},
  keywords = {ambisonics, augmented instruments, expressive spatial motion, playable instruments},
  abstract = {This paper presents research undertaken by the Bent Leather Band investigating the application of live Ambisonics to large digital-instrument ensemble improvisation. Their playable approach to live ambisonic projection is inspired by the work of Trevor Wishart and presents a systematic investigation of the potential for live spatial motion improvisation.}
}

@inproceedings{Collins2010,
  author = {Collins, Nick},
  title = {Contrary Motion : An Oppositional Interactive Music System},
  pages = {125--129},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177747},
  url = {http://www.nime.org/proceedings/2010/nime2010_125.pdf},
  keywords = {contrary, beat tracking, stream analysis, musical agent},
  abstract = {The hypothesis of this interaction research project is that it can be stimulating for experimental musicians to confront a system which ‘opposes’ their musical style. The ‘contrary motion’ of the title is the name of a MIDI-based realtime musical software agent which uses machine listening to establish the musical context, and thereby chooses its own responses to differentiate its position from that of its human interlocutant. To do this requires a deep consideration of the space of musical actions, so as to explicate what opposition should constitute, and machine listening technology (most prominently represented by new online beat and stream tracking algorithms) which gives an accurate measurement of player position so as to consistently avoid it. An initial pilot evaluation was undertaken, feeding back critical data to the developing design.}
}

@inproceedings{Deleflie2010,
  author = {Deleflie, Etienne and Schiemer, Greg},
  title = {Images as Spatial Sound Maps},
  pages = {130--135},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177753},
  url = {http://www.nime.org/proceedings/2010/nime2010_130.pdf},
  keywords = {Spatial audio, surround sound, ambisonics, granular synthesis, decorrelation, diffusion.},
  abstract = {The tools for spatial composition typically model just a small subset of the spatial audio cues known to researchers. As composers explore this medium it has become evident that the nature of spatial sound perception is complex. Yet interfaces for spatial composition are often simplistic and the end results can be disappointing. This paper presents an interface that is designed to liberate the composer from thinking of spatialised sound as points in space. Instead, visual images are used to define sound in terms of shape, size and location. Images can be sequenced into video, thereby creating rich and complex temporal soundscapes. The interface offers both the ability to craft soundscapes and also compose their evolution in time. }
}

@inproceedings{Schlei2010,
  author = {Schlei, Kevin},
  title = {Relationship-Based Instrument Mapping of Multi-Point Data Streams Using a Trackpad Interface},
  pages = {136--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177891},
  url = {http://www.nime.org/proceedings/2010/nime2010_136.pdf},
  keywords = {Multi-point, multi-touch interface, instrument mapping, multi-point data analysis, trackpad instrument},
  abstract = {Multi-point devices are rapidly becoming a practical interface choice for electronic musicians. Interfaces that generate multiple simultaneous streams of point data present a unique mapping challenge. This paper describes an analysis system for point relationships that acts as a bridge between raw streams of multi-point data and the instruments they control, using a multipoint trackpad to test various configurations. The aim is to provide a practical approach for instrument programmers working with multi-point tools, while highlighting the difference between mapping systems based on point coordinate streams, grid evaluations, or object interaction and mapping systems based on multi-point data relationships. }
}

@inproceedings{Wyse2010,
  author = {Wyse, Lonce and Duy, Nguyen D.},
  title = {Instrumentalizing Synthesis Models},
  pages = {140--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177925},
  url = {http://www.nime.org/proceedings/2010/nime2010_140.pdf},
  keywords = {Musical interface, parameter mapping, expressive control.},
  abstract = {An important part of building interactive sound models is designing the interface and control strategy. The multidimensional structure of the gestures natural for a musical or physical interface may have little obvious relationship to the parameters that a sound synthesis algorithm exposes for control. A common situation arises when there is a nonlinear synthesis technique for which a traditional instrumental interface with quasi-independent control of pitch and expression is desired. This paper presents a semi-automatic meta-modeling tool called the Instrumentalizer for embedding arbitrary synthesis algorithms in a control structure that exposes traditional instrument controls for pitch and expression. }
}

@inproceedings{Cassinelli2010,
  author = {Cassinelli, Alavaro and Kuribara, Yusaku and Zerroug, Alexis and Ishikawa, Masatoshi and Manabe, Daito},
  title = {scoreLight : Playing with a Human-Sized Laser Pick-Up},
  pages = {144--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177739},
  url = {http://www.nime.org/proceedings/2010/nime2010_144.pdf},
  keywords = {H5.2 [User Interfaces] interaction styles / H.5.5 [Sound and Music Computing] Methodologies and techniques / J.5 [Arts and Humanities] performing arts},
  abstract = {scoreLight is a playful musical instrument capable of generating sound from the lines of drawings as well as from theedges of three-dimensional objects nearby (including everyday objects, sculptures and architectural details, but alsothe performer's hands or even the moving silhouettes ofdancers). There is no camera nor projector: a laser spotexplores shapes as a pick-up head would search for soundover the surface of a vinyl record --- with the significant difference that the groove is generated by the contours of thedrawing itself.}
}

@inproceedings{Yerkes2010,
  author = {Yerkes, Karl and Shear, Greg and Wright, Matthew},
  title = {Disky : a DIY Rotational Interface with Inherent Dynamics},
  pages = {150--155},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177929},
  url = {http://www.nime.org/proceedings/2010/nime2010_150.pdf},
  keywords = {turntable, dial, encoder, re-purposed, hard drive, scratch-ing, inherent dynamics, DIY},
  abstract = {Disky is a computer hard drive re-purposed into a do-it-yourself USB turntable controller that offers high resolution and low latency for controlling parameters of multimedia performance software. Disky is a response to the challenge “re-purpose something that is often discarded and share it with the do-it-yourself community to promote reuse!”}
}

@inproceedings{Solis2010,
  author = {Solis, Jorge and Petersen, Klaus and Yamamoto, Tetsuro and Takeuchi, Masaki and Ishikawa, Shimpei and Takanishi, Atsuo and Hashimoto, Kunimatsu},
  title = {Development of the Waseda Saxophonist Robot and Implementation of an Auditory Feedback Control},
  pages = {156--161},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177897},
  url = {http://www.nime.org/proceedings/2010/nime2010_156.pdf},
  keywords = {Humanoid Robot, Auditory Feedback, Music, Saxophone.},
  abstract = {Since 2007, our research is related to the development of an anthropomorphic saxophonist robot, which it has been designed to imitate the saxophonist playing by mechanically reproducing the organs involved for playing a saxophone. Our research aims in understanding the motor control from an engineering point of view and enabling the communication. In this paper, the Waseda Saxophone Robot No. 2 (WAS-2) which is composed by 22-DOFs is detailed. The lip mechanism of WAS-2 has been designed with 3-DOFs to control the motion of the lower, upper and sideway lips. In addition, a human-like hand (16 DOF-s) has been designed to enable to play all the keys of the instrument. Regarding the improvement of the control system, a feed-forward control system with dead-time compensation has been implemented to assure the accurate control of the air pressure. In addition, the implementation of an auditory feedback control system has been proposed and implemented in order to adjust the positioning of the physical parameters of the components of the robot by providing a pitch feedback and defining a recovery position (off-line). A set of experiments were carried out to verify the mechanical design improvements and the dynamic response of the air pressure. As a result, the range of sound pressure has been increased and the proposed control system improved the dynamic response of the air pressure control. }
}

@inproceedings{Kapur2010,
  author = {Kapur, Ajay and Darling, Michael},
  title = {A Pedagogical Paradigm for Musical Robotics},
  pages = {162--165},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177821},
  url = {http://www.nime.org/proceedings/2010/nime2010_162.pdf},
  keywords = {dartron,digital classroom,laptop orchestra,machine orchestra,musical robotics,nime pedagogy,nime10,solenoid},
  abstract = {This paper describes the making of a class to teach the history and art of musical robotics. The details of the curriculum are described as well as designs for our custom schematics for robotic solenoid driven percussion. This paper also introduces four new robotic instruments that were built during the term of this course. This paper also introduces the Machine Orchestra, a laptop orchestra with ten human performers and our five robotic instruments. }
}

@inproceedings{Pan2010,
  author = {Pan, Ye and Kim, Min-Gyu and Suzuki, Kenji},
  title = {A Robot Musician Interacting with a Human Partner through Initiative Exchange},
  pages = {166--169},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177875},
  url = {http://www.nime.org/proceedings/2010/nime2010_166.pdf},
  keywords = {Human-robot interaction, initiative exchange, prediction},
  abstract = {This paper proposes a novel method to realize an initiativeexchange for robot. A humanoid robot plays vibraphone exchanging initiative with a human performer by perceivingmultimodal cues in real time. It understands the initiative exchange cues through vision and audio information.In order to achieve the natural initiative exchange betweena human and a robot in musical performance, we built thesystem and the software architecture and carried out the experiments for fundamental algorithms which are necessaryto the initiative exchange.}
}

@inproceedings{Bukvic2010,
  author = {Bukvic, Ivika and Martin, Thomas and Standley, Eric and Matthews, Michael},
  title = {Introducing L2Ork : Linux Laptop Orchestra},
  pages = {170--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177731},
  url = {http://www.nime.org/proceedings/2010/nime2010_170.pdf},
  keywords = {l2ork,laptop orchestra,linux,nime10},
  abstract = {Virginia Tech Department of Music’s Digital Interactive Sound & Intermedia Studio in collaboration with the College of Engineering and School of Visual Arts presents the latest addition to the *Ork family, the Linux Laptop Orchestra. Apart from maintaining compatibility with its precursors and sources of inspiration, Princeton’s PLOrk, and Stanford’s SLOrk, L2Ork’s particular focus is on delivering unprecedented affordability without sacrificing quality, as well as flexibility necessary to encourage a more widespread adoption and standardization of the laptop orchestra ensemble. The newfound strengths of L2Ork’s design have resulted in opportunities in K-12 education with a particular focus on cross-pollinating STEM and Arts, as well as research of an innovative content delivery system that can seamlessly engage students regardless of their educational background. In this document we discuss key components of the L2Ork initiative, their benefits, and offer resources necessary for the creation of other Linux-based *Orks}
}

@inproceedings{Bryan2010,
  author = {Bryan, Nicholas J. and Herrera, Jorge and Oh, Jieun and Wang, Ge},
  title = {MoMu : A Mobile Music Toolkit},
  pages = {174--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177725},
  url = {http://www.nime.org/proceedings/2010/nime2010_174.pdf},
  keywords = {instrument design, iPhone, mobile music, software develop- ment, toolkit},
  abstract = {The Mobile Music (MoMu) toolkit is a new open-sourcesoftware development toolkit focusing on musical interaction design for mobile phones. The toolkit, currently implemented for iPhone OS, emphasizes usability and rapidprototyping with the end goal of aiding developers in creating real-time interactive audio applications. Simple andunified access to onboard sensors along with utilities forcommon tasks found in mobile music development are provided. The toolkit has been deployed and evaluated in theStanford Mobile Phone Orchestra (MoPhO) and serves asthe primary software platform in a new course exploringmobile music.}
}

@inproceedings{Dahl2010,
  author = {Dahl, Luke and Wang, Ge},
  title = {Sound Bounce : Physical Metaphors in Designing Mobile Music Performance},
  pages = {178--181},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177751},
  url = {http://www.nime.org/proceedings/2010/nime2010_178.pdf},
  keywords = {Mobile music, design, metaphor, performance, gameplay.},
  abstract = {The use of metaphor has a prominent role in HCI, both as a device to help users understand unfamiliar technologies, and as a tool to guide the design process. Creators of new computerbased instruments face similar design challenges as those in HCI. In the course of creating a new piece for Mobile Phone Orchestra we propose the metaphor of a sound as a ball and explore the interactions and sound mappings it suggests. These lead to the design of a gesture-controlled instrument that allows players to "bounce" sounds, "throw" them to other players, and compete in a game to "knock out" others' sounds. We composed the piece SoundBounce based on these interactions, and note that audiences seem to find performances of the piece accessible and engaging, perhaps due to the visibility of the metaphor. }
}

@inproceedings{Essl2010a,
  author = {Essl, Georg and Rohs, Michael and Kratz, Sven},
  title = {Use the Force (or something) --- Pressure and Pressure --- Like Input for Mobile Music Performance},
  pages = {182--185},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177761},
  url = {http://www.nime.org/proceedings/2010/nime2010_182.pdf},
  keywords = {Force, impact, pressure, multi-touch, mobile phone, mobile music making.},
  abstract = {Impact force is an important dimension for percussive musical instruments such as the piano. We explore three possible mechanisms how to get impact forces on mobile multi-touch devices: using built-in accelerometers, the pressure sensing capability of Android phones, and external force sensing resistors. We find that accelerometers are difficult to control for this purpose. Android's pressure sensing shows some promise, especially when combined with augmented playing technique. Force sensing resistors can offer good dynamic resolution but this technology is not currently offered in commodity devices and proper coupling of the sensor with the applied impact is difficult. }
}

@inproceedings{Mills2010a,
  author = {Mills, Roger},
  title = {Dislocated Sound : A Survey of Improvisation in Networked Audio Platforms},
  pages = {186--191},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177857},
  url = {http://www.nime.org/proceedings/2010/nime2010_186.pdf},
  keywords = {improvisation, internet audio, networked collaboration, sound art},
  abstract = {The evolution of networked audio technologies has created unprecedented opportunities for musicians to improvise with instrumentalists from a diverse range of cultures and disciplines. As network speeds increase and latency is consigned to history, tele-musical collaboration, and in particular improvisation will be shaped by new methodologies that respond to this potential. While networked technologies eliminate distance in physical space, for the remote improviser, this creates a liminality of experience through which their performance is mediated. As a first step in understanding the conditions arising from collaboration in networked audio platforms, this paper will examine selected case studies of improvisation in a variety of networked interfaces. The author will examine how platform characteristics and network conditions influence the process of collective improvisation and the methodologies musicians are employing to negotiate their networked experiences.}
}

@inproceedings{Berthaut2010,
  author = {Berthaut, Florent and Desainte-Catherine, Myriam and Hachet, Martin},
  title = {DRILE : An Immersive Environment for Hierarchical Live-Looping},
  pages = {192--197},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177721},
  url = {http://www.nime.org/proceedings/2010/nime2010_192.pdf},
  keywords = {Drile, immersive instrument, hierarchical live-looping, 3D interac- tion},
  abstract = {We present Drile, a multiprocess immersive instrument built uponthe hierarchical live-looping technique and aimed at musical performance. This technique consists in creating musical trees whosenodes are composed of sound effects applied to a musical content.In the leaves, this content is a one-shot sound, whereas in higherlevel nodes this content is composed of live-recorded sequencesof parameters of the children nodes. Drile allows musicians tointeract efficiently with these trees in an immersive environment.Nodes are represented as worms, which are 3D audiovisual objects. Worms can be manipulated using 3D interaction techniques,and several operations can be applied to the live-looping trees. Theenvironment is composed of several virtual rooms, i.e. group oftrees, corresponding to specific sounds and effects. Learning Drileis progressive since the musical control complexity varies according to the levels in live-looping trees. Thus beginners may havelimited control over only root worms while still obtaining musically interesting results. Advanced users may modify the trees andmanipulate each of the worms.}
}

@inproceedings{Fencott2010,
  author = {Fencott, Robin and Bryan-Kinns, Nick},
  title = {Hey Man, You're Invading my Personal Space ! Privacy and Awareness in Collaborative Music},
  pages = {198--203},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177763},
  url = {http://www.nime.org/proceedings/2010/nime2010_198.pdf},
  keywords = {Awareness, Privacy, Collaboration, Music, Interaction, En- gagement, Group Music Making, Design, Evaluation.},
  abstract = {This research is concerned with issues of privacy, awareness and the emergence of roles in the process of digitallymediated collaborative music making. Specifically we areinterested in how providing collaborators with varying degrees of privacy and awareness of one another influencesthe group interaction. A study is presented whereby ninegroups of co-located musicians compose music together using three different interface designs. We use qualitative andquantitative data to study and characterise the musician'sinteraction with each other and the software. We show thatwhen made available to them, participants make extensiveuse of a private working area to develop musical contributions before they are introduced to the group. We also arguethat our awareness mechanisms change the perceived quality of the musical interaction, but have no impact on theway musicians interact with the software. We then reflecton implications for the design of new collaborative musicmaking tools which exploit the potential of digital technologies, while at the same time support creative musicalinteraction.}
}

@inproceedings{Martin2010a,
  author = {Martin, Charles and Forster, Benjamin and Cormick, Hanna},
  title = {Cross-Artform Performance Using Networked Interfaces : Last Man to Die's Vital LMTD},
  pages = {204--207},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177843},
  url = {http://www.nime.org/proceedings/2010/nime2010_204.pdf},
  keywords = {cross-artform performance, networked performance, physi- cal computing},
  abstract = {In 2009 the cross artform group, Last Man to Die, presenteda series of performances using new interfaces and networkedperformance to integrate the three artforms of its members(actor, Hanna Cormick, visual artist, Benjamin Forster andpercussionist, Charles Martin). This paper explains ourartistic motivations and design for a computer vision surfaceand networked heartbeat sensor as well as the experience ofmounting our first major work, Vital LMTD.}
}

@inproceedings{Jensenius2010,
  author = {Jensenius, Alexander R. and Innervik, Kjell Tore and Frounberg, Ivar},
  title = {Evaluating the Subjective Effects of Microphone Placement on Glass Instruments},
  pages = {208--211},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177817},
  url = {http://www.nime.org/proceedings/2010/nime2010_208.pdf},
  keywords = {glass instruments, microphone placement, sound analysis},
  abstract = {We report on a study of perceptual and acoustic featuresrelated to the placement of microphones around a custommade glass instrument. Different microphone setups weretested: above, inside and outside the instrument and at different distances. The sounds were evaluated by an expertperformer, and further qualitative and quantitative analyses have been carried out. Preference was given to therecordings from microphones placed close to the rim of theinstrument, either from the inside or the outside.}
}

@inproceedings{Quintas2010,
  author = {Quintas, Rudolfo},
  title = {Glitch Delighter : Lighter's Flame Base Hyper-Instrument for Glitch Music in Burning The Sound Performance},
  pages = {212--216},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177879},
  url = {http://www.nime.org/proceedings/2010/nime2010_212.pdf},
  keywords = {Hyper-Instruments, Glitch Music, Interactive Systems, Electronic Music Performance.},
  abstract = {Glitch DeLighter is a HyperInstrument conceived for Glitch music, based on the idea of using fire expressiveness to digitally distort sound, pushing the body and primitive ritualism into a computer mediated sound performance. Glitch DeLighter uses ordinary lighters as physical controllers that can be played by creating a flame and moving it in the air. Droned sounds are played by sustaining the flame and beats by generating sparks and fast flames. The pitch of every sound can be changed moving the flame vertically in the air. This is achieved by using a custom computer vision system as an interface which maps the real-time the data extracted from the flame and transmits those parameters to the sound generator. As a result, the flame visual dynamics are deeply connected to the aural perception of the sound - ‘the sound seems to be burning’. This process establishes a metaphor dramaturgically engaging for an audience. This paper contextualizes the glitch music aesthetics, prior research, the design and development of the instrument and reports on Burning The Sound– the first music composition created and performed with the instrument (by the author).}
}

@inproceedings{McPherson2010,
  author = {McPherson, Andrew and Kim, Youngmoo},
  title = {Augmenting the Acoustic Piano with Electromagnetic String Actuation and Continuous Key Position Sensing},
  pages = {217--222},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177849},
  url = {http://www.nime.org/proceedings/2010/nime2010_217.pdf},
  keywords = {Augmented instruments, piano, interfaces, electromagnetic actuation, gesture measurement},
  abstract = {This paper presents the magnetic resonator piano, an augmented instrument enhancing the capabilities of the acoustic grand piano. Electromagnetic actuators induce the stringsto vibration, allowing each note to be continuously controlled in amplitude, frequency, and timbre without external loudspeakers. Feedback from a single pickup on thepiano soundboard allows the actuator waveforms to remainlocked in phase with the natural motion of each string. Wealso present an augmented piano keyboard which reportsthe continuous position of every key. Time and spatial resolution are sufficient to capture detailed data about keypress, release, pretouch, aftertouch, and other extended gestures. The system, which is designed with cost and setupconstraints in mind, seeks to give pianists continuous control over the musical sound of their instrument. The instrument has been used in concert performances, with theelectronically-actuated sounds blending with acoustic instruments naturally and without amplification.}
}

@inproceedings{Grossmann2010,
  author = {Grossmann, Cesar M.},
  title = {Developing a Hybrid Contrabass Recorder Resistances, Expression, Gestures and Rhetoric},
  pages = {223--228},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177781},
  url = {http://www.nime.org/proceedings/2010/nime2010_223.pdf},
  keywords = {live processing,new instruments,nime10,recorder},
  abstract = {In this paper I describe aspects that have been involved in my experience of developing a hybrid instrument. The process of transformation and extension of the instrument is informed by ideas concerning the intrinsic communication aspects of musical activities. Decisions taken for designing the instrument and performing with it take into account the hypothesis that there are ontological levels of human reception in music that are related to the intercorporeal. Arguing that it is necessary to encounter resistances for achieving expression, it is suggested that new instrumental development ought to reflect on the concern for keeping the natural connections of live performances. }
}

@inproceedings{Carrillo2010,
  author = {Carrillo, Alfonso P. and Bonada, Jordi},
  title = {The Bowed Tube : a Virtual Violin},
  pages = {229--232},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177737},
  url = {http://www.nime.org/proceedings/2010/nime2010_229.pdf},
  keywords = {violin, synthesis, control, spectral, virtual},
  abstract = {This paper presents a virtual violin for real-time performances consisting of two modules: a violin spectral model and a control interface. The interface is composed by a sensing bow and a tube with drawn strings in substitution of a real violin. The spectral model is driven by the bowing controls captured with the control interface and it is able to predict spectral envelopes of the sound corresponding to those controls. The envelopes are filled with harmonic andnoisy content and given to an additive synthesizer in order to produce violin sounds. The sensing system is based on two motion trackers with 6 degrees of freedom. One tracker is attached to the bow and the other to the tube. Bowing controls are computed after a calibration process where the position of virtual strings and the hair-ribbon of the bowis obtained. A real time implementation was developed asa MAX/MSP patch with external objects for each of the modules.}
}

@inproceedings{Hochenbaum2010,
  author = {Hochenbaum, Jordan and Kapur, Ajay and Wright, Matthew},
  title = {Multimodal Musician Recognition},
  pages = {233--237},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177805},
  url = {http://www.nime.org/proceedings/2010/nime2010_233.pdf},
  keywords = {Performer Recognition, Multimodal, HCI, Machine Learning, Hyperinstrument, eSitar},
  abstract = {This research is an initial effort in showing how a multimodal approach can improve systems for gaining insight into a musician's practice and technique. Embedding a variety of sensors inside musical instruments and synchronously recording the sensors' data along with audio, we gather a database of gestural information from multiple performers, then use machine-learning techniques to recognize which musician is performing. Our multimodal approach (using both audio and sensor data) yields promising performer classification results, which we see as a first step in a larger effort to gain insight into musicians' practice and technique. }
}

@inproceedings{Guaus2010,
  author = {Guaus, Enric and Ozaslan, Tan and Palacios, Eric and Arcos, Josep L.},
  title = {A Left Hand Gesture Caption System for Guitar Based on Capacitive Sensors},
  pages = {238--243},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177783},
  url = {http://www.nime.org/proceedings/2010/nime2010_238.pdf},
  keywords = {Guitar; Gesture acquisition; Capacitive sensors},
  abstract = {In this paper, we present our research on the acquisitionof gesture information for the study of the expressivenessin guitar performances. For that purpose, we design a sensor system which is able to gather the movements from lefthand fingers. Our effort is focused on a design that is (1)non-intrusive to the performer and (2) able to detect fromstrong movements of the left hand to subtle movements ofthe fingers. The proposed system is based on capacitive sensors mounted on the fingerboard of the guitar. We presentthe setup of the sensor system and analyze its response toseveral finger movements.}
}

@inproceedings{Schmeder2010,
  author = {Schmeder, Andrew and Freed, Adrian},
  title = {Support Vector Machine Learning for Gesture Signal Estimation with a Piezo-Resistive Fabric Touch Surface},
  pages = {244--249},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177893},
  url = {http://www.nime.org/proceedings/2010/nime2010_244.pdf},
  keywords = {gesture signal processing, support vector machine, touch sensor},
  abstract = {The design of an unusually simple fabric-based touchlocation and pressure sensor is introduced. An analysisof the raw sensor data is shown to have significant nonlinearities and non-uniform noise. Using support vectormachine learning and a state-dependent adaptive filter itis demonstrated that these problems can be overcome.The method is evaluated quantitatively using a statisticalestimate of the instantaneous rate of information transfer.The SVM regression alone is shown to improve the gesturesignal information rate by up to 20% with zero addedlatency, and in combination with filtering by 40% subjectto a constant latency bound of 10 milliseconds.}
}

@inproceedings{Schacher2010,
  author = {Schacher, Jan C.},
  title = {Motion To Gesture To Sound : Mapping For Interactive Dance},
  pages = {250--254},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177889},
  url = {http://www.nime.org/proceedings/2010/nime2010_250.pdf},
  keywords = {Mapping, motion sensing, computer vision, artistic strategies, wearable sensors, mapping tools, splines, delaunay tessellation.},
  abstract = {Mapping in interactive dance performance poses a number of questions related to the perception and expression of gestures in contrast to pure motion-detection and analysis. A specific interactive dance project is discussed, in which two complementary sensing modes are integrated to obtain higherlevel expressive gestures. These are applied to a modular nonlinear composition, in which the exploratory dance performance assumes the role of instrumentalist and conductor. The development strategies and methods for each of the involved artists are discussed and the software tools and wearable devices that were developed for this project are presented. }
}

@inproceedings{Whalley2010,
  author = {Whalley, Ian},
  title = {Generative Improv . \& Interactive Music Project (GIIMP)},
  pages = {255--258},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177923},
  url = {http://www.nime.org/proceedings/2010/nime2010_255.pdf},
  keywords = {Interaction, gesture, genetic algorithm, flocking, improvisation.},
  abstract = {GIIMP addresses the criticism that in many interactive music systems the machine simply reacts. Interaction is addressed by extending Winkler's [18] model toward adapting Paine's [10] conversational model of interaction. Realized using commercial tools, GIIMP implements a machine/human generative improvisation system using human gesture input, machine gesture capture, and a gesture mutation module in conjunction with a flocking patch, mapped through microtonal/spectral techniques to sound. The intention is to meld some established and current practices, and combine aspects of symbolic and sub-symbolic approaches, toward musical outcomes. }
}

@inproceedings{Nymoen2010,
  author = {Nymoen, Kristian and Glette, Kyrre and Skogstad, Ståle A. and Torresen, Jim and Jensenius, Alexander Refsum},
  title = {Searching for Cross-Individual Relationships between Sound and Movement Features using an {SVM} Classifier},
  pages = {259--262},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177869},
  url = {http://www.nime.org/proceedings/2010/nime2010_259.pdf},
  keywords = {nime10},
  abstract = {In this paper we present a method for studying relationships between features of sound and features of movement. The method has been tested by carrying out an experiment with people moving an object in space along with short sounds. 3D position data of the object was recorded and several features were calculated from each of the recordings. These features were provided as input to a classifier which was able to classify the recorded actions satisfactorily, particularly when taking into account that the only link between the actions performed by the different subjects was the sound they heard while making the action.}
}

@inproceedings{Baba2010,
  author = {Baba, Takashi and Hashida, Mitsuyo and Katayose, Haruhiro},
  title = {''VirtualPhilharmony'': A Conducting System with Heuristics of Conducting an Orchestra},
  pages = {263--270},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177715},
  url = {http://www.nime.org/proceedings/2010/nime2010_263.pdf},
  keywords = {Conducting system, heuristics, sensor, template.},
  abstract = {''VirtualPhilharmony'' (V.P.) is a conducting interface that enables users to perform expressive music with conducting action. Several previously developed conducting interfaces do not satisfy users who have conducting experience because the feedback from the conducting action does not always correspond with a natural performance. The tempo scheduler, which is the main engine of a conducting system, must be improved. V.P. solves this problem by introducing heuristics of conducting an orchestra in detecting beats, applying rules regarding the tempo expression in a bar, etc. We confirmed with users that the system realized a high "following" performance and had musical persuasiveness. }
}

@inproceedings{Grosshauser2010,
  author = {Gro{\ss}hauser, Tobias and Gro{\ss}ekath\"{o}fer, Ulf and Hermann, Thomas},
  title = {New Sensors and Pattern Recognition Techniques for String Instruments},
  pages = {271--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177779},
  url = {http://www.nime.org/proceedings/2010/nime2010_271.pdf},
  keywords = {left hand,nime10,ordered means models,pressure,sensor,strings},
  abstract = {Pressure, motion, and gesture are important parameters inmusical instrument playing. Pressure sensing allows to interpret complex hidden forces, which appear during playinga musical instrument. The combination of our new sensorsetup with pattern recognition techniques like the lately developed ordered means models allows fast and precise recognition of highly skilled playing techniques. This includes leftand right hand analysis as well as a combination of both. Inthis paper we show bow position recognition for string instruments by means of support vector regression machineson the right hand finger pressure, as well as bowing recognition and inaccurate playing detection with ordered meansmodels. We also introduce a new left hand and chin pressuresensing method for coordination and position change analysis. Our methods in combination with our audio, video,and gesture recording software can be used for teachingand exercising. Especially studies of complex movementsand finger force distribution changes can benefit from suchan approach. Practical applications include the recognitionof inaccuracy, cramping, or malposition, and, last but notleast, the development of augmented instruments and newplaying techniques.}
}

@inproceedings{Hahnel2010b,
  author = {H\''{a}hnel, Tilo and Berndt, Axel},
  title = {Expressive Articulation for Synthetic Music Performances},
  pages = {277--282},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177789},
  url = {http://www.nime.org/proceedings/2010/nime2010_277.pdf},
  keywords = {Expressive Performance, Articulation, Historically Informed Performance},
  abstract = {As one of the main expressive feature in music, articulationaffects a wide range of tone attributes. Based on experimental recordings we analyzed human articulation in the lateBaroque style. The results are useful for both the understanding of historically informed performance practices andfurther progress in synthetic performance generation. Thispaper reports of our findings and the implementation in aperformance system. Because of its flexibility and universality the system allows more than Baroque articulation.}
}

@inproceedings{Brown2010,
  author = {Brown, Andrew R.},
  title = {Network Jamming : Distributed Performance using Generative Music},
  pages = {283--286},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177723},
  url = {http://www.nime.org/proceedings/2010/nime2010_283.pdf},
  keywords = {collaborative,ensemble,generative,interaction,network,nime10},
  abstract = {Generative music systems can be played by musicians who manipulate the values of algorithmic parameters, and their datacentric nature provides an opportunity for coordinated interaction amongst a group of systems linked over IP networks; a practice we call Network Jamming. This paper outlines the characteristics of this networked performance practice and discusses the types of mediated musical relationships and ensemble configurations that can arise. We have developed and tested the jam2jam network jamming software over recent years. We describe this system, draw from our experiences with it, and use it to illustrate some characteristics of Network Jamming.}
}

@inproceedings{Frounberg2010,
  author = {Frounberg, Ivar and Innervik, Kjell Tore and Jensenius, Alexander R.},
  title = {Glass Instruments -- From Pitch to Timbre},
  pages = {287--290},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177773},
  url = {http://www.nime.org/proceedings/2010/nime2010_287.pdf},
  keywords = {glass instruments,nime,nime10,performance practice},
  abstract = {The paper reports on the development of prototypes of glassinstruments. The focus has been on developing acousticinstruments specifically designed for electronic treatment,and where timbral qualities have had priority over pitch.The paper starts with a brief historical overview of glassinstruments and their artistic use. Then follows an overviewof the glass blowing process. Finally the musical use of theinstruments is discussed.}
}

@inproceedings{Kiefer2010,
  author = {Kiefer, Chris},
  title = {A Malleable Interface for Sonic Exploration},
  pages = {291--296},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177823},
  url = {http://www.nime.org/proceedings/2010/nime2010_291.pdf},
  keywords = {Musical Controller, Reservoir Computing, Human Computer Interaction, Tangible User Interface, Evaluation},
  abstract = {Input devices for controlling music software can benefit fromexploiting the use of perceptual-motor skill in interaction.The project described here is a new musical controller, designed with the aim of enabling intuitive and nuanced interaction through direct physical manipulation of malleablematerial.The controller is made from conductive foam. This foamchanges electrical resistance when deformed; the controllerworks by measuring resistance at multiple points in a single piece of foam in order to track its shape. These measurements are complex and interdependent so an echo statenetwork, a form of recurrent neural network, is employed totranslate the sensor readings into usable control data.A cube shaped controller was built and evaluated in thecontext of the haptic exploration of sound synthesis parameter spaces. Eight participants experimented with the controller and were interviewed about their experiences. Thecontroller achieves its aim of enabling intuitive interaction,but in terms of nuanced interaction, accuracy and repeatability were issues for some participants. It's not clear fromthe short evaluation study whether these issues would improve with practice, a longitudinal study that gives musicians time to practice and find the creative limitations ofthe controller would help to evaluate this fully.The evaluation highlighted interesting issues concerningthe high level nature of malleable control and different approaches to sonic exploration.}
}

@inproceedings{Zappi2010,
  author = {Zappi, Victor and Brogni, Andrea and Caldwell, Darwin},
  title = {OSC Virtual Controller},
  pages = {297--302},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177931},
  url = {http://www.nime.org/proceedings/2010/nime2010_297.pdf},
  keywords = {Glove device, Music controller, Virtual Reality, OSC, con- trol mapping},
  abstract = {The number of artists who express themselves through music in an unconventional way is constantly growing. Thistrend strongly depends on the high diffusion of laptops,which proved to be powerful and flexible musical devices.However laptops still lack in flexible interface, specificallydesigned for music creation in live and studio performances.To resolve this issue many controllers have been developed,taking into account not only the performer's needs andhabits during music creation, but also the audience desire tovisually understand how performer's gestures are linked tothe way music is made. According to the common need ofadaptable visual interface to manipulate music, in this paper we present a custom tridimensional controller, based onOpen Sound Control protocol and completely designed towork inside Virtual Reality: simple geometrical shapes canbe created to directly control loop triggering and parametermodification, just using free hand interaction.}
}

@inproceedings{Dimitrov2010,
  author = {Dimitrov, Smilen},
  title = {Extending the Soundcard for Use with Generic {DC} Sensors Demonstrated by Revisiting a Vintage ISA Design},
  pages = {303--308},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177755},
  url = {http://www.nime.org/proceedings/2010/nime2010_303.pdf},
  keywords = {dc,isa,nime10,sensors,soundcard},
  abstract = {The sound card anno 2010, is an ubiquitous part of almostany personal computing system; what was once considereda high-end, CD-quality audio fidelity, is today found in mostcommon sound cards. The increased presence of multichannel devices, along with the high sampling frequency, makesthe sound card desirable as a generic interface for acquisition of analog signals in prototyping of sensor-based musicinterfaces. However, due to the need for coupling capacitorsat a sound card's inputs and outputs, the use as a genericsignal interface of a sound card is limited to signals not carrying information in a constant DC component. Through arevisit of a card design for the (now defunct) ISA bus, thispaper proposes use of analog gates for bypassing the DCfiltering input sections, controllable from software --- therebyallowing for arbitrary choice by the user, if a soundcardinput channel is to be used as a generic analog-to-digitalsensor interface. Issues regarding use of obsolete technology and educational aspects are discussed as well.}
}

@inproceedings{LeGroux2010,
  author = {Le Groux, Sylvain and Manzolli, Jonatas and Verschure, Paul F. and Marti Sanchez and Andre Luvizotto and Anna Mura and Aleksander Valjamae and Christoph Guger and Robert Prueckl and Ulysses Bernardet},
  title = {Disembodied and Collaborative Musical Interaction in the Multimodal Brain Orchestra},
  pages = {309--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177831},
  url = {http://www.nime.org/proceedings/2010/nime2010_309.pdf},
  keywords = {Brain-computer Interface, Biosignals, Interactive Music System, Collaborative Musical Performance},
  abstract = {Most new digital musical interfaces have evolved upon theintuitive idea that there is a causality between sonic outputand physical actions. Nevertheless, the advent of braincomputer interfaces (BCI) now allows us to directly accesssubjective mental states and express these in the physicalworld without bodily actions. In the context of an interactive and collaborative live performance, we propose to exploit novel brain-computer technologies to achieve unmediated brain control over music generation and expression.We introduce a general framework for the generation, synchronization and modulation of musical material from brainsignal and describe its use in the realization of Xmotion, amultimodal performance for a "brain quartet".}
}

@inproceedings{Hochenbaum2010a,
  author = {Hochenbaum, Jordan and Vallis, Owen and Diakopoulos, Dimitri and Murphy, Jim and Kapur, Ajay},
  title = {Designing Expressive Musical Interfaces for Tabletop Surfaces},
  pages = {315--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177807},
  url = {http://www.nime.org/proceedings/2010/nime2010_315.pdf},
  keywords = {Bricktable, Multi-touch Interface, Tangible Interface, Generative Music, Music Information Retrieval},
  abstract = {This paper explores the evolution of collaborative, multi-user, musical interfaces developed for the Bricktable interactive surface. Two key types of applications are addressed: user interfaces for artistic installation and interfaces for musical performance. In describing our software, we provide insight on the methodologies and practicalities of designing interactive musical systems for tabletop surfaces. Additionally, subtleties of working with custom-designed tabletop hardware are addressed. }
}

@inproceedings{Suiter2010,
  author = {Suiter, Wendy},
  title = {Toward Algorithmic Composition of Expression in Music Using Fuzzy Logic},
  pages = {319--322},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177901},
  url = {http://www.nime.org/proceedings/2010/nime2010_319.pdf},
  keywords = {fuzzy logic,music composition,musical expression,nime10},
  abstract = {This paper introduces the concept of composing expressive music using the principles of Fuzzy Logic. The paper provides a conceptual model of a musical work which follows compositional decision making processes. Significant features of this Fuzzy Logic framework are its inclusiveness through the consideration of all the many and varied musical details, while also incorporating the imprecision that characterises musical terminology and discourse. A significant attribute of my Fuzzy Logic method is that it traces the trajectory of all musical details, since it is both the individual elements and their combination over time which is significant to the effectiveness of a musical work in achieving its goals. The goal of this work is to find a set of elements and rules, which will ultimately enable the construction of a genralised algorithmic compositional system which can produce expressive music if so desired. }
}

@inproceedings{Beilharz2010,
  author = {Beilharz, Kirsty and Vande Moere, Andrew and Stiel, Barbara and Calo, Claudia and Tomitsch, Martin and Lombard, Adrian},
  title = {Expressive Wearable Sonification and Visualisation : Design and Evaluation of a Flexible Display},
  pages = {323--326},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177717},
  url = {http://www.nime.org/proceedings/2010/nime2010_323.pdf},
  keywords = {Wearable display, sonification, visualisation, design aesthetics, physical computing, multimodal expression, bimodal display},
  abstract = {In this paper we examine a wearable sonification and visualisation display that uses physical analogue visualisation and digital sonification to convey feedback about the wearer's activity and environment. Intended to bridge a gap between art aesthetics, fashionable technologies and informative physical computing, the user experience evaluation reveals the wearers' responses and understanding of a novel medium for wearable expression. The study reveals useful insights for wearable device design in general and future iterations of this sonification and visualisation display. }
}

@inproceedings{Nugroho2010,
  author = {Nugroho, Jeremiah and Beilharz, Kirsty},
  title = {Understanding and Evaluating User Centred Design in Wearable Expressions},
  pages = {327--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177867},
  url = {http://www.nime.org/proceedings/2010/nime2010_327.pdf},
  keywords = {Wearable expressions, body, user-centered design.},
  abstract = {In this paper, we describe the shaping factors, which simplify and help us understand the multi-dimensional aspects of designing Wearable Expressions. These descriptive shaping factors contribute to both the design and user-experience evaluation of Wearable Expressions. }
}

@inproceedings{Park2010,
  author = {Park, Sihwa and Kim, Seunghun and Lee, Samuel and Yeo, Woon Seung},
  title = {Online Map Interface for Creative and Interactive},
  pages = {331--334},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177877},
  url = {http://www.nime.org/proceedings/2010/nime2010_331.pdf},
  keywords = {Musical sonification, map interface, online map service, geo- referenced data, composition, mashup},
  abstract = {In this paper, we discuss the musical potential of COMPath --- an online map based music-making tool --- as a noveland unique interface for interactive music composition andperformance. COMPath provides an intuitive environmentfor creative music making by sonification of georeferenceddata. Users can generate musical events with simple andfamiliar actions on an online map interface; a set of local information is collected along the user-drawn route andthen interpreted as sounds of various musical instruments.We discuss the musical interpretation of routes on a map,review the design and implementation of COMPath, andpresent selected sonification results with focus on mappingstrategies for map-based composition.}
}

@inproceedings{Hadjakos2010,
  author = {Hadjakos, Aristotelis and M\''{u}hlh\''{a}user, Max},
  title = {Analysis of Piano Playing Movements Spanning Multiple Touches},
  pages = {335--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177791},
  url = {http://www.nime.org/proceedings/2010/nime2010_335.pdf},
  keywords = {nime10},
  abstract = {Awareness of playing movements can help a piano student to improve technique. We are developing a piano pedagogy application that uses sensor data of hand and arm movement and generates feedback to increase movement awareness. This paper reports on a method for analysis of piano playing movements. The method allows to judge whether an active movement in a joint has occurred during a given time interval. This time interval may include one or more touches. The problem is complicated by the fact that the mechanical interaction between the arm and piano action generates additional movements that are not under direct control of the player. The analysis method is able to ignore these movements and can therefore be used to provide useful feedback.}
}

@inproceedings{Heinz2010,
  author = {Heinz, Sebastian and O'Modhrain, Sile},
  title = {Designing a Shareable Musical TUI},
  pages = {339--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177803},
  url = {http://www.nime.org/proceedings/2010/nime2010_339.pdf},
  keywords = {Tangible User Interfaces, collaborative performances, social factors},
  abstract = {This paper proposes a design concept for a tangible interface forcollaborative performances that incorporates two social factorspresent during performance, the individual creation andadaptation of technology and the sharing of it within acommunity. These factors are identified using the example of alaptop ensemble and then applied to three existing collaborativeperformance paradigms. Finally relevant technology, challengesand the current state of our implementation are discussed.}
}

@inproceedings{Freed2010,
  author = {Freed, Adrian},
  title = {Visualizations and Interaction Strategies for Hybridization Interfaces},
  pages = {343--347},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177769},
  url = {http://www.nime.org/proceedings/2010/nime2010_343.pdf},
  keywords = {Interpolation, dimension reduction, radial basis functions, triangular mesh},
  abstract = {We present two complementary approaches for the visualization and interaction of dimensionally reduced data setsusing hybridization interfaces. Our implementations privilege syncretic systems allowing one to explore combinations(hybrids) of disparate elements of a data set through theirplacement in a 2-D space. The first approach allows for theplacement of data points anywhere on the plane accordingto an anticipated performance strategy. The contribution(weight) of each data point varies according to a power function of the distance from the control cursor. The secondapproach uses constrained vertex colored triangulations ofmanifolds with labels placed at the vertices of triangulartiles. Weights are computed by barycentric projection ofthe control cursor position.}
}

@inproceedings{Woldecke2010,
  author = {W\''{o}ldecke, Bj\''{o}rn and Geiger, Christian and Reckter, Holger and Schulz, Florian},
  title = {ANTracks 2.0 --- Generative Music on Multiple Multitouch Devices Categories and Subject Descriptors},
  pages = {348--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177921},
  url = {http://www.nime.org/proceedings/2010/nime2010_348.pdf},
  keywords = {Generative music, mobile interfaces, multitouch interaction},
  abstract = {In this paper we describe work in progress on generative music generation on multi-touch devices. Our goal is to create a musical application framework for multiple casual users that use state of the art multitouch devices. We choose the metaphor of ants moving on a hexagonal grid to interact with a pitch pattern. The set of devices used includes a custom built multitouch table and a number of iPhones to jointly create musical expressions.}
}

@inproceedings{Kang2010,
  author = {Kang, Laewoo and Chien, Hsin-Yi},
  title = {H\'{e} : Calligraphy as a Musical Interface},
  pages = {352--355},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177819},
  url = {http://www.nime.org/proceedings/2010/nime2010_352.pdf},
  keywords = {Interactive music interface, calligraphy, graphical music composing, sonification},
  abstract = {The project H\'{e}(和, harmony) is a sound installation that enables a user to play music by writing calligraphy. We developed a system where calligraphic symbols can be detected and converted to a sound composed of pitch, pitch length, and volume though MIDI and serial communication. The H\'{e} sound installation involves a micro-controller, photocells, and multiplexers. A DC motor controls the speed of a spooled paper roll that is capable of setting the music tempo. This paper presents the design concept and implementation of H\'{e}. We discuss the major research issues such as using photocells for detecting components of calligraphy like thickness and location. Hardware and software details are also discussed. Finally, we explore the potential for further extending musical and visual experience through this project’s applications and outcomes.}
}

@inproceedings{Marier2010,
  author = {Marier, Martin},
  title = {The Sponge A Flexible Interface},
  pages = {356--359},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177839},
  url = {http://www.nime.org/proceedings/2010/nime2010_356.pdf},
  keywords = {Interface, electroacoustic music, performance, expressivity, mapping},
  abstract = {The sponge is an interface that allows a clear link to beestablished between gesture and sound in electroacousticmusic. The goals in developing the sponge were to reintroduce the pleasure of playing and to improve the interaction between the composer/performer and the audience. Ithas been argued that expenditure of effort or energy is required to obtain expressive interfaces. The sponge favors anenergy-sound relationship in two ways : 1) it senses acceleration, which is closely related to energy; and 2) it is madeout of a flexible material (foam) that requires effort to besqueezed or twisted. Some of the mapping strategies usedin a performance context with the sponge are discussed.}
}

@inproceedings{Fyfe2010,
  author = {Fyfe, Lawrence and Lynch, Sean and Hull, Carmen and Carpendale, Sheelagh},
  title = {SurfaceMusic : Mapping Virtual Touch-based Instruments to Physical Models},
  pages = {360--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177777},
  url = {http://www.nime.org/proceedings/2010/nime2010_360.pdf},
  keywords = {Tabletop, multi-touch, gesture, physical model, Open Sound Control.},
  abstract = {In this paper we discuss SurfaceMusic, a tabletop music system in which touch gestures are mapped to physical modelsof instruments. With physical models, parametric controlover the sound allows for a more natural interaction between gesture and sound. We discuss the design and implementation of a simple gestural interface for interactingwith virtual instruments and a messaging system that conveys gesture data to the audio system.}
}

@inproceedings{Martin2010,
  author = {Martin, Aengus and Ferguson, Sam and Beilharz, Kirsty},
  title = {Mechanisms for Controlling Complex Sound Sources : Applications to Guitar Feedback Control},
  pages = {364--367},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177841},
  url = {http://www.nime.org/proceedings/2010/nime2010_364.pdf},
  keywords = {Concatenative Synthesis, Feedback, Guitar},
  abstract = {Many musical instruments have interfaces which emphasisethe pitch of the sound produced over other perceptual characteristics, such as its timbre. This is at odds with the musical developments of the last century. In this paper, weintroduce a method for replacing the interface of musicalinstruments (both conventional and unconventional) witha more flexible interface which can present the intrument'savailable sounds according to variety of different perceptualcharacteristics, such as their brightness or roughness. Weapply this method to an instrument of our own design whichcomprises an electro-mechanically controlled electric guitarand amplifier configured to produce feedback tones.}
}

@inproceedings{Torresen2010,
  author = {Torresen, Jim and Renton, Eirik and Jensenius, Alexander R.},
  title = {Wireless Sensor Data Collection based on {ZigBee} Communication},
  pages = {368--371},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177911},
  url = {http://www.nime.org/proceedings/2010/nime2010_368.pdf},
  keywords = {wireless communication, ZigBee, microcontroller},
  abstract = {This paper presents a comparison of different configurationsof a wireless sensor system for capturing human motion.The systems consist of sensor elements which wirelesslytransfers motion data to a receiver element. The sensorelements consist of a microcontroller, accelerometer(s) anda radio transceiver. The receiver element consists of a radioreceiver connected through a microcontroller to a computerfor real time sound synthesis. The wireless transmission between the sensor elements and the receiver element is basedon the low rate IEEE 802.15.4/ZigBee standard.A configuration with several accelerometers connected bywire to a wireless sensor element is compared to using multiple wireless sensor elements with only one accelerometer ineach. The study shows that it would be feasable to connect5-6 accelerometers in the given setups.Sensor data processing can be done in either the receiverelement or in the sensor element. For various reasons it canbe reasonable to implement some sensor data processing inthe sensor element. The paper also looks at how much timethat typically would be needed for a simple pre-processingtask.}
}

@inproceedings{Jaimovich2010a,
  author = {Jaimovich, Javier and Knapp, Benjamin},
  title = {Synchronization of Multimodal Recordings for Musical Performance Research},
  pages = {372--374},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177815},
  url = {http://www.nime.org/proceedings/2010/nime2010_372.pdf},
  keywords = {Synchronization, Multimodal Signals, Sensor Data Acquisition, Signal Recording.},
  abstract = {The past decade has seen an increase of low-cost technology for sensor data acquisition, which has been utilized for the expanding field of research in gesture measurement for music performance. Unfortunately, these devices are still far from being compatible with the audiovisual recording platforms which have been used to record synchronized streams of data. In this paper, we describe a practical solution for simultaneous recording of heterogeneous multimodal signals. The recording system presented uses MIDI Time Code to time-stamp sensor data and to synchronize with standard video and audio recording systems. We also present a set of tools for recording sensor data, as well as a set of analysis tools to evaluate in realtime the sample rate of different signals, and the overall synchronization status of the recording system. }
}

@inproceedings{Torre2010,
  author = {Torre, Giuseppe and O'Leary, Mark and Tuohy, Brian},
  title = {POLLEN A Multimedia Interactive Network Installation},
  pages = {375--376},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177909},
  url = {http://www.nime.org/proceedings/2010/nime2010_375.pdf},
  keywords = {Interactive, Installation, Network, 3D Physics Emulator, Educational Tools, Public Spaces, Computer Labs, Sound Design, Site-Specific Art},
  abstract = {This paper describes the development of an interactive 3Daudio/visual and network installation entitled POLLEN.Specifically designed for large computer Laboratories, theartwork explores the regeneration of those spaces throughthe creation of a fully immersive multimedia art experience.The paper describes the technical, aesthetic and educational development of the piece.}
}

@inproceedings{Feng2010,
  author = {Feng, Xiaoyang},
  title = {Irregular Incurve},
  pages = {377--379},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177765},
  url = {http://www.nime.org/proceedings/2010/nime2010_377.pdf},
  keywords = {NIME, Robotics, Acoustic, Interactive, MIDI, Real time Performance, String Instrument, Arduino, Servo, Motor Control},
  abstract = { Irregular Incurve is a MIDI controllable robotic string instrument. The twelve independent string-units compose the complete musical scale of 12 units. Each string can be plucked by a motor control guitar pick. A MIDI keyboard is attached to the instrument and serves as an interface for real-time interactions between the instrument and the audience. Irregular Incurve can also play preprogrammed music by itself. This paper presents the design concept and the technical solutions to realizing the functionality of Irregular Incurve. The future features are also discussed. }
}

@inproceedings{Miyama2010,
  author = {Miyama, Chikashi},
  title = {Peacock : A Non-Haptic {3D} Performance Interface},
  pages = {380--382},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177859},
  url = {http://www.nime.org/proceedings/2010/nime2010_380.pdf},
  keywords = {Musical interface, Sensor technologies, Computer music, Hardware and software design},
  abstract = {Peacock is a newly designed interface for improvisational performances. The interface is equipped with thirty-five proximity sensors arranged in five rows and seven columns. The sensors detect the movements of a performer's hands and arms in a three-dimensional space above them. The interface digitizes the output of the sensors into sets of high precision digital packets, and sends them to a patch running in Pdextended with a sufficiently high bandwidth for performances with almost no computational resource consumption in Pd. The precision, speed, and efficiency of the system enable the sonification of hand gestures in realtime without the need to attach any physical devices to the performer's body. This paper traces the interface's evolution, discussing relevant technologies, hardware construction, system design, and input monitoring. }
}

@inproceedings{Holm2010,
  author = {Holm, Jukka and Holm, Harri and Sepp\''{a}nen, Jarno},
  title = {Associating Emoticons with Musical Genres},
  pages = {383--386},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177809},
  url = {http://www.nime.org/proceedings/2010/nime2010_383.pdf},
  keywords = {Music, music recommendation, context, facial expression, mood, emotion, emoticon, and musical genre.},
  abstract = {Music recommendation systems can observe user's personal preferences and suggest new tracks from a large online catalog. In the case of context-aware recommenders, user's current emotional state plays an important role. One simple way to visualize emotions and moods is graphical emoticons. In this study, we researched a high-level mapping between genres, as descriptions of music, and emoticons, as descriptions of emotions and moods. An online questionnaire with 87 participants was arranged. Based on the results, we present a list of genres that could be used as a starting point for making recommendations fitting the current mood of the user. }
}

@inproceedings{Nagashima2010,
  author = {Nagashima, Yoichi},
  title = {Untouchable Instrument "Peller-Min"},
  pages = {387--390},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177865},
  url = {http://www.nime.org/proceedings/2010/nime2010_387.pdf},
  keywords = {Theremin, untouchable, distance sensor, Propeller processor},
  abstract = {This paper is a report on the development of a new musical instrument in which the main concept is "Untouchable". The key concept of this instrument is "sound generation by body gesture (both hands)" and "sound generation by kneading with hands". The new composition project had completed as the premiere of a new work "controllable untouchableness" with this new instrument in December 2009.}
}

@inproceedings{Jaimovich2010,
  author = {Jaimovich, Javier},
  title = {Ground Me ! An Interactive Sound Art Installation},
  pages = {391--394},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177813},
  url = {http://www.nime.org/proceedings/2010/nime2010_391.pdf},
  keywords = {Interactive sound installation, body impedance, skin conductivity, site-specific sound installation, human network, Sonic Lab, Arduino.},
  abstract = {This paper describes the design, implementation and outcome of Ground Me!, an interactive sound installation set up in the Sonic Lab of the Sonic Arts Research Centre. The site-specific interactive installation consists of multiple copper poles hanging from the Sonic Lab's ceiling panels, which trigger samples of electricity sounds when grounded through the visitor's' body to the space's metallic floor. }
}

@inproceedings{Savage2010,
  author = {Savage, Norma S. and Ali, Syed R. and Chavez, Norma E.},
  title = {Mmmmm: A Multi-modal Mobile Music Mixer},
  pages = {395--398},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177887},
  url = {http://www.nime.org/proceedings/2010/nime2010_395.pdf},
  keywords = {Multi-modal, interaction, music, mixer, mobile, interactive, DJ, smart phones, Nokia, n900, touch screen, accelerometer, phone, audience},
  abstract = {This paper presents Mmmmm; a Multimodal Mobile Music Mixer that provides DJs a new interface for mixing musicon the Nokia N900 phones. Mmmmm presents a novel way for DJ to become more interactive with their audience andvise versa. The software developed for the N900 mobilephone utilizes the phones built-in accelerometer sensor andBluetooth audio streaming capabilities to mix and apply effects to music using hand gestures and have the mixed audiostream to Bluetooth speakers, which allows the DJ to moveabout the environment and get familiarized with their audience, turning the experience of DJing into an interactiveand audience engaging process. Mmmmm is designed so that the DJ can utilize handgestures and haptic feedback to help them perform the various tasks involved in DJing (mixing, applying effects, andetc). This allows the DJ to focus on the crowd, thus providing the DJ a better intuition of what kind of music ormusical mixing style the audience is more likely to enjoyand engage with. Additionally, Mmmmm has an Ambient Tempo Detection mode in which the phones camera is utilized to detect the amount of movement in the environment and suggest to the DJ the tempo of music that should be played. This mode utilizes frame differencing and pixelchange overtime to get a sense of how fast the environmentis changing, loosely correlating to how fast the audience isdancing or the lights are flashing in the scene. By determining the ambient tempo of the environment the DJ canget a better sense for the type of music that would fit bestfor their venue.Mmmmm helps novice DJs achieve a better music repertoire by allowing them to interact with their audience andreceive direct feedback on their performance. The DJ canchoose to utilize these modes of interaction and performance or utilize traditional DJ controls using MmmmmsN900 touch screen based graphics user interface.}
}

@inproceedings{Tsai2010,
  author = {Tsai, Chih-Chieh and Liu, Cha-Lin and Chang, Teng-Wen},
  title = {An Interactive Responsive Skin for Music},
  pages = {399--402},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177915},
  url = {http://www.nime.org/proceedings/2010/nime2010_399.pdf},
  keywords = {Interactive Performance, Ambient Environment, Responsive Skin, Music performance.},
  abstract = {With the decreasing audience of classical music performance, this research aims to develop a performance-enhancement system, called AIDA, to help classical performers better communicating with their audiences. With three procedures Input-Processing-Output, AIDA system can sense and analyze the body information of performers and further reflect it onto the responsive skin. Thus abstract and intangible emotional expressions of performers are transformed into tangible and concrete visual elements, which clearly facilitating the audiences' threshold for music appreciation. }
}

@inproceedings{BryanKinns2010,
  author = {Bryan-Kinns, Nick and Fencott, Robin and Metatla, Oussama and Nabavian, Shahin and Sheridan, Jennifer G.},
  title = {Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art},
  pages = {403--406},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177727},
  url = {http://www.nime.org/proceedings/2010/nime2010_403.pdf},
  keywords = {Interactional, sound, music, mutual engagement, improvisation, composition, collaboration, awareness.},
  abstract = {In this paper we outline the emerging field of Interactional Sound and Music which concerns itself with multi-person technologically mediated interactions primarily using audio. We present several examples of interactive systems in our group, and reflect on how they were designed and evaluated. Evaluation techniques for collective, performative, and task oriented activities are outlined and compared. We emphasise the importance of designing for awareness in these systems, and provide examples of different awareness mechanisms. }
}

@inproceedings{Skogstad2010,
  author = {Skogstad, Ståle A. and Jensenius, Alexander Refsum and Nymoen, Kristian},
  title = {Using {IR} Optical Marker Based Motion Capture for Exploring Musical Interaction},
  pages = {407--410},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177895},
  url = {http://www.nime.org/proceedings/2010/nime2010_407.pdf},
  keywords = {nime10},
  abstract = {The paper presents a conceptual overview of how optical infrared marker based motion capture systems (IrMoCap) can be used in musical interaction. First we present a review of related work of using IrMoCap for musical control. This is followed by a discussion of possible features which can be exploited. Finally, the question of mapping movement features to sound features is presented and discussed.}
}

@inproceedings{Buch2010,
  author = {Buch, Benjamin and Coussement, Pieter and Schmidt, L\''{u}der},
  title = {''playing robot'' : An Interactive Sound Installation in Human-Robot Interaction Design for New Media Art},
  pages = {411--414},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177729},
  url = {http://www.nime.org/proceedings/2010/nime2010_411.pdf},
  keywords = {dynamic mapping,embodiment,finite state au-,human-robot interaction,new media art,nime10,structured,tomata},
  abstract = {In this study artistic human-robot interaction design is introduced as a means for scientific research and artistic investigations. It serves as a methodology for situated cognitionintegrating empirical methodology and computational modeling, and is exemplified by the installation playing robot.Its artistic purpose is to aid to create and explore robots as anew medium for art and entertainment. We discuss the useof finite state machines to organize robots' behavioral reactions to sensor data, and give a brief outlook on structuredobservation as a potential method for data collection.}
}

@inproceedings{Reboursiere2010,
  author = {Reboursi\`{e}re, Lo\"{i}c and Frisson, Christian and L\"{a}hdeoja, Otso and Mills, John A. and Picard-Limpens, C\'{e}cile and Todoroff, Todor},
  title = {Multimodal Guitar : A Toolbox For Augmented Guitar Performances},
  pages = {415--418},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177881},
  url = {http://www.nime.org/proceedings/2010/nime2010_415.pdf},
  keywords = {Augmented guitar, audio synthesis, digital audio effects, multimodal interaction, gestural sensing, polyphonic tran- scription, hexaphonic guitar},
  abstract = {This project aims at studying how recent interactive and interactions technologies would help extend how we play theguitar, thus defining the "multimodal guitar". Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed anddeveloped a freely-available toolbox for augmented guitarperformances, compliant with the PureData and Max/MSPenvironments, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing,modal synthesis, infinite sustain, rearranging looping and "smart" harmonizing.}
}

@inproceedings{Berger2010,
  author = {Berger, Michael},
  title = {The GRIP MAESTRO : Idiomatic Mappings of Emotive Gestures for Control of Live Electroacoustic Music},
  pages = {419--422},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177719},
  url = {http://www.nime.org/proceedings/2010/nime2010_419.pdf},
  keywords = {emotive gesture and music,hall effect,human-controller interaction,musical mapping strategies,nime10,novel musical instrument,passive haptic feedback,sensor-augmented hand-exerciser},
  abstract = {This paper introduces my research in physical interactive design with my "GRIP MAESTRO" electroacoustic performance interface. It then discusses the considerations involved in creating intuitive software mappings of emotive performative gestures such that they are idiomatic not only of the sounds they create but also of the physical nature of the interface itself. }
}

@inproceedings{Headlee2010,
  author = {Headlee, Kimberlee and Koziupa, Tatyana and Siwiak, Diana},
  title = {Sonic Virtual Reality Game : How Does Your Body Sound ?},
  pages = {423--426},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177801},
  url = {http://www.nime.org/proceedings/2010/nime2010_423.pdf},
  keywords = {biomusic, collaborative, expressive, hci, interactive, interactivity design, interface for musical expression, multimodal, musical mapping strategies,nime10,performance,sonification},
  abstract = {In this paper, we present an interactive system that uses the body as a generative tool for creating music. We explore innovative ways to make music, create self-awareness, and provide the opportunity for unique, interactive social experiences. The system uses a multi-player game paradigm, where players work together to add layers to a soundscape of three distinct environments. Various sensors and hardware are attached to the body and transmit signals to a workstation, where they are processed using Max/MSP. The game is divided into three levels, each of a different soundscape. The underlying purpose of our system is to move the player's focus away from complexities of the modern urban world toward a more internalized meditative state. The system is currently viewed as an interactive installation piece, but future iterations have potential applications in music therapy, bio games, extended performance art, and as a prototype for new interfaces for musical expression. }
}

@inproceedings{Stahl2010,
  author = {Stahl, Alex and Clemens, Patricia},
  title = {Auditory Masquing : Wearable Sound Systems for Diegetic Character Voices},
  pages = {427--430},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177899},
  url = {http://www.nime.org/proceedings/2010/nime2010_427.pdf},
  keywords = {magnetostrictive flextensional transducer,nime10,paralinguistics,sound reinforcement,spatialization,speech enhancement,transformation,voice,wearable systems},
  abstract = {Maintaining a sense of personal connection between increasingly synthetic performers and increasingly diffuse audiences is vital to storytelling and entertainment. Sonic intimacy is important, because voice is one of the highestbandwidth channels for expressing our real and imagined selves.New tools for highly focused spatialization could help improve acoustical clarity, encourage audience engagement, reduce noise pollution and inspire creative expression. We have a particular interest in embodied, embedded systems for vocal performance enhancement and transformation. This short paper describes work in progress on a toolkit for high-quality wearable sound suits. Design goals include tailored directionality and resonance, full bandwidth, and sensible ergonomics. Engineering details to accompany a demonstration of recent prototypes are presented, highlighting a novel magnetostrictive flextensional transducer. Based on initial observations we suggest that vocal acoustic output from the torso, and spatial perception of situated low frequency sources, are two areas deserving greater attention and further study.}
}

@inproceedings{Rothman2010,
  author = {Rothman, Paul},
  title = {The Ghost : An Open-Source, User Programmable {MIDI} Performance Controller},
  pages = {431--435},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177885},
  url = {http://www.nime.org/proceedings/2010/nime2010_431.pdf},
  keywords = {Controller, MIDI, Live Performance, Programmable, Open-Source},
  abstract = {The Ghost has been developed to create a merger between the standard MIDI keyboard controller, MIDI/digital guitars and alternative desktop controllers. Using a custom software editor, The Ghost's controls can be mapped to suit the users performative needs. The interface takes its interaction and gestural cues from the guitar but it is not a MIDI guitar. The Ghost's hardware, firmware and software will be open sourced with the hopes of creating a community of users that are invested in creating music with controller.}
}

@inproceedings{Paine2010,
  author = {Paine, Garth},
  title = {Towards a Taxonomy of Realtime Interfaces for Electronic Music Performance},
  pages = {436--439},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177873},
  url = {http://www.nime.org/proceedings/2010/nime2010_436.pdf},
  keywords = {Instrument, Interface, Organology, Taxonomy.},
  abstract = {This paper presents a discussion regarding organology classification and taxonomies for digital musical instruments (DMI), arising from the TIEM (Taxonomy of Interfaces for Electronic Music performance) survey (http://tiem.emf.org/), conducted as part of an Australian Research Council Linkage project titled "Performance Practice in New Interfaces for Realtime Electronic Music Performance". This research is being carried out at the VIPRe Lab at, the University of Western Sydney in partnership with the Electronic Music Foundation (EMF), Infusion Systems1 and The Input Devices and Music Interaction Laboratory (IDMIL) at McGill University. The project seeks to develop a schema of new interfaces for realtime electronic music performance. }
}

@inproceedings{Taylor2010,
  author = {Taylor, Robyn and Schofield, Guy and Shearer, John and Boulanger, Pierre and Wallace, Jayne and Olivier, Patrick},
  title = {humanaquarium : A Participatory Performance System},
  pages = {88--93},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177905},
  url = {http://www.nime.org/proceedings/2010/nime2010_440.pdf},
  keywords = {busking, collaborative interface, creative practice, experience centered design, frustrated total internal reflection (FTIR), multi-touch screen, multimedia, participatory performance},
  abstract = {humanaquarium is a self-contained, transportable performance environment that is used to stage technology-mediated interactive performances in public spaces. Drawing upon the creative practices of busking and street performance, humanaquarium incorporates live musicians, real-time audiovisual content generation, and frustrated total internal reflection (FTIR) technology to facilitate participatory interaction by members of the public. }
}

@inproceedings{Kim2010,
  author = {Kim, Hyun-Soo and Yoon, Je-Han and Jung, Moon-Sik},
  title = {Interactive Music Studio : The Soloist},
  pages = {444--446},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177825},
  url = {http://www.nime.org/proceedings/2010/nime2010_444.pdf},
  keywords = {Mobile device, music composer, pattern composing, MIDI},
  abstract = {In this paper, we present and demonstrate Samsung’s new concept music creation engine and music composer application for mobile devices such as touch phones or MP3 players, ‘Interactive Music Studio : the soloist’.}
}

@inproceedings{Tremblay2010,
  author = {Tremblay, Pierre Alexandre and Schwarz, Diemo},
  title = {Surfing the Waves : Live Audio Mosaicing of an Electric Bass Performance as a Corpus Browsing Interface},
  pages = {447--450},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177913},
  url = {http://www.nime.org/proceedings/2010/nime2010_447.pdf},
  keywords = {laptop improvisation, corpus-based concatenative synthesis, haptic interface, multi-dimensional mapping, audio mosaic},
  abstract = {In this paper, the authors describe how they use an electric bass as a subtle, expressive and intuitive interface to browse the rich sample bank available to most laptop owners. This is achieved by audio mosaicing of the live bass performance audio, through corpus-based concatenative synthesis (CBCS) techniques, allowing a mapping of the multi-dimensional expressivity of the performance onto foreign audio material, thus recycling the virtuosity acquired on the electric instrument with a trivial learning curve. This design hypothesis is contextualised and assessed within the Sandbox#n series of bass+laptop meta-instruments, and the authors describe technical means of the implementation through the use of the open-source CataRT CBCS system adapted for live mosaicing. They also discuss their encouraging early results and provide a list of further explorations to be made with that rich new interface.}
}

@inproceedings{Fyans2010,
  author = {Fyans, A. Cavan and Gurevich, Michael and Stapleton, Paul},
  title = {Examining the Spectator Experience},
  pages = {451--454},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177775},
  url = {http://www.nime.org/proceedings/2010/nime2010_451.pdf},
  keywords = {error,intention,mental model,nime10,qualitative,spectator},
  abstract = {Drawing on a model of spectator understanding of error inperformance in the literature, we document a qualitativeexperiment that explores the relationships between domainknowledge, mental models, intention and error recognitionby spectators of performances with electronic instruments.Participants saw two performances with contrasting instruments, with controls on their mental model and understanding of intention. Based on data from a subsequent structured interview, we identify themes in participants' judgements and understanding of performance and explanationsof their spectator experience. These reveal both elementsof similarity and difference between the two performances,instruments and between domain knowledge groups. Fromthese, we suggest and discuss implications for the design ofnovel performative interactions with technology.}
}

@inproceedings{Collins2010a,
  author = {Collins, Nick and Kiefer, Chris and Patoli, Zeeshan and White, Martin},
  title = {Musical Exoskeletons : Experiments with a Motion Capture Suit},
  pages = {455--458},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177749},
  url = {http://www.nime.org/proceedings/2010/nime2010_455.pdf},
  keywords = {Motion Capture, Musical Controller, Mapping, Agile Design},
  abstract = {Gaining access to a prototype motion capture suit designedby the Animazoo company, the Interactive Systems groupat the University of Sussex have been investigating application areas. This paper describes our initial experimentsin mapping the suit control data to sonic attributes for musical purposes. Given the lab conditions under which weworked, an agile design cycle methodology was employed,with live coding of audio software incorporating fast feedback, and more reflective preparations between sessions, exploiting both individual and pair programming. As the suitprovides up to 66 channels of information, we confront achallenging mapping problem, and techniques are describedfor automatic calibration, and the use of echo state networksfor dimensionality reduction.}
}

@inproceedings{Murphy2010,
  author = {Murphy, Jim and Kapur, Ajay and Burgin, Carl},
  title = {The Helio : A Study of Membrane Potentiometers and Long Force Sensing Resistors for Musical Interfaces},
  pages = {459--462},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177863},
  url = {http://www.nime.org/proceedings/2010/nime2010_459.pdf},
  keywords = {Force Sensing Resistors, Membrane Potentiometers, Force Sensing Resistors, Haptic Feedback, Helio},
  abstract = {This paper describes a study of membrane potentiometers and long force sensing resistors as tools to enable greater interaction between performers and audiences. This is accomplished through the building of a new interface called the Helio. In preparation for the Helio's construction, a variety of brands of membrane potentiometers and long force sensing resistors were analyzed for their suitability for use in a performance interface. Analog and digital circuit design considerations are discussed. We discuss in detail the design process and performance scenarios explored with the Helio. }
}

@inproceedings{Taylor2010a,
  author = {Taylor, Stuart and Hook, Jonathan},
  title = {FerroSynth : A Ferromagnetic Music Interface},
  pages = {463--466},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177907},
  url = {http://www.nime.org/proceedings/2010/nime2010_463.pdf},
  keywords = {Ferromagnetic sensing, ferrofluid, reconfigurable user interface, wave terrain synthesis, MIDI controller.},
  abstract = {We present a novel user interface device based around ferromagnetic sensing. The physical form of the interface can easily be reconfigured by simply adding and removing a variety of ferromagnetic objects to the device's sensing surface. This allows the user to change the physical form of the interface resulting in a variety of different interaction modes. When used in a musical context, the performer can leverage the physical reconfiguration of the device to affect the method of playing and ultimately the sound produced. We describe the implementation of the sensing system, along with a range of mapping techniques used to transform the sensor data into musical output, including both the direct synthesis of sound and also the generation of MIDI data for use with Ableton Live. We conclude with a discussion of future directions for the device. }
}

@inproceedings{Dubrau2010,
  author = {Dubrau, Josh M. and Havryliv, Mark},
  title = {P[a]ra[pra]xis : Towards Genuine Realtime 'Audiopoetry'},
  pages = {467--468},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.117777},
  url = {http://www.nime.org/proceedings/2010/nime2010_467.pdf},
  keywords = {language sonification, new media poetry, realtime, Lacan, semiotics, collaborative environment, psychoanalysis, Freud},
  abstract = {P[a]ra[pra]xis is an ongoing collaborative project incorporating a two-piece software package which explores human relations to language through dynamic sound and text production. Incorporating an exploration of the potential functions and limitations of the ‘sign’ and the intrusions of the Unconscious into the linguistic utterance via parapraxes, or ‘Freudian slips’, our software utilises realtime subject response to automatically- generated changes in a narrative of their own writing to create music. This paper considers the relative paucity of truly interactive realtime text and audio works and provides an account of current and future potential for the simultaneous production of realtime poetry and electronic music through the P[a]ra[pra]xis software. It also provides the basis for a demonstration session in which we hope to show users how the program works, discuss possibilities for different applications of the software, and collect data for future collaborative work.}
}

@inproceedings{Kitani2010,
  author = {Kitani, Kris M. and Koike, Hideki},
  title = {ImprovGenerator : Online Grammatical Induction for On-the-Fly Improvisation Accompaniment},
  pages = {469--472},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177827},
  url = {http://www.nime.org/proceedings/2010/nime2010_469.pdf},
  keywords = {Machine Improvisation, Grammatical Induction, Stochastic Context-Free Grammars, Algorithmic Composition},
  abstract = {We propose an online generative algorithm to enhance musical expression via intelligent improvisation accompaniment.Our framework called the ImprovGenerator, takes a livestream of percussion patterns and generates an improvisedaccompaniment track in real-time to stimulate new expressions in the improvisation. We use a mixture model togenerate an accompaniment pattern, that takes into account both the hierarchical temporal structure of the liveinput patterns and the current musical context of the performance. The hierarchical structure is represented as astochastic context-free grammar, which is used to generateaccompaniment patterns based on the history of temporalpatterns. We use a transition probability model to augmentthe grammar generated pattern to take into account thecurrent context of the performance. In our experiments weshow how basic beat patterns performed by a percussioniston a cajon can be used to automatically generate on-the-flyimprovisation accompaniment for live performance.}
}

@inproceedings{Frisson2010,
  author = {Frisson, Christian and Macq, Beno{\^i}t and Dupont, St\'{e}phane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry},
  title = {DeviceCycle : Rapid and Reusable Prototyping of Gestural Interfaces, Applied to Audio Browsing by Similarity},
  pages = {473--476},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177771},
  url = {http://www.nime.org/proceedings/2010/nime2010_473.pdf},
  keywords = {Human-computer interaction, gestural interfaces, rapid prototyping, browsing by similarity, audio database},
  abstract = {This paper presents the development of rapid and reusablegestural interface prototypes for navigation by similarity inan audio database and for sound manipulation, using theAudioCycle application. For this purpose, we propose andfollow guidelines for rapid prototyping that we apply usingthe PureData visual programming environment. We havemainly developed three prototypes of manual control: onecombining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits andshortcomings we experienced while prototyping using thisapproach.}
}

@inproceedings{Muller2010,
  author = {M\"{u}ller, Alexander and Hemmert, Fabian and Wintergerst, G\"{o}tz and Jagodzinski, Ron},
  title = {Reflective Haptics : Resistive Force Feedback for Musical Performances with Stylus-Controlled Instruments},
  pages = {477--478},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177835},
  url = {http://www.nime.org/proceedings/2010/nime2010_477.pdf},
  keywords = {force feedback, haptic feedback, interactive, pen controller},
  abstract = {In this paper we present a novel system for tactile actuation in stylus-based musical interactions. The proposed controller aims to support rhythmical musical performance. The system builds on resistive force feedback, which is achieved through a brakeaugmented ball pen stylus on a sticky touch-sensitive surface. Along the device itself, we present musical interaction principles that are enabled through the aforementioned tactile response. Further variations of the device and perspectives of the friction-based feedback are outlined. }
}

@inproceedings{Mattek2010,
  author = {Mattek, Alison and Freeman, Mark and Humphrey, Eric},
  title = {Revisiting Cagean Composition Methodology with a Modern Computational Implementation},
  pages = {479--480},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177847},
  url = {http://www.nime.org/proceedings/2010/nime2010_479.pdf},
  keywords = {Multi-touch Interfaces, Computer-Assisted Composition},
  abstract = {The American experimental tradition in music emphasizes a process-oriented – rather than goal-oriented – composition style. According to this tradition, the composition process is considered an experiment beginning with a problem resolved by the composer. The noted experimental composer John Cage believed that the artist’s role in composition should be one of coexistence, as opposed to the traditional view of directly controlling the process. Consequently, Cage devel- oped methods of composing that upheld this philosophy by utilizing musical charts and the I Ching, also known as the Chinese Book of Changes. This project investigates these methods and models them via an interactive computer system to explore the use of modern interfaces in experimental composition.}
}

@inproceedings{Ferguson2010,
  author = {Ferguson, Sam and Schubert, Emery and Stevens, Catherine},
  title = {Movement in a Contemporary Dance Work and its Relation to Continuous Emotional Response},
  pages = {481--484},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177767},
  url = {http://www.nime.org/proceedings/2010/nime2010_481.pdf},
  keywords = {Dance, Emotion, Motion Capture, Continuous Response.},
  abstract = {In this paper, we describe a comparison between parameters drawn from 3-dimensional measurement of a dance performance, and continuous emotional response data recorded from an audience present during this performance. A continuous time series representing the mean movement as the dance unfolds is extracted from the 3-dimensional data. The audiences' continuous emotional response data are also represented as a time series, and the series are compared. We concluded that movement in the dance performance directly influences the emotional arousal response of the audience. }
}

@inproceedings{Ahmaniemi2010,
  author = {Ahmaniemi, Teemu},
  title = {Gesture Controlled Virtual Instrument with Dynamic Vibrotactile Feedback},
  pages = {485--488},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177711},
  url = {http://www.nime.org/proceedings/2010/nime2010_485.pdf},
  keywords = {Virtual instrument, Gesture, Tactile feedback, Motor control},
  abstract = {This paper investigates whether a dynamic vibrotactile feedback improves the playability of a gesture controlled virtual instrument. The instrument described in this study is based on a virtual control surface that player strikes with a hand held sensor-actuator device. We designed two tactile cues to augment the stroke across the control surface: a static and dynamic cue. The static cue was a simple burst of vibration triggered when crossing the control surface. The dynamic cue was continuous vibration increasing in amplitude when approaching the surface. We arranged an experiment to study the influence of the tactile cues in performance. In a tempo follow task, the dynamic cue yielded significantly the best temporal and periodic accuracy and control of movement velocity and amplitude. The static cue did not significantly improve the rhythmic accuracy but assisted the control of movement velocity compared to the condition without tactile feedback at all. The findings of the study indicate that careful design of dynamic vibrotactile feedback can improve the controllability of gesture based virtual instrument. }
}

@inproceedings{Hass2010,
  author = {Hass, Jeffrey},
  title = {Creating Integrated Music and Video for Dance : Lessons Learned and Lessons Ignored},
  pages = {489--492},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177793},
  url = {http://www.nime.org/proceedings/2010/nime2010_489.pdf},
  keywords = {dance, video processing, video tracking, LilyPad Arduino.},
  abstract = {In his demonstration, the author discusses the sequential progress of his technical and aesthetic decisions as composer and videographer for four large-scale works for dance through annotated video examples of live performances and PowerPoint slides. In addition, he discusses his current real-time dance work with wireless sensor interfaces using sewable LilyPad Arduino modules and Xbee radio hardware.}
}

@inproceedings{Burt2010,
  author = {Burt, Warren},
  title = {Packages for ArtWonk : New Mathematical Tools for Composers},
  pages = {493--496},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177733},
  url = {http://www.nime.org/proceedings/2010/nime2010_493.pdf},
  keywords = {Algorithmic composition, mathematical composition, probability distributions, fractals, additive sequences},
  abstract = {This paper describes a series of mathematical functions implemented by the author in the commercial algorithmic software language ArtWonk, written by John Dunn, which are offered with that language as resources for composers. It gives a history of the development of the functions, with an emphasis on how I developed them for use in my compositions.}
}

@inproceedings{Miller2010,
  author = {Miller, Jace and Hammond, Tracy},
  title = {Wiiolin : a Virtual Instrument Using the Wii Remote},
  pages = {497--500},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177853},
  url = {http://www.nime.org/proceedings/2010/nime2010_497.pdf},
  keywords = {Wii remote, virtual instrument, violin, cello, motion recognition, human computer interaction, gesture recognition.},
  abstract = {The console gaming industry is experiencing a revolution in terms of user control, and a large part to Nintendo's introduction of the Wii remote. The online open source development community has embraced the Wii remote, integrating the inexpensive technology into numerous applications. Some of the more interesting applications demonstrate how the remote hardware can be leveraged for nonstandard uses. In this paper we describe a new way of interacting with the Wii remote and sensor bar to produce music. The Wiiolin is a virtual instrument which can mimic a violin or cello. Sensor bar motion relative to the Wii remote and button presses are analyzed in real-time to generate notes. Our design is novel in that it involves the remote's infrared camera and sensor bar as an integral part of music production, allowing users to change notes by simply altering the angle of their wrist, and henceforth, bow. The Wiiolin introduces a more realistic way of instrument interaction than other attempts that rely on button presses and accelerometer data alone. }
}

@inproceedings{Meier2010,
  author = {Meier, Max and Schranner, Max},
  title = {The Planets},
  pages = {501--504},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2010},
  address = {Sydney, Australia},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177851},
  url = {http://www.nime.org/proceedings/2010/nime2010_501.pdf},
  keywords = {algorithmic composition, soft constraints, tangible interaction},
  abstract = {‘The Planets’ combines a novel approach for algorithmic composition with new human-computer interaction paradigms and realistic painting techniques. The main inspiration for it was the composition ‘The Planets’ from Gustav Holst who portrayed each planet in our solar system with music. Our application allows to interactively compose music in real-time by arranging planet constellations on an interactive table. The music generation is controlled by painted miniatures of the planets and the sun which are detected by the table and supplemented with an additional graphical visualization, creating a unique audio-visual experience. A video of the application can be found in [1].}
}

