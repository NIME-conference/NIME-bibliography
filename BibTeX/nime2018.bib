@inproceedings{nime18-Brandtsegg,
  author = {Oeyvind Brandtsegg and Trond Engum and Bernt Isak Wærstad},
  title = {Working methods and instrument design for cross-adaptive sessions},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0001.pdf},
  abstract = {This paper explores working methods and instrument design for musical performance sessions (studio and live) where cross-adaptive techniques for audio processing are utilized.  Cross-adaptive processing uses feature extraction methods and digital processing to allow the actions of one acoustic instrument to influence the timbre of another. Even though the physical interface for the musician is the familiar acoustic instrument, the musical dimensions controlled with the actions on the instrument have been expanded radically. For this reason, and when used in live performance, the cross-adaptive methods constitute new interfaces for musical expression. Not only do the musician control his or her own instrumental expression, but the instrumental actions directly influence the timbre of another instrument in the ensemble, while their own instrument's sound is modified by the actions of other musicians. In the present paper we illustrate and discuss some design issues relating to the configuration and composition of such tools for different musical situations. Such configurations include among other things the mapping of modulators, the choice of applied effects and processing methods.}
}

@inproceedings{nime18-Egozy,
  author = {Eran Egozy and Eun Young Lee},
  title = {*12*: Mobile Phone-Based Audience Participation in a Chamber Music Performance},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0002.pdf},
  abstract = {*12* is chamber music work composed with the goal of letting audience members have an engaging, individualized, and influential role in live music performance using their mobile phones as custom tailored musical instruments. The goals of direct music making, meaningful communication, intuitive interfaces, and technical transparency led to a design that purposefully limits the number of participating audience members, balances the tradeoffs between interface simplicity and control, and prioritizes the role of a graphics and animation display system that is both functional and aesthetically integrated. Survey results from the audience and stage musicians show a successful and engaging experience, and also illuminate the path towards future improvements.}
}

@inproceedings{nime18-Lind,
  author = {Anders Lind},
  title = {Animated Notation in Multiple Parts for Crowd of Non-professional Performers},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0003.pdf},
  abstract = {The Max Maestro – an animated music notation system was developed to enable the exploration of artistic possibilities for composition and performance practices within the field of contemporary art music, more specifically, to enable a large crowd of non-professional performers regardless of their musical background to perform a fixed music compositions written in multiple individual parts. Furthermore, the Max Maestro was developed to facilitate concert hall performances where non-professional performers could be synchronised with an electronic music part. This paper presents the background, the content and the artistic ideas with the Max Maestro system and gives two examples of live concert hall performances where the Max Maestro was used. An artistic research approach with an auto ethnographic method was adopted for the study. This paper contributes with new knowledge to the field of animated music notation.

}
}

@inproceedings{nime18-Brown,
  author = {Andrew R. Brown and Matthew Horrigan and Arne Eigenfeldt and Toby Gifford and Daniel Field and Jon McCormack},
  title = {Interacting with Musebots},
  pages = {19--24},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0004.pdf},
  abstract = {Musebots are autonomous musical agents that interact with other musebots to produce music. Inaugurated in 2015, musebots are now an established practice in the field of musical metacreation, which aims to automate aspects of creative practice. Originally musebot development focused on software-only ensembles of musical agents, coded by a community of developers. More recent experiments have explored humans interfacing with musebot ensembles in various ways: including through electronic interfaces in which parametric control of high-level musebot parameters are used; message-based interfaces which allow human users to communicate with musebots in their own language; and interfaces through which musebots have jammed with human musicians. Here we report on the recent developments of human interaction with musebot ensembles and reflect on some of the implications of these developments for the design of metacreative music systems.}
}

@inproceedings{nime18-Kiefer,
  author = {Chris Kiefer and Cecile Chevalier},
  title = {Towards New Modes of Collective Musical Expression through Audio Augmented Reality},
  pages = {25--28},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0005.pdf},
  abstract = {We investigate how audio augmented reality can engender new collective modes of musical expression in the context of a sound art installation, 'Listening Mirrors', exploring the creation of interactive sound environments for musicians and non-musicians alike. 'Listening Mirrors' is designed to incorporate physical objects and computational systems for altering the acoustic environment, to enhance collective listening and challenge traditional musician-instrument performance. At a formative stage in exploring audio AR technology, we conducted an audience experience study investigating questions around the potential of audio AR in creating sound installation environments for collective musical expression. We collected interview evidence about the participants' experience and analysed the data with using a grounded theory approach.  The results demonstrated that the technology has the potential to create immersive spaces where an audience can feel safe to experiment musically, and showed how AR can intervene in sound perception to instrumentalise an environment.  The results also revealed caveats about the use of audio AR, mainly centred on social inhibition and seamlessness of experience, and finding a balance between mediated worlds so that there is space for interplay between the two.}
}

@inproceedings{nime18-Matsuura,
  author = {Tomoya Matsuura and kazuhiro jo},
  title = {Aphysical Unmodeling Instrument: Sound Installation that Re-Physicalizes a Meta-Wind-Instrument Physical Model, Whirlwind},
  pages = {29--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0006.pdf},
  abstract = {Aphysical Unmodeling Instrument is the title of a sound installation that re-physicalizes the Whirlwind meta-wind-instrument physical model. We re-implemented the Whirlwind by using real-world physical objects to comprise a sound installation. The sound propagation between a speaker and microphone was used as the delay, and a paper cylinder was employed as the resonator. This paper explains the concept and implementation of this work at the 2017 HANARART  exhibition. We examine the characteristics of the work, address its limitations, and discuss the possibility of its interpretation by means of a “re-physicalization.”}
}

@inproceedings{nime18-Holbrook,
  author = {Ulf A. S. Holbrook},
  title = {An approach to stochastic spatialization - A case of Hot Pocket},
  pages = {31--32},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0007.pdf},
  abstract = {Many common and popular sound spatialisation techniques and methods rely on listeners being positioned in a "sweet-spot'' for an optimal listening position in a circle of speakers. This paper discusses a stochastic spatialisation method and its first iteration as implemented for the exhibition Hot Pocket at The Museum of Contemporary Art in Oslo in 2017. This method is implemented in Max and offers a matrix-based amplitude panning methodology which can provide a flexible means for the spatialialisation of sounds.}
}

@inproceedings{nime18-Champion,
  author = {Cory Champion and Mo H Zareei},
  title = {AM MODE: Using AM and FM Synthesis for Acoustic Drum Set Augmentation},
  pages = {33--34},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0008.pdf},
  abstract = {AM MODE is a custom-designed software interface for electronic augmentation of the acoustic drum set. The software is used in the development a series of recordings, similarly titled as AM MODE. Programmed in Max/MSP, the software uses live audio input from individual instruments within the drum set as control parameters for modulation synthesis. By using a combination of microphones and MIDI triggers, audio signal features such as the velocity of the strike of the drum, or the frequency at which the drum resonates, are tracked, interpolated, and scaled to user specifications. The resulting series of recordings is comprised of the digitally generated output of the modulation engine, in addition to both raw and modulated signals from the acoustic drum set. In this way, this project explores drum set augmentation not only at the input and from a performative angle, but also at the output, where the acoustic and the synthesized elements are merged into each other, forming a sonic hybrid.  }
}

@inproceedings{nime18-Haddad,
  author = {Don Derek Haddad and Joe Paradiso},
  title = {Kinesynth: Patching, Modulating, and Mixing a Hybrid Kinesthetic Synthesizer.},
  pages = {35--36},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0009.pdf},
  abstract = {This paper introduces the Kinesynth, a hybrid kinesthetic synthesizer that uses the human body as both an analog mixer and as a modulator using a combination of capacitive sensing in "transmit" mode and skin conductance. This is achieved when the body, through the skin, relays signals from control & audio sources to the inputs of the instrument. These signals can be harnessed from the environment, from within the Kinesynth’s internal synthesizer, or from external instrument, making the Kinesynth a mediator between the body and the environment.}
}

@inproceedings{nime18-Marogna,
  author = {Riccardo Marogna},
  title = {CABOTO: A Graphic-Based Interactive System for Composing and Performing Electronic Music},
  pages = {37--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0010.pdf},
  abstract = {CABOTO is an interactive system for live performance and composition. A graphic score sketched on paper is read by a computer vision system. The graphic elements are scanned following a symbolic-raw hybrid approach, that is, they are recognised and classified according to their shapes  but also scanned as waveforms and optical signals. All this information is mapped into the synthesis engine, which implements different kind of synthesis techniques for different shapes. In CABOTO the score is viewed as a cartographic map explored by some navigators. These navigators traverse the score in a semi-autonomous way, scanning the graphic elements found along their paths. The system tries to challenge the boundaries between the concepts of composition, score, performance, instrument, since the musical result will depend both on the composed score and the way the navigators will traverse it during the live performance. }
}

@inproceedings{nime18-Oliveira,
  author = {Gustavo Oliveira da Silveira},
  title = {The XT Synth: A New Controller for String Players},
  pages = {43--44},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0011.pdf},
  abstract = {This paper describes the concept, design, and realization of two iterations of a new controller called the XT Synth. The development of the instrument came from the desire to maintain the expressivity and familiarity of string instruments, while adding the flexibility and power usually found in keyboard controllers. There are different examples of instruments that bring the physicality and expressiveness of acoustic instruments into electronic music, from “Do it yourself” (DIY) products to commercially available ones. This paper discusses the process and the challenges faced when creating a DIY musical instrument and then subsequently transforming the instrument into a product suitable for commercialization.}
}

@inproceedings{nime18-Bin,
  author = {S. M. Astrid Bin and Nick Bryan-Kinns and Andrew P. McPherson},
  title = {Risky business: Disfluency as a design strategy},
  pages = {45--50},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0012.pdf},
  abstract = {This paper presents a study examining the effects of disfluent design on audience perception of digital musical instrument (DMI) performance. Disfluency, defined as a barrier to effortless cognitive processing, has been shown to generate better results in some contexts as it engages higher levels of cognition. We were motivated to determine if disfluent design in a DMI would result in a risk state that audiences would be able to perceive, and if this would have any effect on their evaluation of the performance. A DMI was produced that incorporated a disfluent characteristic: It would turn itself off if not constantly moved. Six physically identical instruments were produced, each in one of three versions: Control (no disfluent characteristics), mild disfluency (turned itself off slowly), and heightened disfluency (turned itself off more quickly). 6 percussionists each performed on one instrument for a live audience (N=31), and data was collected in the form of real-time feedback (via a mobile phone app), and post-hoc surveys. Though there was little difference in ratings of enjoyment between the versions of the instrument, the real-time and qualitative data suggest that disfluent behaviour in a DMI may be a way for audiences to perceive and appreciate performer skill.}
}

@inproceedings{nime18-Gibson,
  author = {Rachel Gibson},
  title = {The Theremin Textural Expander},
  pages = {51--52},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0013.pdf},
  abstract = {The voice of the theremin is more than just a simple sine wave. Its unique sound is made through two radio frequency oscillators that, when operating at almost identical frequencies, gravitate towards each other. Ultimately, this pull alters the sine wave, creating the signature sound of the theremin. The Theremin Textural Expander (TTE) explores other textures the theremin can produce when its sound is processed and manipulated through a Max/MSP patch and controlled via a MIDI pedalboard. The TTE extends the theremin’s ability, enabling it to produce five distinct new textures beyond the original.  It also features a looping system that the performer can use to layer textures created with the traditional theremin sound. Ultimately, this interface introduces a new way to play and experience the theremin; it extends its expressivity, affording a greater range of compositional possibilities and greater flexibility in free improvisation contexts.   }
}

@inproceedings{nime18-Toka,
  author = {Mert Toka and Can Ince and Mehmet Aydin Baytas},
  title = {Siren: Interface for Pattern Languages},
  pages = {53--58},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0014.pdf},
  abstract = {This paper introduces Siren, a hybrid system for algorithmic composition and live-coding performances. Its hierarchical structure allows small modifications to propagate and aggregate on lower levels for dramatic changes in the musical output. It uses functional programming language TidalCycles as the core pattern creation environment due to its inherent ability to create complex pattern relations with minimal syntax. Borrowing the best from TidalCycles, Siren augments the pattern creation process by introducing various interface level features: a multi-channel sequencer, local and global parameters, mathematical expressions, and pattern history. It presents new opportunities for recording, refining, and reusing the playback information with the pattern roll component. Subsequently, the paper concludes with a preliminary evaluation of Siren in the context of user interface design principles, which originates from the cognitive dimensions framework for musical notation design.}
}

@inproceedings{nime18-Salazar,
  author = {Spencer Salazar and Andrew Piepenbrink and Sarah Reid},
  title = {Developing a Performance Practice for Mobile Music Technology},
  pages = {59--64},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0015.pdf},
  abstract = {This paper documents an extensive and varied series of performances by the authors over the past year using mobile technology, primarily iPad tablets running the Auraglyph musical sketchpad software. These include both solo and group performances, the latter under the auspices of the Mobile Ensemble of CalArts (MECA), a group created to perform music with mobile technology devices. As a whole, this diverse mobile technology-based performance practice leverages Auraglyph's versatility to explore a number of topical issues in electronic music performance, including the use of physical and acoustical space, audience participation, and interaction design of musical instruments. 
}
}

@inproceedings{nime18-Momeni,
  author = {Ali Momeni and Daniel McNamara and Jesse Stiles},
  title = {MOM: an Extensible Platform for Rapid Prototyping and Design of Electroacoustic Instruments},
  pages = {65--71},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0016.pdf},
  abstract = {This paper provides an overview of the design, prototyping, deployment and evaluation of a multi-agent interactive sound instrument named MOM (Mobile Object for Music). MOM combines a real-time signal processing engine implemented with Pure Data on an embedded Linux platform, with gestural interaction implemented via a variety of analog and digital sensors.  Power, sound-input and sound-diffusion subsystems make the instrument autonomous and mobile. This instrument was designed in coordination with the development of an evening-length dance/music performance in which the performing musician is engaged in choreographed movements with the mobile instruments.  The design methodology relied on a participatory process that engaged an interdisciplinary team made up of technologists, musicians, composers, choreographers, and dancers.  The prototyping process relied on a mix of in-house and out-sourced digital fabrication processes intended to make the open source hardware and software design of the system accessible and affordable for other creators. }
}

@inproceedings{nime18-Robertson,
  author = {Ben Luca Robertson and Luke Dahl},
  title = {Harmonic Wand:  An Instrument for Microtonal Control and Gestural Excitation},
  pages = {72--77},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0017.pdf},
  abstract = {The Harmonic Wand is a transducer-based instrument that combines physical excitation, synthesis, and gestural control.  Our objective was to design a device that affords exploratory modes of interaction with the performer’s surroundings, as well as precise control over microtonal pitch content and other concomitant parameters.  The instrument is comprised of a hand-held wand, containing two piezo-electric transducers affixed to a pair of metal probes.  The performer uses the wand to physically excite surfaces in the environment and capture resultant signals.  Input materials are then processed using a novel application of Karplus-Strong synthesis, in which these impulses are imbued with discrete resonances.  We achieved gestural control over synthesis parameters using a secondary tactile interface, consisting of four force-sensitive resistors (FSR), a fader, and momentary switch.  As a unique feature of our instrument, we modeled pitch organization and associated parametric controls according to theoretical principles outlined in Harry Partch’s “monophonic fabric” of Just Intonation—specifically his conception of odentities, udentities, and a variable numerary nexus.  This system classifies pitch content based upon intervallic structures found in both the overtone and undertone series.  Our paper details the procedural challenges in designing the Harmonic Wand.  
}
}

@inproceedings{nime18-Macionis,
  author = {McLean J Macionis and Ajay Kapur},
  title = {Sansa: A Modified Sansula for Extended Compositional Techniques Using Machine Learning},
  pages = {78--81},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0018.pdf},
  abstract = {Sansa is an extended sansula, a hyper-instrument that is similar in design and functionality to a kalimba or thumb piano. At the heart of this interface is a series of sensors that are used to augment the tone and expand the performance capabilities of the instrument. The sensor data is further exploited using the machine learning program Wekinator, which gives users the ability to interact and perform with the instrument using several different modes of operation. In this way, Sansa is capable of both solo acoustic performances as well as complex productions that require interactions between multiple technological mediums. Sansa expands the current community of hyper-instruments by demonstrating the ways that hardware and software can extend an acoustic instrument's functionality and playability in a live performance or studio setting.}
}

@inproceedings{nime18-Turchet,
  author = {Luca Turchet and Mathieu Barthet},
  title = {Demo of interactions between a performer playing a Smart Mandolin and audience members using Musical Haptic Wearables},
  pages = {82--83},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0019.pdf},
  abstract = {This demo will showcase technologically mediated interactions between a performer playing a smart musical instrument (SMIs) and audience members using Musical Haptic Wearables (MHWs). Smart Instruments are a family of musical instruments characterized by embedded computational intelligence, wireless connectivity, an embedded sound delivery system, and an onboard system for feedback to the player. They offer direct point-to-point communication between each other and other portable sensor-enabled devices connected to local networks and to the Internet. MHWs are wearable devices for audience members, which encompass haptic stimulation, gesture tracking, and wireless connectivity features. This demo will present an architecture enabling the multidirectional creative communication between a performer playing a Smart Mandolin and audience members using armband-based MHWs.}
}

@inproceedings{nime18-Kemper,
  author = {Steven Kemper and Scott Barton},
  title = {Mechatronic Expression: Reconsidering Expressivity in Music for Robotic Instruments },
  pages = {84--87},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0020.pdf},
  abstract = {Robotic instrument designers tend to focus on the number of sound control parameters and their resolution when trying to develop expressivity in their instruments. These parameters afford greater sonic nuance related to elements of music that are traditionally associated with expressive human performances including articulation, timbre, dynamics, and phrasing. Equating the capacity for sonic nuance and musical expression stems from the “transitive” perspective that musical expression is an act of emotional communication from performer to listener. However, this perspective is problematic in the case of robotic instruments since we do not typically consider machines to be capable of expressing emotion. Contemporary theories of musical expression focus on an “intransitive” perspective, where musical meaning is generated as an embodied experience. Understanding expressivity from this perspective allows listeners to interpret performances by robotic instruments as possessing their own expressive meaning, even though the performer is a machine. It also enables musicians working with robotic instruments to develop their own unique vocabulary of expressive gestures unique to mechanical instruments. This paper explores these issues of musical expression, introducing the concept of mechatronic expression as a compositional and design strategy that highlights the musical and performative capabilities unique to robotic instruments.}
}

@inproceedings{nime18-Brown-b,
  author = {Courtney Brown},
  title = {Interactive Tango Milonga: Designing DMIs for the Social Dance Context },
  pages = {88--91},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0021.pdf},
  abstract = {Musical participation has brought individuals together in on-going communities throughout human history, aiding in the kinds of social integration essential for wellbeing. The design of Digital Musical Instruments (DMIs), however, has generally been driven by idiosyncratic artistic concerns, Western art music and dance traditions of expert performance, and short-lived interactive art installations engaging a broader public of musical novices. These DMIs rarely engage with the problems of on-going use in musical communities with existing performance idioms, repertoire, and social codes with participants representing the full learning curve of musical skill, such as social dance. Our project, Interactive Tango Milonga, an interactive Argentine tango dance system for social dance addresses these challenges in order to innovate connection, the feeling of intense relation between dance partners, music, and the larger tango community. }
}

@inproceedings{nime18-Kleinberger,
  author = {Rebecca Kleinberger},
  title = {Vocal Musical Expression with a Tactile Resonating Device and its Psychophysiological Effects},
  pages = {92--95},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0022.pdf},
  abstract = {This paper presents an experiment to investigate how new types of vocal practices can affect psychophysiological activity. We know that health can influence the voice, but can a certain use of the voice influence health through modification of mental and physical state? This study took place in the setting of the Vocal Vibrations installation. For the experiment, participants engage in a multi sensory vocal exercise with a limited set of guidance to obtain a wide spectrum of vocal performances across participants. We compare characteristics of those vocal practices to the participant’s heart rate, breathing rate, electrodermal activity and mental states. We obtained significant results suggesting that we can correlate psychophysiological states with characteristics of the vocal practice if we also take into account biographical information, and in particular mea- surement of how much people “like” their own voice.}
}

@inproceedings{nime18-Palsbröker,
  author = {Patrick Palsbröker and Christine Steinmeier and Dominic Becking},
  title = {A Framework for Modular VST-based NIMEs Using EDA and Dependency Injection},
  pages = {96--101},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0023.pdf},
  abstract = {In order to facilitate access to playing music spontaneously, the prototype of an instrument which allows a more natural learning approach was developed as part of the research project Drum-Dance-Music-Machine. The result was a modular system consisting of several VST plug-ins, which on the one hand provides a drum interface to create sounds and tones and on the other hand generates or manipulates music through dance movement, in order to simplify the understanding of more abstract characteristics of music. This paper describes the development of a new software concept for the prototype, which since then has been further developed and evaluated several times. This will improve the maintainability and extensibility of the system and eliminate design weaknesses. To do so, the existing system first will be analyzed and requirements for a new framework, which is based on the concepts of event driven architecture and dependency injection, will be defined. The components are then transferred to the new system and their performance is assessed. The approach chosen in this case study and the lessons learned are intended to provide a viable solution for solving similar problems in the development of modular VST-based NIMEs.}
}

@inproceedings{nime18-Atherton,
  author = {Jack Atherton and Ge Wang},
  title = {Chunity: Integrated Audiovisual Programming in Unity},
  pages = {102--107},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0024.pdf},
  abstract = {Chunity is a programming environment for the design of interactive audiovisual games, instruments, and experiences. It embodies an audio-driven, sound-first approach that integrates audio programming and graphics programming in the same workflow, taking advantage of strongly-timed audio programming features of the ChucK programming language and the state-of-the-art real-time graphics engine found in Unity. We describe both the system and its intended workflow for the creation of expressive audiovisual works. Chunity was evaluated as the primary software platform in a computer music and design course, where students created a diverse assortment of interactive audiovisual software. We present results from the evaluation and discuss Chunity's usability, utility, and aesthetics as a way of working. Through these, we argue for Chunity as a unique and useful way to program sound, graphics, and interaction in tandem, giving users the flexibility to use a game engine to do much more than "just" make games.}
}

@inproceedings{nime18-Ianigro,
  author = {Steffan Carlos Ianigro and Oliver Bown},
  title = {Exploring Continuous Time Recurrent Neural Networks through Novelty Search},
  pages = {108--113},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0025.pdf},
  abstract = {In this paper we expand on prior research into the use of Continuous Time Recurrent Neural Networks (CTRNNs) as evolvable generators of musical structures such as audio waveforms. This type of neural network has a compact structure and is capable of producing a large range of temporal dynamics. Due to these properties, we believe that CTRNNs combined with evolutionary algorithms (EA) could offer musicians many creative possibilities for the exploration of sound. In prior work, we have explored the use of interactive and target-based EA designs to tap into the creative possibilities of CTRNNs. Our results have shown promise for the use of CTRNNs in the audio domain. However, we feel neither EA designs allow both open-ended discovery and effective navigation of the CTRNN audio search space by musicians. Within this paper, we explore the possibility of using novelty search as an alternative algorithm that facilitates both open-ended and rapid discovery of the CTRNN creative search space.}
}

@inproceedings{nime18-Bowers,
  author = {John Bowers and Owen Green},
  title = {All the Noises: Hijacking Listening Machines for Performative Research},
  pages = {114--119},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0026.pdf},
  abstract = {Research into machine listening has intensified in recent years creating a variety of techniques for recognising musical features suitable, for example, in musicological analysis or commercial application in song recognition. Within NIME, several projects exist seeking to make these techniques useful in real-time music making. However, we debate whether the functionally-oriented approaches inherited from engineering domains that much machine listening research manifests is fully suited to the exploratory, divergent, boundary-stretching, uncertainty-seeking, playful and irreverent orientations of many artists. To explore this, we engaged in a concerted collaborative design exercise in which many different listening algorithms were implemented and presented with input which challenged their customary range of application and the implicit norms of musicality which research can take for granted. An immersive 3D spatialised multichannel environment was created in which the algorithms could be explored in a hybrid installation/performance/lecture form of research presentation. The paper closes with reflections on the creative value of ‘hijacking’ formal approaches into deviant contexts, the typically undocumented practical know-how required to make algorithms work, the productivity of a playfully irreverent relationship between engineering and artistic approaches to NIME, and a sketch of a sonocybernetic aesthetics for our work.}
}

@inproceedings{nime18-Schramm,
  author = {Rodrigo Schramm and Federico Visi and André Brasil and Marcelo O Johann},
  title = {A polyphonic pitch tracking embedded system for rapid instrument augmentation},
  pages = {120--125},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0027.pdf},
  abstract = {This paper presents a system for easily augmenting polyphonic pitched instruments. The entire system is designed to run on a low-cost embedded computer, suitable for live performance and easy to customise for different use cases. The core of the system implements real-time spectrum factorisation, decomposing polyphonic audio input signals into music note activations. New instruments can be easily added to the system with the help of custom spectral template dictionaries. Instrument augmentation is achieved by replacing or mixing the instrument's original sounds with a large variety of synthetic or sampled sounds, which follow the polyphonic pitch activations.
}
}

@inproceedings{nime18-Tahiroglu,
  author = {Koray Tahiroglu and Michael Gurevich and R. Benjamin Knapp},
  title = {Contextualising Idiomatic Gestures in Musical Interactions with NIMEs},
  pages = {126--131},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0028.pdf},
  abstract = {This paper introduces various ways that idiomatic gestures emerge in performance practice with new musical instruments. It demonstrates that idiomatic gestures can play an important role in the development of personalized performance practices that can be the basis for the development of style and expression. Three detailed examples -- biocontrollers, accordion-inspired instruments, and a networked intelligent controller --  illustrate how a complex suite of factors throughout the design, composition and performance processes can influence the development of idiomatic gestures. We argue that the explicit consideration of idiomatic gestures throughout the life cycle of new instruments can facilitate the emergence of style and give rise to performances that can develop rich layers of meaning.}
}

@inproceedings{nime18-Hantrakul,
  author = {Lamtharn Hantrakul},
  title = {GestureRNN:  A neural gesture system for the Roli Lightpad Block},
  pages = {132--137},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0029.pdf},
  abstract = {Machine learning and deep learning has recently made a large impact in the artistic community. In many of these applications however, the model is often used to render the high dimensional output directly e.g. every individual pixel in the final image. Humans arguably operate in much lower dimensional spaces during the creative process e.g. the broad movements of a brush. In this paper, we design a neural gesture system for music generation based around this concept. Instead of directly generating audio, we train a Long Short Term Memory (LSTM) recurrent neural network to generate instantaneous position and pressure on the Roli Lightpad instrument. These generated coordinates in turn, give rise to the sonic output defined in the synth engine.  The system relies on learning these movements from a musician who has already developed a palette of musical gestures idiomatic to the Lightpad. Unlike many deep learning systems that render high dimensional output, our low-dimensional system can be run in real-time, enabling the first real time gestural duet of its kind between a player and a recurrent neural network on the Lightpad instrument.}
}

@inproceedings{nime18-DiDonato,
  author = {Di Donato, Balandino  and Jamie Bullock and Atau Tanaka},
  title = {Myo Mapper: a Myo armband to OSC mapper},
  pages = {138--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0030.pdf},
  abstract = {Myo Mapper is a free and open source cross-platform application to map data from the gestural device Myo armband into Open Sound Control (OSC) messages. It represents a `quick and easy' solution for exploring the Myo's potential for realising new interfaces for musical expression. Together with details of the software, this paper reports some applications in which Myo Mapper has been successfully used and a qualitative evaluation. We then proposed guidelines for using Myo data in interactive artworks based on insight gained from the works described and the evaluation. Findings show that Myo Mapper empowers artists and non-skilled developers to easily take advantage of Myo data high-level features for realising interactive artistic works. It also facilitates the recognition of poses and gestures beyond those included with the product by using third-party interactive machine learning software.}
}

@inproceedings{nime18-Visi,
  author = {Federico Visi and Luke Dahl},
  title = {Real-Time Motion Capture Analysis and Music Interaction with the Modosc Descriptor Library},
  pages = {144--147},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0031.pdf},
  abstract = {We present modosc, a set of Max abstractions designed for computing motion descriptors from raw motion capture data in real time. The library contains methods for extracting descriptors useful for expressive movement analysis and sonic interaction design. modosc is designed to address the data handling and synchronization issues that often arise when working with complex marker sets. This is achieved by adopting a multiparadigm approach facilitated by odot and Open Sound Control to overcome some of the limitations of conventional Max programming, and structure incoming and outgoing data streams in a meaningful and easily accessible manner. After describing the contents of the library and how data streams are structured and processed, we report on a sonic interaction design use case involving motion feature extraction and machine learning.}
}

@inproceedings{nime18-Arslan,
  author = {Cagan Arslan and Florent Berthaut and Jean Martinet and Ioan Marius Bilasco and Laurent Grisoni},
  title = {The Phone with the Flow: Combining Touch + Optical Flow in Mobile Instruments},
  pages = {148--151},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0032.pdf},
  abstract = {Mobile devices have been a promising platform for musical performance thanks to the various sensors readily available on board. In particular, mobile cameras can provide rich input as they can capture a wide variety of user gestures or environment dynamics. However, this raw camera input only provides continuous parameters and requires expensive computation. In this paper, we propose to combine motion/gesture input with the touch input, in order to filter movement information both temporally and spatially, thus increasing expressiveness while reducing computation time. We present a design space which demonstrates the diversity of interactions that our technique enables. We also report the results of a user study in which we observe how musicians appropriate the interaction space with an example instrument.}
}

@inproceedings{nime18-Engeln,
  author = {Lars Engeln and Dietrich Kammer and Leon Brandt and Rainer Groh},
  title = {Multi-Touch Enhanced Visual Audio-Morphing},
  pages = {152--155},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0033.pdf},
  abstract = {Many digital interfaces for audio effects still resemble racks and cases of their hardware counterparts. For instance, DSP-algorithms are often adjusted via direct value input, sliders, or knobs. While recent research has started to experiment with the capabilities offered by modern interfaces, there are no examples for productive applications such as audio-morphing. Audio-morphing as a special field of DSP has a high complexity for the morph itself and for the parametrization of the transition between two sources. We propose a multi-touch enhanced interface for visual audiomorphing. This interface visualizes the internal processing and allows direct manipulation of the morphing parameters in the visualization. Using multi-touch gestures to manipulate audio-morphing in a visual way, sound design and music production becomes more unrestricted and creative.}
}

@inproceedings{nime18-Çamcı,
  author = {Anıl Çamcı},
  title = {GrainTrain: A Hand-drawn Multi-touch Interface for Granular Synthesis},
  pages = {156--161},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0034.pdf},
  abstract = {We describe an innovative multi-touch performance tool for real-time granular synthesis based on hand-drawn waveform paths. GrainTrain is a cross-platform web application that can run on both desktop and mobile computers, including tablets and phones. In this paper, we first offer an analysis of existing granular synthesis tools from an interaction stand-point, and outline a taxonomy of common interaction paradigms used in their designs. We then delineate the implementation of GrainTrain, and its unique approach to controlling real-time granular synthesis. We describe practical scenarios in which GrainTrain enables new performance possibilities. Finally, we discuss the results of a user study, and provide reports from expert users who evaluated GrainTrain.}
}

@inproceedings{nime18-xia,
  author = {gus xia and Roger B. Dannenberg},
  title = {ShIFT: A Semi-haptic Interface for Flute Tutoring},
  pages = {162--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0035.pdf},
  abstract = {Traditional instrument learning procedure is time-consuming; it begins with learning music notations and necessitates layers of sophistication and abstraction. Haptic interfaces open another door to the music world for the vast majority of talentless beginners when traditional training methods are not effective. However, the existing haptic interfaces can only be used to learn specially designed pieces with great restrictions on duration and pitch range due to the fact that it is only feasible to guide a part of performance motion haptically for most instruments. Our study breaks such restrictions using a semi-haptic guidance method. For the first time, the pitch range of the haptically learned pieces go beyond an octave (with the fingering motion covers most of the possible choices) and the duration of learned pieces cover a whole phrase. This significant change leads to a more realistic instrument learning process. Experiments show that semi-haptic interface is effective as long as learners are not “tone deaf”. Using our prototype device, the learning rate is about 30% faster compared with learning from videos.}
}

@inproceedings{nime18-Morreale,
  author = {Fabio Morreale and Andrew P. McPherson and Marcelo Wanderley},
  title = {NIME Identity from the Performer's Perspective},
  pages = {168--173},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0036.pdf},
  abstract = {The term `NIME' - New Interfaces for Musical Expression - has come to signify both technical and cultural characteristics. Not all new musical instruments are NIMEs, and not all NIMEs are defined as such for the sole ephemeral condition of being new. So, what are the typical characteristics of NIMEs and what are their roles in performers' practice? Is there a typical NIME repertoire? This paper aims to address these questions with a bottom up approach. We reflect on the answers of 78 NIME performers to an online questionnaire discussing their performance experience with NIMEs. The results of our investigation explore the role of NIMEs in the performers' practice and identify the values that are common among performers. We find that most NIMEs are viewed as exploratory tools created by and for performers, and that they are constantly in development and almost in no occasions in a finite state. The findings of our survey also reflect upon virtuosity with NIMEs, whose peculiar performance practice results in learning trajectories that often do not lead to the development of virtuosity as it is commonly understood in traditional performance.}
}

@inproceedings{nime18-Xambó,
  author = {Anna Xambó},
  title = {Who Are the Women Authors in NIME?–Improving Gender Balance in NIME Research},
  pages = {174--177},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0037.pdf},
  abstract = {In recent years, there has been an increase in awareness of the underrepresentation of women in the sound and music computing fields. The New Interfaces for Musical Expression (NIME) conference is not an exception, with a number of open questions remaining around the issue. In the present paper, we study the presence and evolution over time of women authors in NIME since the beginning of the conference in 2001 until 2017. We discuss the results of such a gender imbalance and potential solutions by summarizing the actions taken by a number of worldwide initiatives that have put an effort into making women's work visible in our field, with a particular emphasis on Women in Music Tech (WiMT), a student-led organization that aims to encourage more women to join music technology, as a case study. We conclude with a hope for an improvement in the representation of women in NIME by presenting WiNIME, a public online database that details who are the women authors in NIME.}
}

@inproceedings{nime18-Reid,
  author = {Sarah Reid and Sara Sithi-Amnuai and Ajay Kapur},
  title = {Women Who Build Things: Gestural Controllers, Augmented Instruments, and Musical Mechatronics},
  pages = {178--183},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0038.pdf},
  abstract = {This paper presents a collection of hardware-based technologies for live performance developed by women over the last few decades. The field of music technology and interface design has a significant gender imbalance, with men greatly outnumbering women. The purpose of this paper is to promote the visibility and representation of women in this field, and to encourage discussion on the importance of mentorship and role models for young women and girls in music technology.}
}

@inproceedings{nime18-Jack,
  author = {Robert H Jack and Jacob Harrison and Fabio Morreale and Andrew P. McPherson},
  title = {Democratising DMIs: the relationship of expertise and control intimacy},
  pages = {184--189},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0039.pdf},
  abstract = {An oft-cited aspiration of digital musical instrument (DMI) design is to create instruments, in the words of Wessel and Wright, with a ‘low entry fee and no ceiling on virtuosity’. This is a difficult task to achieve: many new instruments are aimed at either the expert or amateur musician, with few instruments catering for both. There is often a balance between learning curve and the nuance of musical control in DMIs. In this paper we present a study conducted with non-musicians and guitarists playing guitar-derivative DMIs with variable levels of control intimacy: how the richness and nuance of a performer’s movement translates into the musical output of an instrument. Findings suggest a significant difference in preference for levels of control intimacy between the guitarists and the non-musicians. In particular, the guitarists unanimously preferred the richest of the two settings whereas the non-musicians generally preferred the setting with lower richness. This difference is notable because it is often taken as a given that increasing richness is a way to make instruments more enjoyable to play, however, this result only seems to be true for expert players.}
}

@inproceedings{nime18-Marquez-Borbon,
  author = {Adnan Marquez-Borbon and Juan Pablo Martinez-Avila},
  title = {The Problem of DMI Adoption and Longevity: Envisioning a NIME Performance Pedagogy},
  pages = {190--195},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0040.pdf},
  abstract = {This paper addresses the prevailing longevity problem of digital musical instruments (DMIs) in NIME research and design by proposing a holistic system design approach. Despite recent efforts to examine the main contributing factors of DMI falling into obsolescence, such attempts to remedy this issue largely place focus on the artifacts establishing themselves, their design processes and technologies. However, few existing studies have attempted to proactively build a community around technological platforms for DMIs, whilst bearing in mind the social dynamics and activities necessary for a budding community. We observe that such attempts while important in their undertaking, are limited in their scope. In this paper we will discuss that achieving some sort of longevity must be addressed beyond the device itself and must tackle broader ecosystemic factors. We hypothesize, that a longevous DMI design must not only take into account a target community but it may also require a non-traditional pedagogical system that sustains artistic practice.}
}

@inproceedings{nime18-Martin,
  author = {Martin, Charles Patrick  and Jensenius, Alexander Refsum and Jim Torresen},
  title = {Composing an Ensemble Standstill Work for Myo and Bela},
  pages = {196--197},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0041.pdf},
  abstract = {This paper describes the process of developing a standstill performance work using the Myo gesture control armband and the Bela embedded computing platform. The combination of Myo and Bela allows a portable and extensible version of the standstill performance concept while introducing muscle tension as an additional control parameter.  We describe the technical details of our setup and introduce Myo-to-Bela and Myo-to-OSC software bridges that assist with prototyping compositions using the Myo controller.}
}

@inproceedings{nime18-Nieva,
  author = {Alex Nieva and Johnty Wang and Joseph Malloch and Marcelo Wanderley},
  title = {The T-Stick: Maintaining a 12 year-old Digital Musical Instrument},
  pages = {198--199},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0042.pdf},
  abstract = {This paper presents the work to maintain several copies of the digital musical instrument (DMI) called the T-Stick in the hopes of extending their useful lifetime.  The T-Sticks were originally conceived in 2006 and 20 copies have been built over the last 12 years. While they all preserve the original design concept, their evolution resulted in variations in choice  of  microcontrollers,  and  sensors. We  worked  with eight copies of the second and fourth generation T-Sticks to overcome issues related to the aging of components, changes in external software, lack of documentation, and in general, the problem of technical maintenance.}
}

@inproceedings{nime18-Dewey,
  author = {Christopher Dewey and Jonathan P. Wakefield},
  title = {MIDI Keyboard Defined DJ Performance System},
  pages = {200--201},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0043.pdf},
  abstract = {This paper explores the use of the ubiquitous MIDI keyboard to control a DJ performance system. The prototype system uses a two octave keyboard with each octave controlling one audio track. Each audio track has four two-bar loops which play in synchronisation switchable by its respective octave’s first four black keys. The top key of the keyboard toggles between frequency filter mode and time slicer mode. In frequency filter mode the white keys provide seven bands of latched frequency filtering. In time slicer mode the white keys plus black B flat key provide latched on/off control of eight time slices of the loop. The system was informally evaluated by nine subjects. The frequency filter mode combined with loop switching worked well with the MIDI keyboard interface. All subjects agreed that all tools had creative performance potential that could be developed by further practice.}
}

@inproceedings{nime18-Engum,
  author = {Trond Engum and Otto Jonassen Wittner},
  title = {Democratizing Interactive Music Production over the Internet},
  pages = {202--203},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0044.pdf},
  abstract = {This paper describes an ongoing research project which address challenges and opportunities when collaborating interactively in real time in a "virtual" sound studio with several partners in different locations. "Virtual" in this context referring to an interconnected and inter-domain studio environment consisting of several local production systems connected to public and private networks. This paper reports experiences and challenges related to two different production scenarios conducted in 2017.}
}

@inproceedings{nime18-Charles,
  author = {Jean-Francois Charles and Carlos Cotallo Solares and Carlos Toro Tobon and Andrew Willette},
  title = {Using the Axoloti Embedded Sound Processing Platform to Foster Experimentation and Creativity},
  pages = {204--205},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0045.pdf},
  abstract = {This paper describes how the Axoloti platform is well suited to teach a beginners’ course about new elecro-acoustic musical instruments and how it fits the needs of artists who want to work with an embedded sound processing platform and get creative at the crossroads of acoustics and electronics. First, we present the criteria used to choose a platform for the course titled "Creating New Musical Instruments" given at the University of Iowa in the Fall of 2017. Then, we explain why we chose the Axoloti board and development environment.}
}

@inproceedings{nime18-Tsoukalas,
  author = {Kyriakos Tsoukalas and Ivica Ico Bukvic},
  title = {Introducing a K-12 Mechatronic NIME Kit},
  pages = {206--209},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0046.pdf},
  abstract = {The following paper introduces a new mechatronic NIME kit that uses new additions to the Pd-L2Ork visual programing environment and its K-12 learning module. It is designed to facilitate the creation of simple mechatronics systems for physical sound production in K-12 and production scenarios. The new set of objects builds on the existing support for the Raspberry Pi platform to also include the use of electric actuators via the microcomputer’s GPIO system. Moreover, we discuss implications of the newly introduced kit in the creative and K-12 education scenarios by sharing observations from a series of pilot workshops, with particular focus on using mechatronic NIMEs as a catalyst for the development of programing skills.}
}

@inproceedings{nime18-Bennett,
  author = {Daniel Bennett and Peter Bennett and Anne Roudaut},
  title = {Neurythmic: A Rhythm Creation Tool Based on  Central Pattern Generators},
  pages = {210--215},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0047.pdf},
  abstract = {We describe the development of Neurythmic: an interactive system for the creation and performance of fluid, expressive musical rhythms using Central Pattern Generators (CPGs).  CPGs are neural networks which generate adaptive rhythmic signals. They simulate structures in animals which underly behaviours such as heartbeat, gut peristalsis and complex motor control. Neurythmic is the first such system to use CPGs for interactive rhythm creation. We discuss how Neurythmic uses the entrainment behaviour of these networks to support the creation of rhythms while avoiding the rigidity of grid quantisation approaches. As well as discussing the development, design and evaluation of Neurythmic, we discuss relevant properties of the CPG networks used (Matsuoka's Neural Oscillator), and describe methods for their control. Evaluation with expert and professional musicians shows that Neurythmic is a versatile tool, adapting well to a range of quite different musical approaches.}
}

@inproceedings{nime18-Granger,
  author = {James Granger and Mateo Aviles and Joshua Kirby and Austin Griffin and Johnny Yoon and Raniero A. Lara-Garduno and Tracy Hammond},
  title = {Evaluating LED-based interface for Lumanote composition creation tool},
  pages = {216--221},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0048.pdf},
  abstract = {Composing music typically requires years of music theory experience and knowledge that includes but is not limited to chord progression, melody composition theory, and an understanding of whole-step/half-step passing tones among others. For that reason, certain songwriters such as singers may find a necessity to hire experienced pianists to help compose their music. In order to facilitate the process for beginner and aspiring musicians, we have developed Lumanote, a music composition tool that aids songwriters by presenting real-time suggestions on appropriate melody notes and chord progression. While a preliminary evaluation yielded favorable results for beginners, many commented on the difficulty of having to map the note suggestions displayed on the on-screen interface to the physical keyboard they were playing on. This paper presents the resulting solution: an LED-based feedback system that is designed to be directly attached to any standard MIDI keyboard. This peripheral aims to help map note suggestions directly to the physical keys of a musical keyboard. A study consisting of 22 individuals was conducted to compare the effectiveness of the new LED-based system with the existing computer interface, finding that the vast majority of users preferred the LED system. Three experienced musicians also judged and ranked the compositions, noting significant improvement in song quality when using either system, and citing comparable quality between compositions that used either interface.}
}

@inproceedings{nime18-Meneses,
  author = {Eduardo Meneses and Sergio Freire and Marcelo Wanderley},
  title = {GuitarAMI and GuiaRT: two independent yet complementary projects on augmented nylon guitars},
  pages = {222--227},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0049.pdf},
  abstract = {This paper describes two augmented nylon-string guitar projects developed in different institutions. GuitarAMI uses sensors to modify the classical guitars constraints while GuiaRT uses digital signal processing to create virtual guitarists that interact with the performer in real-time. After a bibliographic review of Augmented Musical Instruments (AMIs) based on guitars, we present the details of the two projects and compare them using an adapted dimensional space representation. Highlighting the complementarity and cross-influences between the projects, we propose avenues for future collaborative work.}
}

@inproceedings{nime18-Stolfi,
  author = {Ariane de Souza Stolfi and Miguel Ceriani and Luca Turchet and Mathieu Barthet},
  title = {Playsound.space: Inclusive Free Music Improvisations Using Audio Commons},
  pages = {228--233},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0050.pdf},
  abstract = {Playsound.space is a web-based tool to search for and play Creative Commons licensed-sounds which can be applied to free improvisation, experimental music production and soundscape composition. It provides a fast access to about 400k non-musical and musical sounds provided by Freesound, and allows users to play/loop single or multiple sounds retrieved through text based search. Sound discovery is facilitated by use of semantic searches and sound visual representations (spectrograms). Guided by the motivation to create an intuitive tool to support music practice that could suit both novice and trained musicians, we developed and improved the system in a continuous process, gathering frequent feedback from a range of users with various skills. We assessed the prototype with 18 non musician and musician participants during free music improvisation sessions. Results indicate that the system was found easy to use and supports creative collaboration and expressiveness irrespective of musical ability. We identified further design challenges linked to creative identification, control and content quality.}
}

@inproceedings{nime18-Harding,
  author = {John Harding and Richard Graham and Edwin Park},
  title = {CTRL: A Flexible, Precision Interface for Analog Synthesis},
  pages = {234--237},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0051.pdf},
  abstract = {This paper provides a new interface for the production and distribution of high resolution analog control signals, particularly aimed toward the control of analog modular synthesisers. Control Voltage/Gate interfaces generate Control Voltage (CV) and Gate Voltage (Gate) as a means of controlling note pitch and length respectively, and have been with us since 1986 [2]. The authors provide a unique custom CV/Gate interface and dedicated communication protocol which leverages standard USB Serial functionality and enables connectivity over a plethora of computing devices, including embedded devices such as the Raspberry Pi and ARM based devices including widely available ‘Android TV Boxes’. We provide a general overview of the unique hardware and communication protocol developments followed by usage case examples toward tuning and embedded platforms, leveraging softwares ranging from Pure Data (Pd), Max, and Max for Live (M4L).
}
}

@inproceedings{nime18-Beyls,
  author = {Peter Beyls},
  title = {Motivated Learning in Human-Machine Improvisation},
  pages = {238--243},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0052.pdf},
  abstract = {This paper describes a machine learning approach in the context of non-idiomatic human-machine improvisation. In an attempt to avoid explicit mapping of user actions to machine responses, an experimental machine learning strategy is suggested where rewards are derived from the implied motivation of the human interactor – two motivations are at work: integration (aiming to connect with machine generated material) and expression (independent activity). By tracking consecutive changes in musical distance (i.e. melodic similarity) between human and machine, such motivations can be inferred. A variation of Q-learning is used featuring a self-optimizing variable length state-action-reward list. The system (called Pock) is tunable into particular behavioral niches by means of a limited number of parameters. Pock is designed as a recursive structure and behaves as a complex dynamical system. When tracking systems variables over time, emergent non-trivial patterns reveal experimental evidence of attractors demonstrating successful adaptation.}
}

@inproceedings{nime18-Chandran,
  author = {Deepak Chandran and Ge Wang},
  title = {InterFACE: new faces for musical expression},
  pages = {244--248},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0053.pdf},
  abstract = {InterFACE is an interactive system for musical creation, mediated primarily through the user’s facial expressions and movements. It aims to take advantage of the expressive capabilities of the human face to create music in a way that is both expressive and whimsical. This paper introduces the designs of three virtual instruments in the InterFACE system: namely, FACEdrum (a drum machine), GrannyFACE (a granular synthesis sampler), and FACEorgan (a laptop mouth organ using both face tracking and audio analysis). We present the design behind these instruments and consider what it means to be able to create music with one’s face. Finally, we discuss the usability and aesthetic criteria for evaluating such a system, taking into account our initial design goals as well as the resulting experience for the performer and audience.}
}

@inproceedings{nime18-Polfreman,
  author = {Richard Polfreman},
  title = {Hand Posture Recognition: IR, IMU and sEMG},
  pages = {249--254},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0054.pdf},
  abstract = {Hands are important anatomical structures for musical performance, and recent developments in input device technology have allowed rather detailed capture of hand gestures using consumer-level products. While in some musical contexts, detailed hand and finger movements are required, in others it is sufficient to communicate discrete hand postures to indicate selection or other state changes. This research compared three approaches to capturing hand gestures where the shape of the hand, i.e. the relative positions and angles of finger joints, are an important part of the gesture. A number of sensor types can be used to capture information about hand posture, each of which has various practical advantages and disadvantages for music applications. This study compared three approaches, using optical, inertial and muscular information, with three sets of 5 hand postures (i.e. static gestures) and gesture recognition algorithms applied to the device data, aiming to determine which methods are most effective.}
}

@inproceedings{nime18-Malloch,
  author = {Joseph Malloch and Marlon Mario Schumacher and Stephen Sinclair and Marcelo Wanderley},
  title = {The Digital Orchestra Toolbox for Max},
  pages = {255--258},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0055.pdf},
  abstract = {The Digital Orchestra Toolbox for Max is an open-source collection of small modular software tools for aiding the development of Digital Musical Instruments. Each tool takes the form of an "abstraction" for the visual programming environment Max, meaning it can be opened and understood by users within the Max environment, as well as copied, modified, and appropriated as desired. This paper describes the origins of the Toolbox and our motivations for creating it, broadly outlines the types of tools included, and follows the development of the project over the last twelve years. We also present examples of several digital musical instruments built using the Toolbox.}
}

@inproceedings{nime18-Manaris,
  author = {Bill Manaris and Pangur Brougham-Cook and Dana Hughes and Andrew R. Brown},
  title = {JythonMusic: An Environment for Developing Interactive Music Systems},
  pages = {259--262},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0056.pdf},
  abstract = {JythonMusic is a software environment for developing interactive musical experiences and systems.  It is based on jMusic, a software environment for computer-assisted composition, which was extended within the last decade into a more comprehensive framework providing composers and software developers with libraries for music making, image manipulation, building graphical user interfaces, and interacting with external devices via MIDI and OSC, among others.  This environment is free and open source.  It is based on Python, therefore it provides more economical syntax relative to Java- and C/C++-like languages.  JythonMusic rests on top of Java, so it provides access to the complete Java API and external Java-based libraries as needed.  Also, it works seamlessly with other software, such as PureData, Max/MSP, and Processing.  The paper provides an overview of important JythonMusic libraries related to constructing interactive musical experiences.  It demonstrates their scope and utility by summarizing several projects developed using JythonMusic, including interactive sound art installations, new interfaces for sound manipulation and spatialization, as well as various explorations on mapping among motion, gesture and music.}
}

@inproceedings{nime18-Leib,
  author = {Steven Leib and Anıl Çamcı},
  title = {Triplexer: An Expression Pedal with New Degrees of Freedom},
  pages = {263--268},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0057.pdf},
  abstract = {We introduce the Triplexer, a novel foot controller that gives the performer 3 degrees of freedom over the control of various effects parameters. With the Triplexer, we aim to expand the performer's control space by augmenting the capabilities of the common expression pedal that is found in most effects rigs. Using industrial-grade weight-detection sensors and widely-adopted communication protocols, the Triplexer offers a flexible platform that can be integrated into various performance setups and situations. In this paper, we detail the design of the Triplexer by describing its hardware, embedded signal processing, and mapping software implementations. We also offer the results of a user study, which we conducted to evaluate the usability of our controller.}
}

@inproceedings{nime18-Úlfarsson,
  author = {Halldór Úlfarsson},
  title = {The halldorophone: The ongoing innovation of a cello-like drone instrument},
  pages = {269--274},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0058.pdf},
  abstract = {This paper reports upon the process of innovation of a new instrument. The author has developed the halldorophone a new electroacoustic string instrument which makes use of positive feedback as a key element in generating its sound. 
  An important objective of the project has been to encourage its use by practicing musicians. After ten years of use, the halldorophone has a growing repertoire of works by prominent composers and performers. During the development of the instrument, the question has been asked: “why do musicians want to use this instrument?” and answers have been found through on-going (informal) user studies and feedback. As the project progresses, a picture emerges of what qualities have led to a culture of acceptance and use around this new instrument.
  This paper describes the halldorophone and presents the rationale for its major design features and ergonomic choices, as they relate to the overarching objective of nurturing a culture of use and connects it to wider trends.
}
}

@inproceedings{nime18-Tsoukalas-b,
  author = {Kyriakos Tsoukalas and Joseph Kubalak and Ivica Ico Bukvic},
  title = {L2OrkMote: Reimagining a Low-Cost Wearable Controller for a Live Gesture-Centric Music Performance},
  pages = {275--280},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0059.pdf},
  abstract = {Laptop orchestras create music, although digitally produced, in a collaborative live performance not unlike a traditional orchestra. The recent increase in interest and investment in this style of music creation has paved the way for novel methods for musicians to create and interact with music. To this end, a number of nontraditional instruments have been constructed that enable musicians to control sound production beyond pitch and volume, integrating filtering, musical effects, etc. Wii Remotes (WiiMotes) have seen heavy use in maker communities, including laptop orchestras, for their robust sensor array and low cost. The placement of sensors and the form factor of the device itself are suited for video games, not necessarily live music creation. In this paper, the authors present a new controller design, based on the WiiMote hardware platform, to address usability in gesture-centric music performance. Based on the pilot-study data, the new controller offers unrestricted two-hand gesture production, smaller footprint, and lower muscle strain.}
}

@inproceedings{nime18-Armitage,
  author = {Jack Armitage and Andrew P. McPherson},
  title = {Crafting Digital Musical Instruments: An Exploratory Workshop Study},
  pages = {281--286},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0060.pdf},
  abstract = {In digital musical instrument design, different tools and methods offer a variety of approaches for constraining the exploration of musical gestures and sounds. Toolkits made of modular components usefully constrain exploration towards simple, quick and functional combinations, and methods such as sketching and model-making alternatively allow imagination and narrative to guide exploration. In this work we sought to investigate a context where these approaches to exploration were combined. We designed a craft workshop for 20 musical instrument designers, where groups were given the same partly-finished instrument to craft for one hour with raw materials, and though the task was open ended, they were prompted to focus on subtle details that might distinguish their instruments. Despite the prompt the groups diverged dramatically in intent and style, and generated gestural language rapidly and flexibly. By the end, each group had developed a distinctive approach to constraint, exploratory style, collaboration and interpretation of the instrument and workshop materials. We reflect on this outcome to discuss advantages and disadvantages to integrating digital musical instrument design tools and methods, and how to further investigate and extend this approach.}
}

@inproceedings{nime18-Kalo,
  author = {Ammar Kalo and Georg Essl},
  title = {Individual Fabrication of Cymbals using Incremental Robotic Sheet Forming},
  pages = {287--292},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0061.pdf},
  abstract = {Incremental robotic sheet forming is used to fabricate a novel cymbal shape based on models of geometric chaos for stadium shaped boundaries. This provides a proof-of-concept that this robotic fabrication technique might be a candidate method for creating novel metallic ideophones that are based on sheet deformations. Given that the technique does not require molding, it is well suited for both rapid and iterative prototyping and the fabrication of individual pieces. With advances in miniaturization, this approach may also be suitable for personal fabrication. In this paper we discuss this technique as well as aspects of the geometry of stadium cymbals and their impact on the resulting instrument.}
}

@inproceedings{nime18-McDowell,
  author = {John McDowell},
  title = {Haptic-Listening and the Classical Guitar},
  pages = {293--298},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0062.pdf},
  abstract = {This paper reports the development of a ‘haptic-listening’ system which presents the listener with a representation of the vibrotactile feedback perceived by a classical guitarist during performance through the use of haptic feedback technology. The paper describes the design of the haptic-listening system which is in two prototypes: the “DIY Haptic Guitar” and a more robust haptic-listening Trial prototype using a Reckhorn BS-200 shaker. Through two experiments, the perceptual significance and overall musical contribution of the addition of haptic feedback in a listening context was evaluated. Subjects preferred listening to the classical guitar presentation with the addition of haptic feedback and the addition of haptic feedback contributed to listeners’ engagement with a performance. The results of the experiments and their implications are discussed in this paper.}
}

@inproceedings{nime18-Harrison,
  author = {Jacob Harrison and Robert H Jack and Fabio Morreale and Andrew P. McPherson},
  title = {When is a Guitar not a Guitar? Cultural Form, Input Modality and Expertise},
  pages = {299--304},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0063.pdf},
  abstract = {The design of traditional musical instruments is a process of incremental refinement over many centuries of innovation. Conversely, digital musical instruments (DMIs), being unconstrained by requirements of efficient acoustic sound production and ergonomics, can take on forms which are more abstract in their relation to the mechanism of control and sound production. In this paper we consider the case of designing DMIs for use in existing musical cultures, and pose questions around the social and technical acceptability of certain design choices relating to global physical form and input modality (sensing strategy and the input gestures that it affords). We designed four guitar-derivative DMIs designed to be suitable to perform a strummed harmonic accompaniment to a folk tune. Each instrument possessed varying degrees of `guitar-likeness', based either on the form and aesthetics of the guitar or the specific mode of interaction. We conducted a study where both non-musicians and guitarists played two versions of the instruments and completed musical tasks with each instrument. The results of this study highlight the complex interaction between global form and input modality when designing for existing musical cultures.}
}

@inproceedings{nime18-Larsen,
  author = {Jeppe Larsen and Hendrik Knoche and Dan Overholt},
  title = {A Longitudinal Field Trial with a Hemiplegic Guitarist Using The Actuated Guitar},
  pages = {305--310},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0064.pdf},
  abstract = {Common emotional effects following a stroke include depression, apathy and lack of motivation. We conducted a longitudinal case study to investigate if enabling a post-stroke former guitarist re-learn to play guitar would help increase motivation for self rehabilitation and quality of life after suffering a stroke. The intervention lasted three weeks during which the participant had a fully functional electrical guitar fitted with a strumming device controlled by a foot pedal at his free disposal. The device replaced right strumming of the strings, and the study showed that the participant, who was highly motivated, played 20 sessions despite system latency and reduced musical expression. He incorporated his own literature and equipment into his playing routine and improved greatly as the study progressed. He was able to play alone and keep a steady rhythm in time with backing tracks that went as fast as 120bpm. During the study he was able to lower his error rate to 33%, while his average flutter also decreased.}
}

@inproceedings{nime18-Stapleton,
  author = {Paul Stapleton and Maarten van Walstijn and Sandor Mehes},
  title = {Co-Tuning Virtual-Acoustic Performance Ecosystems: observations on the development of skill and style in the study of musician-instrument relationships},
  pages = {311--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0065.pdf},
  abstract = {In this paper we report preliminary observations from an ongoing study into how musicians explore and adapt to the parameter space of a virtual-acoustic string bridge plate instrument. These observations inform (and are informed by) a wider approach to understanding the development of skill and style in interactions between musicians and musical instruments. We discuss a performance-driven ecosystemic approach to studying musical relationships, drawing on arguments from the literature which emphasise the need to go beyond simplistic notions of control and usability when assessing exploratory and performatory musical interactions. Lastly, we focus on processes of perceptual learning and co-tuning between musician and instrument, and how these activities may contribute to the emergence of personal style as a hallmark of skilful music-making.}
}

@inproceedings{nime18-Fish,
  author = {Sands A Fish II and Nicole L'Huillier},
  title = {Telemetron: A Musical Instrument  for Performance in Zero Gravity},
  pages = {315--317},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0066.pdf},
  abstract = {The environment of zero gravity affords a unique medium for new modalities of musical performance, both in the design of instruments, and human interactions with said instruments. To explore this medium, we have created and flown Telemetron, the first musical instrument specifically designed for and tested in the zero gravity environment. The resultant instrument (leveraging gyroscopes and wireless telemetry transmission) and recorded performance represent an initial exploration of compositions that are unique to the physics and dynamics of outer space. We describe the motivations for this instrument, and the unique constraints involved in designing for this environment. This initial design suggests possibilities for further experiments in musical instrument design for outer space.}
}

@inproceedings{nime18-Wilcox,
  author = {Dan Wilcox},
  title = {robotcowboy: 10 Years of Wearable Computer Rock},
  pages = {318--323},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0067.pdf},
  abstract = {This paper covers the technical and aesthetic development of robotcowboy, the author's ongoing human-computer wearable performance project. Conceived as an idiosyncratic manifesto on the embodiment of computational sound, the original robotcowboy system was built in 2006-2007 using a belt-mounted industrial wearable computer running GNU/Linux and Pure Data, external USB audio/MIDI interfaces, HID gamepads, and guitar. Influenced by roadworthy analog gear, chief system requirements were mobility, plug-and-play, reliability, and low cost. From 2007 to 2011, this first iteration "Cabled Madness" melded rock music with realtime algorithmic composition and revolved around cyborg human/system tension, aspects of improvisation, audience feedback, and an inherent capability of failure. The second iteration "Onward to Mars" explored storytelling from 2012-2015 through the one-way journey of the first human on Mars with the computing system adapted into a self-contained spacesuit backpack. Now 10 years on, a new "robotcowboy 2.0" system powers a third iteration with only an iPhone and PdParty, the author's open-source iOS application which runs Pure Data patches and provides full duplex stereo audio, MIDI, HID game controller support, and Open Sound Control communication. The future is bright, do you have room to wiggle?}
}

@inproceedings{nime18-Gonzalez,
  author = {Gonzalez Sanchez, Victor Evaristo and Martin, Charles Patrick  and Agata Zelechowska and Bjerkestrand, Kari Anne Vadstensvik  and Victoria Johnson and Jensenius, Alexander Refsum },
  title = {Bela-Based Augmented Acoustic Guitars for Sonic Microinteraction},
  pages = {324--327},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0068.pdf},
  abstract = {This article describes the design and construction of a collection of digitally-controlled augmented acoustic guitars, and the use of these guitars in the installation \textit\{Sverm-Resonans\}. The installation was built around the idea of exploring `inverse' sonic microinteraction, that is, controlling sounds by the micromotion observed when attempting to stand still. It consisted of six acoustic guitars, each equipped with a Bela embedded computer for sound processing (in Pure Data), an infrared distance sensor to detect the presence of users, and an actuator attached to the guitar body to produce sound. With an attached battery pack, the result was a set of completely autonomous instruments that were easy to hang in a gallery space. The installation encouraged explorations on the boundary between the tactile and the kinesthetic, the body and the mind, and between motion and sound. The use of guitars, albeit with an untraditional `performance' technique, made the experience both familiar and unfamiliar at the same time. Many users reported heightened sensations of stillness, sound, and vibration, and that the `inverse' control of the instrument was both challenging and pleasant.}
}

@inproceedings{nime18-Lepri,
  author = {Giacomo Lepri and Andrew P. McPherson},
  title = {Mirroring the past, from typewriting to interactive art: an approach to the re-design of a vintage technology},
  pages = {328--333},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0069.pdf},
  abstract = {Obsolete and old technologies are often used in interactive art and music performance. DIY practices such as hardware hacking and circuit bending provide effective methods to the integration of old machines into new artistic inventions. This paper presents the Cembalo Scrivano .1, an interactive audio-visual installation based on an augmented typewriter. Borrowing concepts from media archaeology studies, tangible interaction design and digital lutherie, we discuss how investigations into the historical and cultural evolution of a technology can suggest directions for the regeneration of obsolete objects. The design approach outlined focuses on the remediation of an old device and aims to evoke cultural and physical properties associated to the source object.}
}

@inproceedings{nime18-Thorn,
  author = {Seth Dominicus Thorn},
  title = {Alto.Glove: New Techniques for Augmented Violin},
  pages = {334--339},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0070.pdf},
  abstract = {This paper describes a performer-centric approach to the design, sensor selection, data interpretation, and mapping schema of a sensor-embedded glove called the “alto.glove” that the author uses to extend his performance abilities on violin. The alto.glove is a response to the limitations—both creative and technical—perceived in feature extraction processes that rely on classification. The hardware answers one problem: how to extend violin playing in a minimal yet powerful way; the software answers another: how to create a rich, evolving response that enhances expression in improvisation. The author approaches this problem from the various roles of violinist, hardware technician, programmer, sound designer, composer, and improviser. Importantly, the alto.glove is designed to be cost-effective and relatively easy to build.}
}

@inproceedings{nime18-Liontiris,
  author = {Thanos Polymeneas Liontiris},
  title = {Low Frequency Feedback Drones: A non-invasive augmentation of the double bass},
  pages = {340--341},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0071.pdf},
  abstract = {This paper illustrates the development of a Feedback Resonating Double Bass. The instrument is essentially the augmentation of an acoustic double bass using positive feedback. The research aimed to reply the question of how to augment and convert a double bass into a feedback resonating one without following an invasive method. The conversion process illustrated here is applicable and adaptable to double basses of any size, without making irreversible alterations to the instruments. }
}

@inproceedings{nime18-Formo,
  author = {Daniel Formo},
  title = {The Orchestra of Speech: a speech-based instrument system},
  pages = {342--343},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0072.pdf},
  abstract = {The Orchestra of Speech is a performance concept resulting from a recent artistic research project exploring the relationship between music and speech, in particular improvised music and everyday conversation. As a tool in this exploration, a digital musical instrument system has been developed for “orchestrating” musical features of speech into music, in real time. Through artistic practice, this system has evolved into a personal electroacoustic performance concept.}
}

@inproceedings{nime18-Weisling,
  author = {Anna Weisling and Anna Xambó and ireti olowe and Mathieu Barthet},
  title = {Surveying the Compositional and Performance Practices of Audiovisual Practitioners},
  pages = {344--345},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0073.pdf},
  abstract = {This paper presents a brief overview of an online survey conducted with the objective of gaining insight into compositional and performance practices of contemporary audiovisual practitioners. The survey gathered information regarding how practitioners relate aural and visual media in their work, and how compositional and performance practices involving multiple modalities might differ from other practices. Discussed here are three themes: compositional approaches, transparency and audience knowledge, and error and risk, which emerged from participants' responses. We believe these themes contribute to a discussion within the NIME community regarding unique challenges and objectives presented when working with multiple media.}
}

@inproceedings{nime18-Marasco,
  author = {Anthony T. Marasco},
  title = {Sound Opinions: Creating a Virtual Tool for Sound Art Installations through Sentiment Analysis of Critical Reviews},
  pages = {346--347},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0074.pdf},
  abstract = {The author presents Sound Opinions, a custom software tool that uses sentiment analysis to create sound art installations and music compositions. The software runs inside the NodeRed.js programming environment. It scrapes text from web pages, pre-processes it, performs sentiment analysis via a remote API, and parses the resulting data for use in external digital audio programs. The sentiment analysis itself is handled by IBM's Watson Tone Analyzer. The author has used this tool to create an interactive multimedia installation, titled Critique.  Sources of criticism of a chosen musical work are analyzed and the negative or positive statements about that composition work to warp and change it.  This allows the audience to only hear the work through the lens of its critics, and not in the original form that its creator intended.}
}

@inproceedings{nime18-Kritsis,
  author = {Kosmas Kritsis and Aggelos Gkiokas and Carlos Árpád Acosta and Quentin Lamerand and Robert Piéchaud and Maximos Kaliakatsos-Papakostas and Vassilis Katsouros},
  title = {A web-based 3D environment for gestural interaction with virtual music instruments as a STEAM education tool},
  pages = {348--349},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0075.pdf},
  abstract = {We present our work in progress on the development of a web-based system for music performance with virtual instruments in a virtual 3D environment, which provides three means of interaction (i.e physical, gestural and mixed), using tracking data from a Leap Motion sensor. Moreover, our system is integrated as a creative tool within the context of a STEAM education platform that promotes science learning through musical activities. The presented system models string and percussion instruments, with realistic sonic feedback based on Modalys, a physical model-based sound synthesis engine. Our proposal meets the performance requirements of real-time interactive systems and is implemented strictly with web technologies.}
}

@inproceedings{nime18-Mannone,
  author = {Maria C. Mannone and Eri Kitamura and Jiawei Huang and Ryo Sugawara and Yoshifumi Kitamura},
  title = {CubeHarmonic: A New Interface from a Magnetic 3D Motion Tracking System to Music Performance},
  pages = {350--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0076.pdf},
  abstract = {We developed a new musical interface, CubeHarmonic, with the magnetic tracking system, IM3D, created at Tohoku University. IM3D system precisely tracks positions of tiny, wireless, battery-less, and identifiable LC coils in real time. The CubeHarmonic is a musical application of the Rubik's cube, with notes on each little piece. Scrambling the cube, we get different chords and chord sequences. Positions of the pieces which contain LC coils are detected through IM3D, and transmitted to the computer, that plays sounds. The central position of the cube is also computed from the LC coils located into the corners of Rubik's cube, and, depending on the computed central position, we can manipulate overall loudness and pitch changes, as in theremin playing. This new instrument, whose first idea comes from mathematical theory of music, can be used as a teaching tool both for math (group theory) and music (music theory, mathematical music theory), as well as a composition device, a new instrument for avant-garde performances, and a recreational tool.}
}

@inproceedings{nime18-Kristoffersen,
  author = {Martin M Kristoffersen and Trond Engum},
  title = {The Whammy Bar as a Digital Effect Controller},
  pages = {352--355},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0077.pdf},
  abstract = {In this paper we present a novel digital effects controller for electric guitar based upon the whammy bar as a user interface. The goal with the project is to give guitarists a way to interact with dynamic effects control that feels familiar to their instrument and playing style. A 3D-printed prototype has been made. It replaces the whammy bar of a traditional Fender vibrato system with a sensor-equipped whammy bar. The functionality of the present prototype includes separate readings of force applied towards and from the guitar body, as well as an end knob for variable control. Further functionality includes a hinged system allowing for digital effect control either with or without the mechanical manipulation of string tension. By incorporating digital sensors to the idiomatic whammy bar interface, one would potentially bring guitarists a high level of control intimacy with the device, and thus lead to a closer interaction with effects.}
}

@inproceedings{nime18-Pond,
  author = {Robert Pond and Alexander Klassen and Kirk McNally},
  title = {Timbre Tuning: Variation in Cello Sprectrum Across Pitches and Instruments},
  pages = {356--359},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0078.pdf},
  abstract = {The process of learning to play a string instrument is a notoriously difficult task. A new student to the instrument is faced with mastering multiple, interconnected physical movements in order to become a skillful player. In their development, one measure of a players quality is their tone, which is the result of the combination of the physical characteristics of the instrument and their technique in playing it. This paper describes preliminary research into creating an intuitive, real-time device for evaluating the quality of tone generation on the cello: a ``timbre-tuner" to aid cellists evaluate their tone quality. Data for the study was collected from six post-secondary music students, consisting of recordings of scales covering the entire range of the cello. Comprehensive spectral audio analysis was performed on the data set in order to evaluate features suitable to describe tone quality. An inverse relationship was found between the harmonic centroid and pitch played, which became more pronounced when restricted to the A string. In addition, a model for predicting the harmonic centroid at different pitches on the A string was created. Results from informal listening tests support the use of the harmonic centroid as an appropriate measure for tone quality.}
}

@inproceedings{nime18-Mosher,
  author = {Matthew Mosher and Danielle Wood and Tony Obr},
  title = {Tributaries of Our Lost Palpability},
  pages = {360--361},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0079.pdf},
  abstract = {This demonstration paper describes the concepts behind Tributaries of Our Distant Palpability, an interactive sonified sculpture.  It takes form as a swelling sea anemone, while the sounds it produces recall the quagmire of a digital ocean.  The sculpture responds to changing light conditions with a dynamic mix of audio tracks, mapping volume to light level.  People passing by the sculpture, or directly engaging it by creating light and shadows with their smart phone flashlights, will trigger the audio.  At the same time, it automatically adapts to gradual environment light changes, such as the rise and fall of the sun.  The piece was inspired by the searching gestures people make, and emotions they have while, idly browsing content on their smart devices.  It was created through an interdisciplinary collaboration between a musician, an interaction designer, and a ceramicist.}
}

@inproceedings{nime18-Piepenbrink,
  author = {Andrew Piepenbrink},
  title = {Embedded Digital Shakers: Handheld Physical Modeling Synthesizers},
  pages = {362--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0080.pdf},
  abstract = {We present a flexible, compact, and affordable embedded physical modeling synthesizer which functions as a digital shaker. The instrument is self-contained, battery-powered, wireless, and synthesizes various shakers, rattles, and other handheld shaken percussion. Beyond modeling existing shakers, the instrument affords new sonic interactions including hand mutes on its loudspeakers and self-sustaining feedback. Both low-cost and high-performance versions of the instrument are discussed.}
}

@inproceedings{nime18-Xambó-b,
  author = {Anna Xambó and Gerard Roma and Alexander Lerch and Mathieu Barthet and György Fazekas},
  title = {Live Repurposing of Sounds: MIR Explorations with Personal and Crowdsourced Databases},
  pages = {364--369},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0081.pdf},
  abstract = {The recent increase in the accessibility and size of personal and crowdsourced digital sound collections brought about a valuable resource for music creation. Finding and retrieving relevant sounds in performance leads to challenges that can be approached using music information retrieval (MIR). In this paper, we explore the use of MIR to retrieve and repurpose sounds in musical live coding. We present a live coding system built on SuperCollider enabling the use of audio content from online Creative Commons (CC) sound databases such as Freesound or personal sound databases. The novelty of our approach lies in exploiting high-level MIR methods (e.g., query by pitch or rhythmic cues) using live coding techniques applied to sounds. We demonstrate its potential through the reflection of an illustrative case study and the feedback from four expert users. The users tried the system with either a personal database or a crowdsourced database and reported its potential in facilitating tailorability of the tool to their own creative workflows.}
}

@inproceedings{nime18-Sarwate,
  author = {Avneesh Sarwate and Ryan Taylor Rose and Jason Freeman and Jack Armitage},
  title = {Performance Systems for Live Coders and Non Coders},
  pages = {370--373},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0082.pdf},
  abstract = {This paper explores the question of how live coding musicians can perform with musicians who are not using code (such as acoustic instrumentalists or those using graphical and tangible electronic interfaces). This paper investigates performance systems that facilitate improvisation where the musicians can interact not just by listening to each other and changing their own output, but also by manipulating the data stream of the other performer(s). In a course of performance-led research four prototypes were built and analyzed them using concepts from NIME and creative collaboration literature. Based on this analysis it was found that the systems should 1) provide a commonly modifiable visual representation of musical data for both coder and non-coder, and 2) provide some independent means of sound production for each user, giving the non-coder the ability to slow down and make non-realtime decisions for greater performance flexibility. }
}

@inproceedings{nime18-Snyder,
  author = {Jeff Snyder and Michael R  Mulshine and Rajeev S Erramilli},
  title = {The Feedback Trombone: Controlling Feedback in Brass Instruments},
  pages = {374--379},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0083.pdf},
  abstract = {This paper presents research on control of electronic signal feedback in brass instruments through the development of a new augmented musical instrument, the Feedback Trombone. The Feedback Trombone (FBT) extends the traditional acoustic trombone interface with a speaker, microphone, and custom analog and digital hardware. }
}

@inproceedings{nime18-Sheffield,
  author = {Eric Sheffield},
  title = {Mechanoise: Mechatronic Sound and Interaction in Embedded Acoustic Instruments},
  pages = {380--381},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0084.pdf},
  abstract = {The use of mechatronic components (e.g. DC motors and solenoids) as both electronic sound source and locus of interaction is explored in a form of embedded acoustic instruments called mechanoise instruments. Micro-controllers and embedded computing devices provide a platform for live control of motor speeds and additional sound processing by a human performer. Digital fabrication and use of salvaged and found materials are emphasized.}
}

@inproceedings{nime18-Pigrem,
  author = {Jon Pigrem Mr and Andrew P. McPherson},
  title = {Do We Speak Sensor? Cultural Constraints of Embodied Interaction },
  pages = {382--385},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0085.pdf},
  abstract = {This paper explores the role of materiality in Digital Musical Instruments and questions the influence of tacit understandings of sensor technology. Existing research investigates the use of gesture, physical interaction and subsequent parameter mapping. We suggest that a tacit knowledge of the ‘sensor layer’ brings with it definitions, understandings and expectations that forge and guide our approach to interaction. We argue that the influence of technology starts before a sound is made, and comes from not only intuition of material properties, but also received notions of what technology can and should do. On encountering an instrument with obvious sensors, a potential performer will attempt to predict what the sensors do and what the designer intends for them to do, becoming influenced by a machine centered understanding of interaction and not a solely material centred one. The paper presents an observational study of interaction using non-functional prototype instruments designed to explore fundamental ideas and understandings of instrumental interaction in the digital realm. We will show that this understanding influences both gestural language and ability to characterise an expected sonic/musical response. }
}

@inproceedings{nime18-Salazar-b,
  author = {Spencer Salazar and Jack Armitage},
  title = {Re-engaging the Body and Gesture in Musical Live Coding},
  pages = {386--389},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0086.pdf},
  abstract = {At first glance, the practice of musical live coding seems distanced from the gestures and sense of embodiment common in musical performance, electronic or otherwise. This workshop seeks to explore the extent to which this assertion is justified, to re-examine notions of gesture and embodiment in the context of musical live coding performance, to consider historical approaches to synthesizing musical programming and gesture, and to look to the future for new ways of doing so. The workshop will consist firstly of a critical discussion of these issues and related literature. This will be followed by applied practical experiments involving ideas generated during these discussions. The workshop will conclude with a recapitulation and examination of these experiments in the context of previous research and proposed future directions. }
}

@inproceedings{nime18-Berdahl,
  author = {Edgar Berdahl and Eric Sheffield and Andrew Pfalz and Anthony T. Marasco},
  title = {Widening the Razor-Thin Edge of Chaos Into a Musical Highway: Connecting Chaotic Maps to Digital Waveguides},
  pages = {390--393},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0087.pdf},
  abstract = {For the purpose of creating new musical instruments, chaotic dynamical systems can be simulated in real time to synthesize complex sounds.  This work investigates a series of discrete-time chaotic maps, which have the potential to generate intriguing sounds when they are adjusted to be on the edge of chaos.  With these chaotic maps as studied historically, the edge of chaos tends to be razor-thin, which can make it difficult to employ them for making new musical instruments.  The authors therefore suggest connecting chaotic maps with digital waveguides, which (1) make it easier to synthesize harmonic tones and (2) make it harder to fall off of the edge of chaos while playing a musical instrument.  The authors argue therefore that this technique widens the razor-thin edge of chaos into a musical highway.}
}

@inproceedings{nime18-Snyder-b,
  author = {Jeff Snyder and Aatish Bhatia and Michael R  Mulshine},
  title = {Neuron-modeled Audio Synthesis: Nonlinear Sound and Control},
  pages = {394--397},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0088.pdf},
  abstract = {This paper describes a project to create a software instrument using a biological model of neuron behavior for audio synthesis. The translation of the model to a usable audio synthesis process is described, and a piece for laptop orchestra created using the instrument is discussed.}
}

@inproceedings{nime18-Cádiz,
  author = {Rodrigo F. Cádiz and Marie Gonzalez-Inostroza},
  title = {Fuzzy Logic Control Toolkit 2.0: composing and synthesis by fuzzyfication},
  pages = {398--402},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0089.pdf},
  abstract = {In computer or electroacoustic music, it is often the case that the compositional act and the parametric control of the underlying synthesis algorithms or hardware are not separable from each other. In these situations, composition and control of the synthesis parameters are not easy to distinguish. One possible solution is by means of fuzzy logic. This approach provides a simple, intuitive but powerful control of the compositional process usually in interesting non-linear ways. Compositional control in this context is achieved by the fuzzification of the relevant internal synthesis parameters and the parallel computation of common sense fuzzy rules of inference specified by the composer. This approach has been implemented computationally as a software package entitled FLCTK (Fuzzy Logic Control Tool Kit) in the form of external objects for the widely used real-time compositional environments Max/MSP and Pd. In this article, we present an updated version of this tool. As a demonstration of the wide range of situations in which this approach could be used, we provide two examples of parametric fuzzy control: first, the fuzzy control of a water tank simulation and second a particle-based sound synthesis technique by a fuzzy approach. }
}

@inproceedings{nime18-Leigh,
  author = {Sang-won Leigh and Pattie Maes},
  title = {Guitar Machine: Robotic Fretting Augmentation for Hybrid Human-Machine Guitar Play},
  pages = {403--408},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0090.pdf},
  abstract = {Playing musical instruments involves producing gradually more challenging body movements and transitions, where the kinematic constraints of the body play a crucial role in structuring the resulting music. We seek to make a bridge between currently accessible motor patterns, and musical possibilities beyond those - afforded through the use of a robotic augmentation. Guitar Machine is a robotic device that presses on guitar strings and assists a musician by fretting alongside her on the same guitar. This paper discusses the design of the system, strategies for using the system to create novel musical patterns, and a user study that looks at the effects of the temporary acquisition of enhanced physical ability. Our results indicate that the proposed human-robot interaction would equip users to explore new musical avenues on the guitar, as well as provide an enhanced understanding of the task at hand on the basis of the robotically acquired ability. }
}

@inproceedings{nime18-Barton,
  author = {Scott Barton and Karl Sundberg and Andrew Walter and Linda Sara Baker and Tanuj Sane and Alexander O'Brien},
  title = {Robotic Percussive Aerophone},
  pages = {409--412},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0091.pdf},
  abstract = {Percussive aerophones are configurable, modular, scalable, and can be constructed from commonly found materials. They can produce rich timbres, a wide range of pitches and complex polyphony. Their use by humans, perhaps most famously by the Blue Man Group, inspired us to build an electromechanically-actuated version of the instrument in order to explore expressive possibilities enabled by machines. The Music, Perception, and Robotics Lab at WPI has iteratively designed, built and composed for a robotic percussive aerophone since 2015, which has both taught lessons in actuation and revealed promising musical capabilities of the instrument. }
}

@inproceedings{nime18-Villicaña-Shaw,
  author = {Nathan Daniel Villicaña-Shaw and Spencer Salazar and Ajay Kapur},
  title = {Mechatronic Performance in Computer Music Compositions},
  pages = {413--418},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  editor = {Luke Dahl, Douglas Bowman, Thomas Martin},
  year = {2018},
  month = {June},
  publisher = {Virginia Tech},
  address = {Blacksburg, Virginia, USA },
  URL = {http://www.nime.org/proceedings/2018/nime2018_paper0092.pdf},
  abstract = {This paper introduces seven mechatronic compositions performed over three years at the xxxxx (xxxx). Each composition is discussed in regard to how it addresses the performative elements of mechatronic music concerts. The compositions are grouped into four classifications according to the types of interactions between human and robotic performers they afford: Non-Interactive, Mechatronic Instruments Played by Humans, Mechatronic Instruments Playing with Humans, and Social Interaction as Performance. The orchestration of each composition is described along with an overview of the piece’s compositional philosophy. Observations on how specific extra-musical compositional techniques can be incorporated into future mechatronic performances by human-robot performance ensembles are addressed. 
}
}