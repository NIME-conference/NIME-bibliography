@inproceedings{Traube2003,
author = {Traube, Caroline and Depalle, Philippe and Wanderley, Marcelo M.},
url = {http://www.nime.org/proceedings/2003/nime2003_042.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Indirect Acquisition of Instrumental Gesture Based on Signal , Physical and Perceptual Information},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {42--47},
}
@inproceedings{Singer2003,
author = {Singer, Eric},
url = {http://www.nime.org/proceedings/2003/nime2003_220.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sonic Banana: A Novel Bend-Sensor-Based MIDI Controller},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {220--221},
}
@inproceedings{Young2003a,
author = {Young, Diana and Serafin, Stefania},
url = {http://www.nime.org/proceedings/2003/nime2003_104.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Playability Evaluation of a Virtual Bowed String Instrument},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {104--108},
}
@inproceedings{Baird2003,
author = {Baird, Kevin C.},
url = {http://www.nime.org/proceedings/2003/nime2003_211.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Multi-Conductor: An Onscreen Polymetrical Conducting and Notation Display System},
year = {2003},
abstract = {This software tool, developed in Max/MSP, presentsperformers with image files consisting of traditional notationas well as conducting in the form of video playback. Theimpetus for this work was the desire to allow the musicalmaterial for each performer of a given piece to differ withregard to content and tempo.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Open form, notation, polymeter, polytempi, Max/MSP. },
pages = {211--212},
}
@inproceedings{Ventura2003,
author = {Ventura, David and Mase, Kenji},
url = {http://www.nime.org/proceedings/2003/nime2003_091.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Duet Musical Companion: Improvisational Interfaces for Children},
year = {2003},
abstract = {We present a sensor-doll interface as a musical outlet forpersonal expression. A doll serves the dual role of being bothan expressive agent and a playmate by allowing solo andaccompanied performance. An internal computer and sensorsystem allow the doll to receive input from the user and itssurroundings, and then respond accordingly with musicalfeedback. Sets of musical timbres and melodies may bechanged by presenting the doll with a series of themed clothhats, each suggesting a different style of play. The doll mayperform by itself and play a number of melodies, or it maycollaborate with the user when its limbs are squeezed or bent.Shared play is further encouraged by a basic set of aural tonesmimicking conversation.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Musical improvisation, toy interface agent, sensor doll, context awareness. },
pages = {91--94},
}
@inproceedings{Hewitt2003,
author = {Hewitt, Donna and Stevenson, Ian},
url = {http://www.nime.org/proceedings/2003/nime2003_122.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {E-mic: Extended Mic-stand Interface Controller},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Alternate controller, gesture, microphone technique, vocal performance, performance interface, electronic music. },
pages = {122--128},
}
@inproceedings{Kartadinata2003,
author = {Kartadinata, Sukandar},
url = {http://www.nime.org/proceedings/2003/nime2003_180.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Gluiph: a Nucleus for Integrated Instruments},
year = {2003},
abstract = {In this paper I present the gluiph, a single-board computer thatwas conceived as a platform for integrated electronic musicalinstruments. It aims to provide new instruments as well asexisting ones with a stronger identity by untethering themfrom the often lab-like stage setups built around general purpose computers. The key additions to its core are a flexiblesensor subsystem and multi-channel audio I/O. In contrast toother stand-alone approaches it retains a higher degree offlexibility by supporting popular music programming languages, with Miller Puckette's pd [1] being the current focus.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {180--183},
}
@inproceedings{Nakra2003,
author = {Nakra, Teresa M.},
url = {http://www.nime.org/proceedings/2003/nime2003_151.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Immersion Music: a Progress Report},
year = {2003},
abstract = {This paper describes the artistic projects undertaken at ImmersionMusic, Inc. (www.immersionmusic.org) during its three-yearexistence. We detail work in interactive performance systems,computer-based training systems, and concert production.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Interactive computer music systems, gestural interaction, Conductor's Jacket, Digital Baton  },
pages = {151--152},
}
@inproceedings{Flety2003,
author = {Fl\'{e}ty, Emmanuel and Sirguy, Marc},
url = {http://www.nime.org/proceedings/2003/nime2003_225.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {EoBody : a Follow-up to AtoMIC Pro’s Technology},
year = {2003},
abstract = {Ircam has been deeply involved into gesture analysis and sensingfor about four years now, as several artistic projects demonstrate.Ircam has often been solicited for sharing software and hardwaretools for gesture sensing, especially devices for the acquisition andconversion of sensor data, such as the AtoMIC Pro [1][2]. Thisdemo-paper describes the recent design of a new sensor to MIDIinterface called EoBody1},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Gestural controller, Sensor, MIDI, Computer Music. },
pages = {225--226},
}
@inproceedings{Dobrian2003,
author = {Dobrian, Christopher and Bevilacqua, Fr\'{e}d\'{e}ric},
url = {http://www.nime.org/proceedings/2003/nime2003_161.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Gestural Control of Music Using the Vicon 8 Motion Capture System},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Motion capture, gestural control, mapping. },
pages = {161--163},
}
@inproceedings{Andersen2003,
author = {Andersen, Tue H.},
url = {http://www.nime.org/proceedings/2003/nime2003_030.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Mixxx : Towards Novel DJ Interfaces},
year = {2003},
abstract = {The Disc Jockey (DJ) software system Mixxx is presented.Mixxx makes it possible to conduct studies of new interaction techniques in connection with the DJ situation, by itsopen design and easy integration of new software modulesand MIDI connection to external controllers. To gain a better understanding of working practices, and to aid the designprocess of new interfaces, interviews with two contemporarymusicians and DJ's are presented. In contact with thesemusicians development of several novel prototypes for DJinteraction have been made. Finally implementation detailsof Mixxx are described.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {30--35},
}
@inproceedings{Howard2003,
author = {Howard, David M. and Rimell, Stuart and Hunt, Andy D.},
url = {http://www.nime.org/proceedings/2003/nime2003_095.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Force Feedback Gesture Controlled Physical Modelling Synthesis},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {95--98},
}
@inproceedings{Cannon2003,
author = {Cannon, Cormac and Hughes, Stephen and O'Modhrain, Sile},
url = {http://www.nime.org/proceedings/2003/nime2003_003.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {EpipE: Exploration of the Uilleann Pipes as a Potential Controller for Computer-based Music},
year = {2003},
abstract = {In this paper we present a design for the EpipE, a newexpressive electronic music controller based on the IrishUilleann Pipes, a 7-note polyphonic reeded woodwind. Thecore of this proposed controller design is a continuouselectronic tonehole-sensing arrangement, equally applicableto other woodwind interfaces like those of the flute, recorder orJapanese shakuhachi. The controller will initially be used todrive a physically-based synthesis model, with the eventualgoal being the development of a mapping layer allowing theEpipE interface to operate as a MIDI-like controller of arbitrarysynthesis models.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Controllers, continuous woodwind tonehole sensor, uilleann pipes, Irish bagpipe, physical modelling, double reed, conical bore, tonehole. },
pages = {3--8},
}
@inproceedings{Young2003,
author = {Young, Diana and Essl, Georg},
url = {http://www.nime.org/proceedings/2003/nime2003_009.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {HyperPuja: A Tibetan Singing Bowl Controller},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {9--14},
}
@inproceedings{Singer2003a,
author = {Singer, Eric and Larke, Kevin and Bianciardi, David},
url = {http://www.nime.org/proceedings/2003/nime2003_188.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {LEMUR GuitarBot: MIDI Robotic String Instrument},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {188--191},
}
@inproceedings{Peiper2003,
author = {Peiper, Chad and Warden, David and Garnett, Guy},
url = {http://www.nime.org/proceedings/2003/nime2003_192.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {An Interface for Real-time Classification of Articulations Produced by Violin Bowing},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {192--196},
}
@inproceedings{Nagashima2003,
author = {Nagashima, Yoichi},
url = {http://www.nime.org/proceedings/2003/nime2003_048.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Bio-Sensing Systems and Bio-Feedback Systems for Interactive Media Arts},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {48--53},
}
@inproceedings{Muth2003,
author = {Muth, David and Burton, Ed},
url = {http://www.nime.org/proceedings/2003/nime2003_222.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sodaconductor},
year = {2003},
abstract = {Sodaconductor is a musical interface for generating OSCcontrol data based on the dynamic physical simulation toolSodaconstructor as it can be seen and heard onhttp://www.sodaplay.com.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {222--224},
}
@inproceedings{Laibowitz2003,
author = {Laibowitz, Mat},
url = {http://www.nime.org/proceedings/2003/nime2003_216.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {BASIS: A Genesis in Musical Interfaces},
year = {2003},
abstract = {This paper is a demo proposal for a new musical interfacebased on a DNA-like double-helix and concepts in charactergeneration. It contains a description of the interface,motivations behind developing such an interface, variousmappings of the interface to musical applications, and therequirements to demo the interface.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Performance, Design, Experimentation, DNA, Big Five. },
pages = {216--217},
}
@inproceedings{Allison2003,
author = {Allison, Jesse T. and Place, Timothy},
url = {http://www.nime.org/proceedings/2003/nime2003_208.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {SensorBox: Practical Audio Interface for Gestural Performance},
year = {2003},
abstract = {SensorBox is a low cost, low latency, high-resolutioninterface for obtaining gestural data from sensors for use inrealtime with a computer-based interactive system. Wediscuss its implementation, benefits, current limitations, andcompare it with several popular interfaces for gestural dataacquisition.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Sensors, gestural acquisition, audio interface, interactive music, SensorBox. },
pages = {208--210},
}
@inproceedings{Scavone2003,
author = {Scavone, Gary},
url = {http://www.nime.org/proceedings/2003/nime2003_015.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {THE PIPE: Explorations with Breath Control},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {MIDI Controller, Wind Controller, Breath Control, Human Computer Interaction. },
pages = {15--18},
}
@inproceedings{Nishimoto2003,
author = {Nishimoto, Kazushi and Oshima, Chika and Miyagawa, Yohei},
url = {http://www.nime.org/proceedings/2003/nime2003_164.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Why Always Versatile? Dynamically Customizable Musical Instruments Facilitate Expressive Performances},
year = {2003},
abstract = {In this paper, we discuss a design principle for the musicalinstruments that are useful for both novices and professionalmusicians and that facilitate musically rich expression. Webelieve that the versatility of conventional musicalinstruments causes difficulty in performance. By dynamicallyspecializing a musical instrument for performing a specific(genre of) piece, the musical instrument could become moreuseful for performing the piece and facilitates expressiveperformance. Based on this idea, we developed two new typesof musical instruments, i.e., a "given-melody-based musicalinstrument" and a "harmonic-function-based musicalinstrument." From the experimental results using twoprototypes, we demonstrate the efficiency of the designprinciple.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {164--169},
}
@inproceedings{Modler2003,
author = {Modler, Paul and Myatt, Tony and Saup, Michael},
url = {http://www.nime.org/proceedings/2003/nime2003_146.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {An Experimental Set of Hand Gestures for Expressive Control of Musical Parameters in Realtime},
year = {2003},
abstract = {This paper describes the implementation of Time Delay NeuralNetworks (TDNN) to recognize gestures from video images.Video sources are used because they are non-invasive and do notinhibit performer's physical movement or require specialistdevices to be attached to the performer which experience hasshown to be a significant problem that impacts musiciansperformance and can focus musical rehearsals and performancesupon technical rather than musical concerns (Myatt 2003).We describe a set of hand gestures learned by an artificial neuralnetwork to control musical parameters expressively in real time.The set is made up of different types of gestures in order toinvestigate:-aspects of the recognition process-expressive musical control-schemes of parameter mapping-generalization issues for an extended set for musicalcontrolThe learning procedure of the Neural Network is describedwhich is based on variations by affine transformations of imagesequences of the hand gestures.The whole application including the gesture capturing isimplemented in jMax to achieve real time conditions and easyintegration into a musical environment to realize differentmappings and routings of the control stream.The system represents a practice-based research using actualmusic models like compositions and processes of compositionwhich will follow the work described in the paper.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Gesture Recognition, Artificial Neural Network, Expressive Control, Real-time Interaction },
pages = {146--150},
}
@inproceedings{Hunt2003,
author = {Hunt, Andy D. and Kirk, Ross},
url = {http://www.nime.org/proceedings/2003/nime2003_135.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MidiGrid: Past, Present and Future},
year = {2003},
abstract = {MidiGrid is a computer-based musical instrument, primarilycontrolled with the computer mouse, which allows liveperformance of MIDI-based musical material by mapping 2dimensional position onto musical events. Since itsinvention in 1987, it has gained a small, but enthusiastic,band of users, and has become the primary instrument forseveral people with physical disabilities. This paper reviewsits development, uses and user interface issues, and highlightsthe work currently in progress for its transformation intoMediaGrid.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {135--139},
}
@inproceedings{Orio2003,
author = {Orio, Nicola and Lemouton, Serge and Schwarz, Diemo},
url = {http://www.nime.org/proceedings/2003/nime2003_036.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Score Following: State of the Art and New Developments},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {36--41},
}
@inproceedings{Ryan2003,
author = {Ryan, Joel and Salter, Christopher L.},
url = {http://www.nime.org/proceedings/2003/nime2003_087.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {TGarden: Wearable Instruments and Augmented Physicality},
year = {2003},
abstract = {This report details work on the interdisciplinary mediaproject TGarden. The authors discuss the challengesencountered while developing a responsive musicalenvironment for the general public involving wearable,sensor-integrated clothing as the central interface and input device. The project's dramaturgical andtechnical/implementation background are detailed toprovide a framework for the creation of a responsive hardwareand software system that reinforces a tangible relationshipbetween the participant's improvised movement and musicalresponse. Finally, the  authors take into consideration testingscenarios gathered from public prototypes in two Europeanlocales in 2001 to evaluate user experience of the system.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Gesture, interaction, embodied action, enaction, physical model, responsive environment, interactive musical systems, affordance, interface, phenomenology, energy, kinetics, time constant, induced ballistics, wearable computing, accelerometer, audience participation, dynamical system, dynamic compliance, effort, wearable instrument, augmented physicality. },
pages = {87--90},
}
@inproceedings{Hatanaka2003,
author = {Hatanaka, Motohide},
url = {http://www.nime.org/proceedings/2003/nime2003_077.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Ergonomic Design of A Portable Musical Instrument},
year = {2003},
abstract = {A handheld electronic musical instrument, named the BentoBox, was developed. The motivation was to develop aninstrument which one can easily carry around and play inmoments of free time, for example when riding public transportation or during short breaks at work. The device wasdesigned to enable quick learning by having various scalesprogrammed for different styles of music, and also beexpressive by having hand controlled timbral effects whichcan be manipulated while playing. Design analysis anditeration lead to a compact and ergonomic device. This paperfocuses on the ergonomic design process of the hardware.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {MIDI controller, electronic musical instrument, musical instrument design, ergonomics, playability, human computer interface. },
pages = {77--82},
}
@inproceedings{Baalman2003,
author = {Baalman, Marije A.},
url = {http://www.nime.org/proceedings/2003/nime2003_019.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The STRIMIDILATOR: a String Controlled MIDI-Instrument},
year = {2003},
abstract = {The STRIMIDILATOR is an instrument that uses the deviation and the vibration of strings as MIDI-controllers. Thismethod of control gives the user direct tactile force feedbackand allows for subtle control. The development of the instrument and its different functions are described.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {MIDI controllers, tactile force feedback, strings. Figure The STRIMIDILATOR },
pages = {19--23},
}
@inproceedings{Hoskinson2003,
author = {Hoskinson, Reynald and van den Doel, Kees and Fels, Sidney S.},
url = {http://www.nime.org/proceedings/2003/nime2003_099.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Real-time Adaptive Control of Modal Synthesis},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {99--103},
}
@inproceedings{Jorda2003,
author = {Jord\`{a}, Sergi},
url = {http://www.nime.org/proceedings/2003/nime2003_070.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sonigraphical Instruments: From FMOL to the reacTable*},
year = {2003},
abstract = {This paper first introduces two previous software-based musicinstruments designed by the author, and analyses the crucialimportance of the visual feedback introduced by theirinterfaces. A quick taxonomy and analysis of the visualcomponents in current trends of interactive music software isthen proposed, before introducing the reacTable*, a newproject that is currently under development. The reacTable* isa collaborative music instrument, aimed both at novices andadvanced musicians, which employs computer vision andtangible interfaces technologies, and pushes further the visualfeedback interface ideas and techniques aforementioned.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {70--76},
}
@inproceedings{PalacioQuintin2003,
author = {Palacio-Quintin, Cl\'{e}o},
url = {http://www.nime.org/proceedings/2003/nime2003_206.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Hyper-Flute},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {206--207},
}
@inproceedings{Cadoz2003,
author = {Cadoz, Claude and Luciani, Annie and Florens, Jean-Loup and Castagn\'{e}, Nicolas},
url = {http://www.nime.org/proceedings/2003/nime2003_235.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {ACROE - ICA Artistic Creation and Computer Interactive Multisensory Simulation Force Feedback Gesture Transducers},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {235--246},
}
@inproceedings{Momeni2003,
author = {Momeni, Ali and Wessel, David},
url = {http://www.nime.org/proceedings/2003/nime2003_054.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Characterizing and Controlling Musical Material Intuitively with Geometric Models},
year = {2003},
abstract = {In this paper, we examine the use of spatial layouts of musicalmaterial for live performance control. Emphasis is given tosoftware tools that provide for the simple and intuitivegeometric organization of sound material, sound processingparameters, and higher-level musical structures.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {54--62},
}
@inproceedings{Wilson2003,
author = {Wilson, Scott and Gurevich, Michael and Verplank, Bill and Stang, Pascal},
url = {http://www.nime.org/proceedings/2003/nime2003_024.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Microcontrollers in Music HCI Instruction: Reflections on our Switch to the Atmel AVR Platform},
year = {2003},
abstract = {Over the past year the instructors of the Human ComputerInteraction courses at CCRMA have undertaken a technology shift to a much more powerful teaching platform. Wedescribe the technical features of the new Atmel AVR basedplatform, contrasting it with the Parallax BASIC Stampplatform used in the past. The successes and failures ofthe new platform are considered, and some student projectsuccess stories described.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {24--29},
}
@inproceedings{Merrill2003,
author = {Merrill, David},
url = {http://www.nime.org/proceedings/2003/nime2003_218.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Head-Tracking for Gestural and Continuous Control of Parameterized Audio Effects},
year = {2003},
abstract = {This paper describes a system which uses the output fromhead-tracking and gesture recognition software to drive aparameterized guitar effects synthesizer in real-time.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Head-tracking, gestural control, continuous control, parameterized effects processor. },
pages = {218--219},
}
@inproceedings{Newton-Dunn2003,
author = {Newton-Dunn, Henry and Nakano, Hiroaki and Gibson, James},
url = {http://www.nime.org/proceedings/2003/nime2003_170.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Block Jam: A Tangible Interface for Interactive Music},
year = {2003},
abstract = {In this paper, we introduce Block Jam, a Tangible UserInterface that controls a dynamic polyrhythmic sequencerusing 26 physical artifacts. These physical artifacts, that wecall blocks, are a new type of input device for manipulatingan interactive music system. The blocks' functional andtopological statuses are tightly coupled to an ad hocsequencer, interpreting the user's arrangement of the blocksas meaningful musical phrases and structures.We demonstrate that we have created both a tangible andvisual language that enables both the novice and musicallytrained users by taking advantage of both their explorativeand intuitive abilities. The tangible nature of the blocks andthe intuitive interface promotes face-to-face collaborationand social interaction within a single system. The principleof collaboration is further extended by linking two BlockJam systems together to create a network.We discuss our project vision, design rational, relatedworks, and the implementation of Block Jam prototypes.Figure 1. A cluster of blocks, note the mother block on thebottom right},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Tangible interface, modular system, polyrhythmic sequencer. VISION We believe in a future where music will no longer be considered a linear composition, but a dynamic structure, and musical composition will extend to interaction. We also believe that through the },
pages = {170--177},
}
@inproceedings{Shiraiwa2003,
author = {Shiraiwa, Hiroko and Segnini, Rodrigo and Woo, Vivian},
url = {http://www.nime.org/proceedings/2003/nime2003_083.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sound Kitchen: Designing a Chemically Controlled Musical Performance},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Chemical music, Applied chemistry, Battery Controller. },
pages = {83--86},
}
@inproceedings{Burtner2003,
author = {Burtner, Matthew},
url = {http://www.nime.org/proceedings/2003/nime2003_063.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Composing for the (dis)Embodied Ensemble : Notational Systems in (dis)Appearances},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {63--69},
}
@inproceedings{Blaine2003,
author = {Blaine, Tina and Fels, Sidney S.},
url = {http://www.nime.org/proceedings/2003/nime2003_129.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Contexts of Collaborative Musical Experiences},
year = {2003},
abstract = {We explore a variety of design criteria applicable to thecreation of collaborative interfaces for musical experience. Themain factor common to the design of most collaborativeinterfaces for novices is that musical control is highlyrestricted, which makes it possible to easily learn andparticipate in the collective experience. Balancing this tradeoff is a key concern for designers, as this happens at theexpense of providing an upward path to virtuosity with theinterface. We attempt to identify design considerationsexemplified by a sampling of recent collaborative devicesprimarily oriented toward novice interplay. It is our intentionto provide a non-technical overview of design issues inherentin configuring multiplayer experiences, particularly for entrylevel players.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Design, collaborative interface, musical experience, multiplayer, novice, musical control. },
pages = {129--134},
}
@inproceedings{Gaye2003,
author = {Gaye, Lalya and Maz\'{e}, Ramia and Holmquist, Lars E.},
url = {http://www.nime.org/proceedings/2003/nime2003_109.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sonic City: The Urban Environment as a Musical Interface},
year = {2003},
abstract = {In the project Sonic City, we have developed a system thatenables users to create electronic music in real time by walkingthrough and interacting with the urban environment. Weexplore the use of public space and everyday behaviours forcreative purposes, in particular the city as an interface andmobility as an interaction model for electronic music making.A multi-disciplinary design process resulted in theimplementation of a wearable, context-aware prototype. Thesystem produces music by retrieving information aboutcontext and user action and mapping it to real-time processingof urban sounds. Potentials, constraints, and implications ofthis type of music creation are discussed.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {109--115},
}
@inproceedings{Kleinsasser2003,
author = {Kleinsasser, William},
url = {http://www.nime.org/proceedings/2003/nime2003_213.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Dsp.rack: Laptop-based Modular, Programmable Digital Signal Processing and Mixing for Live Performance},
year = {2003},
abstract = {This document describes modular software supporting livesignal processing and sound file playback within theMax/MSP environment. Dsp.rack integrates signalprocessing, memory buffer recording, and pre-recordedmulti-channel file playback using an interconnected,programmable signal flow matrix, and an eight-channel i/oformat.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Digital signal processing, Max/MSP, computer music performance, matrix routing, live performance processing. },
pages = {213--215},
}
@inproceedings{Lyons2003,
author = {Lyons, Michael J. and Haehnel, Michael and Tetsutani, Nobuji},
url = {http://www.nime.org/proceedings/2003/nime2003_116.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Designing, Playing, and Performing with a Vision-based Mouth Interface},
year = {2003},
abstract = {The role of the face and mouth in speech production as well asnon-verbal communication suggests the use of facial action tocontrol musical sound. Here we document work on theMouthesizer, a system which uses a headworn miniaturecamera and computer vision algorithm to extract shapeparameters from the mouth opening and output these as MIDIcontrol changes. We report our experience with variousgesture-to-sound mappings and musical applications, anddescribe a live performance which used the Mouthesizerinterface.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Video-based interface; mouth controller; alternative input devices. },
pages = {116--121},
}
@inproceedings{Paradiso2003,
author = {Paradiso, Joseph A.},
url = {http://www.nime.org/proceedings/2003/nime2003_228.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Dual-Use Technologies for Electronic Music Controllers: A Personal Perspective},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {228--234},
}
@inproceedings{Choi2003,
author = {Choi, Insook},
url = {http://www.nime.org/proceedings/2003/nime2003_201.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Component Model of Gestural Primitive Throughput},
year = {2003},
abstract = {This paper suggests that there is a need for formalizing acomponent model of gestural primitive throughput in musicinstrument design. The purpose of this model is to construct acoherent and meaningful interaction between performer andinstrument. Such a model has been implicit in previous researchfor interactive performance systems. The model presented heredistinguishes gestural primitives from units of measure ofgestures. The throughput model identifies symmetry betweenperformance gestures and musical gestures, and indicates a rolefor gestural primitives when a performer navigates regions ofstable oscillations in a musical instrument. The use of a highdimensional interface tool is proposed for instrument design, forfine-tuning the mapping between movement sensor data andsound synthesis control data.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Performance gestures, musical gestures, instrument design, mapping, tuning, affordances, stability. },
pages = {201--204},
}
@inproceedings{Settel2003,
author = {Settel, Zack and Lippe, Cort},
url = {http://www.nime.org/proceedings/2003/nime2003_197.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Convolution Brother’s Instrument Design},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {197--200},
}
@inproceedings{Wright2003,
author = {Wright, Matthew and Freed, Adrian and Momeni, Ali},
url = {http://www.nime.org/proceedings/2003/nime2003_153.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {OpenSound Control: State of the Art 2003},
year = {2003},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
pages = {153--159},
}
@inproceedings{Couturier2003,
author = {Couturier, Jean-Michel and Arfib, Daniel},
url = {http://www.nime.org/proceedings/2003/nime2003_184.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Pointing Fingers: Using Multiple Direct Interactions with Visual Objects to Perform Music},
year = {2003},
abstract = {In this paper, we describe a new interface for musicalperformance, using the interaction with a graphical userinterface in a powerful manner: the user directly touches ascreen where graphical objects are displayed and can useseveral fingers simultaneously to interact with the objects. Theconcept of this interface is based on the superposition of thegesture spatial place and the visual feedback spatial place; i tgives the impression that the graphical objects are real. Thisconcept enables a huge freedom in designing interfaces. Thegesture device we have created gives the position of fourfingertips using 3D sensors and the data is performed in theMax/MSP environment. We have realized two practicalexamples of musical use of such a device, using PhotosonicSynthesis and Scanned Synthesis.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {HCI, touch screen, multimodality, mapping, direct interaction, gesture devices, bimanual interaction, two-handed, Max/MSP. },
pages = {184--187},
}
@inproceedings{Kessous2003,
author = {Kessous, Lo\"{\i}c and Arfib, Daniel},
url = {http://www.nime.org/proceedings/2003/nime2003_140.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Bimanuality in Alternate Musical Instruments},
year = {2003},
abstract = {This paper presents a study of bimanual control applied tosound synthesis. This study deals with coordination,cooperation, and abilities of our hands in musical context. Wedescribe examples of instruments made using subtractivesynthesis, scanned synthesis in Max/MSP and commercialstand-alone software synthesizers via MIDI communicationprotocol. These instruments have been designed according to amulti-layer-mapping model, which provides modular design.They have been used in concerts and performanceconsiderations are discussed too.},
editor = {Wanderley, Marcelo M. and McKenzie, Richard and Ostiguy, Louise},
address = {Montreal},
date = {22-24 May, 2003},
keywords = {Gesture control, mapping, alternate controllers, musical instruments. },
pages = {140--145},
}

