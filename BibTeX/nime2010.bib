% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Ahmaniemi2010,
  Title                    = {Gesture Controlled Virtual Instrument with Dynamic Vibrotactile Feedback},
  Author                   = {Ahmaniemi, Teemu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {485--488},

  Abstract                 = {This paper investigates whether a dynamic vibrotactile feedback improves the playability of a gesture controlled virtual instrument. The instrument described in this study is based on a virtual control surface that player strikes with a hand held sensor-actuator device. We designed two tactile cues to augment the stroke across the control surface: a static and dynamic cue. The static cue was a simple burst of vibration triggered when crossing the control surface. The dynamic cue was continuous vibration increasing in amplitude when approaching the surface. We arranged an experiment to study the influence of the tactile cues in performance. In a tempo follow task, the dynamic cue yielded significantly the best temporal and periodic accuracy and control of movement velocity and amplitude. The static cue did not significantly improve the rhythmic accuracy but assisted the control of movement velocity compared to the condition without tactile feedback at all. The findings of the study indicate that careful design of dynamic vibrotactile feedback can improve the controllability of gesture based virtual instrument. },
  Keywords                 = {Virtual instrument, Gesture, Tactile feedback, Motor control},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_485.pdf}
}

@InProceedings{Alt2010,
  Title                    = {Creating Meaningful Melodies from Text Messages},
  Author                   = {Alt, Florian and Shirazi, Alireza S. and Legien, Stefan and Schmidt, Albrecht and Mennen\"{o}h, Julian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {63--68},

  Keywords                 = {instant messaging,nime10,sms,sonority,text sonification},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_063.pdf}
}

@InProceedings{Baba2010,
  Title                    = {"VirtualPhilharmony" : A Conducting System with Heuristics of Conducting an Orchestra},
  Author                   = {Baba, Takashi and Hashida, Mitsuyo and Katayose, Haruhiro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {263--270},

  Abstract                 = {"VirtualPhilharmony" (V.P.) is a conducting interface that enables users to perform expressive music with conducting action. Several previously developed conducting interfaces do not satisfy users who have conducting experience because the feedback from the conducting action does not always correspond with a natural performance. The tempo scheduler, which is the main engine of a conducting system, must be improved. V.P. solves this problem by introducing heuristics of conducting an orchestra in detecting beats, applying rules regarding the tempo expression in a bar, etc. We confirmed with users that the system realized a high "following" performance and had musical persuasiveness. },
  Keywords                 = {Conducting system, heuristics, sensor, template.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_263.pdf}
}

@InProceedings{Beilharz2010,
  Title                    = {Expressive Wearable Sonification and Visualisation : Design and Evaluation of a Flexible Display},
  Author                   = {Beilharz, Kirsty and {Vande Moere}, Andrew and Stiel, Barbara and Calo, Claudia and Tomitsch, Martin and Lombard, Adrian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {323--326},

  Abstract                 = {In this paper we examine a wearable sonification and visualisation display that uses physical analogue visualisation and digital sonification to convey feedback about the wearer's activity and environment. Intended to bridge a gap between art aesthetics, fashionable technologies and informative physical computing, the user experience evaluation reveals the wearers' responses and understanding of a novel medium for wearable expression. The study reveals useful insights for wearable device design in general and future iterations of this sonification and visualisation display. },
  Keywords                 = {Wearable display, sonification, visualisation, design aesthetics, physical computing, multimodal expression, bimodal display},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_323.pdf}
}

@InProceedings{Berger2010,
  Title                    = {The GRIP MAESTRO : Idiomatic Mappings of Emotive Gestures for Control of Live Electroacoustic Music},
  Author                   = {Berger, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {419--422},

  Abstract                 = {This paper introduces my research in physical interactive design with my "GRIP MAESTRO" electroacoustic performance interface. It then discusses the considerations involved in creating intuitive software mappings of emotive performative gestures such that they are idiomatic not only of the sounds they create but also of the physical nature of the interface itself. },
  Keywords                 = {emotive gesture and music,hall effect,human-controller interaction,musical mapping strategies,nime10,novel musical instrument,passive haptic feedback,sensor-augmented hand-exerciser},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_419.pdf}
}

@InProceedings{Berthaut2010,
  Title                    = {DRILE : An Immersive Environment for Hierarchical Live-Looping},
  Author                   = {Berthaut, Florent and Desainte-Catherine, Myriam and Hachet, Martin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {192--197},

  Abstract                 = {We present Drile, a multiprocess immersive instrument built uponthe hierarchical live-looping technique and aimed at musical performance. This technique consists in creating musical trees whosenodes are composed of sound effects applied to a musical content.In the leaves, this content is a one-shot sound, whereas in higherlevel nodes this content is composed of live-recorded sequencesof parameters of the children nodes. Drile allows musicians tointeract efficiently with these trees in an immersive environment.Nodes are represented as worms, which are 3D audiovisual objects. Worms can be manipulated using 3D interaction techniques,and several operations can be applied to the live-looping trees. Theenvironment is composed of several virtual rooms, i.e. group oftrees, corresponding to specific sounds and effects. Learning Drileis progressive since the musical control complexity varies according to the levels in live-looping trees. Thus beginners may havelimited control over only root worms while still obtaining musically interesting results. Advanced users may modify the trees andmanipulate each of the worms.},
  Keywords                 = {Drile, immersive instrument, hierarchical live-looping, 3D interac- tion},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_192.pdf}
}

@InProceedings{Brown2010,
  Title                    = {Network Jamming : Distributed Performance using Generative Music},
  Author                   = {Brown, Andrew R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {283--286},

  Abstract                 = {Generative music systems can be played by musicians who manipulate the values of algorithmic parameters, and their datacentric nature provides an opportunity for coordinated interaction amongst a group of systems linked over IP networks; a practice we call Network Jamming. This paper outlines the characteristics of this networked performance practice and discusses the types of mediated musical relationships and ensemble configurations that can arise. We have developed and tested the jam2jam network jamming software over recent years. We describe this system, draw from our experiences with it, and use it to illustrate some characteristics of Network Jamming.},
  Keywords                 = {collaborative,ensemble,generative,interaction,network,nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_283.pdf}
}

@InProceedings{Bryan2010,
  Title                    = {MoMu : A Mobile Music Toolkit},
  Author                   = {Bryan, Nicholas J. and Herrera, Jorge and Oh, Jieun and Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {174--177},

  Abstract                 = {The Mobile Music (MoMu) toolkit is a new open-sourcesoftware development toolkit focusing on musical interaction design for mobile phones. The toolkit, currently implemented for iPhone OS, emphasizes usability and rapidprototyping with the end goal of aiding developers in creating real-time interactive audio applications. Simple andunified access to onboard sensors along with utilities forcommon tasks found in mobile music development are provided. The toolkit has been deployed and evaluated in theStanford Mobile Phone Orchestra (MoPhO) and serves asthe primary software platform in a new course exploringmobile music.},
  Keywords                 = {instrument design, iPhone, mobile music, software develop- ment, toolkit},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_174.pdf}
}

@InProceedings{BryanKinns2010,
  Title                    = {Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art},
  Author                   = {Bryan-Kinns, Nick and Fencott, Robin and Metatla, Oussama and Nabavian, Shahin and Sheridan, Jennifer G.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {403--406},

  Abstract                 = {In this paper we outline the emerging field of Interactional Sound and Music which concerns itself with multi-person technologically mediated interactions primarily using audio. We present several examples of interactive systems in our group, and reflect on how they were designed and evaluated. Evaluation techniques for collective, performative, and task oriented activities are outlined and compared. We emphasise the importance of designing for awareness in these systems, and provide examples of different awareness mechanisms. },
  Keywords                 = {Interactional, sound, music, mutual engagement, improvisation, composition, collaboration, awareness.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_403.pdf}
}

@InProceedings{Buch2010,
  Title                    = {"playing robot" : An Interactive Sound Installation in Human-Robot Interaction Design for New Media Art},
  Author                   = {Buch, Benjamin and Coussement, Pieter and Schmidt, L\"{u}der},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {411--414},

  Abstract                 = {In this study artistic human-robot interaction design is introduced as a means for scientific research and artistic investigations. It serves as a methodology for situated cognitionintegrating empirical methodology and computational modeling, and is exemplified by the installation playing robot.Its artistic purpose is to aid to create and explore robots as anew medium for art and entertainment. We discuss the useof finite state machines to organize robots' behavioral reactions to sensor data, and give a brief outlook on structuredobservation as a potential method for data collection.},
  Keywords                 = {dynamic mapping,embodiment,finite state au-,human-robot interaction,new media art,nime10,structured,tomata},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_411.pdf}
}

@InProceedings{Bukvic2010,
  Title                    = {Introducing L2Ork : Linux Laptop Orchestra},
  Author                   = {Bukvic, Ivika and Martin, Thomas and Standley, Eric and Matthews, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {170--173},

  Keywords                 = {l2ork,laptop orchestra,linux,nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_170.pdf}
}

@InProceedings{Burt2010,
  Title                    = {Packages for ArtWonk : New Mathematical Tools for Composers},
  Author                   = {Burt, Warren},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {493--496},

  Abstract                 = {This paper describes a series of mathematical functions implemented by the ,
,
author in the commercial algorithmic software language ArtWonk, written by John Dunn, which are offered with that language as resources for composers. It gives a history of the development of the functions, with an emphasis on how I developed them for use in my compositions. },
  Keywords                 = {Algorithmic composition, mathematical composition, probability distributions, fractals, additive sequences},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_493.pdf}
}

@InProceedings{Cannon2010,
  Title                    = {Expression and Spatial Motion : Playable Ambisonics},
  Author                   = {Cannon, Joanne and Favilla, Stuart},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {120--124},

  Keywords                 = {augmented instruments,expressive spatial,nime10,playable instruments},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_120.pdf}
}

@InProceedings{Carrillo2010,
  Title                    = {The Bowed Tube : a Virtual Violin},
  Author                   = {Carrillo, Alfonso P. and Bonada, Jordi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {229--232},

  Abstract                 = {This paper presents a virtual violin for real-time performances consisting of two modules: a violin spectral modeland a control interface. The interface is composed by asensing bow and a tube with drawn strings in substitutionof a real violin. The spectral model is driven by the bowingcontrols captured with the control interface and it is ableto predict spectral envelopes of the sound corresponding tothose controls. The envelopes are filled with harmonic andnoisy content and given to an additive synthesizer in orderto produce violin sounds. The sensing system is based ontwo motion trackers with 6 degrees of freedom. One trackeris attached to the bow and the other to the tube. Bowingcontrols are computed after a calibration process where theposition of virtual strings and the hair-ribbon of the bowis obtained. A real time implementation was developed asa MAX/MSP patch with external objects for each of themodules.},
  Keywords                 = {violin, synthesis, control, spectral, virtual},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_229.pdf}
}

@InProceedings{Cassinelli2010,
  Title                    = {scoreLight : Playing with a Human-Sized Laser Pick-Up},
  Author                   = {Cassinelli, Alavaro and Kuribara, Yusaku and Zerroug, Alexis and Ishikawa, Masatoshi and Manabe, Daito},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {144--149},

  Abstract                 = {scoreLight is a playful musical instrument capable of generating sound from the lines of drawings as well as from theedges of three-dimensional objects nearby (including everyday objects, sculptures and architectural details, but alsothe performer's hands or even the moving silhouettes ofdancers). There is no camera nor projector: a laser spotexplores shapes as a pick-up head would search for soundover the surface of a vinyl record - with the significant difference that the groove is generated by the contours of thedrawing itself.},
  Keywords                 = {H5.2 [User Interfaces] interaction styles / H.5.5 [Sound and Music Computing] Methodologies and techniques / J.5 [Arts and Humanities] performing arts},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_144.pdf}
}

@InProceedings{Choi2010,
  Title                    = {LUSH : An Organic Eco + Music System},
  Author                   = {Choi, Hongchan and Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {112--115},

  Abstract                 = {We propose an environment that allows users to create music by leveraging playful visualization and organic interaction. Our attempt to improve ideas drawn from traditional sequencer paradigm has been made in terms of extemporizing music and associating with visualization in real-time. In order to offer different user experience and musical possibility, this system incorporates many techniques, including; flocking simulation, nondeterministic finite automata (NFA), score file analysis, vector calculation, OpenGL animation, and networking. We transform a sequencer into an audiovisual platform for composition and performance, which is furnished with artistry and ease of use. Thus we believe that it is suitable for not only artists such as algorithmic composers or audiovisual performers, but also anyone who wants to play music and imagery in a different way. },
  Keywords                 = {algorithmic composition,audiovisual,automata,behavior simulation,music,music sequencer,musical interface,nime10,visualization},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_112.pdf}
}

@InProceedings{Chun2010,
  Title                    = {Freepad : A Custom Paper-based MIDI Interface},
  Author                   = {Chun, Sungkuk and Hawryshkewich, Andrew and Jung, Keechul and Pasquier, Philippe},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {31--36},

  Abstract                 = {The field of mixed-reality interface design is relatively young and in regards to music, has not been explored in great depth. Using computer vision and collision detection techniques, Freepad further explores the development of mixed-reality interfaces for music. The result is an accessible user-definable MIDI interface for anyone with a webcam, pen and paper, which outputs MIDI notes with velocity values based on the speed of the strikes on drawn pads. },
  Keywords                 = {Computer vision, form recognition, collision detection, mixed- reality, custom interface, MIDI},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_031.pdf}
}

@InProceedings{Ciglar2010,
  Title                    = {An Ultrasound Based Instrument Generating Audible and Tactile Sound},
  Author                   = {Ciglar, Miha},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {19--22},

  Abstract                 = {This paper, describes the second phase of an ongoing research project dealing with the implementation of an interactive interface. It is a "hands free" instrument, utilizing a non-contact tactile feedback method based on airborne ultrasound. The three main elements/components of the interface that will be discussed in this paper are: 1. Generation of audible sound by self-demodulation of an ultrasound signal during its propagation through air; 2. The condensation of the ultrasound energy in one spatial point generating a precise tactile reproduction of the audible sound; and 3. The feed-forward method enabling a real-time intervention of the musician, by shaping the tactile (ultra)sound directly with his hands.},
  Keywords                 = {haptics, vibro-tactility, feedback, ultrasound, hands-free interface, nonlinear acoustics, parametric array.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_019.pdf}
}

@InProceedings{Collins2010,
  Title                    = {Contrary Motion : An Oppositional Interactive Music System},
  Author                   = {Collins, Nick},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {125--129},

  Keywords                 = {contrary, beat tracking, stream analysis, musical agent},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_125.pdf}
}

@InProceedings{Collins2010a,
  Title                    = {Musical Exoskeletons : Experiments with a Motion Capture Suit},
  Author                   = {Collins, Nick and Kiefer, Chris and Patoli, Zeeshan and White, Martin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {455--458},

  Abstract                 = {Gaining access to a prototype motion capture suit designedby the Animazoo company, the Interactive Systems groupat the University of Sussex have been investigating application areas. This paper describes our initial experimentsin mapping the suit control data to sonic attributes for musical purposes. Given the lab conditions under which weworked, an agile design cycle methodology was employed,with live coding of audio software incorporating fast feedback, and more reflective preparations between sessions, exploiting both individual and pair programming. As the suitprovides up to 66 channels of information, we confront achallenging mapping problem, and techniques are describedfor automatic calibration, and the use of echo state networksfor dimensionality reduction.},
  Keywords                 = {Motion Capture, Musical Controller, Mapping, Agile De- sign},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_455.pdf}
}

@InProceedings{Dahl2010,
  Title                    = {Sound Bounce : Physical Metaphors in Designing Mobile Music Performance},
  Author                   = {Dahl, Luke and Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {178--181},

  Abstract                 = {The use of metaphor has a prominent role in HCI, both as a device to help users understand unfamiliar technologies, and as a tool to guide the design process. Creators of new computerbased instruments face similar design challenges as those in HCI. In the course of creating a new piece for Mobile Phone Orchestra we propose the metaphor of a sound as a ball and explore the interactions and sound mappings it suggests. These lead to the design of a gesture-controlled instrument that allows players to "bounce" sounds, "throw" them to other players, and compete in a game to "knock out" others' sounds. We composed the piece SoundBounce based on these interactions, and note that audiences seem to find performances of the piece accessible and engaging, perhaps due to the visibility of the metaphor. },
  Keywords                 = {Mobile music, design, metaphor, performance, gameplay.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_178.pdf}
}

@InProceedings{Deleflie2010,
  Title                    = {Images as Spatial Sound Maps},
  Author                   = {Deleflie, Etienne and Schiemer, Greg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {130--135},

  Abstract                 = {The tools for spatial composition typically model just a small subset of the spatial audio cues known to researchers. As composers explore this medium it has become evident that the nature of spatial sound perception is complex. Yet interfaces for spatial composition are often simplistic and the end results can be disappointing. This paper presents an interface that is designed to liberate the composer from thinking of spatialised sound as points in space. Instead, visual images are used to define sound in terms of shape, size and location. Images can be sequenced into video, thereby creating rich and complex temporal soundscapes. The interface offers both the ability to craft soundscapes and also compose their evolution in time. },
  Keywords                 = {Spatial audio, surround sound, ambisonics, granular synthesis, decorrelation, diffusion.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_130.pdf}
}

@InProceedings{Dimitrov2010,
  Title                    = {Extending the Soundcard for Use with Generic {DC} Sensors Demonstrated by Revisiting a Vintage ISA Design},
  Author                   = {Dimitrov, Smilen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {303--308},

  Abstract                 = {The sound card anno 2010, is an ubiquitous part of almostany personal computing system; what was once considereda high-end, CD-quality audio fidelity, is today found in mostcommon sound cards. The increased presence of multichannel devices, along with the high sampling frequency, makesthe sound card desirable as a generic interface for acquisition of analog signals in prototyping of sensor-based musicinterfaces. However, due to the need for coupling capacitorsat a sound card's inputs and outputs, the use as a genericsignal interface of a sound card is limited to signals not carrying information in a constant DC component. Through arevisit of a card design for the (now defunct) ISA bus, thispaper proposes use of analog gates for bypassing the DCfiltering input sections, controllable from software - therebyallowing for arbitrary choice by the user, if a soundcardinput channel is to be used as a generic analog-to-digitalsensor interface. Issues regarding use of obsolete technology and educational aspects are discussed as well.},
  Keywords                 = {dc,isa,nime10,sensors,soundcard},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_303.pdf}
}

@InProceedings{Dubrau2010,
  Title                    = {P[a]ra[pra]xis : Towards Genuine Realtime 'Audiopoetry'},
  Author                   = {Dubrau, Josh M. and Havryliv, Mark},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {467--468},

  Keywords                 = {nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_467.pdf}
}

@InProceedings{Essl2010,
  Title                    = {Designing Mobile Musical Instruments and Environments with urMus},
  Author                   = {Essl, Georg and M\"{u}ller, Alexander},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {76--81},

  Abstract                 = {We discuss how the environment urMus was designed to allow creation of mobile musical instruments on multi-touch smartphones. The design of a mobile musical instrument consists of connecting sensory capabilities to output modalities through various means of processing. We describe how the default mapping interface was designed which allows to set up such a pipeline and how visual and interactive multi-touch UIs for musical instruments can be designed within the system. },
  Keywords                 = {Mobile music making, meta-environment, design, mapping, user interface},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_076.pdf}
}

@InProceedings{Essl2010a,
  Title                    = {Use the Force (or something) - Pressure and Pressure - Like Input for Mobile Music Performance},
  Author                   = {Essl, Georg and Rohs, Michael and Kratz, Sven},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {182--185},

  Abstract                 = {Impact force is an important dimension for percussive musical instruments such as the piano. We explore three possible mechanisms how to get impact forces on mobile multi-touch devices: using built-in accelerometers, the pressure sensing capability of Android phones, and external force sensing resistors. We find that accelerometers are difficult to control for this purpose. Android's pressure sensing shows some promise, especially when combined with augmented playing technique. Force sensing resistors can offer good dynamic resolution but this technology is not currently offered in commodity devices and proper coupling of the sensor with the applied impact is difficult. },
  Keywords                 = {Force, impact, pressure, multi-touch, mobile phone, mobile music making.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_182.pdf}
}

@InProceedings{Fencott2010,
  Title                    = {Hey Man, You're Invading my Personal Space ! Privacy and Awareness in Collaborative Music},
  Author                   = {Fencott, Robin and Bryan-Kinns, Nick},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {198--203},

  Abstract                 = {This research is concerned with issues of privacy, awareness and the emergence of roles in the process of digitallymediated collaborative music making. Specifically we areinterested in how providing collaborators with varying degrees of privacy and awareness of one another influencesthe group interaction. A study is presented whereby ninegroups of co-located musicians compose music together using three different interface designs. We use qualitative andquantitative data to study and characterise the musician'sinteraction with each other and the software. We show thatwhen made available to them, participants make extensiveuse of a private working area to develop musical contributions before they are introduced to the group. We also arguethat our awareness mechanisms change the perceived quality of the musical interaction, but have no impact on theway musicians interact with the software. We then reflecton implications for the design of new collaborative musicmaking tools which exploit the potential of digital technologies, while at the same time support creative musicalinteraction.},
  Keywords                 = {Awareness, Privacy, Collaboration, Music, Interaction, En- gagement, Group Music Making, Design, Evaluation.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_198.pdf}
}

@InProceedings{Feng2010,
  Title                    = {Irregular Incurve},
  Author                   = {Feng, Xiaoyang},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {377--379},

  Abstract                 = { Irregular Incurve is a MIDI controllable robotic string instrument. The twelve independent string-units compose the complete musical scale of 12 units. Each string can be plucked by a motor control guitar pick. A MIDI keyboard is attached to the instrument and serves as an interface for real-time interactions between the instrument and the audience. Irregular Incurve can also play preprogrammed music by itself. This paper presents the design concept and the technical solutions to realizing the functionality of Irregular Incurve. The future features are also discussed. },
  Keywords                 = {NIME, Robotics, Acoustic, Interactive, MIDI, Real time Performance, String Instrument, Arduino, Servo, Motor Control},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_377.pdf}
}

@InProceedings{Ferguson2010,
  Title                    = {Movement in a Contemporary Dance Work and its Relation to Continuous Emotional Response},
  Author                   = {Ferguson, Sam and Schubert, Emery and Stevens, Catherine},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {481--484},

  Abstract                 = {In this paper, we describe a comparison between parameters drawn from 3-dimensional measurement of a dance performance, and continuous emotional response data recorded from an audience present during this performance. A continuous time series representing the mean movement as the dance unfolds is extracted from the 3-dimensional data. The audiences' continuous emotional response data are also represented as a time series, and the series are compared. We concluded that movement in the dance performance directly influences the emotional arousal response of the audience. },
  Keywords                 = {Dance, Emotion, Motion Capture, Continuous Response.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_481.pdf}
}

@InProceedings{Freed2010,
  Title                    = {Visualizations and Interaction Strategies for Hybridization Interfaces},
  Author                   = {Freed, Adrian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {343--347},

  Abstract                 = {We present two complementary approaches for the visualization and interaction of dimensionally reduced data setsusing hybridization interfaces. Our implementations privilege syncretic systems allowing one to explore combinations(hybrids) of disparate elements of a data set through theirplacement in a 2-D space. The first approach allows for theplacement of data points anywhere on the plane accordingto an anticipated performance strategy. The contribution(weight) of each data point varies according to a power function of the distance from the control cursor. The secondapproach uses constrained vertex colored triangulations ofmanifolds with labels placed at the vertices of triangulartiles. Weights are computed by barycentric projection ofthe control cursor position.},
  Keywords                 = {Interpolation, dimension reduction, radial basis functions, triangular mesh},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_343.pdf}
}

@InProceedings{Frisson2010,
  Title                    = {DeviceCycle : Rapid and Reusable Prototyping of Gestural Interfaces, Applied to Audio Browsing by Similarity},
  Author                   = {Frisson, Christian and Macq, Beno\^{\i}t and Dupont, St\'{e}phane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {473--476},

  Abstract                 = {This paper presents the development of rapid and reusablegestural interface prototypes for navigation by similarity inan audio database and for sound manipulation, using theAudioCycle application. For this purpose, we propose andfollow guidelines for rapid prototyping that we apply usingthe PureData visual programming environment. We havemainly developed three prototypes of manual control: onecombining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits andshortcomings we experienced while prototyping using thisapproach.},
  Keywords                 = {Human-computer interaction, gestural interfaces, rapid pro- totyping, browsing by similarity, audio database},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_473.pdf}
}

@InProceedings{Frounberg2010,
  Title                    = {Glass Instruments -- From Pitch to Timbre},
  Author                   = {Frounberg, Ivar and Innervik, Kjell Tore and Jensenius, Alexander R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {287--290},

  Abstract                 = {The paper reports on the development of prototypes of glassinstruments. The focus has been on developing acousticinstruments specifically designed for electronic treatment,and where timbral qualities have had priority over pitch.The paper starts with a brief historical overview of glassinstruments and their artistic use. Then follows an overviewof the glass blowing process. Finally the musical use of theinstruments is discussed.},
  Keywords                 = {glass instruments,nime,nime10,performance practice},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_287.pdf}
}

@InProceedings{Fyans2010,
  Title                    = {Examining the Spectator Experience},
  Author                   = {Fyans, A. Cavan and Gurevich, Michael and Stapleton, Paul},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {451--454},

  Abstract                 = {Drawing on a model of spectator understanding of error inperformance in the literature, we document a qualitativeexperiment that explores the relationships between domainknowledge, mental models, intention and error recognitionby spectators of performances with electronic instruments.Participants saw two performances with contrasting instruments, with controls on their mental model and understanding of intention. Based on data from a subsequent structured interview, we identify themes in participants' judgements and understanding of performance and explanationsof their spectator experience. These reveal both elementsof similarity and difference between the two performances,instruments and between domain knowledge groups. Fromthese, we suggest and discuss implications for the design ofnovel performative interactions with technology.},
  Keywords                 = {error,intention,mental model,nime10,qualitative,spectator},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_451.pdf}
}

@InProceedings{Fyfe2010,
  Title                    = {SurfaceMusic : Mapping Virtual Touch-based Instruments to Physical Models},
  Author                   = {Fyfe, Lawrence and Lynch, Sean and Hull, Carmen and Carpendale, Sheelagh},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {360--363},

  Abstract                 = {In this paper we discuss SurfaceMusic, a tabletop music system in which touch gestures are mapped to physical modelsof instruments. With physical models, parametric controlover the sound allows for a more natural interaction between gesture and sound. We discuss the design and implementation of a simple gestural interface for interactingwith virtual instruments and a messaging system that conveys gesture data to the audio system.},
  Keywords                 = {Tabletop, multi-touch, gesture, physical model, Open Sound Control.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_360.pdf}
}

@InProceedings{Grosshauser2010,
  Title                    = {New Sensors and Pattern Recognition Techniques for String Instruments},
  Author                   = {Gro\ss hauser, Tobias and Gro\ss ekath\"{o}fer, Ulf and Hermann, Thomas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {271--276},

  Abstract                 = {Pressure, motion, and gesture are important parameters inmusical instrument playing. Pressure sensing allows to interpret complex hidden forces, which appear during playinga musical instrument. The combination of our new sensorsetup with pattern recognition techniques like the lately developed ordered means models allows fast and precise recognition of highly skilled playing techniques. This includes leftand right hand analysis as well as a combination of both. Inthis paper we show bow position recognition for string instruments by means of support vector regression machineson the right hand finger pressure, as well as bowing recognition and inaccurate playing detection with ordered meansmodels. We also introduce a new left hand and chin pressuresensing method for coordination and position change analysis. Our methods in combination with our audio, video,and gesture recording software can be used for teachingand exercising. Especially studies of complex movementsand finger force distribution changes can benefit from suchan approach. Practical applications include the recognitionof inaccuracy, cramping, or malposition, and, last but notleast, the development of augmented instruments and newplaying techniques.},
  Keywords                 = {left hand,nime10,ordered means models,pressure,sensor,strings},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_271.pdf}
}

@InProceedings{Grossmann2010,
  Title                    = {Developing a Hybrid Contrabass Recorder Resistances, Expression, Gestures and Rhetoric},
  Author                   = {Grossmann, Cesar M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {223--228},

  Abstract                 = {In this paper I describe aspects that have been involved in my experience of developing a hybrid instrument. The process of transformation and extension of the instrument is informed by ideas concerning the intrinsic communication aspects of musical activities. Decisions taken for designing the instrument and performing with it take into account the hypothesis that there are ontological levels of human reception in music that are related to the intercorporeal. Arguing that it is necessary to encounter resistances for achieving expression, it is suggested that new instrumental development ought to reflect on the concern for keeping the natural connections of live performances. },
  Keywords                 = {live processing,new instruments,nime10,recorder},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_223.pdf}
}

@InProceedings{Guaus2010,
  Title                    = {A Left Hand Gesture Caption System for Guitar Based on Capacitive Sensors},
  Author                   = {Guaus, Enric and Ozaslan, Tan and Palacios, Eric and Arcos, Josep L.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {238--243},

  Abstract                 = {In this paper, we present our research on the acquisitionof gesture information for the study of the expressivenessin guitar performances. For that purpose, we design a sensor system which is able to gather the movements from lefthand fingers. Our effort is focused on a design that is (1)non-intrusive to the performer and (2) able to detect fromstrong movements of the left hand to subtle movements ofthe fingers. The proposed system is based on capacitive sensors mounted on the fingerboard of the guitar. We presentthe setup of the sensor system and analyze its response toseveral finger movements.},
  Keywords                 = {Guitar; Gesture acquisition; Capacitive sensors},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_238.pdf}
}

@InProceedings{Gurevich2010,
  Title                    = {Style and Constraint in Electronic Musical Instruments},
  Author                   = {Gurevich, Michael and Stapleton, Paul and Marquez-Borbon, Adnan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {106--111},

  Abstract                 = {A qualitative study to investigate the development of stylein performance with a highly constrained musical instrument is described. A new one-button instrument was designed, with which several musicians were each asked topractice and develop a solo performance. Observations oftrends in attributes of these performances are detailed in relation to participants' statements in structured interviews.Participants were observed to develop stylistic variationsboth within the domain of activities suggested by the constraint, and by discovering non-obvious techniques througha variety of strategies. Data suggest that stylistic variationsoccurred in spite of perceived constraint, but also becauseof perceived constraint. Furthermore, participants tendedto draw on unique experiences, approaches and perspectivesthat shaped individual performances.},
  Keywords                 = {design, interaction, performance, persuasive technology},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_106.pdf}
}

@InProceedings{Hahnel2010,
  Title                    = {From Mozart to MIDI : A Rule System for Expressive Articulation},
  Author                   = {H\"{a}hnel, Tilo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {72--75},

  Abstract                 = {The propriety of articulation, especially of notes that lackannotations, is influenced by the origin of the particularmusic. This paper presents a rule system for articulationderived from late Baroque and early Classic treatises on performance. Expressive articulation, in this respect, is understood as a combination of alterable tone features like duration, loudness, and timbre. The model differentiates globalcharacteristics and local particularities, provides a generalframework for human-like music performances, and, therefore, serves as a basis for further and more complex rulesystems.},
  Keywords                 = {Articulation, Historically Informed Performance, Expres- sive Performance, Synthetic Performance},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_072.pdf}
}

@InProceedings{Hahnel2010b,
  Title                    = {Expressive Articulation for Synthetic Music Performances},
  Author                   = {H\"{a}hnel, Tilo and Berndt, Axel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {277--282},

  Abstract                 = {As one of the main expressive feature in music, articulationaffects a wide range of tone attributes. Based on experimental recordings we analyzed human articulation in the lateBaroque style. The results are useful for both the understanding of historically informed performance practices andfurther progress in synthetic performance generation. Thispaper reports of our findings and the implementation in aperformance system. Because of its flexibility and universality the system allows more than Baroque articulation.},
  Keywords                 = {Expressive Performance, Articulation, Historically Informed Performance},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_277.pdf}
}

@InProceedings{Hadjakos2010,
  Title                    = {Analysis of Piano Playing Movements Spanning Multiple Touches},
  Author                   = {Hadjakos, Aristotelis and M\"{u}hlh\"{a}user, Max},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {335--338},

  Keywords                 = {nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_335.pdf}
}

@InProceedings{Hass2010,
  Title                    = {Creating Integrated Music and Video for Dance : Lessons Learned and Lessons Ignored},
  Author                   = {Hass, Jeffrey},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {489--492},

  Abstract                 = {In his demonstration, the ,
,
author discusses the sequential progress of his technical and aesthetic decisions as composer and videographer for four large-scale works for dance through annotated video examples of live performances and PowerPoint slides. In addition, he discusses his current real-time dance work with wireless sensor interfaces using sewable LilyPad Arduino modules and Xbee radio hardware. Keywords },
  Keywords                 = {dance, video processing, video tracking, LilyPad Arduino.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_489.pdf}
}

@InProceedings{Havryliv2010,
  Title                    = {Composing For Improvisation with Chaotic Oscillators},
  Author                   = {Havryliv, Mark},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {94--99},

  Abstract                 = {This paper describes a novel method for composing andimprovisation with real-time chaotic oscillators. Recentlydiscovered algebraically simple nonlinear third-order differential equations are solved and acoustical descriptors relating to their frequency spectrums are determined accordingto the MPEG-7 specification. A second nonlinearity is thenadded to these equations: a real-time audio signal. Descriptive properties of the complex behaviour of these equationsare then determined as a function of difference tones derived from a Just Intonation scale and the amplitude ofthe audio signal. By using only the real-time audio signalfrom live performer/s as an input the causal relationshipbetween acoustic performance gestures and computer output, including any visual or performer-instruction output,is deterministic even if the chaotic behaviours are not.},
  Keywords                 = {audio de-,chaos and music,chaotic dynamics and oscillators,dif-,ferential equations and music,mathematica,nime10,scriptors and mpeg-7},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_094.pdf}
}

@InProceedings{Hawryshkewich2010,
  Title                    = {Beatback : A Real-time Interactive Percussion System for Rhythmic Practise and Exploration},
  Author                   = {Hawryshkewich, Andrew and Pasquier, Philippe and Eigenfeldt, Arne},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {100--105},

  Keywords                 = {Interactive music interface, real-time, percussion, machine learning, Markov models, MIDI.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_100.pdf}
}

@InProceedings{Hayes2010,
  Title                    = {Neurohedron : A Nonlinear Sequencer Interface},
  Author                   = {Hayes, Ted},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {23--25},

  Abstract                 = {The Neurohedron is a multi-modal interface for a nonlinear sequencer software model, embodied physically in a dodecahedron. The faces of the dodecahedron are both inputs and outputs, allowing the device to visualize the activity of the software model as well as convey input to it. The software model maps MIDI notes to the faces of the device, and defines and controls the behavior of the sequencer's progression around its surface, resulting in a unique instrument for computer-based performance and composition. },
  Keywords                 = {controller,human computer interaction,interface,live performance,neural network,nime10,sequencer},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_023.pdf}
}

@InProceedings{Headlee2010,
  Title                    = {Sonic Virtual Reality Game : How Does Your Body Sound ?},
  Author                   = {Headlee, Kimberlee and Koziupa, Tatyana and Siwiak, Diana},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {423--426},

  Abstract                 = {In this paper, we present an interactive system that uses the body as a generative tool for creating music. We explore innovative ways to make music, create self-awareness, and provide the opportunity for unique, interactive social experiences. The system uses a multi-player game paradigm, where players work together to add layers to a soundscape of three distinct environments. Various sensors and hardware are attached to the body and transmit signals to a workstation, where they are processed using Max/MSP. The game is divided into three levels, each of a different soundscape. The underlying purpose of our system is to move the player's focus away from complexities of the modern urban world toward a more internalized meditative state. The system is currently viewed as an interactive installation piece, but future iterations have potential applications in music therapy, bio games, extended performance art, and as a prototype for new interfaces for musical expression. },
  Keywords                 = {biomusic,collaborative,expressive,hci,interactive,interactivity design,interface for musical expression,multimodal,musical mapping strategies,nime10,performance,sonification},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_423.pdf}
}

@InProceedings{Heinz2010,
  Title                    = {Designing a Shareable Musical TUI},
  Author                   = {Heinz, Sebastian and O'Modhrain, Sile},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {339--342},

  Abstract                 = {This paper proposes a design concept for a tangible interface forcollaborative performances that incorporates two social factorspresent during performance, the individual creation andadaptation of technology and the sharing of it within acommunity. These factors are identified using the example of alaptop ensemble and then applied to three existing collaborativeperformance paradigms. Finally relevant technology, challengesand the current state of our implementation are discussed.},
  Keywords                 = {Tangible User Interfaces, collaborative performances, social factors},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_339.pdf}
}

@InProceedings{Hochenbaum2010,
  Title                    = {Multimodal Musician Recognition},
  Author                   = {Hochenbaum, Jordan and Kapur, Ajay and Wright, Matthew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {233--237},

  Abstract                 = {This research is an initial effort in showing how a multimodal approach can improve systems for gaining insight into a musician's practice and technique. Embedding a variety of sensors inside musical instruments and synchronously recording the sensors' data along with audio, we gather a database of gestural information from multiple performers, then use machine-learning techniques to recognize which musician is performing. Our multimodal approach (using both audio and sensor data) yields promising performer classification results, which we see as a first step in a larger effort to gain insight into musicians' practice and technique. },
  Keywords                 = {Performer Recognition, Multimodal, HCI, Machine Learning, Hyperinstrument, eSitar},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_233.pdf}
}

@InProceedings{Hochenbaum2010a,
  Title                    = {Designing Expressive Musical Interfaces for Tabletop Surfaces},
  Author                   = {Hochenbaum, Jordan and Vallis, Owen and Diakopoulos, Dimitri and Murphy, Jim and Kapur, Ajay},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {315--318},

  Abstract                 = {This paper explores the evolution of collaborative, multi-user, musical interfaces developed for the Bricktable interactive surface. Two key types of applications are addressed: user interfaces for artistic installation and interfaces for musical performance. In describing our software, we provide insight on the methodologies and practicalities of designing interactive musical systems for tabletop surfaces. Additionally, subtleties of working with custom-designed tabletop hardware are addressed. },
  Keywords                 = {Bricktable, Multi-touch Interface, Tangible Interface, Generative Music, Music Information Retrieval},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_315.pdf}
}

@InProceedings{Holm2010,
  Title                    = {Associating Emoticons with Musical Genres},
  Author                   = {Holm, Jukka and Holm, Harri and Sepp\"{a}nen, Jarno},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {383--386},

  Abstract                 = {Music recommendation systems can observe user's personal preferences and suggest new tracks from a large online catalog. In the case of context-aware recommenders, user's current emotional state plays an important role. One simple way to visualize emotions and moods is graphical emoticons. In this study, we researched a high-level mapping between genres, as descriptions of music, and emoticons, as descriptions of emotions and moods. An online questionnaire with 87 participants was arranged. Based on the results, we present a list of genres that could be used as a starting point for making recommendations fitting the current mood of the user. },
  Keywords                 = {Music, music recommendation, context, facial expression, mood, emotion, emoticon, and musical genre.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_383.pdf}
}

@InProceedings{Humphrey2010,
  Title                    = {Epi-thet : A Musical Performance Installation and a Choreography of Stillness},
  Author                   = {Humphrey, Tim and Flynn, Madeleine and Stevens, Jesse},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {69--71},

  Abstract                 = {This paper articulates an interest in a kind of interactive musical instrument and artwork that defines the mechanisms for instrumental interactivity from the iconic morphologies of "ready-mades", casting historical utilitarian objects as the basis for performed musical experiences by spectators. The interactive repertoires are therefore partially pre-determined through enculturated behaviors that are associated with particular objects, but more importantly, inextricably linked to the thematic and meaningful assemblage of the work itself. Our new work epi-thet gathers data from individual interactions with common microscopes placed on platforms within a large space. This data is correlated with public domain genetic datasets obtained from micro-array analysis. A sonification algorithm generates unique compositions associated with the spectator "as measured" through their individual specification in performing an iconic measurement action. The apparatus is a receptacle for unique compositions in sound, and invites a participatory choreography of stillness that is available for reception as a live musical performance. },
  Keywords                 = {Sonification installation spectator-choreography micro-array ready-mades morphology stillness},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_069.pdf}
}

@InProceedings{Jaimovich2010,
  Title                    = {Ground Me ! An Interactive Sound Art Installation},
  Author                   = {Jaimovich, Javier},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {391--394},

  Abstract                 = {This paper describes the design, implementation and outcome of Ground Me!, an interactive sound installation set up in the Sonic Lab of the Sonic Arts Research Centre. The site-specific interactive installation consists of multiple copper poles hanging from the Sonic Lab's ceiling panels, which trigger samples of electricity sounds when grounded through the visitor's' body to the space's metallic floor. },
  Keywords                 = {Interactive sound installation, body impedance, skin conductivity, site-specific sound installation, human network, Sonic Lab, Arduino.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_391.pdf}
}

@InProceedings{Jaimovich2010a,
  Title                    = {Synchronization of Multimodal Recordings for Musical Performance Research},
  Author                   = {Jaimovich, Javier and Knapp, Benjamin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {372--374},

  Abstract                 = {The past decade has seen an increase of low-cost technology for sensor data acquisition, which has been utilized for the expanding field of research in gesture measurement for music performance. Unfortunately, these devices are still far from being compatible with the audiovisual recording platforms which have been used to record synchronized streams of data. In this paper, we describe a practical solution for simultaneous recording of heterogeneous multimodal signals. The recording system presented uses MIDI Time Code to time-stamp sensor data and to synchronize with standard video and audio recording systems. We also present a set of tools for recording sensor data, as well as a set of analysis tools to evaluate in realtime the sample rate of different signals, and the overall synchronization status of the recording system. },
  Keywords                 = {Synchronization, Multimodal Signals, Sensor Data Acquisition, Signal Recording.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_372.pdf}
}

@InProceedings{Jensenius2010,
  Title                    = {Evaluating the Subjective Effects of Microphone Placement on Glass Instruments},
  Author                   = {Jensenius, Alexander R. and Innervik, Kjell Tore and Frounberg, Ivar},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {208--211},

  Abstract                 = {We report on a study of perceptual and acoustic featuresrelated to the placement of microphones around a custommade glass instrument. Different microphone setups weretested: above, inside and outside the instrument and at different distances. The sounds were evaluated by an expertperformer, and further qualitative and quantitative analyses have been carried out. Preference was given to therecordings from microphones placed close to the rim of theinstrument, either from the inside or the outside.},
  Keywords                 = {glass instruments, microphone placement, sound analysis},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_208.pdf}
}

@InProceedings{Kang2010,
  Title                    = {H\'{e} : Calligraphy as a Musical Interface},
  Author                   = {Kang, Laewoo and Chien, Hsin-Yi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {352--355},

  Keywords                 = {Interactive music interface, calligraphy, graphical music composing, sonification},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_352.pdf}
}

@InProceedings{Kapur2010,
  Title                    = {A Pedagogical Paradigm for Musical Robotics},
  Author                   = {Kapur, Ajay and Darling, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {162--165},

  Abstract                 = {This paper describes the making of a class to teach the history and art of musical robotics. The details of the curriculum are described as well as designs for our custom schematics for robotic solenoid driven percussion. This paper also introduces four new robotic instruments that were built during the term of this course. This paper also introduces the Machine Orchestra, a laptop orchestra with ten human performers and our five robotic instruments. },
  Keywords                 = {dartron,digital classroom,laptop orchestra,machine orchestra,musical robotics,nime pedagogy,nime10,solenoid},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_162.pdf}
}

@InProceedings{Kiefer2010,
  Title                    = {A Malleable Interface for Sonic Exploration},
  Author                   = {Kiefer, Chris},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {291--296},

  Abstract                 = {Input devices for controlling music software can benefit fromexploiting the use of perceptual-motor skill in interaction.The project described here is a new musical controller, designed with the aim of enabling intuitive and nuanced interaction through direct physical manipulation of malleablematerial.The controller is made from conductive foam. This foamchanges electrical resistance when deformed; the controllerworks by measuring resistance at multiple points in a single piece of foam in order to track its shape. These measurements are complex and interdependent so an echo statenetwork, a form of recurrent neural network, is employed totranslate the sensor readings into usable control data.A cube shaped controller was built and evaluated in thecontext of the haptic exploration of sound synthesis parameter spaces. Eight participants experimented with the controller and were interviewed about their experiences. Thecontroller achieves its aim of enabling intuitive interaction,but in terms of nuanced interaction, accuracy and repeatability were issues for some participants. It's not clear fromthe short evaluation study whether these issues would improve with practice, a longitudinal study that gives musicians time to practice and find the creative limitations ofthe controller would help to evaluate this fully.The evaluation highlighted interesting issues concerningthe high level nature of malleable control and different approaches to sonic exploration.},
  Keywords                 = {Musical Controller, Reservoir Computing, Human Computer Interaction, Tangible User Interface, Evaluation},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_291.pdf}
}

@InProceedings{Kim2010,
  Title                    = {Interactive Music Studio : The Soloist},
  Author                   = {Kim, Hyun-Soo and Yoon, Je-Han and Jung, Moon-Sik},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {444--446},

  Keywords                 = {Mobile device, music composer, pattern composing, MIDI},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_444.pdf}
}

@InProceedings{Kitani2010,
  Title                    = {ImprovGenerator : Online Grammatical Induction for On-the-Fly Improvisation Accompaniment},
  Author                   = {Kitani, Kris M. and Koike, Hideki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {469--472},

  Abstract                 = {We propose an online generative algorithm to enhance musical expression via intelligent improvisation accompaniment.Our framework called the ImprovGenerator, takes a livestream of percussion patterns and generates an improvisedaccompaniment track in real-time to stimulate new expressions in the improvisation. We use a mixture model togenerate an accompaniment pattern, that takes into account both the hierarchical temporal structure of the liveinput patterns and the current musical context of the performance. The hierarchical structure is represented as astochastic context-free grammar, which is used to generateaccompaniment patterns based on the history of temporalpatterns. We use a transition probability model to augmentthe grammar generated pattern to take into account thecurrent context of the performance. In our experiments weshow how basic beat patterns performed by a percussioniston a cajon can be used to automatically generate on-the-flyimprovisation accompaniment for live performance.},
  Keywords                 = {Machine Improvisation, Grammatical Induction, Stochastic Context-Free Grammars, Algorithmic Composition},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_469.pdf}
}

@InProceedings{Kocaballi2010,
  Title                    = {Investigating the Potential for Shared Agency using Enactive Interfaces},
  Author                   = {Kocaballi, A. Baki and Gemeinboeck, Petra and Saunders, Rob},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {47--50},

  Abstract                 = {Human agency, our capacity for action, has been at the hub of discussions centring upon philosophical enquiry for a long period of time. Sensory supplementation devices can provide us with unique opportunities to investigate the different aspects of our agency by enabling new modes of perception and facilitating the emergence of novel interactions, all of which is impossible without the aforesaid devices. Our preliminary study investigates the non-verbal strategies employed for negotiation of our capacity for action with other bodies and the surrounding space through body-to-body and body-to-space couplings enabled by sensory supplementation devices. We employed a lowfi rapid prototyping approach to build this device, enabling distal perception by sonic and haptic feedback. Further, we conducted a workshop in which participants equipped with this device engaged in game-like activities. },
  Keywords                 = {Human agency, sensory supplementation, distal perception, sonic feedback, tactile feedback, enactive interfaces},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_047.pdf}
}

@InProceedings{LeGroux2010,
  Title                    = {Disembodied and Collaborative Musical Interaction in the Multimodal Brain Orchestra},
  Author                   = {Le Groux, Sylvain and Manzolli, Jonatas and Verschure, Paul F. and Marti Sanchez and Andre Luvizotto and Anna Mura and Aleksander Valjamae and Christoph Guger and Robert Prueckl and Ulysses Bernardet},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {309--314},

  Abstract                 = {Most new digital musical interfaces have evolved upon theintuitive idea that there is a causality between sonic outputand physical actions. Nevertheless, the advent of braincomputer interfaces (BCI) now allows us to directly accesssubjective mental states and express these in the physicalworld without bodily actions. In the context of an interactive and collaborative live performance, we propose to exploit novel brain-computer technologies to achieve unmediated brain control over music generation and expression.We introduce a general framework for the generation, synchronization and modulation of musical material from brainsignal and describe its use in the realization of Xmotion, amultimodal performance for a "brain quartet".},
  Date-modified            = {2013-06-06 20:01:54 +0000},
  Keywords                 = {Brain-computer Interface, Biosignals, Interactive Music Sys- tem, Collaborative Musical Performance},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_309.pdf}
}

@InProceedings{Liebman2010,
  Title                    = {Cuebert : A New Mixing Board Concept for Musical Theatre},
  Author                   = {Liebman, Noah and Nagara, Michael and Spiewla, Jacek and Zolkosky, Erin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {51--56},

  Keywords                 = {audio,control surfaces,mixing board,multitouch,nime10,screen,sound,theatre,touch-,user-centered design},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_051.pdf}
}

@InProceedings{Muller2010,
  Title                    = {Reflective Haptics : Resistive Force Feedback for Musical Performances with Stylus-Controlled Instruments},
  Author                   = {M\"{u}ller, Alexander and Hemmert, Fabian and Wintergerst, G\"{o}tz and Jagodzinski, Ron},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {477--478},

  Abstract                 = {In this paper we present a novel system for tactile actuation in stylus-based musical interactions. The proposed controller aims to support rhythmical musical performance. The system builds on resistive force feedback, which is achieved through a brakeaugmented ball pen stylus on a sticky touch-sensitive surface. Along the device itself, we present musical interaction principles that are enabled through the aforementioned tactile response. Further variations of the device and perspectives of the friction-based feedback are outlined. },
  Keywords                 = {force feedback,haptic feedback,interactive,nime10,pen controller},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_477.pdf}
}

@InProceedings{Magnusson2010,
  Title                    = {An Epistemic Dimension Space for Musical Devices},
  Author                   = {Magnusson, Thor},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {43--46},

  Abstract                 = {The analysis of digital music systems has traditionally been characterized by an approach that can be defined as phenomenological. The focus has been on the body and its relationship to the machine, often neglecting the system's conceptual design. This paper brings into focus the epistemic features of digital systems, which implies emphasizing the cognitive, conceptual and music theoretical side of our musical instruments. An epistemic dimension space for the analysis of musical devices is proposed. },
  Keywords                 = {Epistemic tools, music theory, dimension space, analysis.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_043.pdf}
}

@InProceedings{Marier2010,
  Title                    = {The Sponge A Flexible Interface},
  Author                   = {Marier, Martin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {356--359},

  Abstract                 = {The sponge is an interface that allows a clear link to beestablished between gesture and sound in electroacousticmusic. The goals in developing the sponge were to reintroduce the pleasure of playing and to improve the interaction between the composer/performer and the audience. Ithas been argued that expenditure of effort or energy is required to obtain expressive interfaces. The sponge favors anenergy-sound relationship in two ways : 1) it senses acceleration, which is closely related to energy; and 2) it is madeout of a flexible material (foam) that requires effort to besqueezed or twisted. Some of the mapping strategies usedin a performance context with the sponge are discussed.},
  Keywords                 = {Interface, electroacoustic music, performance, expressivity, mapping},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_356.pdf}
}

@InProceedings{Martin2010,
  Title                    = {Mechanisms for Controlling Complex Sound Sources : Applications to Guitar Feedback Control},
  Author                   = {Martin, Aengus and Ferguson, Sam and Beilharz, Kirsty},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {364--367},

  Abstract                 = {Many musical instruments have interfaces which emphasisethe pitch of the sound produced over other perceptual characteristics, such as its timbre. This is at odds with the musical developments of the last century. In this paper, weintroduce a method for replacing the interface of musicalinstruments (both conventional and unconventional) witha more flexible interface which can present the intrument'savailable sounds according to variety of different perceptualcharacteristics, such as their brightness or roughness. Weapply this method to an instrument of our own design whichcomprises an electro-mechanically controlled electric guitarand amplifier configured to produce feedback tones.},
  Keywords                 = {Concatenative Synthesis, Feedback, Guitar},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_364.pdf}
}

@InProceedings{Martin2010a,
  Title                    = {Cross-Artform Performance Using Networked Interfaces : Last Man to Die's Vital LMTD},
  Author                   = {Martin, Charles and Forster, Benjamin and Cormick, Hanna},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {204--207},

  Abstract                 = {In 2009 the cross artform group, Last Man to Die, presenteda series of performances using new interfaces and networkedperformance to integrate the three artforms of its members(actor, Hanna Cormick, visual artist, Benjamin Forster andpercussionist, Charles Martin). This paper explains ourartistic motivations and design for a computer vision surfaceand networked heartbeat sensor as well as the experience ofmounting our first major work, Vital LMTD.},
  Keywords                 = {cross-artform performance, networked performance, physi- cal computing},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_204.pdf}
}

@InProceedings{Maruyama2010,
  Title                    = {UnitInstrument : Easy Configurable Musical Instruments},
  Author                   = {Maruyama, Yutaro and Takegawa, Yoshinari and Terada, Tsutomu and Tsukamoto, Masahiko},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {7--12},

  Keywords                 = {Musical instruments, Script language},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_007.pdf}
}

@InProceedings{Mattek2010,
  Title                    = {Revisiting Cagean Composition Methodology with a Modern Computational Implementation},
  Author                   = {Mattek, Alison and Freeman, Mark and Humphrey, Eric},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {479--480},

  Keywords                 = {Multi-touch Interfaces, Computer-Assisted Composition},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_479.pdf}
}

@InProceedings{McPherson2010,
  Title                    = {Augmenting the Acoustic Piano with Electromagnetic String Actuation and Continuous Key Position Sensing},
  Author                   = {McPherson, Andrew and Kim, Youngmoo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {217--222},

  Abstract                 = {This paper presents the magnetic resonator piano, an augmented instrument enhancing the capabilities of the acoustic grand piano. Electromagnetic actuators induce the stringsto vibration, allowing each note to be continuously controlled in amplitude, frequency, and timbre without external loudspeakers. Feedback from a single pickup on thepiano soundboard allows the actuator waveforms to remainlocked in phase with the natural motion of each string. Wealso present an augmented piano keyboard which reportsthe continuous position of every key. Time and spatial resolution are sufficient to capture detailed data about keypress, release, pretouch, aftertouch, and other extended gestures. The system, which is designed with cost and setupconstraints in mind, seeks to give pianists continuous control over the musical sound of their instrument. The instrument has been used in concert performances, with theelectronically-actuated sounds blending with acoustic instruments naturally and without amplification.},
  Keywords                 = {Augmented instruments, piano, interfaces, electromagnetic actuation, gesture measurement},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_217.pdf}
}

@InProceedings{Meier2010,
  Title                    = {The Planets},
  Author                   = {Meier, Max and Schranner, Max},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {501--504},

  Keywords                 = {algorithmic composition,nime10,soft constraints,tangible interaction},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_501.pdf}
}

@InProceedings{Miller2010,
  Title                    = {Wiiolin : a Virtual Instrument Using the Wii Remote},
  Author                   = {Miller, Jace and Hammond, Tracy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {497--500},

  Abstract                 = {The console gaming industry is experiencing a revolution in terms of user control, and a large part to Nintendo's introduction of the Wii remote. The online open source development community has embraced the Wii remote, integrating the inexpensive technology into numerous applications. Some of the more interesting applications demonstrate how the remote hardware can be leveraged for nonstandard uses. In this paper we describe a new way of interacting with the Wii remote and sensor bar to produce music. The Wiiolin is a virtual instrument which can mimic a violin or cello. Sensor bar motion relative to the Wii remote and button presses are analyzed in real-time to generate notes. Our design is novel in that it involves the remote's infrared camera and sensor bar as an integral part of music production, allowing users to change notes by simply altering the angle of their wrist, and henceforth, bow. The Wiiolin introduces a more realistic way of instrument interaction than other attempts that rely on button presses and accelerometer data alone. },
  Keywords                 = {Wii remote, virtual instrument, violin, cello, motion recognition, human computer interaction, gesture recognition.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_497.pdf}
}

@InProceedings{Mills2010,
  Title                    = {Music Programming in Minim},
  Author                   = {Mills, John A. and {Di Fede}, Damien and Brix, Nicolas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {37--42},

  Abstract                 = {Our team realized that a need existed for a music programming interface in the Minim audio library of the Processingprogramming environment. The audience for this new interface would be the novice programmer interested in usingmusic as part of the learning experience, though the interface should also be complex enough to benefit experiencedartist-programmers. We collected many ideas from currently available music programming languages and librariesto design and create the new capabilities in Minim. Thebasic mechanisms include chained unit generators, instruments, and notes. In general, one "patches" unit generators(for example, oscillators, delays, and envelopes) together inorder to create synthesis algorithms. These algorithms canthen either create continuous sound, or be used in instruments to play notes with specific start time and duration.We have written a base set of unit generators to enablea wide variety of synthesis options, and the capabilities ofthe unit generators, instruments, and Processing allow fora wide range of composition techniques.},
  Keywords                 = {Minim, music programming, audio library, Processing, mu- sic software},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_037.pdf}
}

@InProceedings{Mills2010a,
  Title                    = {Dislocated Sound : A Survey of Improvisation in Networked Audio Platforms},
  Author                   = {Mills, Roger},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {186--191},

  Abstract                 = {The evolution of networked audio technologies has created unprecedented opportunities for musicians to improvise with instrumentalists from a diverse range of cultures and disciplines. As network speeds increase and latency is consigned to history, tele-musical collaboration, and in particular improvisation will be shaped by new methodologies that respond to this potential. While networked technologies eliminate distance in physical space, for the remote improviser, this creates a liminality of experience through which their performance is mediated. As a first step in understanding the conditions arising from collaboration in networked audio platforms, this paper will examine selected case studies of improvisation in a variety of networked interfaces. The ,
,
author will examine how platform characteristics and network conditions influence the process of collective improvisation and the methodologies musicians are employing to negotiate their networked experiences. },
  Keywords                 = {improvisation,internet audio,networked collaboration,nime10,sound},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_186.pdf}
}

@InProceedings{Miyama2010,
  Title                    = {Peacock : A Non-Haptic {3D} Performance Interface},
  Author                   = {Miyama, Chikashi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {380--382},

  Abstract                 = {Peacock is a newly designed interface for improvisational performances. The interface is equipped with thirty-five proximity sensors arranged in five rows and seven columns. The sensors detect the movements of a performer's hands and arms in a three-dimensional space above them. The interface digitizes the output of the sensors into sets of high precision digital packets, and sends them to a patch running in Pdextended with a sufficiently high bandwidth for performances with almost no computational resource consumption in Pd. The precision, speed, and efficiency of the system enable the sonification of hand gestures in realtime without the need to attach any physical devices to the performer's body. This paper traces the interface's evolution, discussing relevant technologies, hardware construction, system design, and input monitoring. },
  Keywords                 = {Musical interface, Sensor technologies, Computer music, Hardware and software design},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_380.pdf}
}

@InProceedings{Mulder2010,
  Title                    = {The Loudspeaker as Musical Instrument},
  Author                   = {Mulder, Jos},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {13--18},

  Keywords                 = {amplification,modal perception,multi,musical instruments,nime10,performance practice,sound technology},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_013.pdf}
}

@InProceedings{Murphy2010,
  Title                    = {The Helio : A Study of Membrane Potentiometers and Long Force Sensing Resistors for Musical Interfaces},
  Author                   = {Murphy, Jim and Kapur, Ajay and Burgin, Carl},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {459--462},

  Abstract                 = {This paper describes a study of membrane potentiometers and long force sensing resistors as tools to enable greater interaction between performers and audiences. This is accomplished through the building of a new interface called the Helio. In preparation for the Helio's construction, a variety of brands of membrane potentiometers and long force sensing resistors were analyzed for their suitability for use in a performance interface. Analog and digital circuit design considerations are discussed. We discuss in detail the design process and performance scenarios explored with the Helio. },
  Keywords                 = {Force Sensing Resistors, Membrane Potentiometers, Force Sensing Resistors, Haptic Feedback, Helio},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_459.pdf}
}

@InProceedings{Nagashima2010,
  Title                    = {Untouchable Instrument "Peller-Min"},
  Author                   = {Nagashima, Yoichi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {387--390},

  Abstract                 = {This paper is a report on the development of a new musical instrument in which the main concept is "Untouchable". The key concept of this instrument is "sound generation by body gesture (both hands)" and "sound generation by kneading with hands". The new composition project had completed as the premiere of a new work "controllable untouchableness" with this new instrument in December 2009.},
  Keywords                 = {Theremin, untouchable, distance sensor, Propeller processor},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_387.pdf}
}

@InProceedings{Nugroho2010,
  Title                    = {Understanding and Evaluating User Centred Design in Wearable Expressions},
  Author                   = {Nugroho, Jeremiah and Beilharz, Kirsty},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {327--330},

  Abstract                 = {In this paper, we describe the shaping factors, which simplify and help us understand the multi-dimensional aspects of designing Wearable Expressions. These descriptive shaping factors contribute to both the design and user-experience evaluation of Wearable Expressions. },
  Keywords                 = {Wearable expressions, body, user-centered design.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_327.pdf}
}

@InProceedings{Nymoen2010,
  Title                    = {Searching for Cross-Individual Relationships between Sound and Movement Features using an {SVM} Classifier},
  Author                   = {Nymoen, Kristian and Glette, Kyrre and Skogstad, St\aa le A. and Torresen, Jim and Jensenius, Alexander R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {259--262},

  Keywords                 = {nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_259.pdf}
}

@InProceedings{Oh2010,
  Title                    = {Evolving The Mobile Phone Orchestra},
  Author                   = {Oh, Jieun and Herrera, Jorge and Bryan, Nicholas J. and Dahl, Luke and Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {82--87},

  Abstract                 = {In this paper, we describe the development of the Stanford Mobile Phone Orchestra (MoPhO) since its inceptionin 2007. As a newly structured ensemble of musicians withiPhones and wearable speakers, MoPhO takes advantageof the ubiquity and mobility of smartphones as well asthe unique interaction techniques offered by such devices.MoPhO offers a new platform for research, instrument design, composition, and performance that can be juxtaposedto that of a laptop orchestra. We trace the origins of MoPhO,describe the motivations behind the current hardware andsoftware design in relation to the backdrop of current trendsin mobile music making, detail key interaction conceptsaround new repertoire, and conclude with an analysis onthe development of MoPhO thus far.},
  Keywords                 = {mobile phone orchestra, live performance, iPhone, mobile music},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_082.pdf}
}

@InProceedings{Paine2010,
  Title                    = {Towards a Taxonomy of Realtime Interfaces for Electronic Music Performance},
  Author                   = {Paine, Garth},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {436--439},

  Abstract                 = {This paper presents a discussion regarding organology classification and taxonomies for digital musical instruments (DMI), arising from the TIEM (Taxonomy of Interfaces for Electronic Music performance) survey (http://tiem.emf.org/), conducted as part of an Australian Research Council Linkage project titled "Performance Practice in New Interfaces for Realtime Electronic Music Performance". This research is being carried out at the VIPRe Lab at, the University of Western Sydney in partnership with the Electronic Music Foundation (EMF), Infusion Systems1 and The Input Devices and Music Interaction Laboratory (IDMIL) at McGill University. The project seeks to develop a schema of new interfaces for realtime electronic music performance. },
  Keywords                 = {Instrument, Interface, Organology, Taxonomy.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_436.pdf}
}

@InProceedings{Pan2010,
  Title                    = {A Robot Musician Interacting with a Human Partner through Initiative Exchange},
  Author                   = {Pan, Ye and Kim, Min-Gyu and Suzuki, Kenji},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {166--169},

  Abstract                 = {This paper proposes a novel method to realize an initiativeexchange for robot. A humanoid robot plays vibraphone exchanging initiative with a human performer by perceivingmultimodal cues in real time. It understands the initiative exchange cues through vision and audio information.In order to achieve the natural initiative exchange betweena human and a robot in musical performance, we built thesystem and the software architecture and carried out the experiments for fundamental algorithms which are necessaryto the initiative exchange.},
  Keywords                 = {Human-robot interaction, initiative exchange, prediction},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_166.pdf}
}

@InProceedings{Park2010,
  Title                    = {Online Map Interface for Creative and Interactive},
  Author                   = {Park, Sihwa and Kim, Seunghun and Lee, Samuel and Yeo, Woon Seung},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {331--334},

  Abstract                 = {In this paper, we discuss the musical potential of COMPath - an online map based music-making tool - as a noveland unique interface for interactive music composition andperformance. COMPath provides an intuitive environmentfor creative music making by sonification of georeferenceddata. Users can generate musical events with simple andfamiliar actions on an online map interface; a set of local information is collected along the user-drawn route andthen interpreted as sounds of various musical instruments.We discuss the musical interpretation of routes on a map,review the design and implementation of COMPath, andpresent selected sonification results with focus on mappingstrategies for map-based composition.},
  Keywords                 = {Musical sonification, map interface, online map service, geo- referenced data, composition, mashup},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_331.pdf}
}

@InProceedings{Quintas2010,
  Title                    = {Glitch Delighter : Lighter's Flame Base Hyper-Instrument for Glitch Music in Burning The Sound Performance},
  Author                   = {Quintas, Rudolfo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {212--216},

  Keywords                 = {Hyper-Instruments, Glitch Music, Interactive Systems, Electronic Music Performance.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_212.pdf}
}

@InProceedings{Reboursiere2010,
  Title                    = {Multimodal Guitar : A Toolbox For Augmented Guitar Performances},
  Author                   = {Reboursi\`{e}re, Lo\"{\i}c and Frisson, Christian and L\"{a}hdeoja, Otso and Mills, John A. and Picard-Limpens, C\'{e}cile and Todoroff, Todor},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {415--418},

  Abstract                 = {This project aims at studying how recent interactive and interactions technologies would help extend how we play theguitar, thus defining the "multimodal guitar". Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed anddeveloped a freely-available toolbox for augmented guitarperformances, compliant with the PureData and Max/MSPenvironments, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing,modal synthesis, infinite sustain, rearranging looping and"smart" harmonizing.},
  Keywords                 = {Augmented guitar, audio synthesis, digital audio effects, multimodal interaction, gestural sensing, polyphonic tran- scription, hexaphonic guitar},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_415.pdf}
}

@InProceedings{Roberts2010,
  Title                    = {Dynamic Interactivity Inside the AlloSphere},
  Author                   = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn and Putnam, Lance},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {57--62},

  Keywords                 = {AlloSphere, mapping, performance, HCI, interactivity, Vir- tual Reality, OSC, multi-user, network},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_057.pdf}
}

@InProceedings{Rothman2010,
  Title                    = {The Ghost : An Open-Source, User Programmable MIDI Performance Controller},
  Author                   = {Rothman, Paul},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {431--435},

  Abstract                 = {The Ghost has been developed to create a merger between the standard MIDI keyboard controller, MIDI/digital guitars and alternative desktop controllers. Using a custom software editor, The Ghost's controls can be mapped to suit the users performative needs. The interface takes its interaction and gestural cues from the guitar but it is not a MIDI guitar. The Ghost's hardware, firmware and software will be open sourced with the hopes of creating a community of users that are invested in creating music with controller.},
  Keywords                 = {Controller, MIDI, Live Performance, Programmable, Open- Source},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_431.pdf}
}

@InProceedings{Savage2010,
  Title                    = {Mmmmm : A Multi-modal Mobile Music Mixer},
  Author                   = {Savage, Norma S. and Ali, Syed R. and Chavez, Norma E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {395--398},

  Abstract                 = {This paper presents Mmmmm; a Multimodal Mobile MusicMixer that provides DJs a new interface for mixing musicon the Nokia N900 phones. Mmmmm presents a novel wayfor DJ to become more interactive with their audience andvise versa. The software developed for the N900 mobilephone utilizes the phones built-in accelerometer sensor andBluetooth audio streaming capabilities to mix and apply effects to music using hand gestures and have the mixed audiostream to Bluetooth speakers, which allows the DJ to moveabout the environment and get familiarized with their audience, turning the experience of DJing into an interactiveand audience engaging process.Mmmmm is designed so that the DJ can utilize handgestures and haptic feedback to help them perform the various tasks involved in DJing (mixing, applying effects, andetc). This allows the DJ to focus on the crowd, thus providing the DJ a better intuition of what kind of music ormusical mixing style the audience is more likely to enjoyand engage with. Additionally, Mmmmm has an "Ambient Tempo Detection mode in which the phones camera isutilized to detect the amount of movement in the environment and suggest to the DJ the tempo of music that shouldbe played. This mode utilizes frame differencing and pixelchange overtime to get a sense of how fast the environmentis changing, loosely correlating to how fast the audience isdancing or the lights are flashing in the scene. By determining the ambient tempo of the environment the DJ canget a better sense for the type of music that would fit bestfor their venue.Mmmmm helps novice DJs achieve a better music repertoire by allowing them to interact with their audience andreceive direct feedback on their performance. The DJ canchoose to utilize these modes of interaction and performance or utilize traditional DJ controls using MmmmmsN900 touch screen based graphics user interface.},
  Keywords                 = {Multi-modal, interaction, music, mixer, mobile, interactive, DJ, smart phones, Nokia, n900, touch screen, accelerome- ter, phone, audience},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_395.pdf}
}

@InProceedings{Schacher2010,
  Title                    = {Motion To Gesture To Sound : Mapping For Interactive Dance},
  Author                   = {Schacher, Jan C.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {250--254},

  Abstract                 = {Mapping in interactive dance performance poses a number of questions related to the perception and expression of gestures in contrast to pure motion-detection and analysis. A specific interactive dance project is discussed, in which two complementary sensing modes are integrated to obtain higherlevel expressive gestures. These are applied to a modular nonlinear composition, in which the exploratory dance performance assumes the role of instrumentalist and conductor. The development strategies and methods for each of the involved artists are discussed and the software tools and wearable devices that were developed for this project are presented. },
  Keywords                 = {Mapping, motion sensing, computer vision, artistic strategies, wearable sensors, mapping tools, splines, delaunay tessellation.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_250.pdf}
}

@InProceedings{Schlei2010,
  Title                    = {Relationship-Based Instrument Mapping of Multi-Point Data Streams Using a Trackpad Interface},
  Author                   = {Schlei, Kevin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {136--139},

  Abstract                 = {Multi-point devices are rapidly becoming a practical interface choice for electronic musicians. Interfaces that generate multiple simultaneous streams of point data present a unique mapping challenge. This paper describes an analysis system for point relationships that acts as a bridge between raw streams of multi-point data and the instruments they control, using a multipoint trackpad to test various configurations. The aim is to provide a practical approach for instrument programmers working with multi-point tools, while highlighting the difference between mapping systems based on point coordinate streams, grid evaluations, or object interaction and mapping systems based on multi-point data relationships. },
  Keywords                 = {Multi-point, multi-touch interface, instrument mapping, multi- point data analysis, trackpad instrument},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_136.pdf}
}

@InProceedings{Schmeder2010,
  Title                    = {Support Vector Machine Learning for Gesture Signal Estimation with a Piezo-Resistive Fabric Touch Surface},
  Author                   = {Schmeder, Andrew and Freed, Adrian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {244--249},

  Abstract                 = {The design of an unusually simple fabric-based touchlocation and pressure sensor is introduced. An analysisof the raw sensor data is shown to have significant nonlinearities and non-uniform noise. Using support vectormachine learning and a state-dependent adaptive filter itis demonstrated that these problems can be overcome.The method is evaluated quantitatively using a statisticalestimate of the instantaneous rate of information transfer.The SVM regression alone is shown to improve the gesturesignal information rate by up to 20% with zero addedlatency, and in combination with filtering by 40% subjectto a constant latency bound of 10 milliseconds.},
  Keywords                 = {gesture signal processing,nime10,support vector,touch sensor},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_244.pdf}
}

@InProceedings{Skogstad2010,
  Title                    = {Using {IR} Optical Marker Based Motion Capture for Exploring Musical Interaction},
  Author                   = {Skogstad, St\aa le A. and Jensenius, Alexander R. and Nymoen, Kristian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {407--410},

  Keywords                 = {nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_407.pdf}
}

@InProceedings{Solis2010,
  Title                    = {Development of the Waseda Saxophonist Robot and Implementation of an Auditory Feedback Control},
  Author                   = {Solis, Jorge and Petersen, Klaus and Yamamoto, Tetsuro and Takeuchi, Masaki and Ishikawa, Shimpei and Takanishi, Atsuo and Hashimoto, Kunimatsu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {156--161},

  Abstract                 = {Since 2007, our research is related to the development of an anthropomorphic saxophonist robot, which it has been designed to imitate the saxophonist playing by mechanically reproducing the organs involved for playing a saxophone. Our research aims in understanding the motor control from an engineering point of view and enabling the communication. In this paper, the Waseda Saxophone Robot No. 2 (WAS-2) which is composed by 22-DOFs is detailed. The lip mechanism of WAS-2 has been designed with 3-DOFs to control the motion of the lower, upper and sideway lips. In addition, a human-like hand (16 DOF-s) has been designed to enable to play all the keys of the instrument. Regarding the improvement of the control system, a feed-forward control system with dead-time compensation has been implemented to assure the accurate control of the air pressure. In addition, the implementation of an auditory feedback control system has been proposed and implemented in order to adjust the positioning of the physical parameters of the components of the robot by providing a pitch feedback and defining a recovery position (off-line). A set of experiments were carried out to verify the mechanical design improvements and the dynamic response of the air pressure. As a result, the range of sound pressure has been increased and the proposed control system improved the dynamic response of the air pressure control. },
  Keywords                 = {Humanoid Robot, Auditory Feedback, Music, Saxophone.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_156.pdf}
}

@InProceedings{Stahl2010,
  Title                    = {Auditory Masquing : Wearable Sound Systems for Diegetic Character Voices},
  Author                   = {Stahl, Alex and Clemens, Patricia},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {427--430},

  Abstract                 = {Maintaining a sense of personal connection between increasingly synthetic performers and increasingly diffuse audiences is vital to storytelling and entertainment. Sonic intimacy is important, because voice is one of the highestbandwidth channels for expressing our real and imagined selves.New tools for highly focused spatialization could help improve acoustical clarity, encourage audience engagement, reduce noise pollution and inspire creative expression. We have a particular interest in embodied, embedded systems for vocal performance enhancement and transformation. This short paper describes work in progress on a toolkit for high-quality wearable sound suits. Design goals include tailored directionality and resonance, full bandwidth, and sensible ergonomics. Engineering details to accompany a demonstration of recent prototypes are presented, highlighting a novel magnetostrictive flextensional transducer. Based on initial observations we suggest that vocal acoustic output from the torso, and spatial perception of situated low frequency sources, are two areas deserving greater attention and further study.},
  Keywords                 = {magnetostrictive flextensional transducer,nime10,paralinguistics,sound reinforcement,spatialization,speech enhancement,transformation,voice,wearable systems},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_427.pdf}
}

@InProceedings{Suiter2010,
  Title                    = {Toward Algorithmic Composition of Expression in Music Using Fuzzy Logic},
  Author                   = {Suiter, Wendy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {319--322},

  Abstract                 = {This paper introduces the concept of composing expressive music using the principles of Fuzzy Logic. The paper provides a conceptual model of a musical work which follows compositional decision making processes. Significant features of this Fuzzy Logic framework are its inclusiveness through the consideration of all the many and varied musical details, while also incorporating the imprecision that characterises musical terminology and discourse. A significant attribute of my Fuzzy Logic method is that it traces the trajectory of all musical details, since it is both the individual elements and their combination over time which is significant to the effectiveness of a musical work in achieving its goals. The goal of this work is to find a set of elements and rules, which will ultimately enable the construction of a genralised algorithmic compositional system which can produce expressive music if so desired. },
  Keywords                 = {fuzzy logic,music composition,musical expression,nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_319.pdf}
}

@InProceedings{Tanaka2010,
  Title                    = {Mapping Out Instruments, Affordances, and Mobiles},
  Author                   = {Tanaka, Atau},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {88--93},

  Abstract                 = {This paper reviews and extends questions of the scope of an interactive musical instrument and mapping strategies for expressive performance. We apply notions of embodiment and affordance to characterize gestural instruments. We note that the democratization of sensor technology in consumer devices has extended the cultural contexts for interaction. We revisit questions of mapping drawing upon the theory of affordances to consider mapping and instrument together. This is applied to recent work by the ,
,
author and his collaborators in the development of instruments based on mobile devices designed for specific performance situations. },
  Keywords                 = {Musical affordance, NIME, mapping, instrument definition, mobile, multimodal interaction.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_088.pdf}
}

@InProceedings{Taylor2010,
  Title                    = {humanaquarium : A Participatory Performance System},
  Author                   = {Taylor, Robyn and Schofield, Guy and Shearer, John and Boulanger, Pierre and Wallace, Jayne and Olivier, Patrick},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {88--93},

  Abstract                 = {humanaquarium is a self-contained, transportable performance environment that is used to stage technology-mediated interactive performances in public spaces. Drawing upon the creative practices of busking and street performance, humanaquarium incorporates live musicians, real-time audiovisual content generation, and frustrated total internal reflection (FTIR) technology to facilitate participatory interaction by members of the public. },
  Keywords                 = {busking,collaborative interface,creative practice,experience centered design,frustrated total internal reflection,ftir,multi-touch screen,multimedia,nime10,participatory performance},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_440.pdf}
}

@InProceedings{Taylor2010a,
  Title                    = {FerroSynth : A Ferromagnetic Music Interface},
  Author                   = {Taylor, Stuart and Hook, Jonathan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {463--466},

  Abstract                 = {We present a novel user interface device based around ferromagnetic sensing. The physical form of the interface can easily be reconfigured by simply adding and removing a variety of ferromagnetic objects to the device's sensing surface. This allows the user to change the physical form of the interface resulting in a variety of different interaction modes. When used in a musical context, the performer can leverage the physical reconfiguration of the device to affect the method of playing and ultimately the sound produced. We describe the implementation of the sensing system, along with a range of mapping techniques used to transform the sensor data into musical output, including both the direct synthesis of sound and also the generation of MIDI data for use with Ableton Live. We conclude with a discussion of future directions for the device. },
  Keywords                 = {Ferromagnetic sensing, ferrofluid, reconfigurable user interface, wave terrain synthesis, MIDI controller.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_463.pdf}
}

@InProceedings{Torre2010,
  Title                    = {POLLEN A Multimedia Interactive Network Installation},
  Author                   = {Torre, Giuseppe and O'Leary, Mark and Tuohy, Brian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {375--376},

  Abstract                 = {This paper describes the development of an interactive 3Daudio/visual and network installation entitled POLLEN.Specifically designed for large computer Laboratories, theartwork explores the regeneration of those spaces throughthe creation of a fully immersive multimedia art experience.The paper describes the technical, aesthetic and educational development of the piece.},
  Keywords                 = {Interactive, Installation, Network, 3D Physics Emulator, Educational Tools, Public Spaces, Computer Labs, Sound Design, Site-Specific Art},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_375.pdf}
}

@InProceedings{Torresen2010,
  Title                    = {Wireless Sensor Data Collection based on {ZigBee} Communication},
  Author                   = {Torresen, Jim and Renton, Eirik and Jensenius, Alexander R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {368--371},

  Abstract                 = {This paper presents a comparison of different configurationsof a wireless sensor system for capturing human motion.The systems consist of sensor elements which wirelesslytransfers motion data to a receiver element. The sensorelements consist of a microcontroller, accelerometer(s) anda radio transceiver. The receiver element consists of a radioreceiver connected through a microcontroller to a computerfor real time sound synthesis. The wireless transmission between the sensor elements and the receiver element is basedon the low rate IEEE 802.15.4/ZigBee standard.A configuration with several accelerometers connected bywire to a wireless sensor element is compared to using multiple wireless sensor elements with only one accelerometer ineach. The study shows that it would be feasable to connect5-6 accelerometers in the given setups.Sensor data processing can be done in either the receiverelement or in the sensor element. For various reasons it canbe reasonable to implement some sensor data processing inthe sensor element. The paper also looks at how much timethat typically would be needed for a simple pre-processingtask.},
  Keywords                 = {wireless communication, ZigBee, microcontroller},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_368.pdf}
}

@InProceedings{Tremblay2010,
  Title                    = {Surfing the Waves : Live Audio Mosaicing of an Electric Bass Performance as a Corpus Browsing Interface},
  Author                   = {Tremblay, Pierre Alexandre and Schwarz, Diemo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {447--450},

  Keywords                 = {1,audio mosaic,context - the sandboxes,corpus-based concatenative synthesis,haptic interface,laptop improvisation,multi-dimensional mapping,nime10},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_447.pdf}
}

@InProceedings{Tsai2010,
  Title                    = {An Interactive Responsive Skin for Music},
  Author                   = {Tsai, Chih-Chieh and Liu, Cha-Lin and Chang, Teng-Wen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {399--402},

  Abstract                 = {With the decreasing audience of classical music performance, this research aims to develop a performance-enhancement system, called AIDA, to help classical performers better communicating with their audiences. With three procedures Input-Processing-Output, AIDA system can sense and analyze the body information of performers and further reflect it onto the responsive skin. Thus abstract and intangible emotional expressions of performers are transformed into tangible and concrete visual elements, which clearly facilitating the audiences' threshold for music appreciation. },
  Keywords                 = {Interactive Performance, Ambient Environment, Responsive Skin, Music performance.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_399.pdf}
}

@InProceedings{Umetani2010,
  Title                    = {Designing Custom-made Metallophone with Concurrent Eigenanalysis},
  Author                   = {Umetani, Nobuyuki and Mitani, Jun and Igarashi, Takeo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {26--30},

  Abstract                 = {We introduce an interactive interface for the custom designof metallophones. The shape of each plate must be determined in the design process so that the metallophone willproduce the proper tone when struck with a mallet. Unfortunately, the relationship between plate shape and tone iscomplex, which makes it difficult to design plates with arbitrary shapes. Our system addresses this problem by runninga concurrent numerical eigenanalysis during interactive geometry editing. It continuously presents a predicted tone tothe user with both visual and audio feedback, thus makingit possible to design a plate with any desired shape and tone.We developed this system to demonstrate the effectivenessof integrating real-time finite element method analysis intogeometric editing to facilitate the design of custom-mademusical instruments. An informal study demonstrated theability of technically unsophisticated user to apply the system to complex metallophone design.},
  Keywords                 = {Modeling - Modeling Interfaces, Modeling - Geometric Mod- eling, Modeling - CAD, Methods and Applications - Edu- cation, Real-time FEM},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_026.pdf}
}

@InProceedings{Vallis2010,
  Title                    = {A Shift Towards Iterative and Open-Source Design for Musical Interfaces},
  Author                   = {Vallis, Owen and Hochenbaum, Jordan and Kapur, Ajay},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {1--6},

  Abstract                 = {The aim of this paper is to define the process of iterative interface design as it pertains to musical performance. Embodying this design approach, the Monome OSC/MIDI USB controller represents a minimalist, open-source hardware device. The open-source nature of the device has allowed for a small group of Monome users to modify the hardware, firmware, and software associated with the interface. These user driven modifications have allowed the re-imagining of the interface for new and novel purposes, beyond even that of the device's original intentions. With development being driven by a community of users, a device can become several related but unique generations of musical controllers, each one focused on a specific set of needs. },
  Keywords                 = {Iterative Design, Monome, Arduinome, Arduino.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_001.pdf}
}

@InProceedings{Woldecke2010,
  Title                    = {ANTracks 2.0 - Generative Music on Multiple Multitouch Devices Categories and Subject Descriptors},
  Author                   = {W\"{o}ldecke, Bj\"{o}rn and Geiger, Christian and Reckter, Holger and Schulz, Florian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {348--351},

  Keywords                 = {Generative music, mobile interfaces, multitouch interaction},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_348.pdf}
}

@InProceedings{Whalley2010,
  Title                    = {Generative Improv . \& Interactive Music Project (GIIMP)},
  Author                   = {Whalley, Ian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {255--258},

  Abstract                 = {GIIMP addresses the criticism that in many interactive music systems the machine simply reacts. Interaction is addressed by extending Winkler's [18] model toward adapting Paine's [10] conversational model of interaction. Realized using commercial tools, GIIMP implements a machine/human generative improvisation system using human gesture input, machine gesture capture, and a gesture mutation module in conjunction with a flocking patch, mapped through microtonal/spectral techniques to sound. The intention is to meld some established and current practices, and combine aspects of symbolic and sub-symbolic approaches, toward musical outcomes. },
  Keywords                 = {Interaction, gesture, genetic algorithm, flocking, improvisation.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_255.pdf}
}

@InProceedings{Wyse2010,
  Title                    = {Instrumentalizing Synthesis Models},
  Author                   = {Wyse, Lonce and Duy, Nguyen D.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {140--143},

  Abstract                 = {An important part of building interactive sound models is designing the interface and control strategy. The multidimensional structure of the gestures natural for a musical or physical interface may have little obvious relationship to the parameters that a sound synthesis algorithm exposes for control. A common situation arises when there is a nonlinear synthesis technique for which a traditional instrumental interface with quasi-independent control of pitch and expression is desired. This paper presents a semi-automatic meta-modeling tool called the Instrumentalizer for embedding arbitrary synthesis algorithms in a control structure that exposes traditional instrument controls for pitch and expression. },
  Keywords                 = {Musical interface, parameter mapping, expressive control.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_140.pdf}
}

@InProceedings{Yamaguchi2010,
  Title                    = {TwinkleBall : A Wireless Musical Interface for Embodied Sound Media},
  Author                   = {Yamaguchi, Tomoyuki and Kobayashi, Tsukasa and Ariga, Anna and Hashimoto, Shuji},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {116--119},

  Abstract                 = {In this paper, we introduce a wireless musical interface driven by grasping forces and human motion. The sounds generated by the traditional digital musical instruments are dependent on the physical shape of the musical instruments. The freedom of the musical performance is restricted by its structure. Therefore, the sounds cannot be generated with the body expression like the dance. We developed a ball-shaped interface, TwinkleBall, to achieve the free-style performance. A photo sensor is embedded in the translucent rubber ball to detect the grasping force of the performer. The grasping force is translated into the luminance intensity for processing. Moreover, an accelerometer is also embedded in the interface for motion sensing. By using these sensors, a performer can control the note and volume by varying grasping force and motion respectively. The features of the proposed interface are ball-shaped, wireless, and handheld size. As a result, the proposed interface is able to generate the sound from the body expression such as dance. },
  Keywords                 = {Musical Interface, Embodied Sound Media, Dance Performance.},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_116.pdf}
}

@InProceedings{Yerkes2010,
  Title                    = {Disky : a DIY Rotational Interface with Inherent Dynamics},
  Author                   = {Yerkes, Karl and Shear, Greg and Wright, Matthew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {150--155},

  Keywords                 = {turntable, dial, encoder, re-purposed, hard drive, scratch- ing, inherent dynamics, DIY},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_150.pdf}
}

@InProceedings{Zappi2010,
  Title                    = {OSC Virtual Controller},
  Author                   = {Zappi, Victor and Brogni, Andrea and Caldwell, Darwin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2010},

  Address                  = {Sydney, Australia},
  Pages                    = {297--302},

  Abstract                 = {The number of artists who express themselves through music in an unconventional way is constantly growing. Thistrend strongly depends on the high diffusion of laptops,which proved to be powerful and flexible musical devices.However laptops still lack in flexible interface, specificallydesigned for music creation in live and studio performances.To resolve this issue many controllers have been developed,taking into account not only the performer's needs andhabits during music creation, but also the audience desire tovisually understand how performer's gestures are linked tothe way music is made. According to the common need ofadaptable visual interface to manipulate music, in this paper we present a custom tridimensional controller, based onOpen Sound Control protocol and completely designed towork inside Virtual Reality: simple geometrical shapes canbe created to directly control loop triggering and parametermodification, just using free hand interaction.},
  Keywords                 = {Glove device, Music controller, Virtual Reality, OSC, con- trol mapping},
  Url                      = {http://www.nime.org/proceedings/2010/nime2010_297.pdf}
}

