% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Allison:2013,
  Title                    = {NEXUS: Collaborative Performance for the Masses, Handling Instrument Interface Distribution through the Web},
  Author                   = {Jesse Allison and Yemin Oh and Benjamin Taylor},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {1--6},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Distributed performance systems present many challenges to the artist inmanaging performance information, distribution and coordination of interface tomany users, and cross platform support to provide a reasonable level ofinteraction to the widest possible user base.Now that many features of HTML 5 are implemented, powerful browser basedinterfaces can be utilized for distribution across a variety of static andmobile devices. The author proposes leveraging the power of a web applicationto handle distribution of user interfaces and passing interactions via OSC toand from realtime audio/video processing software. Interfaces developed in thisfashion can reach potential performers by distributing a unique user interfaceto any device with a browser anywhere in the world.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {NIME, distributed performance systems, Ruby on Rails, collaborative performance, distributed instruments, distributed interface, HTML5, browser based interface},
  Url                      = {http://nime.org/proceedings/2013/nime2013_287.pdf}
}

@InProceedings{Altavilla:2013,
  Title                    = {Towards Gestural Sonic Affordances},
  Author                   = {Alessandro Altavilla and Baptiste Caramiaux and Atau Tanaka},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {61--64},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present a study that explores the affordance evoked by sound andsound-gesture mappings. In order to do this, we make use of a sensor systemwith minimal form factor in a user study that minimizes cultural associationThe present study focuses on understanding how participants describe sounds andgestures produced while playing designed sonic interaction mappings. Thisapproach seeks to move from object-centric affordance towards investigatingembodied gestural sonic affordances.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Gestural embodiment of sound, Affordances, Mapping},
  Url                      = {http://nime.org/proceedings/2013/nime2013_145.pdf}
}

@InProceedings{Andersson:2013,
  Title                    = {Designing Empowering Vocal and Tangible Interaction},
  Author                   = {Anders-Petter Andersson and Birgitta Cappelen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {406--412},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Our voice and body are important parts of our self-experience, andcommunication and relational possibilities. They gradually become moreimportant for Interaction Design, due to increased development of tangibleinteraction and mobile communication. In this paper we present and discuss ourwork with voice and tangible interaction in our ongoing research project XXXXX.The goal is to improve health for families, adults and children withdisabilities through use of collaborative, musical, tangible media. We build onuse of voice in Music Therapy and on a humanistic health approach. Ourchallenge is to design vocal and tangible interactive media that through usereduce isolation and passivity and increase empowerment for the users. We usesound recognition, generative sound synthesis, vibrations and cross-mediatechniques, to create rhythms, melodies and harmonic chords to stimulatebody-voice connections, positive emotions and structures for actions.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Vocal Interaction, Tangible Interaction, Music & Health, Voice, Empowerment, Music Therapy, Resource-Oriented},
  Url                      = {http://nime.org/proceedings/2013/nime2013_210.pdf}
}

@InProceedings{Astrinaki:2013,
  Title                    = {MAGE 2.0: New Features and its Application in the Development of a Talking Guitar},
  Author                   = {Maria Astrinaki and Nicolas d'Alessandro and Lo{\"\i}c Reboursi{\`e}re and Alexis Moinet and Thierry Dutoit},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {547--550},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes the recent progress in our approach to generateperformative and controllable speech. The goal of the performative HMM-basedspeech and singing syn- thesis library, called Mage, is to have the ability togenerate natural sounding speech with arbitrary speaker's voicecharacteristics, speaking styles and expressions and at the same time to haveaccurate reactive user control over all the available production levels. Mageallows to arbitrarily change between voices, control speaking style or vocalidentity, manipulate voice characteristics or alter the targeted contexton-the-fly and also maintain the naturalness and intelligibility of the output.To achieve these controls, it was essential to redesign and improve the initiallibrary. This paper focuses on the improvements of the architectural design,the additional user controls and provides an overview of a prototype, where aguitar is used to reactively control the generation of a synthetic voice invarious levels.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {speech synthesis, augmented guitar, hexaphonic guitar},
  Url                      = {http://nime.org/proceedings/2013/nime2013_214.pdf}
}

@InProceedings{Baldan:2013,
  Title                    = {Sonic Tennis: a rhythmic interaction game for mobile devices},
  Author                   = {Stefano Baldan and Amalia De G{\"o}tzen and Stefania Serafin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {200--201},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents an audio-based tennis simulation game for mobile devices, which uses motion input and non-verbal audio feedback as exclusive means of interaction. Players have to listen carefully to the provided auditory clues, like racquet hits and ball bounces, rhythmically synchronizing their movements in order to keep the ball into play. The device can be swung freely and act as a full-fledged motionbased controller, as the game does not rely at all on visual feedback and the device display can thus be ignored. The game aims to be entertaining but also effective for educational purposes, such as ear training or improvement of the sense of timing, and enjoyable both by visually-impaired and sighted users.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Audio game, mobile devices, sonic interaction design, rhythmic interaction, motion-based},
  Url                      = {http://nime.org/proceedings/2013/nime2013_288.pdf}
}

@InProceedings{Batula:2013,
  Title                    = {Using Audio and Haptic Feedback to Improve Pitched Percussive Instrument Performance in Humanoids},
  Author                   = {Alyssa Batula and Manu Colacot and David Grunberg and Youngmoo Kim},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {295--300},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present a system which allows an adult-sized humanoid to determine whetheror not it is correctly playing a pitched percussive instrument to produce adesired sound. As hu- man musicians utilize sensory feedback to determine ifthey are successfully using their instruments to generate certain pitches,robot performers should be capable of the same feat. We present a noteclassification algorithm that uses auditory and haptic feedback to decide if anote was well- or poorly-struck. This system is demonstrated using Hubo, anadult-sized humanoid, which has been enabled to actu- ate pitched pipes usingmallets. We show that, with this system, Hubo is able to determine whether ornot a note was played correctly.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Musical robots, humanoids, auditory feedback, haptic feedback},
  Url                      = {http://nime.org/proceedings/2013/nime2013_235.pdf}
}

@InProceedings{Ben-Asher:2013,
  Title                    = {Toward an Emotionally Intelligent Piano: Real-Time Emotion Detection and Performer Feedback via Kinesthetic Sensing in Piano Performance},
  Author                   = {Matan Ben-Asher and Colby Leider},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {21--24},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {A system is presented for detecting common gestures, musical intentions andemotions of pianists in real-time using only kinesthetic data retrieved bywireless motion sensors. The algorithm can detect common Western musicalstructures such as chords, arpeggios, scales, and trills as well as musicallyintended emotions: cheerful, mournful, vigorous, dreamy, lyrical, and humorouscompletely and solely based on low-sample-rate motion sensor data. Thealgorithm can be trained per performer in real-time or can work based onprevious training sets. The system maps the emotions to a color set andpresents them as a flowing emotional spectrum on the background of a pianoroll. This acts as a feedback mechanism for emotional expression as well as aninteractive display of the music. The system was trained and tested on a numberof pianists and it classified structures and emotions with promising results ofup to 92\% accuracy.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Motion Sensors, IMUs, Expressive Piano Performance, Machine Learning, Computer Music, Music and Emotion},
  Url                      = {http://nime.org/proceedings/2013/nime2013_48.pdf}
}

@InProceedings{Berdahl:2013,
  Title                    = {Embedded Networking and Hardware-Accelerated Graphics with Satellite CCRMA},
  Author                   = {Edgar Berdahl and Spencer Salazar and Myles Borins},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {325--330},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Satellite CCRMA is a platform for making embedded musical instruments andembedded installations. The project aims to help prototypes live longer byproviding a complete prototyping platform in a single, small, and stand-aloneembedded form factor. A set of scripts makes it easier for artists andbeginning technical students to access powerful features, while advanced usersenjoy the flexibility of the open-source software and open-source hardwareplatform.This paper focuses primarily on networking capabilities of Satellite CCRMA andnew software for enabling interactive control of the hardware-acceleratedgraphical output. In addition, some results are presented from robustness testsalongside specific advice and software support for increasing the lifespan ofthe flash memory.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Satellite CCRMA, embedded musical instruments, embedded installations, Node.js, Interface.js, hardware-accelerated graphics, OpenGLES, SimpleGraphicsOSC, union file system, write endurance},
  Url                      = {http://nime.org/proceedings/2013/nime2013_277.pdf}
}

@InProceedings{Berthaut:2013,
  Title                    = {Rouages: Revealing the Mechanisms of Digital Musical Instruments to the Audience},
  Author                   = {Florent Berthaut and Mark T. Marshall and Sriram Subramanian and Martin Hachet},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {164--169},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Digital musical instruments bring new possibilities for musical performance.They are also more complex for the audience to understand, due to the diversityof their components and the magical aspect of the musicians' actions whencompared to acoustic instruments. This complexity results in a loss of livenessand possibly a poor experience for the audience. Our approach, called Rouages,is based on a mixed-reality display system and a 3D visualization application.It reveals the mechanisms of digital musical instruments by amplifyingmusicians' gestures with virtual extensions of the sensors, by representingthe sound components with 3D shapes and specific behaviors and by showing theimpact ofmusicians gestures on these components. We believe that Rouages opens up newperspectives to help instrument makers and musicians improve audienceexperience with their digital musical instruments.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {rouages, digital musical instruments, mappings, 3D interface, mixed-reality,},
  Url                      = {http://nime.org/proceedings/2013/nime2013_51.pdf}
}

@InProceedings{Bisig:2013,
  Title                    = {Coral - a Physical and Haptic Extension of a Swarm Simulation},
  Author                   = {Daniel Bisig and S{\'e}bastien Schiesser},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {385--388},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a proof of concept implementation of an interface entitledCoral. The interface serves as a physical and haptic extension of a simulatedcomplex system, which will be employed as an intermediate mechanism for thecreation of generative music and imagery. The paper discusses the motivationand conceptual context that underly the implementation, describes its technicalrealisation and presents some first interaction experiments. The paper focuseson the following two aspects: the interrelation between the physical andvirtual behaviours and properties of the interface and simulation, and thecapability of the interface to enable an intuitive and tangible exploration ofthis hybrid dynamical system.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {haptic interface, swarm simulation, generative art},
  Url                      = {http://nime.org/proceedings/2013/nime2013_126.pdf}
}

@InProceedings{Bortz:2013,
  Title                    = {Lantern Field: Exploring Participatory Design of a Communal, Spatially Responsive Installation},
  Author                   = {Brennon Bortz and Aki Ishida and Ivica Ico Bukvic and R. Benjamin Knapp},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {73--78},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Mountains and Valleys (an anonymous name for confidentiality) is a communal,site-specific installation that takes shape as a spatially-responsiveaudio-visual field. The public participates in the creation of theinstallation, resulting in shared ownership of the work between both theartists and participants. Furthermore, the installation takes new shape in eachrealization, both to incorporate the constraints and affordances of eachspecific site, as well as to address the lessons learned from the previousiteration. This paper describes the development and execution of Mountains andValleys over its most recent version, with an eye toward the next iteration ata prestigious art museum during a national festival in Washington, D.C.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Participatory creation, communal interaction, fields, interactive installation, Japanese lanterns},
  Url                      = {http://nime.org/proceedings/2013/nime2013_192.pdf}
}

@InProceedings{Bragg:2013,
  Title                    = {Synchronous Data Flow Modeling for DMIs},
  Author                   = {Danielle Bragg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {237--242},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a graph-theoretic model that supports the design andanalysis of data flow within digital musical instruments (DMIs). The state ofthe art in DMI design fails to provide any standards for the scheduling ofcomputations within a DMI's data flow. It does not provide a theoreticalframework within which we can analyze different scheduling protocols and theirimpact on the DMI's performance. Indeed, the mapping between the DMI's sensoryinputs and sonic outputs is classically treated as a black box. DMI designersand builders are forced to design and schedule the flow of data through thisblack box on their own. Improper design of the data flow can produceundesirable results, ranging from overflowing buffers that cause system crashesto misaligned sensory data that result in strange or disordered sonic events.In this paper, we attempt to remedy this problem by providing a framework forthe design and analysis of the DMI data flow. We also provide a schedulingalgorithm built upon that framework that guarantees desirable properties forthe resulting DMI.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {DMI design, data flow, mapping function},
  Url                      = {http://nime.org/proceedings/2013/nime2013_139.pdf}
}

@InProceedings{Burlet:2013,
  Title                    = {Stompboxes: Kicking the Habit},
  Author                   = {Gregory Burlet and Ichiro Fujinaga},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {41--44},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Sensor-based gesture recognition is investigated as a possible solution to theproblem of managing an overwhelming number of audio effects in live guitarperformances. A realtime gesture recognition system, which automaticallytoggles digital audio effects according to gestural information captured by anaccelerometer attached to the body of a guitar, is presented. To supplement theseveral predefined gestures provided by the recognition system, personalizedgestures may be trained by the user. Upon successful recognition of a gesture,the corresponding audio effects are applied to the guitar signal and visualfeedback is provided to the user. An evaluation of the system yielded 86%accuracy for user-independent recognition and 99% accuracy for user-dependentrecognition, on average.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Augmented instrument, gesture recognition, accelerometer, pattern recognition, performance practice},
  Url                      = {http://nime.org/proceedings/2013/nime2013_109.pdf}
}

@InProceedings{Caramiaux:2013,
  Title                    = {Machine Learning of Musical Gestures},
  Author                   = {Baptiste Caramiaux and Atau Tanaka},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {513--518},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present an overview of machine learning (ML) techniques and theirapplication in interactive music and new digital instruments design. We firstgive to the non-specialist reader an introduction to two ML tasks,classification and regression, that are particularly relevant for gesturalinteraction. We then present a review of the literature in current NIMEresearch that uses ML in musical gesture analysis and gestural sound control.We describe the ways in which machine learning is useful for creatingexpressive musical interaction, and in turn why live music performance presentsa pertinent and challenging use case for machine learning.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Machine Learning, Data mining, Musical Expression, Musical Gestures, Analysis, Control, Gesture, Sound},
  Url                      = {http://nime.org/proceedings/2013/nime2013_84.pdf}
}

@InProceedings{Cerqueira:2013,
  Title                    = {SoundCraft: Transducing StarCraft 2},
  Author                   = {Mark Cerqueira and Spencer Salazar and Ge Wang},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {243--247},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {SoundCraft is a framework that enables real-time data gathering from aStarCraft 2 game to external software applications, allowing for musicalinterpretation of the game's internal structure and strategies in novel ways.While players battle each other for victory within the game world, a customStarCraft 2 map collects and writes out data about players' decision-making,performance, and current focus on the map. This data is parsed and transmittedover Open Sound Control (OSC) in real-time, becoming the source for thesoundscape that accompanies the player's game. Using SoundCraft, we havecomposed a musical work for two em StarCraft 2 players, entitled GG Music. Thispaper details the technical and aesthetic development of SoundCraft, includingdata collection and sonic mapping. Please see the attached video file for a performance of GG Music using theSoundCraft framework.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {interactive sonification, interactive game music, StarCraft 2},
  Url                      = {http://nime.org/proceedings/2013/nime2013_146.pdf}
}

@InProceedings{Choi:2013,
  Title                    = {WAAX: Web Audio {API} eXtension},
  Author                   = {Hongchan Choi and Jonathan Berger},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {499--502},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The advent of Web Audio API in 2011 marked a significant advance for web-basedmusic systems by enabling real-time sound synthesis on web browsers simply bywriting JavaScript code. While this powerful functionality has arrived there isa yet unaddressed need for an extension to the API to fully reveal itspotential. To meet this need, a JavaScript library dubbed WAAX was created tofacilitate music and audio programming based on Web Audio API bypassingunderlying tasks and augmenting useful features. In this paper, we describecommon issues in web audio programming, illustrate how WAAX can speed up thedevelopment, and discuss future developments.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Web Audio API, Chrome, JavaScript, web-based music system, collaborative music making, audience participation},
  Url                      = {http://nime.org/proceedings/2013/nime2013_119.pdf}
}

@InProceedings{Christopher:2013,
  Title                    = {Kontrol: Hand Gesture Recognition for Music and Dance Interaction},
  Author                   = {Kameron Christopher and Jingyin He and Raakhi Kapur and Ajay Kapur},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {267--270},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes Kontrol, a new hand interface that extends the intuitivecontrol of electronic music to traditional instrumentalist and dancers. Thegoal of the authors has been to provide users with a device that is capable ofdetecting the highly intricate and expressive gestures of the master performer,in order for that information to be interpreted and used for control ofelectronic music. This paper discusses related devices, the architecture ofKontrol, it's potential as a gesture recognition device, and severalperformance applications.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Hand controller, computational ethnomusicology, dance interface, conducting interface, Wekinator, wearable sensors},
  Url                      = {http://nime.org/proceedings/2013/nime2013_164.pdf}
}

@InProceedings{Dezfouli:2013,
  Title                    = {Notesaaz: a new controller and performance idiom},
  Author                   = {Erfan Abdi Dezfouli and Edwin van der Heide},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {115--117},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Notesaaz is both a new physical interface meant for musical performance and aproposal for a three-stage process where the controller is used to navigatewithin a graphical score that on its turn controls the sound generation. It canbe seen as a dynamic and understandable way of using dynamic mapping betweenthe sensor input and the sound generation. Furthermore by presenting thegraphical score to both the performer and the audience a new engagement of theaudience can be established.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {musical instrument, custom controller, gestural input, dynamic score},
  Url                      = {http://nime.org/proceedings/2013/nime2013_4.pdf}
}

@InProceedings{Diakopoulos:2013,
  Title                    = {Netpixl: Towards a New Paradigm for Networked Application Development},
  Author                   = {Dimitri Diakopoulos and Ajay Kapur},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {206--209},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Netpixl is a new micro-toolkit built to network devices within interactiveinstallations and environments. Using a familiar client-server model, Netpixlcentrally wraps an important aspect of ubiquitous computing: real-timemessaging. In the context of sound and music computing, the role of Netpixl isto fluidly integrate endpoints like OSC and MIDI within a larger multi-usersystem. This paper considers useful design principles that may be applied totoolkits like Netpixl while also emphasizing recent approaches to applicationdevelopment via HTML5 and Javascript, highlighting an evolution in networkedcreative computing.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {networking, ubiquitious computing, toolkits, html5},
  Url                      = {http://nime.org/proceedings/2013/nime2013_49.pdf}
}

@InProceedings{Dobda:2013,
  Title                    = {Applied and Proposed Installations with Silent Disco Headphones for Multi-Elemental Creative Expression},
  Author                   = {Russell Eric Dobda},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {69--72},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Breaking musical and creative expression into elements, layers, and formulas, we explore how live listeners create unique sonic experiences from a palette of these elements and their interactions. Bringing us to present-day creative applications, a social and historical overview of silent disco is presented. The advantages of this active listening interface are outlined by the author's expressions requiring discrete elements, such as binaural beats, 3D audio effects, and multiple live music acts in the same space. Events and prototypes as well as hardware and software proposals for live multi-listener manipulation of multielemental sound and music are presented. Examples in audio production, sound healing, music composition, tempo phasing, and spatial audio illustrate the applications.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {wireless headphones, music production, silent disco, headphone concert, binaural beats, multi-track audio, active music listening, sound healing, mobile clubbing, smart-phone apps},
  Url                      = {http://nime.org/proceedings/2013/nime2013_161.pdf}
}

@InProceedings{Donnarumma:2013,
  Title                    = {Muscular Interactions. Combining {EMG} and MMG sensing for musical practice},
  Author                   = {Marco Donnarumma and Baptiste Caramiaux and Atau Tanaka},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {128--131},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present the first combined use of the electromyogram (EMG) andmechanomyogram (MMG), two biosignals that result from muscular activity, forinteractive music applications. We exploit differences between these twosignals, as reported in the biomedical literature, to create bi-modalsonification and sound synthesis mappings that allow performers to distinguishthe two components in a single complex arm gesture. We study non-expertplayers' ability to articulate the different modalities. Results show thatpurposely designed gestures and mapping techniques enable novices to rapidlylearn to independently control the two biosignals.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {NIME, sensorimotor system, EMG, MMG, biosignal, multimodal, mapping},
  Url                      = {http://nime.org/proceedings/2013/nime2013_90.pdf}
}

@InProceedings{El-Shimy:2013,
  Title                    = {Reactive Environment for Network Music Performance},
  Author                   = {Dalia El-Shimy and Jeremy R. Cooperstock},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {158--163},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {For a number of years, musicians in different locations have been able toperform with one another over a network as though present on the same stage.However, rather than attempt to re-create an environment for Network MusicPerformance (NMP) that mimics co-present performance as closely as possible, wepropose focusing on providing musicians with additional controls that can helpincrease the level of interaction between them. To this end, we have developeda reactive environment for distributed performance that provides participantsdynamic, real-time control over several aspects of their performance, enablingthem to change volume levels and experience exaggerated stereo panning. Inaddition, our reactive environment reinforces a feeling of a ``shared space''between musicians. It differs most notably from standard ventures into thedesign of novel musical interfaces and installations in its reliance onuser-centric methodologies borrowed from the field of Human-ComputerInteraction (HCI). Not only does this research enable us to closely examine thecommunicative aspects of performance, it also allows us to explore newinterpretations of the network as a performance space. This paper describes themotivation and background behind our project, the work that has been undertakentowards its realization and the future steps that have yet to be explored.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Url                      = {http://nime.org/proceedings/2013/nime2013_66.pdf}
}

@InProceedings{Everett:2013,
  Title                    = {Sonifying Chemical Evolution},
  Author                   = {Steve Everett},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {277--278},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This presentation-demonstration discusses the creation of FIRST LIFE, a75-minute mixed media performance for string quartet, live audio processing,live motion capture video, and audience participation utilizing stochasticmodels of chemical data provided by Martha Grover's Research Group at theSchool of Chemical and Biomolecular Engineering at Georgia Institute ofTechnology. Each section of this work is constructed from contingent outcomesdrawn from biochemical research exploring possible early Earth formations oforganic compounds. Audio-video excerpts of the composition will be played during the presentation.Max patches for sonification and for generating stochastic processes will bedemonstrated as well.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Data-driven composition, sonification, live electronics-video},
  Url                      = {http://nime.org/proceedings/2013/nime2013_198.pdf}
}

@InProceedings{Everman:2013,
  Title                    = {Toward DMI Evaluation Using Crowd-Sourced Tagging Techniques},
  Author                   = {Michael Everman and Colby Leider},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {437--440},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Few formal methods exist for evaluating digital musical instruments (DMIs) .This paper proposes a novel method of DMI evaluation using crowd-sourcedtagging. One of the challenges in devising such methods is that the evaluationof a musical instrument is an inherently qualitative task. While previouslyproposed methods have focused on quantitative methods and largely ignored thequalitative aspects of the task, tagging is well-suited to this and is alreadyused to classify things such as websites and musical genres. These, like DMIs,do not lend themselves to simple categorization or parameterization. Using the social tagging method, participating individuals assign descriptivelabels, or tags, to a DMI. A DMI can then be evaluated by analyzing the tagsassociated with it. Metrics can be generated from the tags assigned to theinstrument, and comparisons made to other instruments. This can give thedesigner valuable insight into the where the strengths of the design lie andwhere improvements may be needed. A prototype system for testing the method is proposed in the paper and iscurrently being implemented as part of an ongoing DMI evaluation project. It isexpected that results from the prototype will be available to report by thetime of the conference in May.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Evaluation, tagging, digital musical instrument},
  Url                      = {http://nime.org/proceedings/2013/nime2013_251.pdf}
}

@InProceedings{Fan:2013,
  Title                    = {Air Violin: A Body-centric Style Musical Instrument},
  Author                   = {Xin Fan and Georg Essl},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {122--123},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We show how body-centric sensing can be integrated in musical interface toenable more flexible gestural control. We present a barehanded body-centricinteraction paradigm where users are able to interact in a spontaneous waythroughperforming gestures. The paradigm employs a wearable camera and see-throughdisplay to enable flexible interaction in the 3D space. We designed andimplemented a prototype called Air Violin, a virtual musical instrument usingdepth camera, to demonstrate the proposed interaction paradigm. We describedthe design and implementation details.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {NIME, musical instrument, interaction, gesture, Kinect},
  Url                      = {http://nime.org/proceedings/2013/nime2013_149.pdf}
}

@InProceedings{Fan:2013a,
  Title                    = {BioSync: An Informed Participatory Interface for Audience Dynamics and Audiovisual Content Co-creation using Mobile PPG and {EEG}},
  Author                   = {Yuan-Yi Fan and Myles Sciotto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {248--251},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The BioSync interface presented in this paper merges the heart-rate basedparadigm with the brain-wave based paradigm into one mobile unit which isscalable for large audience real-time applications. The goal of BioSync is toprovide a hybrid interface, which uses audience biometric responses foraudience participation techniques. To provide an affordable and scalablesolution, BioSync collects the user's heart rate via mobile phone pulseoximetry and the EEG data via Bluetooth communication with the off-the-shelfMindWave Mobile hardware. Various interfaces have been designed and implementedin the development of audience participation techniques and systems. In thedesign and concept of BioSync, we first summarize recent interface research foraudience participation within the NIME-related context, followed by the outlineof the BioSync methodology and interface design. We then present a techniquefor dynamic tempo control based on the audience biometric responses and anearly prototype of a mobile dual-channel pulse oximetry and EEG bi-directionalinterface for iOS device (BioSync). Finally, we present discussions and ideasfor future applications, as well as plans for a series of experiments, whichinvestigate if temporal parameters of an audience's physiological metricsencourage crowd synchronization during a live event or performance, acharacteristic, which we see as having great potential in the creation offuture live musical and audiovisual performance applications.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Mobile, Biometrics, Synchronous Interaction, Social, Audience, Experience},
  Url                      = {http://nime.org/proceedings/2013/nime2013_152.pdf}
}

@InProceedings{Fasciani:2013,
  Title                    = {A Self-Organizing Gesture Map for a Voice-Controlled Instrument Interface},
  Author                   = {Stefano Fasciani and Lonce Wyse},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {507--512},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {ABSTRACT: Mapping gestures to digital musical instrument parameters is nottrivial when the dimensionality of the sensor-captured data is high and themodel relating the gesture to sensor data is unknown. In these cases, afront-end processing system for extracting gestural information embedded in thesensor data is essential. In this paper we propose an unsupervised offlinemethod that learns how to reduce and map the gestural data to a genericinstrument parameter control space. We make an unconventional use of theSelf-Organizing Maps to obtain only a geometrical transformation of thegestural data, while dimensionality reduction is handled separately. Weintroduce a novel training procedure to overcome two main Self- Organizing Mapslimitations which otherwise corrupt the interface usability. As evaluation, weapply this method to our existing Voice-Controlled Interface for musicalinstruments, obtaining sensible usability improvements.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Self-Organizing Maps, Gestural Controller, Multi Dimensional Control, Unsupervised Gesture Mapping, Voice Control},
  Url                      = {http://nime.org/proceedings/2013/nime2013_50.pdf}
}

@InProceedings{Ferguson:2013,
  Title                    = {A corpus-based method for controlling guitar feedback},
  Author                   = {Sam Ferguson and Aengus Martin and Andrew Johnston},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {541--546},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Feedback created by guitars and amplifiers is difficult to use in musicalsettings -- parameters such as pitch and loudness are hard to specify preciselyby fretting a string or by holding the guitar near an amplifier. This researchinvestigates methods for controlling the level and pitch of the feedbackproduced by a guitar and amplifier, which are based on incorporatingcorpus-based control into the system. Two parameters are used to define thecontrol parameter space -- a simple automatic gain control system to controlthe output level, and a band-pass filter frequency for controlling the pitch ofthe feedback. This control parameter space is mapped to a corpus of soundscreated by these parameters and recorded, and these sounds are analysed usingsoftware created for concatenative synthesis. Following this process, thedescriptors taken from the analysis can be used to select control parametersfrom the feedback system.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Url                      = {http://nime.org/proceedings/2013/nime2013_200.pdf}
}

@InProceedings{Feugere:2013,
  Title                    = {Digitartic: bi-manual gestural control of articulation in performative singing synthesis},
  Author                   = {Lionel Feug{\`e}re and Christophe d'Alessandro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {331--336},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Digitartic, a system for bi-manual gestural control of Vowel-Consonant-Vowelperformative singing synthesis is presented. This system is an extension of areal-time gesture-controlled vowel singing instrument developed in the Max/MSPlanguage. In addition to pitch, vowels and voice strength control, Digitarticis designed for gestural control of articulation parameters for a wide set onconsonant, including various places and manners of articulation. The phases ofarticulation between two phonemes are continuously controlled and can bedriven in real time without noticeable delay, at any stage of the syntheticphoneme production. Thus, as in natural singing, very accurate rhythmicpatterns are produced and adapted while playing with other musicians. Theinstrument features two (augmented) pen tablets for controlling voiceproduction: one is dealing with the glottal source and vowels, the second oneis dealing with consonant/vowel articulation. The results show very naturalconsonant and vowel synthesis. Virtual choral practice confirms theeffectiveness of Digitartic as an expressive musical instrument.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {singing voice synthesis, gestural control, syllabic synthesis, articulation, formants synthesis},
  Url                      = {http://nime.org/proceedings/2013/nime2013_143.pdf}
}

@InProceedings{Fohl:2013,
  Title                    = {A Gesture Control Interface for a Wave Field Synthesis System},
  Author                   = {Wolfgang Fohl and Malte Nogalski},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {341--346},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents the design and implementation of agesture control interface for a wave field synthesis system.The user's motion is tracked by a IR-camera-based trackingsystem. The developed connecting software processes thetracker data to modify the positions of the virtual soundsources of the wave field synthesis system. Due to the mod-ular design of the software, the triggered actions of the ges-tures may easily be modified. Three elementary gestureswere designed and implemented: Select / deselect, circularmovement and radial movement. The guidelines for gesturedesign and detection are presented, and the user experiencesare discussed.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Wave field synthesis, gesture control},
  Url                      = {http://nime.org/proceedings/2013/nime2013_106.pdf}
}

@InProceedings{Freed:2013a,
  Title                    = {``Old'' is the New ``New'': a Fingerboard Case Study in Recrudescence as a NIME Development Strategy},
  Author                   = {Adrian Freed and John MacCallum and Sam Mansfield},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {441--445},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper positively addresses the problem that most NIME devices are ephemeralasting long enough to signal academic and technical prowess but rarely longerthan a few musical performances. We offer a case study that shows thatlongevity of use depends on stabilizing the interface and innovating theimplementation to maintain the required performance of the controller for theplayer.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Fingerboard controller, Best practices, Recrudescence, Organology, Unobtainium},
  Url                      = {http://nime.org/proceedings/2013/nime2013_265.pdf}
}

@InProceedings{Freed:2013,
  Title                    = {Agile Interface Development using OSC Expressions and Process Migration},
  Author                   = {Adrian Freed and John MacCallum and David Wessel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {347--351},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We describe ``o.expr'' an expression language for dynamic, object- andagent-oriented computation of gesture signal processing workflows using OSCbundles. We illustrate the use of o.expr for a range of gesture processingtasks showing how stateless programming and homoiconicity simplify applicationsdevelopment and provide support for heterogeneous computational networks.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Gesture Signal Processing, Open Sound Control, Functional Programming, Homoiconicity, Process Migration.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_266.pdf}
}

@InProceedings{Fried:2013,
  Title                    = {Cross-modal Sound Mapping Using Deep Learning},
  Author                   = {Ohad Fried and Rebecca Fiebrink},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {531--534},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present a method for automatic feature extraction and cross-modal mappingusing deep learning. Our system uses stacked autoencoders to learn a layeredfeature representation of the data. Feature vectors from two (or more)different domains are mapped to each other, effectively creating a cross-modalmapping. Our system can either run fully unsupervised, or it can use high-levellabeling to fine-tune the mapping according a user's needs. We show severalapplications for our method, mapping sound to or from images or gestures. Weevaluate system performance both in standalone inference tasks and incross-modal mappings.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Deep learning, feature learning, mapping, gestural control},
  Url                      = {http://nime.org/proceedings/2013/nime2013_111.pdf}
}

@InProceedings{Fuhrmann:2013,
  Title                    = {Multi Sensor Tracking for Live Sound Transformation},
  Author                   = {Anton Fuhrmann and Johannes Kretz and Peter Burwik},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {358--362},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper demonstrates how to use multiple Kinect(TM) sensors to map aperformers motion to music. We merge skeleton data streams from multiplesensors to compensate for occlusions of the performer. The skeleton jointpositions drive the performance via open sound control data. We discuss how toregister the different sensors to each other and how to smoothly merge theresulting data streams and how to map position data in a general framework tothe live electronics applied to a chamber music ensemble.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {kinect, multi sensor, sensor fusion, open sound control, motion tracking, parameter mapping, live electronics},
  Url                      = {http://nime.org/proceedings/2013/nime2013_44.pdf}
}

@InProceedings{Gelineck:2013,
  Title                    = {Towards an Interface for Music Mixing based on Smart Tangibles and Multitouch},
  Author                   = {Steven Gelineck and Dan Overholt and Morten B{\"u}chert and Jesper Andersen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {180--185},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents the continuous work towards the development of an interfacefor music mixing targeted towards expert sound technicians and producers. Themixing interface uses a stage metaphor mapping scheme where audio channels arerepresented as digital widgets on a 2D surface. These can be controlled bymulti touch or by smart tangibles, which are tangible blocks with embeddedsensors. The smart tangibles developed for this interface are able to sense howthey are grasped by the user. The paper presents the design of the mixinginterface including the smart tangible as well as a preliminary user studyinvolving a hands-on focus group session where 5 different control technologiesare contrasted and discussed. Preliminary findings suggest that smart tangibleswere preferred, but that an optimal interface would include a combination oftouch, smart tangibles and an extra function control tangible for extending thefunctionality of the smart tangibles. Finally, the interface should incorporateboth an edit and mix mode - the latter displaying very limited visual feedbackin order to force users to focus their attention to listening instead of theinterface.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {music mixing, tangibles, smart objects, multi-touch, control surface, graspables, physical-digital interface, tangible user interface, wireless sensing, sketching in hardware},
  Url                      = {http://nime.org/proceedings/2013/nime2013_206.pdf}
}

@InProceedings{Greenlee:2013,
  Title                    = {Graphic Waveshaping},
  Author                   = {Shawn Greenlee},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {287--290},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In the design of recent systems, I have advanced techniques that positiongraphic synthesis methods in the context of solo, improvisational performance.Here, the primary interfaces for musical action are prepared works on paper,scanned by digital video cameras which in turn pass image data on to softwarefor analysis and interpretation as sound synthesis and signal processingprocedures. The focus of this paper is on one of these techniques, a process Idescribe as graphic waveshaping. A discussion of graphic waveshaping in basicform and as utilized in my performance work, (title omitted), is offered. Inthe latter case, the performer's objective is to guide the interpretation ofimages as sound, constantly tuning and retuning the conversion while selectingand scanning images from a large catalog. Due to the erratic nature of thesystem and the precondition that image to sound relationships are unfixed, theperformance situation is replete with the discovery of new sounds and thecircumstances that bring them into play. Graphic waveshaping may be understood as non-linear distortion synthesis withtime-varying transfer functions stemming from visual scan lines. As a form ofgraphic synthesis, visual images function as motivations for sound generation.There is a strategy applied for creating one out of the other. However, counterto compositionally oriented forms of graphic synthesis where one may assignimage characteristics to musical parameters such as pitches, durations,dynamics, etc., graphic waveshaping is foremost a processing technique, as itdistorts incoming signals according to graphically derived transfer functions.As such, it may also be understood as an audio effect; one that in myimplementations is particularly feedback dependent, oriented towards shapingthe erratic behavior of synthesis patches written in Max/MSP/Jitter. Used inthis manner, graphic waveshaping elicits an emergent system behaviorconditioned by visual features.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Graphic waveshaping, graphic synthesis, waveshaping synthesis, graphic sound, drawn sound},
  Url                      = {http://nime.org/proceedings/2013/nime2013_232.pdf}
}

@InProceedings{Grierson:2013,
  Title                    = {NoiseBear: A Malleable Wireless Controller Designed In Participation with Disabled Children},
  Author                   = {Mick Grierson and Chris Kiefer},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {413--416},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {NoiseBear is a wireless malleable controller designed for, and in participationwith, physically and cognitively disabled children. The aim of the project wasto produce a musical controller that was robust, and flexible enough to be usedin a wide range of interactive scenarios in participatory design workshops. NoiseBear demonstrates an open ended system for designing wireless malleablecontrollers in different shapes. It uses pressure sensitive material made fromconductive thread and polyester cushion stuffing, to give the feel of a softtoy. The sensor networks with other devices using the Bluetooth Low Energyprotocol, running on a BlueGiga BLE112 chip. This contains an embedded 8051processor which manages the sensor. NoiseBear has undergone an initialformative evaluation in a workshop session with four autistic children, andcontinues to evolve in series of participatory design workshops. The evaluationshowed that controller could be engaging for the children to use, andhighlighted some technical limitations of the design. Solutions to theselimitations are discussed, along with plans for future design iterations.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {malleable controllers, assistive technology, multiparametric mapping},
  Url                      = {http://nime.org/proceedings/2013/nime2013_227.pdf}
}

@InProceedings{Grosshauser:2013,
  Title                    = {Finger Position and Pressure Sensing Techniques for String and Keyboard Instruments},
  Author                   = {Tobias Grosshauser and Gerhard Tr{\"o}ster},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {479--484},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Several new technologies to capture motion, gesture and forces for musical instrument players' analyses have been developed in the last years. In research and for augmented instruments one parameter is underrepresented so far. It is finger position and pressure measurement, applied by the musician while playing the musical instrument. In this paper we show a flexible linear-potentiometer and forcesensitive-resistor (FSR) based solution for position, pressure and force sensing between the contact point of the fingers and the musical instrument. A flexible matrix printed circuit board (PCB) is fixed on a piano key. We further introduce linear potentiometer based left hand finger position sensing for string instruments, integrated into a violin and a guitar finger board. Several calibration and measurement scenarios are shown. The violin sensor was evaluated with 13 music students regarding playability and robustness of the system. Main focus was a the integration of the sensors into these two traditional musical instruments as unobtrusively as possible to keep natural haptic playing sensation. The musicians playing the violin in different performance situations stated good playability and no differences in the haptic sensation while playing. The piano sensor is rated, due to interviews after testing it in a conventional keyboard quite unobtrusive, too, but still evokes a different haptic sensation.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Sensor, Piano, Violin, Guitar, Position, Pressure, Keyboard},
  Url                      = {http://nime.org/proceedings/2013/nime2013_286.pdf}
}

@InProceedings{Hadjakos:2013,
  Title                    = {Motion and Synchronization Analysis of Musical Ensembles with the Kinect},
  Author                   = {Aristotelis Hadjakos and Tobias Grosshauser},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {106--110},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Music ensembles have to synchronize themselves with a very high precision inorder to achieve the desired musical results. For that purpose the musicians donot only rely on their auditory perception but also perceive and interpret themovements and gestures of their ensemble colleges. In this paper we present aKinect-based method to analyze ensemble play based on head tracking. We discussfirst experimental results with a violin duo performance.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Kinect, Ensemble, Synchronization, Strings, Functional Data Analysis, Cross-Correlogram},
  Url                      = {http://nime.org/proceedings/2013/nime2013_304.pdf}
}

@InProceedings{Hamano:2013,
  Title                    = {Generating an Integrated Musical Expression with a Brain--Computer Interface},
  Author                   = {Takayuki Hamano and Tomasz Rutkowski and Hiroko Terasawa and Kazuo Okanoya and Kiyoshi Furukawa},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {49--54},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Brain-computer interface (BCI), qualitative and quantitative information, classification, sonification},
  Url                      = {http://nime.org/proceedings/2013/nime2013_120.pdf}
}

@InProceedings{Hamilton:2013,
  Title                    = {Sonifying Game-Space Choreographies With UDKOSC},
  Author                   = {Rob Hamilton},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {446--449},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {With a nod towards digital puppetry and game-based film genres such asmachinima, recent additions to UDKOSC of- fer an Open Sound Control (OSC)control layer for external control over both third-person ''pawn'' entitiesand camera controllers in fully rendered game-space. Real-time OSC input,driven by algorithmic process or parsed from a human-readable timed scriptingsyntax allows users to shape choreographies of gesture, in this case actormotion and action, as well as an audiences view into the game-spaceenvironment. As UDKOSC outputs real-time coordinate and action data generatedby UDK pawns and players with OSC, individual as well as aggregate virtualactor gesture and motion can be leveraged as a driver for both creative andprocedural/adaptive gaming music and audio concerns.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {procedural music, procedural audio, interactive sonification, game music, Open Sound Control},
  Url                      = {http://nime.org/proceedings/2013/nime2013_268.pdf}
}

@InProceedings{Han:2013,
  Title                    = {A Musical Performance Evaluation System for Beginner Musician based on Real-time Score Following},
  Author                   = {Yoonchang Han and Sejun Kwon and Kibeom Lee and Kyogu Lee},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {120--121},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper proposes a musical performance feedback system based on real-time audio-score alignment for musical instrument education of beginner musicians. In the proposed system, we do not make use of symbolic data such as MIDI, but acquire a real-time audio input from on-board microphone of smartphone. Then, the system finds onset and pitch of the note from the signal, to align this information with the ground truth musical score. Real-time alignment allows the system to evaluate whether the user played the correct note or not, regardless of its timing, which enables user to play at their own speed, as playing same tempo with original musical score is problematic for beginners. As an output of evaluation, the system notifies the user about which part they are currently performing, and which note were played incorrectly.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Music performance analysis, Music education, Real-time score following},
  Url                      = {http://nime.org/proceedings/2013/nime2013_60.pdf}
}

@InProceedings{Han:2013a,
  Title                    = {Digiti Sonus: Advanced Interactive Fingerprint Sonification Using Visual Feature Analysis},
  Author                   = {Yoon Chung Han and Byeong-jun Han and Matthew Wright},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {136--141},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a framework that transforms fingerprint patterns intoaudio. We describe Digiti Sonus, an interactive installation performingfingerprint sonification and visualization, including novel techniques forrepresenting user-intended fingerprint expression as audio parameters. In orderto enable personalized sonification and broaden timbre of sound, theinstallation employs sound synthesis based on various visual feature analysissuch as minutiae extraction, area, angle, and push pressure of fingerprints.The sonification results are discussed and the diverse timbres of soundretrieved from different fingerprints are compared.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Fingerprint, Fingerprint sonification, interactive sonification, sound synthesis, biometric data},
  Url                      = {http://nime.org/proceedings/2013/nime2013_170.pdf}
}

@InProceedings{Hindle:2013,
  Title                    = {{SW}ARMED: Captive Portals, Mobile Devices, and Audience Participation in Multi-User Music Performance},
  Author                   = {Abram Hindle},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {174--179},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Audience participation in computer music has long been limited byresources such as sensor technology or the material goods necessary toshare such an instrument. A recent paradigm is to take advantageof the incredible popularity of the smart-phone, a pocket sizedcomputer, and other mobile devices, to provide the audience aninterface into a computer music instrument. In this paper we discuss amethod of sharing a computer music instrument's interface with anaudience to allow them to interact via their smartphone. We propose amethod that is relatively cross-platform and device-agnostic, yetstill allows for a rich user-interactive experience. By emulating acaptive-portal or hotspot we reduce the adoptability issues and configurationproblems facing performers and their audience. We share ourexperiences with this system, as well as an implementation of thesystem itself.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Wifi, Smartphone, Audience Interaction, Adoption, Captive Portal, Multi-User, Hotspot},
  Url                      = {http://nime.org/proceedings/2013/nime2013_62.pdf}
}

@InProceedings{Hochenbaum:2013,
  Title                    = {Toward The Future Practice Room: Empowering Musical Pedagogy through Hyperinstruments},
  Author                   = {Jordan Hochenbaum and Ajay Kapur},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {307--312},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Music education is a rich subject with many approaches and methodologies thathave developed over hundreds of years. More than ever, technology playsimportant roles at many levels of a musician's practice. This paper begins toexplore some of the ways in which technology developed out of the NIMEcommunity (specifically hyperinstruments), can inform a musician's dailypractice, through short and long term metrics tracking and data visualization.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Hyperinstruments, Pedagogy, Metrics, Ezither, Practice Room},
  Url                      = {http://nime.org/proceedings/2013/nime2013_116.pdf}
}

@InProceedings{Hong:2013,
  Title                    = {Laptap: Laptop Computer as a Musical Instrument using Audio Feedback},
  Author                   = {Dae Ryong Hong and Woon Seung Yeo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {233--236},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Laptap is a laptop-based, real-time sound synthesis/control system for musicand multimedia performance. The system produces unique sounds by positive audiofeedback between the on-board microphone and the speaker of a laptop com-puter. Users can make a variety of sounds by touching the laptop computer inseveral different ways, and control their timbre with the gestures of the otherhand above the mi- crophone and the speaker to manipulate the characteristicsof the acoustic feedback path. We introduce the basic con- cept of this audiofeedback system, describe its features for sound generation and manipulation,and discuss the result of an experimental performance. Finally we suggest somerelevant research topics that might follow in the future.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Laptop music, laptop computer, audio feedback, hand gesture, gestural control, musical mapping, audio visualization, musical notation},
  Url                      = {http://nime.org/proceedings/2013/nime2013_137.pdf}
}

@InProceedings{Honigman:2013,
  Title                    = {The Third Room: A {3D} Virtual Music Framework},
  Author                   = {Colin Honigman and Andrew Walton and Ajay Kapur},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {29--34},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes a new framework for music creation using 3D audio andvisual techniques. It describes the Third Room, which uses a Kinect to placeusers in a virtual environment to interact with new instruments for musicalexpression. Users can also interact with smart objects, including the Ember(modified mbira digital interface) and the Fluid (a wireless six degrees offreedom and touch controller). This project also includes new techniques for 3Daudio connected to a 3D virtual space using multi-channel speakers anddistributed robotic instruments.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Kinect Camera, Third Space, Interface, Virtual Reality, Natural Interaction, Robotics, Arduino},
  Url                      = {http://nime.org/proceedings/2013/nime2013_92.pdf}
}

@InProceedings{Hoste:2013,
  Title                    = {Expressive Control of Indirect Augmented Reality During Live Music Performances},
  Author                   = {Lode Hoste and Beat Signer},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {13--18},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Nowadays many music artists rely on visualisations and light shows to enhanceand augment their live performances. However, the visualisation and triggeringof lights is normally scripted in advance and synchronised with the concert,severely limiting the artist's freedom for improvisation, expression and ad-hocadaptation of their show. These scripts result in performances where thetechnology enforces the artist and their music to stay in synchronisation withthe pre-programmed environment. We argue that these limitations can be overcomebased on emerging non-invasive tracking technologies in combination with anadvanced gesture recognition engine.We present a solution that uses explicit gestures and implicit dance moves tocontrol the visual augmentation of a live music performance. We furtherillustrate how our framework overcomes existing limitations of gestureclassification systems by delivering a precise recognition solution based on asingle gesture sample in combination with expert knowledge. The presentedsolution enables a more dynamic and spontaneous performance and, when combinedwith indirect augmented reality, results in a more intense interaction betweenthe artist and their audience.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Expressive control, augmented reality, live music performance, 3D gesture recognition, Kinect, declarative language},
  Url                      = {http://nime.org/proceedings/2013/nime2013_32.pdf}
}

@InProceedings{Jackie:2013,
  Title                    = {SoloTouch: A Capacitive Touch Controller with Lick-based Note Selector},
  Author                   = {Jackie and Yi Tang Chui and Mubarak Marafa and Samson and Ka Fai Young},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {389--393},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {SoloTouch is a guitar inspired pocket sized controller system that consists ofa capacitive touch trigger and a lick-based note selector. The touch triggerallows an intuitive way to play both velocity sensitive notes and vibratoexpressively using only one finger. The lick-based note selector is an originalconcept that provides the player an easy way to play expressive melodic linesby combining pre-programmed ``licks'' without the need to learn the actualnotes. The two-part controller is primarily used as a basic MIDI controller forplaying MIDI controlled virtual instruments, normally played by keyboardcontrollers. The controller is targeted towards novice musicians, playerswithout prior musical training could play musical and expressive solos,suitable for improvised jamming along modern popular music.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Capacitive touch controller, automated note selector, virtual instrument MIDI controller, novice musicians.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_130.pdf}
}

@InProceedings{Jenkins:2013,
  Title                    = {An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet},
  Author                   = {Leonardo Jenkins and Shawn Trail and George Tzanetakis and Peter Driessen and Wyatt Page},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {352--357},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a minimally-invasive, wireless optical sensorsystem for use with any conventional piston valve acoustic trumpet. Itis designed to be easy to install and remove by any trumpeter. Ourgoal is to offer the extended control afforded by hyperinstrumentswithout the hard to reverse or irreversible invasive modificationsthat are typically used for adding digital sensing capabilities. Weutilize optical sensors to track the continuous position displacementvalues of the three trumpet valves. These values are trasmittedwirelessly and can be used by an external controller. The hardware hasbeen designed to be reconfigurable by having the housing 3D printed sothat the dimensions can be adjusted for any particular trumpetmodel. The result is a low cost, low power, easily replicable sensorsolution that offers any trumpeter the ability to augment their ownexisting trumpet without compromising the instrument's structure orplaying technique. The extended digital control afforded by our systemis interweaved with the natural playing gestures of an acoustictrumpet. We believe that this seemless integration is critical forenabling effective and musical human computer interaction.Keywords: hyperinstrument, trumpet, minimally-invasive, gesture sensing,wireless, I2C},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {hyperinstrument, trumpet, minimally-invasive, gesture sensing, wireless, I2C},
  Url                      = {http://nime.org/proceedings/2013/nime2013_261.pdf}
}

@InProceedings{Jensenius:2013,
  Title                    = {Kinectofon: Performing with Shapes in Planes},
  Author                   = {Alexander Refsum Jensenius},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {196--197},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {"The paper presents the Kinectofon, an instrument for creating sounds throughfree-hand interaction in a 3D space. The instrument is based on the RGB anddepth image streams retrieved from a Microsoft Kinect sensor device. These twoimage streams are used to create different types of motiongrams, which, again,are used as the source material for a sonification process based on inverseFFT. The instrument is intuitive to play, allowing the performer to createsound by ""touching"" a virtual sound wall."},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Kinect, motiongram, sonification, video analysis},
  Url                      = {http://nime.org/proceedings/2013/nime2013_110.pdf}
}

@InProceedings{Barbosa:2013,
  Title                    = {A Drawing-Based Digital Music Instrument},
  Author                   = {Jer{\^{o}}nimo Barbosa, Filipe Calegario, Veronica Teichrieb, Geber Ramalho and Giordano Cabral},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {499--502},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents an innovative digital musical instrument, the Illusio, based on an augmented multi-touch interface that combines a traditional multi-touch surface and a device similar to a guitar pedal. Illusio allows users to perform by drawing and by associating the sketches with live loops. These loops are manipulated based on a concept called hierarchical live looping, which extends traditional live looping through the use of a musical tree, in which any music operation applied to a given node affects all its children nodes. Finally, we evaluate the instrument considering the performer and the audience, which are two of the most important stakeholders involved in the use, conception, and perception of a musical device. The results achieved are encouraging and led to useful insights about how to improve instrument features, performance and usability.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Digital musical instruments, augmented multi-touch, hierarchical live looping, interaction techniques, evaluation methodology},
  Url                      = {http://nime.org/proceedings/2013/nime2013_220.pdf}
}

@InProceedings{John:2013,
  Title                    = {Updating the Classifications of Mobile Music Projects},
  Author                   = {David John},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {301--306},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper reviews the mobile music projects that have been presented at NIMEin the past ten years in order to assess whether the changes in technology haveaffected the activities of mobile music research. An overview of mobile musicprojects is presented using the categories that describe the main activities:projects that explore the influence and make use of location; applications thatshare audio or promote collaborative composition; interaction using wearabledevices; the use of mobile phones as performance devices; projects that exploreHCI design issues. The relative activity between different types of activity isassessed in order to identify trends. The classification according totechnological, social or geographic showed an overwhelming bias to thetechnological, followed by social investigations. An alternative classificationof survey product, or artifact reveals an increase in the number of productsdescribed with a corresponding decline in the number of surveys and artisticprojects. The increase in technical papers appears to be due to an enthusiasmto make use of increased capability of mobile phones, although there are signsthat the initial interest has already peaked, and researchers are againinterested to explore technologies and artistic expression beyond existingmobile phones.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Mobile Music, interactive music, proximity sensing, wearable devices, mobile phone performance, interaction design},
  Url                      = {http://nime.org/proceedings/2013/nime2013_273.pdf}
}

@InProceedings{Johnson:2013,
  Title                    = {MULTI-TOUCH INTERFACES FOR PHANTOM SOURCE POSITIONING IN LIVE SOUND DIFFUSION},
  Author                   = {Bridget Johnson and Ajay Kapur},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {213--216},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a new technique for interface-driven diffusion performance.Details outlining the development of a new tabletop surface-based performanceinterface, named tactile.space, are discussed. User interface and amplitudepanning processes employed in the creation of tactile.space are focused upon,and are followed by a user study-based evaluation of the interface. It is hopedthat the techniques described in this paper afford performers and composers anenhanced level of creative expression in the diffusion performance practice.This paper introduces and evaluates tactile.space, a multi-touch performanceinterface for diffusion built on the BrickTable. It describes how tactile.spaceimplements Vector Base Amplitude Panning to achieve real- time sourcepositioning. The final section of this paper presents the findings of a userstudy that was conducted by those who performed with the interface, evaluatingthe interface as a performance tool with a focus on the increased creativeexpression the interface affords, and directly comparing it to the traditionaldiffusion user interface.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Multi touch, diffusion, VBAP, tabletop surface},
  Url                      = {http://nime.org/proceedings/2013/nime2013_75.pdf}
}

@InProceedings{Johnston:2013,
  Title                    = {Fluid Simulation as Full Body Audio-Visual Instrument},
  Author                   = {Andrew Johnston},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {132--135},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes an audio-visual performance system based on real-timefluid simulation. The aim is to provide a rich environment for works whichblur the boundaries between dance and instrumental performance -- and sound andvisuals -- while maintaining transparency for audiences and new performers. The system uses infra-red motion tracking to allow performers to manipulate areal-time fluid simulation, which in turn provides control data forcomputer-generated audio and visuals. It also provides a control andconfiguration system which allows the behaviour of the interactive system to bechanged over time, enabling the structure within which interactions take placeto be `composed'.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {performance, dance, fluid simulation, composition},
  Url                      = {http://nime.org/proceedings/2013/nime2013_151.pdf}
}

@InProceedings{Kaneko:2013,
  Title                    = {A Function-Oriented Interface for Music Education and Musical Expressions: ``the Sound Wheel''},
  Author                   = {Shoken Kaneko},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {202--205},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper, a function-oriented musical interface, named the sound wheel_x0011_,is presented. This interface is designed to manipulate musical functions likepitch class sets, tonal centers and scale degrees, rather than the _x0010_musicalsurface_x0011_, i.e. the individual notes with concrete note heights. The sound wheelhas an interface summarizing harmony theory, and the playing actions haveexplicit correspondencewith musical functions. Easy usability is realized by semi-automatizing theconversion process from musical functions into the musical surface. Thus, theplayer can use this interface with concentration on the harmonic structure,without having his attention caught by manipulating the musical surface.Subjective evaluation indicated the e_x001B_ffectiveness of this interface as a toolhelpful for understanding the music theory. Because of such features, thisinterface can be used for education and interactive training of tonal musictheory.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Music education, Interactive tonal music generation},
  Url                      = {http://nime.org/proceedings/2013/nime2013_21.pdf}
}

@InProceedings{Kapur:2013,
  Title                    = {New Interfaces for Traditional Korean Music and Dance},
  Author                   = {Ajay Kapur and Dae Hong Kim and Raakhi Kapur and Kisoon Eom},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {45--48},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes the creation of new interfaces that extend traditionalKorean music and dance. Specifically, this research resulted in the design ofthe eHaegum (Korean bowed instrument), eJanggu (Korean drum), and ZiOm wearableinterfaces. The paper describes the process of making these new interfaces aswell as how they have been used to create new music and forms of digital artmaking that blend traditional practice with modern techniques.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Hyperinstrument, Korean interface design, wearable sensors, dance controllers, bowed controllers, drum controllers},
  Url                      = {http://nime.org/proceedings/2013/nime2013_113.pdf}
}

@InProceedings{jo:2013,
  Title                    = {cutting record - a record without (or with) prior acoustic information},
  Author                   = {kazuhiro jo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {283--286},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper, we present a method to produce analog records with standardvector graphics software (i.e. Adobe Illustrator) and two different types ofcutting machines: laser cutter, and paper cutter. The method enables us toengrave wave forms on a surface of diverse materials such as paper, wood,acrylic, and leather without or with prior acoustic information (i.e. digitalaudio data). The results could be played with standard record players. Wepresent the method with its technical specification and explain our initialtrials with two performances and a workshop. The work examines the role ofmusical reproduction in the age of personal fabrication. ---p.s. If it's possible, we also would like to submit the work for performanceand workshop.A video of performance < it contains information on the authorshttp://www.youtube.com/watch?v=vbCLe06P7j0},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Analog Record, Personal Fabrication, Media Archaeology},
  Url                      = {http://nime.org/proceedings/2013/nime2013_228.pdf}
}

@InProceedings{Kikukawa:2013,
  Title                    = {Development of A Learning Environment for Playing Erhu by Diagnosis and Advice regarding Finger Position on Strings},
  Author                   = {Fumitaka Kikukawa and Sojiro Ishihara and Masato Soga and Hirokazu Taki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {271--276},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {So far, there are few studies of string instruments with bows because there aremany parameters to acquire skills and it is difficult to measure theseparameters. Therefore, the aim of this paper is to propose a design of alearning environment for a novice learner to acquire an accurate fingerposition skill. For achieving the aim, we developed a learning environmentwhich can diagnose learner's finger position and give the learner advice byusing magnetic position sensors. The system shows three windows; a fingerposition window for visualization of finger position, a score window fordiagnosing finger position along the score and command prompt window forshowing states of system and advices. Finally, we evaluated the system by anexperiment. The experimental group improved accuracy values about fingerpositions and also improved accuracy of pitches of sounds compared withcontrol group. These results shows significant differences.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Magnetic Position Sensors, String Instruments, Skill, Learning Environment, Finger Position},
  Url                      = {http://nime.org/proceedings/2013/nime2013_181.pdf}
}

@InProceedings{Kim:2013,
  Title                    = {Modelling Gestures in Music Performance with Statistical Latent-State Models},
  Author                   = {Taehun Kim and Stefan Weinzierl},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {427--430},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {"We discuss how to model ""gestures"" in music performance with statisticallatent-states models. A music performance can be described with compositionaland expressive properties varying over time. In those property changes we oftenobserve particular patterns, and such a pattern can be understood as a""gesture"", since it serves as a medium transferring specific emotions. Assuminga finite number of latent states on each property value changes, we candescribe those gestures with statistical latent-states models, and train themby unsupervised learning algorithms. In addition, model entropy provides us ameasure for different effects of each properties on the gesture implementation.Test result on some of real performances indicates that the trained modelscould capture the structure of gestures observed in the given performances, anddetect their boundaries. The entropy-based measure was informative tounderstand the effectiveness of each property on the gesture implementation.Test result on large corpora indicates that our model has potentials for afurther model improvement."},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Musical gestures, performance analysis, unsupervised machine learning},
  Url                      = {http://nime.org/proceedings/2013/nime2013_244.pdf}
}

@InProceedings{KITA:2013,
  Title                    = {Providing a feeling of other remote learners' presence in an online learning environment via realtime sonification of Moodle access log},
  Author                   = {Toshihiro KITA and Naotoshi Osaka},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {198--199},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {When people learn using Web-based educational resources such as an LMS(Learning Management System) or other e-learning related systems, they aresitting in front of their own computer at home and are often physicallyisolated from other online learners. In some courses they are typically gettingin touch online with each others for doing some particular group workassignments, but most of the time they must do their own learning tasks alone.In other courses simply the individual assignments and quizzes are provided, sothe learners are alone all the time from the beginning until the end of thecourse.In order to keep the learners' motivation, it helps to feel other learnersdoing the same learning activities and belonging to the same course.Communicating formally or informally with other learners via Social NetworkingServices or something is one way for learners to get such a feeling, though ina way it might sometimes disturb their learning. Sonification of the access logof the e-learning system could be another indirect way to provide such afeeling.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {e-learning, online learners, Moodle, Csound, realtime sonification, OSC (Open Sound Control)},
  Url                      = {http://nime.org/proceedings/2013/nime2013_203.pdf}
}

@InProceedings{Klugel:2013,
  Title                    = {Towards Mapping Timbre to Emotional Affect},
  Author                   = {Niklas Kl{\"u}gel and Georg Groh},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {525--530},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Controlling the timbre generated by an audio synthesizerin a goal-oriented way requires a profound understandingof the synthesizer's manifold structural parameters. Especially shapingtimbre expressively to communicate emotional affect requires expertise.Therefore, novices in particular may not be able to adequately control timbrein viewof articulating the wealth of affects musically. In this context, the focus ofthis paper is the development of a model that can represent a relationshipbetween timbre and an expected emotional affect . The results of the evaluationof the presented model are encouraging which supports its use in steering oraugmenting the control of the audio synthesis. We explicitly envision thispaper as a contribution to the field of Synthesis by Analysis in the broadersense, albeit being potentially suitable to other related domains.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Emotional affect,Timbre, Machine Learning, Deep Belief Networks, Analysis by Synthesis},
  Url                      = {http://nime.org/proceedings/2013/nime2013_23.pdf}
}

@InProceedings{Kleinberger:2013,
  Title                    = {{PAM}DI Music Box: Primarily Analogico-Mechanical, Digitally Iterated Music Box},
  Author                   = {Rebecca Kleinberger},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {19--20},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {PAMDI is an electromechanical music controller based on an expansion of thecommon metal music boxes. Our system enables an augmentation of the musicalproperties by adding different musical channels triggered and parameterized bynatural gestures during the ``performance''. All the channels are generatedform the original melody recorded once at the start.To capture and treat the different expressive parameters both natural andintentional, our platform is composed of a metallic structure supportingsensors. The measured values are processed by an arduino system that finallysends the results by serial communication to a Max/MSP patch for signaltreatment and modification. We will explain how our embedded instrument aims to bring a certain awarenessto the player of the mapping and the potential musical freedom of the veryspecific -- and not that much automatic - instrument that is a music box. Wewill also address how our design tackles the different questions of mapping,ergonomics and expressiveness while choosing the controller modalities and theparameters to be sensed.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Tangible interface, musical controller, music box, mechanical and electronic coupling, mapping.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_24.pdf}
}

@InProceedings{Lai:2013,
  Title                    = {Audience Experience in Sound Performance},
  Author                   = {Chi-Hsia Lai and Till Bovermann},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {170--173},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents observations from investigating audience experience of apractice-based research in live sound performance with electronics. In seekingto understand the communication flow and the engagement between performer andaudience in this particular performance context, we designed an experiment thatinvolved the following steps: (a) performing WOSAWIP at a new media festival,(b) conducting a qualitative research study with audience members and (c)analyzing the data for new insights.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Audience Experience Study, Live Performance, Evaluation, Research Methods},
  Url                      = {http://nime.org/proceedings/2013/nime2013_197.pdf}
}

@InProceedings{Lee:2013a,
  Title                    = {Live Coding The Mobile Music Instrument},
  Author                   = {Sang Won Lee and Georg Essl},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {493--498},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We introduce a form of networked music performance where a performer plays amobile music instrument while it is being implemented on the fly by a livecoder. This setup poses a set of challenges in performing a music instrumentwhich changes over time and we suggest design guidelines such as making asmooth transition, varying adoption of change, and sharing information betweenthe pair of two performers. A proof-of-concept instrument is implemented on amobile device using UrMus, applying the suggested guidelines. We wish that thismodel would expand the scope of live coding to the distributed interactivesystem, drawing existing performance ideas of NIMEs.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {live coding, network music, on-the-fly instrument, mobile music},
  Url                      = {http://nime.org/proceedings/2013/nime2013_216.pdf}
}

@InProceedings{Lee:2013,
  Title                    = {echobo : Audience Participation Using The Mobile Music Instrument},
  Author                   = {Sang Won Lee and Jason Freeman},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {450--455},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This work aims at a music piece for large-scale audience participation usingmobile phones as musical instruments at a music performance. Utilizing theubiquity of smart phones, we attempted to accomplish audience engagement bycrafting an accessible musical instrument with which audience can be a part ofthe performance. Drawing lessons learnt from the creative works of mobilemusic, audience participation, and the networked instrument a mobile musicalinstrument application is developed so that audience can download the app atthe concert, play the instrument instantly, interact with other audiencemembers, and contribute to the music by sound generated from their mobilephones. The post-survey results indicate that the instrument was easy to use,and the audience felt connected to the music and other musicians.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {mobile music, audience participation, networked instrument},
  Url                      = {http://nime.org/proceedings/2013/nime2013_291.pdf}
}

@InProceedings{Liu:2013,
  Title                    = {Cloud Bridge: a Data-driven Immersive Audio-Visual Software Interface},
  Author                   = {Qian Liu and Yoon Chung Han and JoAnn Kuchera-Morin and Matthew Wright},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {431--436},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Cloud Bridge is an immersive interactive audiovisual software interface forboth data exploration and artistic creation. It explores how information can besonified and visualized to facilitate findings, and eventually becomeinteractive musical compositions. Cloud Bridge functions as a multi-user,multimodal instrument. The data represents the history of items checked out bypatrons of the Seattle Public Library. A single user or agroup of users functioning as a performance ensemble participate in the pieceby interactively querying the database using iOS devices. Each device isassociated with aunique timbre and color for contributing to the piece, whichappears on large shared screens and a surround-sound system for allparticipants and observers. Cloud Bridge leads to a new media interactiveinterface utilizing audio synthesis, visualization and real-time interaction.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Data Sonification, Data Visualization, Sonification, User Interface, Sonic Interaction Design, Open Sound Control},
  Url                      = {http://nime.org/proceedings/2013/nime2013_250.pdf}
}

@InProceedings{Lo:2013,
  Title                    = {Mobile DJ: a Tangible, Mobile Platform for Active and Collaborative Music Listening},
  Author                   = {Kenneth W.K. Lo and Chi Kin Lau and Michael Xuelin Huang and Wai Wa Tang and Grace Ngai and Stephen C.F. Chan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {217--222},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Mobile DJ is a music-listening system that allows multiple users to interactand collaboratively contribute to a single song over a social network. Activelistening through a tangible interface facilitates users to manipulate musicaleffects, such as incorporating chords or ``scratching'' the record. Acommunication and interaction server further enables multiple users to connectover the Internet and collaborate and interact through their music. User testsindicate that the device is successful at facilitating user immersion into theactive listening experience, and that users enjoy the added sensory input aswell as the novel way of interacting with the music and each other.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Mobile, music, interaction design, tangible user interface},
  Url                      = {http://nime.org/proceedings/2013/nime2013_81.pdf}
}

@InProceedings{Lui:2013,
  Title                    = {A Compact Spectrum-Assisted Human Beatboxing Reinforcement Learning Tool On Smartphone},
  Author                   = {Simon Lui},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {25--28},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Music is expressive and hard to be described by words. Learning music istherefore not a straightforward task especially for vocal music such as humanbeatboxing. People usually learn beatboxing in the traditional way of imitatingaudio sample without steps and instructions. Spectrogram contains a lot ofinformation about audio, but it is too complicated to be understood inreal-time. Reinforcement learning is a psychological method, which makes use ofreward and/or punishment as stimulus to train the decision-making process ofhuman. We propose a novel music learning approach based on the reinforcementlearning method, which makes use of compact and easy-to-read spectruminformation as visual clue to assist human beatboxing learning on smartphone.Experimental result shows that the visual information is easy to understand inreal-time, which improves the effectiveness of beatboxing self-learning.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Audio analysis, music learning tool, reinforcement learning, smartphone app, audio information retrieval.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_79.pdf}
}

@InProceedings{Martin:2013,
  Title                    = {Performing with a Mobile Computer System for Vibraphone},
  Author                   = {Charles Martin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {377--380},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes the development of an Apple iPhone based mobile computersystem for vibraphone and its use in a series of the author's performanceprojects in 2011 and 2012.This artistic research was motivated by a desire to develop an alternative tolaptop computers for the author's existing percussion and computer performancepractice. The aims were to develop a light, compact and flexible system usingmobile devices that would allow computer music to infiltrate solo and ensembleperformance situations where it is difficult to use a laptop computer.The project began with a system that brought computer elements to NordligVinter, a suite of percussion duos, using an iPhone, RjDj, Pure Data and ahome-made pickup system. This process was documented with video recordings andanalysed using ethnographic methods.The mobile computer music setup proved to be elegant and convenient inperformance situations with very little time and space to set up, as well as inperformance classes and workshops. The simple mobile system encouragedexperimentation and the platforms used enabled sharing with a wider audience.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {percussion, mobile computer music, Apple iOS, collaborative performance practice, ethnography, artistic research},
  Url                      = {http://nime.org/proceedings/2013/nime2013_121.pdf}
}

@InProceedings{McGee:2013,
  Title                    = {VOSIS: a Multi-touch Image Sonification Interface},
  Author                   = {Ryan McGee},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {460--463},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {VOSIS is an interactive image sonification interface that creates complexwavetables by raster scanning greyscale image pixel data. Using a multi-touchscreen to play image regions of unique frequency content rather than a linearscale of frequencies, it becomes a unique performance tool for experimental andvisual music. A number of image filters controlled by multi-touch gestures addvariation to the sound palette. On a mobile device, parameters controlled bythe accelerometer add another layer expressivity to the resulting audio-visualmontages.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {image sonification, multi-touch, visual music},
  Url                      = {http://nime.org/proceedings/2013/nime2013_310.pdf}
}

@InProceedings{McKinney:2013,
  Title                    = {An Interactive {3D} Network Music Space},
  Author                   = {Chad McKinney and Nick Collins},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {400--405},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper we present Shoggoth, a 3D graphics based program for performingnetwork music. In Shoggoth, users utilize video game style controls to navigateand manipulate a grid of malleable height maps. Sequences can be created bydefining paths through the maps which trigger and modulate audio playback. Withrespect to a context of computer music performance, and specific problems innetwork music, design goals and technical challenges are outlined. The systemis evaluated through established taxonomies for describing interfaces, followedby an enumeration of the merits of 3D graphics in networked performance. Indiscussing proposed improvements to Shoggoth, design suggestions for otherdevelopers and network musicians are drawn out.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {3D, Generative, Network, Environment},
  Url                      = {http://nime.org/proceedings/2013/nime2013_199.pdf}
}

@InProceedings{McLean:2013,
  Title                    = {Paralinguistic Microphone},
  Author                   = {Alex McLean and EunJoo Shin and Kia Ng},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {381--384},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The Human vocal tract is considered for its sonorous qualities incarrying prosodic information, which implicates vision in theperceptual processes of speech. These considerations are put in thecontext of previous work in NIME, forming background for theintroduction of two sound installations; ``Microphone'', which uses acamera and computer vision to translate mouth shapes to sounds, and``Microphone II'', a work-in-progress, which adds physical modellingsynthesis as a sound source, and visualisation of mouth movements.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {face tracking, computer vision, installation, microphone},
  Url                      = {http://nime.org/proceedings/2013/nime2013_122.pdf}
}

@InProceedings{McPherson:2013,
  Title                    = {Portable Measurement and Mapping of Continuous Piano Gesture},
  Author                   = {Andrew McPherson},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {152--157},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a portable optical measurement system for capturingcontinuous key motion on any piano. Very few concert venues have MIDI-enabledpianos, and many performers depend on the versatile but discontinued MoogPianoBar to provide MIDI from a conventional acoustic instrument. The scannerhardware presented in this paper addresses the growing need for alternativesolutions while surpassing existing systems in the level of detail measured.Continuous key position on both black and white keys is gathered at 1kHz samplerate. Software extracts traditional and novel features of keyboard touch fromeach note, which can be flexibly mapped to sound using MIDI or Open SoundControl. RGB LEDs provide rich visual feedback to assist the performer ininteracting with more complex sound mapping arrangements. An application ispresented to the magnetic resonator piano, an electromagnetically-augmentedacoustic grand piano which is performed using continuous key positionmeasurements.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Piano, keyboard, optical sensing, gesture sensing, visual feedback, mapping, magnetic resonator piano},
  Url                      = {http://nime.org/proceedings/2013/nime2013_240.pdf}
}

@InProceedings{Michon:2013,
  Title                    = {The Black Box},
  Author                   = {Romain Michon and Myles Borins and David Meisenholder},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {464--465},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Black Box is a site based installation that allows users to create uniquesounds through physical interaction. The installation consists of a geodesicdome, surround sound speakers, and a custom instrument suspended from the apexof thedome. Audience members entering the space are able to create sound by strikingor rubbing the cube, and are able to control a delay system by moving the cubewithin the space.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Satellite CCRMA, Beagleboard, PureData, Faust, Embedded-Linux, Open Sound Control},
  Url                      = {http://nime.org/proceedings/2013/nime2013_117.pdf}
}

@InProceedings{Mital:2013,
  Title                    = {Mining Unlabeled Electronic Music Databases through {3D} Interactive Visualization of Latent Component Relationships},
  Author                   = {Parag Kumar Mital and Mick Grierson},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {227--232},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present an interactive content-based MIR environment specifically designedto aid in the exploration of databases of experimental electronic music,particularly in cases where little or no metadata exist. In recent years,several rare archives of early experimental electronic music have becomeavailable. The Daphne Oram Collection contains one such archive, consisting ofapproximately 120 hours of 1/4 inch tape recordings and representing a perioddating from circa 1957. This collection is recognized as an importantmusicological resource, representing aspects of the evolution of electronicmusic practices, including early tape editing methods, experimental synthesistechniques and composition. However, it is extremely challenging to derivemeaningful information from this dataset, primarily for three reasons. First,the dataset is very large. Second, there is limited metadata - some titles,track lists, and occasional handwritten notes exist, but where this is true,the reliability of the annotations are unknown. Finally, and mostsignificantly, as this is a collection of early experimental electronic music,the sonic characteristics of the material are often not consistent withtraditional musical information. In other words, there is no score, no knowninstrumentation, and often no recognizable acoustic source. We present amethod for the construction of a frequency component dictionary derived fromthe collection via Probabilistic Latent Component Analysis (PLCA), anddemonstrate how an interactive 3D visualization of the relationships betweenthe PLCA-derived dictionary and the archive is facilitating researcher'sunderstanding of the data.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {mir, plca, mfcc, 3d browser, daphne oram, content-based information retrieval, interactive visualization},
  Url                      = {http://nime.org/proceedings/2013/nime2013_132.pdf}
}

@InProceedings{Mudd:2013,
  Title                    = {Feeling for Sound: Mapping Sonic Data to Haptic Perceptions},
  Author                   = {Tom Mudd},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {369--372},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents a system for exploring different dimensions of a soundthrough the use of haptic feedback. The Novint Falcon force feedback interfaceis used to scan through soundfiles as a subject moves their hand horizontallyfrom left to right, and to relay information about volume, frequency content,noisiness, or potentially any analysable parameter back to the subject throughforces acting on their hand. General practicalities of mapping sonic elements to physical forces areconsidered, such as the problem of representing detailed data through vaguephysical sensation, approaches to applying forces to the hand that do notinterfering with the smooth operation of the device, and the relative merits ofdiscreet and continuous mappings. Three approaches to generating the forcevector are discussed: 1) the use of simulated detents to identify areas of anaudio parameter over a certain threshold, 2) applying friction proportional tothe level of the audio parameter along the axis of movement, and 3) creatingforces perpendicular to the subject's hand movements.Presentation of audio information in this manner could be beneficial for`pre-feeling' as a method for selecting material to play during a liveperformance, assisting visually impaired audio engineers, and as a generalaugmentation of standard audio editing environments.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Haptics, force feedback, mapping, human-computer interaction},
  Url                      = {http://nime.org/proceedings/2013/nime2013_46.pdf}
}

@InProceedings{Murphy:2013,
  Title                    = {Designing and Building Expressive Robotic Guitars},
  Author                   = {Jim Murphy and James McVay and Ajay Kapur and Dale Carnegie},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {557--562},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper provides a history of robotic guitars and bass guitars as well as adiscussion of the design, construction, and evaluation of two new roboticinstruments. Throughout the paper, a focus is made on different techniques toextend the expressivity of robotic guitars. Swivel and MechBass, two newrobots, are built and discussed. Construction techniques of likely interest toother musical roboticists are included. These robots use a variety oftechniques, both new and inspired by prior work, to afford composers andperformers with the ability to precisely control pitch and plucking parameters.Both new robots are evaluated to test their precision, repeatability, andspeed. The paper closes with a discussion of the compositional and performativeimplications of such levels of control, and how it might affect humans who wishto interface with the systems.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {musical robotics, kinetic sculpture, mechatronics},
  Url                      = {http://nime.org/proceedings/2013/nime2013_36.pdf}
}

@InProceedings{Nakanishi:2013,
  Title                    = {POWDER BOX: An Interactive Device with Sensor Based Replaceable Interface For Musical Session},
  Author                   = {Yoshihito Nakanishi and Seiichiro Matsumura and Chuichi Arakawa},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {373--376},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper, the authors introduce an interactive device, ``POWDER BOX''for use by novices in musical sessions. ``POWDER BOX'' is equipped withsensor-based replaceable interfaces, which enable participants to discover andselect their favorite playing styles of musical instruments during a musicalsession. In addition, it has a wireless communication function thatsynchronizes musical scale and BPM between multiple devices. To date, various kinds of ``inventive'' electronic musical instruments havebeen created in the field of Computer Music field. The authors are interestedin formations of musical sessions, aiming for a balance between simpleinteraction and musical expression. This study focuses on the development ofperformance playing styles.Musicians occasionally change their playing styles (e.g., guitar pluckingstyle) during a musical session. Generally, it is difficult for nonmusicians toachieve this kind of smooth changing depends on levels of their skillacquisition. However, it is essentially important for enjoying musical sessionswhether people could acquire these skills. Here, the authors attempted to develop the device that supports nonmusicians toconquer this point using replaceable interfaces. The authors expected thatchanging interfaces would bring similar effect as changing playing style by theskillful player. This research aims to establish an environment in whichnonmusicians and musicians share their individual musical ideas easily. Here,the interaction design and configuration of the ``POWDER BOX'' is presented.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Musical instrument, synthesizer, replaceable interface, sensors},
  Url                      = {http://nime.org/proceedings/2013/nime2013_101.pdf}
}

@InProceedings{Nam:2013,
  Title                    = {Musical Poi (mPoi)},
  Author                   = {Sangbong Nam},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {148--151},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes the Musical Poi (mPoi), a unique sensor-based musicalinstrument rooted in the ancient art of poi spinning. The trajectory ofcircular motion drawn by the performance and the momentum of the mPoiinstrument are converted to the energetic and vibrant sound, which makesspiritual and meditative soundscape that opens everyone up the aura and clearsthe thought forms away. The mPoi project and its concepts will be introducedfirst and then its interaction with a performer will be discussed.The mPoi project seeks to develop a prototype for a set of mobile musicalinstrument based on electronic motion sensors and circuit boards. Thistechnology is installed in egg-shaped structure and allows communicationbetween a performer and the mPoi instrument. The principal motivation for themPoi project has been a desire to develop an extensible interface that willsupport the Poi performance, which is a style of performance art originatedwith the Maori people of New Zealand involving swinging tethered weightsthrough a variety of rhythmical and geometric patterns. As an extension of the body and the expansion of the movement, the mPoiutilizes the creative performance of Poi to make spatial and spiritual soundand music. The aims of the mPoi project are:to create a prototype of mPoi instrument that includes circuit board thatconnects the instrument to a sensor.to develop a software, which includes programming of the circuit board and forthe sound generation.to make a new artistic expression to refine the captured sound into artisticmusical notes. The creative part of the project is to design a unique method to translate theperformer's gesture into sound. A unique algorithm was developed to extractfeatures of the swing motion and translate them into various patterns of sound.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {mPoi, Musical Poi, Jwibulnori, Poi, sensor-based musical instrument},
  Url                      = {http://nime.org/proceedings/2013/nime2013_254.pdf}
}

@InProceedings{Oda:2013,
  Title                    = {Towards Note-Level Prediction for Networked Music Performance},
  Author                   = {Reid Oda and Adam Finkelstein and Rebecca Fiebrink},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {94--97},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The Internet allows musicians and other artists to collaborate remotely.However, network latency presents a fundamental challenge for remotecollaborators who need to coordinate and respond to each other's performancein real time. In this paper, we investigate the viability of predictingpercussion hits before they have occurred, so that information about thepredicted drum hit can be sent over a network, and the sound can be synthesizedat a receiver's location at approximately the same moment the hit occurs atthe sender's location. Such a system would allow two percussionists to playin perfect synchrony despite the delays caused by computer networks. Toinvestigate the feasibility of such an approach, we record vibraphone malletstrikes with a high-speed camera and track the mallet head position. We showthat 30 ms before the strike occurs, it is possible to predict strike time andvelocity with acceptable accuracy. Our method fits a second-order polynomial tothe data to produce a strike time prediction that is within the bounds ofperceptual synchrony, and a velocity estimate that will enable the soundpressure level of the synthesized strike to be accurate within 3 dB.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Networked performance, prediction, computer vision},
  Url                      = {http://nime.org/proceedings/2013/nime2013_258.pdf}
}

@InProceedings{Oh:2013,
  Title                    = {LOLOL: Laugh Out Loud On Laptop},
  Author                   = {Jieun Oh and Ge Wang},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {190--195},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {"Significant progress in the domains of speech- and singing-synthesis hasenhanced communicative potential of machines. To make computers more vocallyexpressive, however, we need a deeper understanding of how nonlinguistic socialsignals are patterned and perceived. In this paper, we focus on laughterexpressions: how a phrase of vocalized notes that we call ""laughter"" may bemodeled and performed to implicate nuanced meaning imbued in the acousticsignal. In designing our model, we emphasize (1) using high-level descriptorsas control parameters, (2) enabling real-time performable laughter, and (3)prioritizing expressiveness over realism. We present an interactive systemimplemented in ChucK that allows users to systematically play with the musicalingredients of laughter. A crowdsourced study on the perception of synthesizedlaughter showed that our model is capable of generating a range of laughtertypes, suggesting an exciting potential for expressive laughter synthesis."},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {laughter, vocalization, synthesis model, real-time controller, interface for musical expression},
  Url                      = {http://nime.org/proceedings/2013/nime2013_86.pdf}
}

@InProceedings{Pardue:2013a,
  Title                    = {Near-Field Optical Reflective Sensing for Bow Tracking},
  Author                   = {Laurel Pardue and Andrew McPherson},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {363--368},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper explores the potential of near-field optical reflective sensing formusical instrument gesture capture. Near-field optical sensors are inexpensive,portable and non-intrusive, and their high spatial and temporal resolutionmakes them ideal for tracking the finer motions of instrumental performance.The paper discusses general optical sensor performance with detailedinvestigations of three sensor models. An application is presented to violinbow position tracking using reflective sensors mounted on the stick. Bowtracking remains a difficult task, and many existing solutions are expensive,bulky, or offer limited temporal resolution. Initial results indicate that bowposition and pressure can be derived from optical measurements of thehair-string distance, and that similar techniques may be used to measure bowtilt.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {optical sensor, reflectance, LED, photodiode, phototransistor, violin, bow tracking, gesture, near-field sensing},
  Url                      = {http://nime.org/proceedings/2013/nime2013_247.pdf}
}

@InProceedings{Pardue:2013,
  Title                    = {Hand-Controller for Combined Tactile Control and Motion Tracking},
  Author                   = {Laurel Pardue and William Sebastian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {90--93},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The Hand Controller is a new interface designed to enable a performer toachieve detailed control of audio and visual parameters through a tangibleinterface combined with motion tracking of the hands to capture large scalephysical movement. Such movement empowers an expressive dynamic for bothperformer and audience. However tracking movements in free space isnotoriously difficult for virtuosic performance. The lack of tactile feedbackleads to difficulty learning the repeated muscle movements required for precisecontrol. In comparison, the hands have shown an impressive ability to mastercomplex motor tasks through feel. The hand controller uses both modes ofinteraction. Electro-magnetic field tracking enables 6D hand motion trackingwhile two options provide tactile interaction- a set of tracks that providelinear positioning and applied finger pressure, or a set of trumpet like sliderkeys that provide continuous data describing key depth. Thumbs actuateadditional pressure sensitive buttons. The two haptic interfaces are mountedto a comfortable hand grip that allows a significant range of reach, andpressure to be applied without restricting hand movement highly desirable inexpressive motion.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {hand, interface, free gesture, force sensing resistor, new musical instrument, tactile feedback, position tracking},
  Url                      = {http://nime.org/proceedings/2013/nime2013_245.pdf}
}

@InProceedings{Park:2013,
  Title                    = {Rainboard and Musix: Building dynamic isomorphic interfaces},
  Author                   = {Brett Park and David Gerhard},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {319--324},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {"Since Euler's development of the Tonnetz in 1739, musicians, composers andinstrument designers have been fascinated with the concept of musicalisomorphism, the idea that by arranging tones by their harmonic relationshipsrather than by their physical properties, the common shapes of musicalconstructs will appear, facilitating learning and new ways of exploringharmonic spaces. The construction of isomorphic instruments, beyond limitedsquare isomorphisms present in many stringed instruments, has been a challengein the past for two reasons: The first problem, that of re-arranging noteactuators from their sounding elements, has been solved by digital instrumentdesign. The second, more conceptual problem, is that only a single isomorphismcan be designed for any one instrument, requiring the instrument designer (aswell as composer and performer) to ""lock in"" to a single isomorphism, or tohave a different instrument for each isomorphism in order to experiment. Musix(an iOS application) and Rainboard (a physical device) are two new musicalinstruments built to overcome this and other limitations of existing isomorphicinstruments. Musix was developed to allow experimentation with a wide varietyof different isomorphic layouts, to assess the advantages and disadvantages ofeach. The Rainboard consists of a hexagonal array of arcade buttons embeddedwith RGB-LEDs, which are used to indicate characteristics of the isomorphismcurrently in use on the Rainboard. The creation of these two instruments /experimentation platforms allows for isomorphic layouts to be explored in waysthat are not possible with existing instruments."},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {isomorphic, mobile application, hexagon, keyboard},
  Url                      = {http://nime.org/proceedings/2013/nime2013_65.pdf}
}

@InProceedings{Park:2013a,
  Title                    = {Sound Spray - can-shaped sound effect device},
  Author                   = {Gibeom Park and Kyogu Lee},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {65--68},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper, we designed a sound effect device, which was applicable forspray paint art process. For the applicability research of the device, wedesigned a prototype which had a form not far off the traditional spray cans,using Arduino and various sensors. Through the test process of the prototype,we verified the elements that would be necessary to apply our newly designeddevice to real spray paint art activities. Thus we checked the possibility ofvarious musical expressions by expanding the functions of the designed device.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Sound effect device, Spray paint art, Arduino, Pure Data},
  Url                      = {http://nime.org/proceedings/2013/nime2013_158.pdf}
}

@InProceedings{Park:2013b,
  Title                    = {Sound Surfing Network (SSN): Mobile Phone-based Sound Spatialization with Audience Collaboration},
  Author                   = {Saebyul Park and Seonghoon Ban and Dae Ryong Hong and Woon Seung Yeo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {111--114},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {SSN (Sound Surfing Network) is a performance system that provides a new musicalexperience by incorporating mobile phone-based spatial sound control tocollaborative music performance. SSN enables both the performer and theaudience to manipulate the spatial distribution of sound using the smartphonesof the audience as distributed speaker system. Proposing a new perspective tothe social aspect music appreciation, SSN will provide a new possibility tomobile music performances in the context of interactive audience collaborationas well as sound spatialization.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Mobile music, smartphone, audience participation, spatial sound control, digital performance},
  Url                      = {http://nime.org/proceedings/2013/nime2013_305.pdf}
}

@InProceedings{Park:2013c,
  Title                    = {Fortissimo: Force-Feedback for Mobile Devices},
  Author                   = {Tae Hong Park and Oriol Nieto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {291--294},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper we present a highly expressive, robust, and easy-to-build systemthat provides force-feedback interaction for mobile computing devices (MCD).Our system, which we call Fortissimo (ff), utilizes standard built-inaccelerometer measurements in conjunction with generic foam padding that can beeasily placed under a device to render an expressive force-feedback performancesetup. Fortissimo allows for musically expressive user-interaction with addedforce-feedback which is integral for any musical controller --a feature that isabsent for touchscreen-centric MCDs. This paper details ff core concepts,hardware and software designs, and expressivity of musical features.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {force-feedback, expression, mobile computing devices, mobile music},
  Url                      = {http://nime.org/proceedings/2013/nime2013_233.pdf}
}

@InProceedings{Perrotin:2013,
  Title                    = {Adaptive mapping for improved pitch accuracy on touch user interfaces},
  Author                   = {Olivier Perrotin and Christophe d'Alessandro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {186--189},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Touch user interfaces such as touchpad or pen tablet are often used forcontinuous pitch control in synthesis devices. Usually, pitch is set at thecontact point on the interface, thus introducing possible pitch inaccuracies atthe note onset. This paper proposes a new algorithm, based on an adaptiveattraction mapping, for improving initial pitch accuracy with touch userinterfaces with continuous control. At each new contact on the interface, thealgorithm adjusts the mapping to produce the most likely targeted note of thescale in the vicinity of the contact point. Then, pitch remains continuouslyadjustable as long as the contact is maintained, allowing for vibrato,portamento and other subtle melodic control. The results of experimentscomparing the users' pitch accuracy with and without the help of the algorithmshow that such a correction enables to play sharply in tune at the contact withthe interface, regardless the musical background of the player. Therefore, thedynamic mapping algorithm allows for a clean and accurate attack when playing touch user interfaces for controlling continuous pitch instruments like voicesynthesizers.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Sound synthesis control, touch user interfaces, pen tablet, automatic correction, accuracy, precision},
  Url                      = {http://nime.org/proceedings/2013/nime2013_178.pdf}
}

@InProceedings{Place:2013,
  Title                    = {AlphaSphere},
  Author                   = {Adam Place and Liam Lacey and Thomas Mitchell},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {491--492},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {A two-page demonstration paper.Abstract:The AlphaSphere is an electronic musical instrument featuring a series oftactile, pressure sensitive touch pads arranged in a spherical form. It isdesigned to offer a new playing style, while allowing for the expressivereal-time modulation of sound available in electronic-based music. It is alsodesigned to be programmable, enabling the flexibility to map a series ofdifferent notational arrangements to the pad-based interface.The AlphaSphere functions as an HID, MIDI and OSC device, which connects to acomputer and/or independent MIDI device, and its control messages can be mappedthrough the AlphaLive software. Our primary motivations for creating theAlphaSphere are to design an expressive music interface which can exploit thesound palate of synthesizers in a design which allows for the mapping ofnotational arrangements.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {AlphaSphere, MIDI, HID, polyphonic aftertouch, open source},
  Url                      = {http://nime.org/proceedings/2013/nime2013_300.pdf}
}

@InProceedings{Resch:2013,
  Title                    = {note~ for Max - An extension for Max/MSP for Media Arts \& music},
  Author                   = {Thomas Resch},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {210--212},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {"note~ for Max - An extension for Max/MSP for Media Arts & musicShort Paper/Poster & Workshop proposal for the NIME 2013At the Electronic Studio/Research Department of the University of Music Baselwe have been developing the software note~, which consists of 4 in theC-Programming language developed objects for Max/MSP. The essential concept ofnote~ is that of a classic MIDI sequencer: Recording, editing and playing backcontrol data. It provides a GUI, a scripting interface and feedback for everyinteraction so its functionality can be extended by common Max/MSP objects. Instead of using the limited MIDI format , note~ allows storing floating pointlists up to 1024 elements plus text within one event.In order to get a quick overview about note~'s capabilities I recommendwatching the video ""noteForMax_IntroductionToNote~"".Instead of doing a demonstration for the short paper I would like to propose a3 hours workshop for note~:The first half of the workshop would be an introduction to note~, its basicfeatures and how to integrate it into the Max/MSP environment as it is shown inthe videos""noteForMax_Webcast1"" and ""noteForMax_Webcast2"".In the second half, some of the examples shown in the videos""noteForMax_IntroductionToNote~"", ""noteForMax_theMakingOfWings&Halos"" and""noteForMax_WhatsNewInTheUpcomingRelease""would be explained in detail:- Extending note~'s functionality with Max Programming- Synchronizing several note~ objects in order to create complex polyrhythmicstructures- Synchronizing audio-, event- and video- playback with note~- Using the note.score object for live score generation- Importing data from SDIF- and Excel-Files and work with them algorithmically"},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Max/MSP, composing, timeline, GUI, sequencing, score, notation.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_57.pdf}
}

@InProceedings{Roberts:2013,
  Title                    = {Enabling Multimodal Mobile Interfaces for Musical Performance},
  Author                   = {Charles Roberts and Angus Forbes and Tobias H{\"o}llerer},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {102--105},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We present research that extends the scope of the mobile application Control, aprototyping environment for defining multimodal interfaces that controlreal-time artistic and musical performances. Control allows users to rapidlycreate interfaces employing a variety of modalities, including: speechrecognition, computer vision, musical feature extraction, touchscreen widgets,and inertial sensor data. Information from these modalities can be transmittedwirelessly to remote applications. Interfaces are declared using JSON and canbe extended with JavaScript to add complex behaviors, including the concurrentfusion of multimodal signals. By simplifying the creation of interfaces viathese simple markup files, Control allows musicians and artists to make novelapplications that use and combine both discrete and continuous data from thewide range of sensors available on commodity mobile devices.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Music, mobile, multimodal, interaction},
  Url                      = {http://nime.org/proceedings/2013/nime2013_303.pdf}
}

@InProceedings{Roberts:2013a,
  Title                    = {The Web Browser As Synthesizer And Interface},
  Author                   = {Charles Roberts and Graham Wakefield and Matthew Wright},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {313--318},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Web technologies provide an incredible opportunity to present new musicalinterfaces to new audiences. Applications written in JavaScript and designed torun in the browser offer remarkable performance, mobile/desktop portability andlongevity due to standardization. Our research examines the use and potentialof native web technologies for musical expression. We introduce two librariestowards this end: Gibberish.js, a heavily optimized audio DSP library, andInterface.js, a GUI toolkit that works with mouse, touch and motion events.Together these libraries provide a complete system for defining musicalinstruments that can be used in both desktop and mobile browsers. Interface.jsalso enables control of remote synthesis applications by including anapplication that translates the socket protocol used by browsers into both MIDIand OSC messages.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {mobile devices, javascript, browser-based NIMEs, web audio, websockets},
  Url                      = {http://nime.org/proceedings/2013/nime2013_282.pdf}
}

@InProceedings{Rosselet:2013,
  Title                    = {Jam On: a new interface for web-based collective music performance},
  Author                   = {Ulysse Rosselet and Alain Renaud},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {394--399},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper presents the musical interactions aspects of the design anddevelopment of a web-based interactive music collaboration system called JamOn. Following a design science approach, this system is being built accordingto principles taken from usability engineering and human computer interaction(HCI). The goal of the system is to allow people with no to little musicalbackground to play a song collaboratively. The musicians control the musicalcontent and structure of the song thanks to an interface relying on the freeinking metaphor. One contribution of this interface is that it displays musicalpatterns of different lengths in the same space. The design of Jam On is basedon a list of performance criteria aimed at ensuring the musicality of theperformance and the interactivity of the technical system. The paper comparestwo alternative interfaces used for the system and explores the various stagesof the design process aimed at making the system as musical and interactive aspossible.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Networked performance, interface design, mapping, web-based music application},
  Url                      = {http://nime.org/proceedings/2013/nime2013_196.pdf}
}

@InProceedings{Sanganeria:2013,
  Title                    = {GrainProc: a real-time granular synthesis interface for live performance},
  Author                   = {Mayank Sanganeria and Kurt Werner},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {223--226},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {GrainProc is a touchscreen interface for real-time granular synthesis designedfor live performance. The user provides a real-time audio input (electricguitar, for example) as a granularization source and controls various synthesisparameters with their fingers or toes. The control parameters are designed togive the user access to intuitive and expressive live granular manipulations.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Granular synthesis, touch screen interface, toe control, real-time, CCRMA},
  Url                      = {http://nime.org/proceedings/2013/nime2013_99.pdf}
}

@InProceedings{Sarwate:2013,
  Title                    = {Variator: A Creativity Support Tool for Music Composition},
  Author                   = {Avneesh Sarwate and Rebecca Fiebrink},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {279--282},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The Variator is a compositional assistance tool that allows users to quicklyproduce and experiment with variations on musical objects, such as chords,melodies, and chord progressions. The transformations performed by the Variatorcan range from standard counterpoint transformations (inversion, retrograde,transposition) to more complicated custom transformations, and the system isbuilt to encourage the writing of custom transformations.This paper explores the design decisions involved in creating a compositionalassistance tool, describes the Variator interface and a preliminary set ofimplemented transformation functions, analyzes the results of the evaluationsof a prototype system, and lays out future plans for expanding upon thatsystem, both as a stand-alone application and as the basis for an opensource/collaborative community where users can implement and share their owntransformation functions.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Composition assistance tool, computer-aided composition, social composition},
  Url                      = {http://nime.org/proceedings/2013/nime2013_224.pdf}
}

@InProceedings{Schacher:2013,
  Title                    = {Hybrid Musicianship - Teaching Gestural Interaction with Traditional and Digital Instruments},
  Author                   = {Jan C. Schacher},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {55--60},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This article documents a class that teaches gestural interaction and juxtaposestraditional instrumental skills with digital musical instrument concepts. Inorder to show the principles and reflections that informed the choices made indeveloping this syllabus, fundamental elements of an instrument-bodyrelationship and the perceptual import of sensori-motor integration areinvestigated. The methods used to let participants learn in practicalexperimental settings are discussed, showing a way to conceptualise andexperience the entire workflow from instrumental sound to electronictransformations by blending gestural interaction with digital musicalinstrument techniques and traditional instrumental playing skills. Thetechnical interfaces and software that were deployed are explained, focussingof the interactive potential offered by each solution. In an attempt tosummarise and evaluate the impact of this course, a number of insights relatingto this specific pedagogical situation are put forward. Finally, concreteexamples of interactive situations that were developed by the participants areshown in order to demonstrate the validity of this approach.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {gestural interaction, digital musical instruments, pedagogy, mapping, enactive approach},
  Url                      = {http://nime.org/proceedings/2013/nime2013_127.pdf}
}

@InProceedings{Schacher:2013a,
  Title                    = {The Quarterstaff, a Gestural Sensor Instrument},
  Author                   = {Jan C. Schacher},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {535--540},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This article describes the motivations and reflections that led to thedevelopment of a gestural sensor instrument called the Quarterstaff. In aniterative design and fabrication process, several versions of this interfacewere build, tested and evaluated in performances. A detailed explanation of thedesign choices concerning the shape but also the sensing capabilities of theinstrument illustrates the emphasis on establishing an `enactive'instrumental relationship. A musical practice for this type of instrument isshown by discussing the methods used in the exploration of the gesturalpotential of the interface and the strategies deployed for the development ofmappings and compositions. Finally, to gain more information about how thisinstrument compares with similar designs, two dimension-space analyses are madethat show a clear positioning in relation to instruments that precede theQuarterstaff.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Gestural sensor interface, instrument design, body-object relation, composition and performance practice, dimension space analysis},
  Url                      = {http://nime.org/proceedings/2013/nime2013_144.pdf}
}

@InProceedings{Scott:2013,
  Title                    = {Personalized Song Interaction Using a Multi Touch Interface},
  Author                   = {Jeffrey Scott and Mickey Moorhead and Justin Chapman and Ryan Schwabe and Youngmoo E. Kim},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {417--420},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Digital music technology has transformed the listener experience and creatednew avenues for creative interaction and expression within the musical domain.The barrier to music creation, distribution and collaboration has been reduced,leading to entirely new ecosystems of musical experience. Software editingtools such as digital audio workstations (DAW) allow nearly limitlessmanipulation of source audio into new sonic elements and textures and havepromoted a culture of recycling and repurposing of content via mashups andremixes. We present a multi-touch application that allows a user to customizetheir listening experience by blending various versions of a song in real time.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Multi-track, Multi-touch, Mobile devices, Interactive media},
  Url                      = {http://nime.org/proceedings/2013/nime2013_234.pdf}
}

@InProceedings{Skogstad:2013,
  Title                    = {Filtering Motion Capture Data for Real-Time Applications},
  Author                   = {St{\aa}le A. Skogstad},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {142--147},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In this paper we present some custom designed filters for real-time motioncapture applications. Our target application is so-called motion controllers,i.e. systems that interpret hand motion for musical interaction. In earlierresearch we found effective methods to design nearly optimal filters forreal-time applications. However, to be able to design suitable filters for ourtarget application, it is necessary to establish the typical frequency contentof the motion capture data we want to filter. This will again allow us todetermine a reasonable cutoff frequency for the filters. We have thereforeconducted an experiment in which we recorded the hand motion of 20 subjects.The frequency spectra of these data together with a method similar to theresidual analysis method were then used to determine reasonable cutofffrequencies. Based on this experiment, we propose three cutoff frequencies fordifferent scenarios and filtering needs: 5, 10 and 15 Hz, which corresponds toheavy, medium and light filtering respectively. Finally, we propose a range ofreal-time filters applicable to motion controllers. In particular, low-passfilters and low-pass differentiators of degrees one and two, which in ourexperience are the most useful filters for our target application.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Url                      = {http://nime.org/proceedings/2013/nime2013_238.pdf}
}

@InProceedings{Soria:2013,
  Title                    = {Multidimensional sound spatialization by means of chaotic dynamical systems},
  Author                   = {Edmar Soria and Roberto Morales-Manzanares},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {79--83},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This work presents a general framework method for cre-ating spatialization systems focused on electroacoustic andacousmatic music performance and creation. Although weused the logistic equation as orbit generator, any dynami-cal system could be suitable. The main idea lies on generating vectors of Rn with entriesfrom data series of di_x000B_erent orbits from an speci_x000C_c dynami-cal system. Such vectors will be called system vectors. Ourproposal is to create ordered paths between those pointsor system vectors using the Splines Quark library by Felix,1which allow us to generate smooth curves joining the points.Finally, interpolating that result with a _x000C_xed sample value,we are able to obtain speci_x000C_c and independent multidimen-sional panning trajectories for each speaker array and forany number of sound sources.Our contribution is intended to be at the very root of the compositionalprocess giving to the creator a method for exploring new ways for spatialsound placement over time for a wide range of speakers ar-rangements. The advantage of using controlled chaotic dy-namical systems like the logistic equation, lies on the factthat the composer can freely and consciously choose be-tween stable or irregular behaviour for the orbits that willgenerate his/her panning trajectories. Besides, with the useof isometries, it is possible to generate di_x000B_erent related or-bits with one single evaluation of the system. The use ofthe spline method in SuperCollider allows the possibilityof joining and relating those values from orbits into a wellde_x000C_ned and coherent general system. Further research willinclude controlling synthesis parameters in the same waywe created panning trajectories.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {NIME, spatialization, dynamical systems, chaos},
  Url                      = {http://nime.org/proceedings/2013/nime2013_195.pdf}
}

@InProceedings{Tahiroglu:2013,
  Title                    = {PESI Extended System: In Space, On Body, with 3 Musicians},
  Author                   = {Koray Tahiro{\u g}lu and Nuno N. Correia and Miguel Espada},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {35--40},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper introduces a novel collaborative environment (PESI) in whichperformers are not only free to move and interact with each other but wheretheir social interactions contribute to the sonic outcome. PESI system isdesigned for co-located collaboration and provides embodied and spatialopportunities for musical exploration. To evaluate PESI with skilled musicians,a user-test jam session was conducted. Musicians' comments indicate that thesystem facilitates group interaction finely to bring up further intentions tomusical ideas. Results from our user-test jam session indicate that, through some modificationof the 'in-space' response to the improvisation, and through more intuitiveinteractions with the 'on-body' mobile instruments, we could make thecollaborative music activity a more engaging and active experience. Despitebeing only user-tested once with musicians, the group interview has raisedfruitful discussions on the precise details of the system components.Furthermore, the paradigms of musical interaction and social actions in groupactivities need to be questioned when we seek design requirements for such acollaborative environment. We introduced a system that we believe can open upnew ways of musical exploration in group music activity with a number ofmusicians. The system brings up the affordances of accessible technologieswhile creating opportunities for novel design applications to be explored. Ourresearch proposes further development of the system, focusing on movementbehavior in long-term interaction between performers. We plan to implement thisversion and evaluate design and implementation with distinct skilled musicians.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Affordances, collaboration, social interaction, mobile music, extended system, NIME},
  Url                      = {http://nime.org/proceedings/2013/nime2013_97.pdf}
}

@InProceedings{Tang:2013,
  Title                    = {Computer Assisted Melo-rhythmic Generation of Traditional Chinese Music from Ink Brush Calligraphy},
  Author                   = {Will W. W. Tang and Stephen Chan and Grace Ngai and Hong-va Leong},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {84--89},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {CalliMusic, is a system developed for users to generate traditional Chinesemusic by writing Chinese ink brush calligraphy, turning the long-believedstrong linkage between the two art forms with rich histories into reality. Inaddition to traditional calligraphy writing instruments (brush, ink and paper),a camera is the only addition needed to convert the motion of the ink brushinto musical notes through a variety of mappings such as human-inspired,statistical and a hybrid. The design of the system, including details of eachmapping and research issues encountered are discussed. A user study of systemperformance suggests that the result is quite encouraging. The technique is,obviously, applicable to other related art forms with a wide range ofapplications.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Chinese Calligraphy, Chinese Music, Assisted Music Generation},
  Url                      = {http://nime.org/proceedings/2013/nime2013_208.pdf}
}

@InProceedings{Tarakajian:2013,
  Title                    = {Mira: Liveness in iPad Controllers for Max/MSP},
  Author                   = {Sam Tarakajian and David Zicarelli and Joshua Clayton},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {421--426},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Mira is an iPad app for controlling Max patchers in real time with minimalconfiguration. This submission includes a paper describing Mira's design andimplementation, as well as a demo showing how Mira works with Max.The Mira iPad app discovers open Max patchers automatically using the Bonjourprotocol, connects to them over WiFi and negotiates a description of the Maxpatcher. As objects change position and appearance, Mira makes sure that theinterface on the iPad is kept up to date. Mira eliminates the need for anexplicit mapping step between the interface and the system being controlled.The user is never asked to input an IP address, nor to configure the mappingbetween interface objects on the iPad and those in the Max patcher. So theprototyping composer is free to rapidly configure and reconfigure theinterface.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {NIME, Max/MSP/Jitter, Mira, ipad, osc, bonjour, zeroconf},
  Url                      = {http://nime.org/proceedings/2013/nime2013_241.pdf}
}

@InProceedings{Taylor:2013,
  Title                    = {Plum St: Live Digital Storytelling with Remote Browsers},
  Author                   = {Ben Taylor and Jesse Allison},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {477--478},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {What is the place for Internet Art within the paradigm of remote musicperformance? In this paper, we discuss techniques for live audiovisualstorytelling through the Web browsers of remote viewers. We focus on theincorporation of socket technology to create a real-time link between performerand audience, enabling manipulation of Web media directly within the eachaudience member's browser. Finally, we describe Plum Street, an onlinemultimedia performance, and suggest that by involving remote performance,appropriating Web media such as Google Maps, social media, and Web Audio intothe work, we can tell stories in a way that more accurately addresses modernlife and holistically fulfills the Web browser's capabilities as a contemporaryperformance instrument.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Remote Performance, Network Music, Internet Art, Storytelling},
  Url                      = {http://nime.org/proceedings/2013/nime2013_281.pdf}
}

@InProceedings{Thorogood:2013,
  Title                    = {Impress: A Machine Learning Approach to Soundscape Affect Classification for a Music Performance Environment},
  Author                   = {Miles Thorogood and Philippe Pasquier},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {256--260},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Soundscape composition in improvisation and performance contexts involves manyprocesses that can become over- whelming for a performer, impacting on thequality of the composition. One important task is evaluating the mood of acomposition for evoking accurate associations and mem- ories of a soundscape. Anew system that uses supervised machine learning is presented for theacquisition and re- altime feedback of soundscape affect. A model of sound-scape mood is created by users entering evaluations of au- dio environmentsusing a mobile device. The same device then provides feedback to the user ofthe predicted mood of other audio environments. We used a features vector ofTotal Loudness and MFCC extracted from an audio signal to build a multipleregression models. The evaluation of the system shows the tool is effective inpredicting soundscape affect.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {soundscape, performance, machine learning, audio features, affect grid},
  Url                      = {http://nime.org/proceedings/2013/nime2013_157.pdf}
}

@InProceedings{Tobise:2013,
  Title                    = {Construction of a System for Recognizing Touch of Strings for Guitar},
  Author                   = {Hayami Tobise and Yoshinari Takegawa and Tsutomu Terada and Masahiko Tsukamoto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {261--266},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {In guitar performance, fingering is an important factor, and complicated. In particular, the fingering of the left hand comprises various relationshipsbetween the finger and the string, such as a finger touching the strings, afinger pressing the strings, and a finger releasing the strings. The recognition of the precise fingering of the left hand is applied to aself-learning support system, which is able to detect strings being muted by afinger, and which transcribes music automatically, including the details offingering techniques. Therefore, the goal of our study is the construction of a system forrecognizing the touch of strings for the guitar. We propose a method for recognizing the touch of strings based on theconductive characteristics of strings and frets. We develop a prototype system, and evaluate its effectiveness.Furthermore, we propose an application which utilizes our system.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Guitar, Touched strings, Fingering recognition},
  Url                      = {http://nime.org/proceedings/2013/nime2013_159.pdf}
}

@InProceedings{Tokunaga:2013,
  Title                    = {Enactive Mandala: Audio-visualizing Brain Waves},
  Author                   = {Tomohiro Tokunaga and Michael Lyons},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {118--119},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {We are exploring the design and implementation of artificial expressions,kinetic audio-visual representations of real-time physiological data whichreflect emotional and cognitive state. In this work we demonstrate a prototype,the Enactive Mandala, which maps real-time EEG signals to modulate ambientmusic and animated visual music. The design draws inspiration from the visualmusic of the Whitney brothers as well as traditional meditative practices.Transparent real-time audio-visual feedback ofbrainwave qualities supports intuitive insight into the connection betweenthoughts and physiological states. Our method is constructive: by linkingphysiology with an dynamic a/v display, and embedding the human-machine systemin the social contexts that arise in real-time play, we hope to seed new, andas yet unknown forms, of non-verbal communication, or ``artificialexpressions''.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Brain-computer Interfaces, BCI, EEG, Sonification, Visualization, Artificial Expressions, NIME, Visual Music},
  Url                      = {http://nime.org/proceedings/2013/nime2013_16.pdf}
}

@InProceedings{Torresen:2013,
  Title                    = {A New Wi-Fi based Platform for Wireless Sensor Data Collection},
  Author                   = {Jim Torresen and Yngve Hafting and Kristian Nymoen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {337--340},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {A custom designed WLAN (Wireless Local Area Network) based sensor interface ispresented in this paper. It is aimed at wirelessly interfacing a large varietyof sensors to supplement built-in sensors in smart phones and media players.The target application area is collection of human related motions andcondition to be applied in musical applications. The interface is based oncommercially available units and allows for up to nine sensors. The benefit ofusing WLAN based communication is high data rate with low latency. Ourexperiments show that the average transmission time is less than 2ms for asingle sensor. Further, it is operational for a whole day without batteryrecharging.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {wireless communication, sensor data collection, WLAN, Arduino},
  Url                      = {http://nime.org/proceedings/2013/nime2013_236.pdf}
}

@InProceedings{Trento:2013,
  Title                    = {Flag beat: a novel interface for rhythmic musical expression for kids},
  Author                   = {Stefano Trento and Stefania Serafin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {456--459},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {This paper describes the development of a prototype of a sonic toy forpre-scholar kids. The device, which is a mod- ified version of a footballratchet, is based on the spinning gesture and it allows to experience fourdifferent types of auditory feedback. These algorithms let a kid play withmusic rhythm, generate a continuous sound feedback and control the pitch of apiece of music. An evaluation test of the device has been performed withfourteen kids in a kindergarten. Results and observations showed that kidspreferred the algorithms based on the exploration of the music rhythm and onpitch shifting.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Sonic toy, children, auditory feedback.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_295.pdf}
}

@InProceedings{Walther:2013,
  Title                    = {Rocking the Keys with a Multi-Touch Interface},
  Author                   = {Thomas Walther and Damir Ismailovi{\'c} and Bernd Br{\"u}gge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {98--101},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Although multi-touch user interfaces have become a widespread form of humancomputer interaction in many technical areas, they haven't found their way intolive performances of musicians and keyboarders yet. In this paper, we present anovel multi-touch interface method aimed at professional keyboard players. Themethod, which is inspired by computer trackpads, allows controlling up to tencontinuous parameters of a keyboard with one hand, without requiring the userto look at the touch area - a significant improvement over traditional keyboardinput controls. We discuss optimizations needed to make our interface reliable,and show in an evaluation with four keyboarders of different skill level thatthis method is both intuitive and powerful, and allows users to more quicklyalter the sound of their keyboard than they could with current input solutions.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {multi-touch, mobile, keyboard, interface},
  Url                      = {http://nime.org/proceedings/2013/nime2013_275.pdf}
}

@InProceedings{Wang:2013,
  Title                    = {PENny: An Extremely Low-Cost Pressure-Sensitive Stylus for Existing Capacitive Touchscreens},
  Author                   = {Johnty Wang and Nicolas d'Alessandro and Aura Pon and Sidney Fels},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {input interfaces, touch screens, tablets, pressure-sensitive, low-cost},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {By building a wired passive stylus we have added pressure sensitivity toexisting capacitive touch screen devices for less than },
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {10 in materials, about1/10th the cost of existing solutions. The stylus makes use of the built inaudio interface that is available on most smartphones and tablets on the markettoday. Limitations of the device include the physical constraint of wires, theoccupation of one audio input and output channel, and increased latency equalto the period of at least one audio buffer duration. The stylus has beendemonstrated in two cases thus far: a visual musical score drawing and asinging synthesis application.},
  Url                      = {http://nime.org/proceedings/2013/nime2013_150.pdf}
}

@InProceedings{Wiriadjaja:2013,
  Title                    = {Gamelan Sampul: Laptop Sleeve Gamelan},
  Author                   = {Antonius Wiriadjaja},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {469--470},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The Gamelan Sampul is a laptop sleeve with embedded circuitry that allows usersto practice playing Javanese gamelan instruments without a full set ofinstruments. It is part of a larger project that aims to develop a set ofportable and mobile tools for learning, recording and performing classicalJavanese gamelan music.The accessibility of a portable Javanese gamelan set introduces the musicalgenre to audiences who have never experienced this traditional music before,passing down long established customs to future generations. But it also raisesthe question of what is and what isn't appropriate to the musical tradition.The Gamelan Sampul attempts to introduce new technology to traditional folkmusic while staying sensitive to cultural needs.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Physical computing, product design, traditional folk arts, gamelan},
  Url                      = {http://nime.org/proceedings/2013/nime2013_246.pdf}
}

@InProceedings{Wolf:2013,
  Title                    = {SonNet: A Code Interface for Sonifying Computer Network Data},
  Author                   = {KatieAnna E Wolf and Rebecca Fiebrink},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {503--506},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {As any computer user employs the Internet to accomplish everyday activities, a flow of data packets moves across the network, forming their own patterns in response to his or her actions. Artists and sound designers who are interested in accessing that data to make music must currently possess low-level knowledge of Internet protocols and spend signifi-cant effort working with low-level networking code. We have created SonNet, a new software tool that lowers these practical barriers to experimenting and composing with network data. SonNet executes packet-sniffng and network connection state analysis automatically, and it includes an easy-touse ChucK object that can be instantiated, customized, and queried from a user's own code. In this paper, we present the design and implementation of the SonNet system, and we discuss a pilot evaluation of the system with computer music composers. We also discuss compositional applications of SonNet and illustrate the use of the system in an example composition.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Sonification, network data, compositional tools},
  Url                      = {http://nime.org/proceedings/2013/nime2013_94.pdf}
}

@InProceedings{Xiao:2013,
  Title                    = {Conjuring the Recorded Pianist: A New Medium to Experience Musical Performance},
  Author                   = {Xiao Xiao and Anna Pereira and Hiroshi Ishii},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {7--12},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The body channels rich layers of information when playing music, from intricatemanipulations of the instrument to vivid personifications of expression. Butwhen music is captured and replayed across distance and time, the performer'sbody is too often trapped behind a small screen or absent entirely.This paper introduces an interface to conjure the recorded performer bycombining the moving keys of a player piano with life-sized projection of thepianist's hands and upper body. Inspired by reflections on a lacquered grandpiano, our interface evokes the sense that the virtual pianist is playing thephysically moving keys.Through our interface, we explore the question of how to viscerally simulate aperformer's presence to create immersive experiences. We discuss designchoices, outline a space of usage scenarios and report reactions from users.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {piano performance, musical expressivity, body language, recorded music, player piano, augmented reality, embodiment},
  Url                      = {http://nime.org/proceedings/2013/nime2013_28.pdf}
}

@InProceedings{Yang:2013,
  Title                    = {Visual Associations in Augmented Keyboard Performance},
  Author                   = {Qi Yang and Georg Essl},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {252--255},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {What is the function of visuals in the design of an augmented keyboardperformance device with projection? We address this question by thinkingthrough the impact of choices made in three examples on notions of locus ofattention, visual anticipation and causal gestalt to articulate a space ofdesign choices. Visuals can emphasize and deemphasize aspects of performanceand help clarify the role input has to the performance. We suggest that thisprocess might help thinking through visual feedback design in NIMEs withrespect to the performer or the audience.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Visual feedback, interaction, NIME, musical instrument, interaction, augmented keyboard, gesture, Kinect},
  Url                      = {http://nime.org/proceedings/2013/nime2013_156.pdf}
}

@InProceedings{You:2013,
  Title                    = {Remix_Dance 3: Improvisatory Sound Displacing on Touch Screen-Based Interface},
  Author                   = {Jaeseong You and Red Wierenga},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {124--127},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {Remix_Dance Music 3 is a four-channel quasi-fixed media piece that can beimprovised by a single player operating the Max/MSP-based controller on atablet such as iPad. Within the fixed time limit of six minutes, the performercan freely (de)activate and displace the eighty seven precomposed audio filesthat are simultaneously running, generating a sonic structure to one's likingout of the given network of musical possibilities. The interface is designed toinvite an integral musical structuring particularly in the dimensions ofperformatively underexplored (but still sonically viable) parameters that arelargely based on MPEG-7 audio descriptors.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Novel controllers, interface for musical expression, musical mapping strategy, music cognition, music perception, MPEG-7},
  Url                      = {http://nime.org/proceedings/2013/nime2013_219.pdf}
}

@InProceedings{Zhang:2013,
  Title                    = {KIB: Simplifying Gestural Instrument Creation Using Widgets},
  Author                   = {Edward Zhang},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2013},

  Address                  = {Daejeon, Republic of Korea},
  Month                    = May,
  Pages                    = {519--524},
  Publisher                = {Graduate School of Culture Technology, KAIST},

  Abstract                 = {The Microsoft Kinect is a popular and versatile input devicefor musical interfaces. However, using the Kinect for suchinterfaces requires not only signi_x000C_cant programming experience,but also the use of complex geometry or machinelearning techniques to translate joint positions into higherlevel gestures. We created the Kinect Instrument Builder(KIB) to address these di_x000E_culties by structuring gesturalinterfaces as combinations of gestural widgets. KIB allowsthe user to design an instrument by con_x000C_guring gesturalprimitives, each with a set of simple but attractive visualfeedback elements. After designing an instrument on KIB'sweb interface, users can play the instrument on KIB's performanceinterface, which displays visualizations and transmitsOSC messages to other applications for sound synthesisor further remapping.},
  Date-modified            = {2013-08-16 18:22:08 +0000},
  Keywords                 = {Kinect, gesture, widgets, OSC, mapping},
  Url                      = {http://nime.org/proceedings/2013/nime2013_114.pdf}
}

